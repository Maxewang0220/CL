{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T22:22:16.589380Z",
     "start_time": "2024-12-14T22:22:16.583968Z"
    }
   },
   "source": [
    "import torch\n",
    "import datasets"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load the dataset\n",
    "dataset = datasets.load_dataset(path=\"universal_dependencies\", name=\"de_gsd\", trust_remote_code=True)\n",
    "print(dataset)\n",
    "train_dataset = dataset[\"train\"]"
   ],
   "id": "5433d570f218c340",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:14:18.556204Z",
     "start_time": "2024-12-14T23:14:18.245716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read the first 10 examples\n",
    "print(dataset)\n",
    "print(train_dataset[\"text\"][:10])\n",
    "print(train_dataset[\"tokens\"][:10])\n",
    "print(train_dataset[\"upos\"][:10])\n",
    "\n",
    "upos_mapping = dataset[\"train\"].features[\"upos\"].feature\n",
    "\n",
    "# store the possible pos tags\n",
    "pos_list = upos_mapping.names\n",
    "print(pos_list)"
   ],
   "id": "e9f17e7fd8306a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 13814\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 799\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 977\n",
      "    })\n",
      "})\n",
      "['Sehr gute Beratung, schnelle Behebung der Probleme, so stelle ich mir Kundenservice vor.', 'Die Kosten sind definitiv auch im Rahmen.', 'Nette Gespräche, klasse Ergebnis', 'Ich bin seit längerer Zeit zur Behandlung verschiedenster \"Leiden\" in der Physiotherapieraxis \"Gaby Montag\" im Vital Center und kann ausschließlich Positives berichten!', 'Ob bei der Terminvergabe, den Behandlungsräumen oder den individuell zugeschnittenen Trainingsplänen sind alle Mitarbeiter äußerst kompetent und flexibel.', 'Sauberkeit, Ordnung und Freundlichkeit brauche ich hier nicht zu erwähnen, denn das gehört für mich zum Standard, der aber auch noch übertroffen wird.', 'Physiotherapie ist zwar oftmals auch anstrengend, aber in dieser Umgebeung freut man sich auf jede Minute Behandlung.', 'Das nächste mal rief ich extra vorher an, um einen Termin zu vereinbaren, damit der Konditor auch Zeit für uns hätte.', 'Eine Stunde später gab man uns dann endlich einen Tisch, der allerdings noch nicht einmal abgeräumt war.', 'Die Bedienung verschwand sofort wieder und kam auch erstmal nicht mehr.']\n",
      "[['Sehr', 'gute', 'Beratung', ',', 'schnelle', 'Behebung', 'der', 'Probleme', ',', 'so', 'stelle', 'ich', 'mir', 'Kundenservice', 'vor', '.'], ['Die', 'Kosten', 'sind', 'definitiv', 'auch', 'im', 'in', 'dem', 'Rahmen', '.'], ['Nette', 'Gespräche', ',', 'klasse', 'Ergebnis'], ['Ich', 'bin', 'seit', 'längerer', 'Zeit', 'zur', 'zu', 'der', 'Behandlung', 'verschiedenster', '\"', 'Leiden', '\"', 'in', 'der', 'Physiotherapieraxis', '\"', 'Gaby', 'Montag', '\"', 'im', 'in', 'dem', 'Vital', 'Center', 'und', 'kann', 'ausschließlich', 'Positives', 'berichten', '!'], ['Ob', 'bei', 'der', 'Terminvergabe', ',', 'den', 'Behandlungsräumen', 'oder', 'den', 'individuell', 'zugeschnittenen', 'Trainingsplänen', 'sind', 'alle', 'Mitarbeiter', 'äußerst', 'kompetent', 'und', 'flexibel', '.'], ['Sauberkeit', ',', 'Ordnung', 'und', 'Freundlichkeit', 'brauche', 'ich', 'hier', 'nicht', 'zu', 'erwähnen', ',', 'denn', 'das', 'gehört', 'für', 'mich', 'zum', 'zu', 'dem', 'Standard', ',', 'der', 'aber', 'auch', 'noch', 'übertroffen', 'wird', '.'], ['Physiotherapie', 'ist', 'zwar', 'oftmals', 'auch', 'anstrengend', ',', 'aber', 'in', 'dieser', 'Umgebeung', 'freut', 'man', 'sich', 'auf', 'jede', 'Minute', 'Behandlung', '.'], ['Das', 'nächste', 'mal', 'rief', 'ich', 'extra', 'vorher', 'an', ',', 'um', 'einen', 'Termin', 'zu', 'vereinbaren', ',', 'damit', 'der', 'Konditor', 'auch', 'Zeit', 'für', 'uns', 'hätte', '.'], ['Eine', 'Stunde', 'später', 'gab', 'man', 'uns', 'dann', 'endlich', 'einen', 'Tisch', ',', 'der', 'allerdings', 'noch', 'nicht', 'einmal', 'abgeräumt', 'war', '.'], ['Die', 'Bedienung', 'verschwand', 'sofort', 'wieder', 'und', 'kam', 'auch', 'erstmal', 'nicht', 'mehr', '.']]\n",
      "[[14, 6, 0, 1, 6, 0, 8, 0, 1, 14, 16, 11, 11, 0, 2, 1], [8, 0, 16, 14, 14, 13, 2, 8, 0, 1], [6, 0, 1, 6, 0], [11, 16, 2, 6, 0, 13, 2, 8, 0, 6, 1, 0, 1, 2, 8, 0, 1, 10, 10, 1, 13, 2, 8, 10, 10, 9, 17, 14, 0, 16, 1], [9, 2, 8, 0, 1, 8, 0, 9, 8, 14, 6, 0, 17, 11, 0, 14, 6, 9, 6, 1], [0, 1, 0, 9, 0, 16, 11, 14, 7, 7, 16, 1, 5, 11, 16, 2, 11, 13, 2, 8, 0, 1, 11, 14, 14, 14, 16, 17, 1], [0, 17, 14, 14, 14, 6, 1, 9, 2, 11, 0, 16, 11, 11, 2, 11, 0, 0, 1], [8, 6, 0, 16, 11, 14, 14, 2, 1, 2, 8, 0, 7, 16, 1, 5, 8, 0, 14, 0, 2, 11, 16, 1], [3, 0, 14, 16, 11, 11, 14, 14, 8, 0, 1, 11, 14, 14, 7, 14, 16, 17, 1], [8, 0, 16, 14, 14, 9, 16, 14, 14, 7, 14, 1]]\n",
      "['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', '_', 'ADV', 'INTJ', 'VERB', 'AUX']\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:28:31.110059Z",
     "start_time": "2024-12-14T23:28:31.106193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer, label_all_tokens=False, skip_index=-100):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True)\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(examples[\"upos\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids: list[int] = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(skip_index)\n",
    "\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else skip_index)\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "id": "6131d5ac04745ca9",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:42:41.406838Z",
     "start_time": "2024-12-14T23:42:41.401545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert numerical labels to string labels\n",
    "def upos_id_to_label(upos_mapping, i):\n",
    "    return upos_mapping.int2str(i)"
   ],
   "id": "6c29f3ba9849f9bf",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:30:51.365890Z",
     "start_time": "2024-12-14T23:30:50.785528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "# test tokenization\n",
    "tokenized_inputs = tokenized_dataset = tokenize_and_align_labels(train_dataset[:5], tokenizer)\n",
    "print(tokenized_inputs)"
   ],
   "id": "4f47e2d9a81a1240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 93404, 25989, 58860, 6, 4, 17230, 13, 873, 195285, 122, 39344, 6, 4, 221, 91151, 654, 2296, 14829, 22584, 1248, 6, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 622, 30882, 1276, 44836, 921, 566, 23, 745, 36070, 6, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 43268, 13, 64225, 13, 6, 4, 38411, 86909, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 2484, 2394, 10743, 60915, 56, 5502, 2957, 404, 122, 68006, 186567, 1515, 44, 185113, 44, 23, 122, 13000, 53, 9619, 88562, 219, 33102, 44, 53186, 53, 53757, 44, 566, 23, 745, 116393, 11588, 165, 1876, 100320, 156060, 90, 120833, 711, 2], [0, 3545, 1079, 122, 27366, 814, 18018, 6, 4, 168, 68006, 7, 161960, 1367, 168, 85675, 404, 429, 103262, 33, 33, 44284, 7008, 42144, 33, 1276, 747, 28735, 196486, 56036, 165, 119997, 6, 5, 2, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 14, 6, 0, 1, -100, 6, -100, 0, -100, 8, 0, 1, -100, 14, 16, 11, 11, 0, -100, 2, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 8, 0, 16, 14, 14, 13, 2, 8, 0, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 6, -100, 0, -100, 1, -100, 6, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 11, 16, 2, 6, -100, 0, 13, 2, 8, 0, 6, -100, 1, 0, 1, 2, 8, 0, -100, -100, -100, -100, -100, 1, 10, -100, 10, 1, 13, 2, 8, 10, 10, 9, 17, 14, 0, -100, 16, 1, -100], [-100, 9, 2, 8, 0, -100, -100, 1, -100, 8, 0, -100, -100, 9, 8, 14, 6, -100, -100, -100, -100, 0, -100, -100, -100, 17, 11, 0, 14, 6, 9, 6, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100]]}\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:53:30.633480Z",
     "start_time": "2024-12-14T23:53:30.265027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test if the tokenization and alignment worked\n",
    "for i in range(len(tokenized_inputs[\"labels\"])):\n",
    "    print(\"text\", i)\n",
    "    print(tokenizer.decode(tokenized_inputs[\"input_ids\"][i]))\n",
    "    print([upos_id_to_label(upos_mapping, x) for x in tokenized_inputs[\"labels\"][i] if x != -100])\n",
    "    print([upos_id_to_label(upos_mapping, x) for x in train_dataset[\"upos\"][i]])"
   ],
   "id": "c773d48dbb57b2e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 0\n",
      "<s> Sehr gute Beratung, schnelle Behebung der Probleme, so stelle ich mir Kundenservice vor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "['ADV', 'ADJ', 'NOUN', 'PUNCT', 'ADJ', 'NOUN', 'DET', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'PRON', 'PRON', 'NOUN', 'ADP', 'PUNCT']\n",
      "['ADV', 'ADJ', 'NOUN', 'PUNCT', 'ADJ', 'NOUN', 'DET', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'PRON', 'PRON', 'NOUN', 'ADP', 'PUNCT']\n",
      "text 1\n",
      "<s> Die Kosten sind definitiv auch im in dem Rahmen.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "['DET', 'NOUN', 'VERB', 'ADV', 'ADV', '_', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "['DET', 'NOUN', 'VERB', 'ADV', 'ADV', '_', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "text 2\n",
      "<s> Nette Gespräche, klasse Ergebnis</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "['ADJ', 'NOUN', 'PUNCT', 'ADJ', 'NOUN']\n",
      "['ADJ', 'NOUN', 'PUNCT', 'ADJ', 'NOUN']\n",
      "text 3\n",
      "<s> Ich bin seit längerer Zeit zur zu der Behandlung verschiedenster \" Leiden \" in der Physiotherapieraxis \" Gaby Montag \" im in dem Vital Center und kann ausschließlich Positives berichten!</s>\n",
      "['PRON', 'VERB', 'ADP', 'ADJ', 'NOUN', '_', 'ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'NOUN', 'PUNCT', 'ADP', 'DET', 'NOUN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', '_', 'ADP', 'DET', 'PROPN', 'PROPN', 'CCONJ', 'AUX', 'ADV', 'NOUN', 'VERB', 'PUNCT']\n",
      "['PRON', 'VERB', 'ADP', 'ADJ', 'NOUN', '_', 'ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'NOUN', 'PUNCT', 'ADP', 'DET', 'NOUN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', '_', 'ADP', 'DET', 'PROPN', 'PROPN', 'CCONJ', 'AUX', 'ADV', 'NOUN', 'VERB', 'PUNCT']\n",
      "text 4\n",
      "<s> Ob bei der Terminvergabe, den Behandlungsräumen oder den individuell zugeschnittenen Trainingsplänen sind alle Mitarbeiter äußerst kompetent und flexibel.</s><pad><pad><pad><pad><pad><pad><pad>\n",
      "['CCONJ', 'ADP', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'CCONJ', 'DET', 'ADV', 'ADJ', 'NOUN', 'AUX', 'PRON', 'NOUN', 'ADV', 'ADJ', 'CCONJ', 'ADJ', 'PUNCT']\n",
      "['CCONJ', 'ADP', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'CCONJ', 'DET', 'ADV', 'ADJ', 'NOUN', 'AUX', 'PRON', 'NOUN', 'ADV', 'ADJ', 'CCONJ', 'ADJ', 'PUNCT']\n"
     ]
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
