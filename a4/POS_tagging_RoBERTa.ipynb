{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-12-15T21:20:56.609051Z",
     "start_time": "2024-12-15T21:20:56.602343Z"
    }
   },
   "source": [
    "import torch\n",
    "import datasets"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "5433d570f218c340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:21:00.567089Z",
     "start_time": "2024-12-15T21:20:57.607337Z"
    }
   },
   "source": [
    "# load the dataset\n",
    "dataset = datasets.load_dataset(path=\"universal_dependencies\", name=\"de_gsd\", trust_remote_code=True)\n",
    "print(dataset)\n",
    "train_dataset = dataset[\"train\"]\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 13814\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 799\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 977\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "e9f17e7fd8306a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:21:02.741947Z",
     "start_time": "2024-12-15T21:21:02.485194Z"
    }
   },
   "source": [
    "# read the first 10 examples\n",
    "print(dataset)\n",
    "print(train_dataset[\"text\"][:10])\n",
    "print(train_dataset[\"tokens\"][:10])\n",
    "print(train_dataset[\"upos\"][:10])\n",
    "\n",
    "upos_mapping = dataset[\"train\"].features[\"upos\"].feature\n",
    "\n",
    "# store the possible pos tags\n",
    "pos_list = upos_mapping.names\n",
    "print(pos_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 13814\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 799\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 977\n",
      "    })\n",
      "})\n",
      "['Sehr gute Beratung, schnelle Behebung der Probleme, so stelle ich mir Kundenservice vor.', 'Die Kosten sind definitiv auch im Rahmen.', 'Nette Gespräche, klasse Ergebnis', 'Ich bin seit längerer Zeit zur Behandlung verschiedenster \"Leiden\" in der Physiotherapieraxis \"Gaby Montag\" im Vital Center und kann ausschließlich Positives berichten!', 'Ob bei der Terminvergabe, den Behandlungsräumen oder den individuell zugeschnittenen Trainingsplänen sind alle Mitarbeiter äußerst kompetent und flexibel.', 'Sauberkeit, Ordnung und Freundlichkeit brauche ich hier nicht zu erwähnen, denn das gehört für mich zum Standard, der aber auch noch übertroffen wird.', 'Physiotherapie ist zwar oftmals auch anstrengend, aber in dieser Umgebeung freut man sich auf jede Minute Behandlung.', 'Das nächste mal rief ich extra vorher an, um einen Termin zu vereinbaren, damit der Konditor auch Zeit für uns hätte.', 'Eine Stunde später gab man uns dann endlich einen Tisch, der allerdings noch nicht einmal abgeräumt war.', 'Die Bedienung verschwand sofort wieder und kam auch erstmal nicht mehr.']\n",
      "[['Sehr', 'gute', 'Beratung', ',', 'schnelle', 'Behebung', 'der', 'Probleme', ',', 'so', 'stelle', 'ich', 'mir', 'Kundenservice', 'vor', '.'], ['Die', 'Kosten', 'sind', 'definitiv', 'auch', 'im', 'in', 'dem', 'Rahmen', '.'], ['Nette', 'Gespräche', ',', 'klasse', 'Ergebnis'], ['Ich', 'bin', 'seit', 'längerer', 'Zeit', 'zur', 'zu', 'der', 'Behandlung', 'verschiedenster', '\"', 'Leiden', '\"', 'in', 'der', 'Physiotherapieraxis', '\"', 'Gaby', 'Montag', '\"', 'im', 'in', 'dem', 'Vital', 'Center', 'und', 'kann', 'ausschließlich', 'Positives', 'berichten', '!'], ['Ob', 'bei', 'der', 'Terminvergabe', ',', 'den', 'Behandlungsräumen', 'oder', 'den', 'individuell', 'zugeschnittenen', 'Trainingsplänen', 'sind', 'alle', 'Mitarbeiter', 'äußerst', 'kompetent', 'und', 'flexibel', '.'], ['Sauberkeit', ',', 'Ordnung', 'und', 'Freundlichkeit', 'brauche', 'ich', 'hier', 'nicht', 'zu', 'erwähnen', ',', 'denn', 'das', 'gehört', 'für', 'mich', 'zum', 'zu', 'dem', 'Standard', ',', 'der', 'aber', 'auch', 'noch', 'übertroffen', 'wird', '.'], ['Physiotherapie', 'ist', 'zwar', 'oftmals', 'auch', 'anstrengend', ',', 'aber', 'in', 'dieser', 'Umgebeung', 'freut', 'man', 'sich', 'auf', 'jede', 'Minute', 'Behandlung', '.'], ['Das', 'nächste', 'mal', 'rief', 'ich', 'extra', 'vorher', 'an', ',', 'um', 'einen', 'Termin', 'zu', 'vereinbaren', ',', 'damit', 'der', 'Konditor', 'auch', 'Zeit', 'für', 'uns', 'hätte', '.'], ['Eine', 'Stunde', 'später', 'gab', 'man', 'uns', 'dann', 'endlich', 'einen', 'Tisch', ',', 'der', 'allerdings', 'noch', 'nicht', 'einmal', 'abgeräumt', 'war', '.'], ['Die', 'Bedienung', 'verschwand', 'sofort', 'wieder', 'und', 'kam', 'auch', 'erstmal', 'nicht', 'mehr', '.']]\n",
      "[[14, 6, 0, 1, 6, 0, 8, 0, 1, 14, 16, 11, 11, 0, 2, 1], [8, 0, 16, 14, 14, 13, 2, 8, 0, 1], [6, 0, 1, 6, 0], [11, 16, 2, 6, 0, 13, 2, 8, 0, 6, 1, 0, 1, 2, 8, 0, 1, 10, 10, 1, 13, 2, 8, 10, 10, 9, 17, 14, 0, 16, 1], [9, 2, 8, 0, 1, 8, 0, 9, 8, 14, 6, 0, 17, 11, 0, 14, 6, 9, 6, 1], [0, 1, 0, 9, 0, 16, 11, 14, 7, 7, 16, 1, 5, 11, 16, 2, 11, 13, 2, 8, 0, 1, 11, 14, 14, 14, 16, 17, 1], [0, 17, 14, 14, 14, 6, 1, 9, 2, 11, 0, 16, 11, 11, 2, 11, 0, 0, 1], [8, 6, 0, 16, 11, 14, 14, 2, 1, 2, 8, 0, 7, 16, 1, 5, 8, 0, 14, 0, 2, 11, 16, 1], [3, 0, 14, 16, 11, 11, 14, 14, 8, 0, 1, 11, 14, 14, 7, 14, 16, 17, 1], [8, 0, 16, 14, 14, 9, 16, 14, 14, 7, 14, 1]]\n",
      "['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', '_', 'ADV', 'INTJ', 'VERB', 'AUX']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "6131d5ac04745ca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:21:04.672071Z",
     "start_time": "2024-12-15T21:21:04.667258Z"
    }
   },
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer, label_all_tokens=False, skip_index=-100):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=200\n",
    "    )\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(examples[\"upos\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids: list[int] = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(skip_index)\n",
    "\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else skip_index)\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "6c29f3ba9849f9bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:21:06.914375Z",
     "start_time": "2024-12-15T21:21:06.910485Z"
    }
   },
   "source": [
    "# convert numerical labels to string labels\n",
    "def upos_id_to_label(upos_mapping, i):\n",
    "    return upos_mapping.int2str(i)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "4f47e2d9a81a1240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:21:08.639838Z",
     "start_time": "2024-12-15T21:21:07.996946Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "# test tokenization\n",
    "tokenized_inputs = tokenize_and_align_labels(train_dataset[:5], tokenizer)\n",
    "print(tokenized_inputs)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WangEntang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 93404, 25989, 58860, 6, 4, 17230, 13, 873, 195285, 122, 39344, 6, 4, 221, 91151, 654, 2296, 14829, 22584, 1248, 6, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 622, 30882, 1276, 44836, 921, 566, 23, 745, 36070, 6, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 43268, 13, 64225, 13, 6, 4, 38411, 86909, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 2484, 2394, 10743, 60915, 56, 5502, 2957, 404, 122, 68006, 186567, 1515, 44, 185113, 44, 23, 122, 13000, 53, 9619, 88562, 219, 33102, 44, 53186, 53, 53757, 44, 566, 23, 745, 116393, 11588, 165, 1876, 100320, 156060, 90, 120833, 711, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 3545, 1079, 122, 27366, 814, 18018, 6, 4, 168, 68006, 7, 161960, 1367, 168, 85675, 404, 429, 103262, 33, 33, 44284, 7008, 42144, 33, 1276, 747, 28735, 196486, 56036, 165, 119997, 6, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 14, 6, 0, 1, -100, 6, -100, 0, -100, 8, 0, 1, -100, 14, 16, 11, 11, 0, -100, 2, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 8, 0, 16, 14, 14, 13, 2, 8, 0, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 6, -100, 0, -100, 1, -100, 6, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 11, 16, 2, 6, -100, 0, 13, 2, 8, 0, 6, -100, 1, 0, 1, 2, 8, 0, -100, -100, -100, -100, -100, 1, 10, -100, 10, 1, 13, 2, 8, 10, 10, 9, 17, 14, 0, -100, 16, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 9, 2, 8, 0, -100, -100, 1, -100, 8, 0, -100, -100, 9, 8, 14, 6, -100, -100, -100, -100, 0, -100, -100, -100, 17, 11, 0, 14, 6, 9, 6, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]]}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "c773d48dbb57b2e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:21:11.543148Z",
     "start_time": "2024-12-15T21:21:11.167457Z"
    }
   },
   "source": [
    "# test if the tokenization and alignment worked\n",
    "for i in range(len(tokenized_inputs[\"labels\"])):\n",
    "    print(\"text\", i, len(tokenized_inputs[\"input_ids\"][i]))\n",
    "    print(tokenizer.decode(tokenized_inputs[\"input_ids\"][i]))\n",
    "    print([upos_id_to_label(upos_mapping, x) for x in tokenized_inputs[\"labels\"][i] if x != -100])\n",
    "    print([upos_id_to_label(upos_mapping, x) for x in train_dataset[\"upos\"][i]])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 0 200\n",
      "<s> Sehr gute Beratung, schnelle Behebung der Probleme, so stelle ich mir Kundenservice vor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "['ADV', 'ADJ', 'NOUN', 'PUNCT', 'ADJ', 'NOUN', 'DET', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'PRON', 'PRON', 'NOUN', 'ADP', 'PUNCT']\n",
      "['ADV', 'ADJ', 'NOUN', 'PUNCT', 'ADJ', 'NOUN', 'DET', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'PRON', 'PRON', 'NOUN', 'ADP', 'PUNCT']\n",
      "text 1 200\n",
      "<s> Die Kosten sind definitiv auch im in dem Rahmen.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "['DET', 'NOUN', 'VERB', 'ADV', 'ADV', '_', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "['DET', 'NOUN', 'VERB', 'ADV', 'ADV', '_', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "text 2 200\n",
      "<s> Nette Gespräche, klasse Ergebnis</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "['ADJ', 'NOUN', 'PUNCT', 'ADJ', 'NOUN']\n",
      "['ADJ', 'NOUN', 'PUNCT', 'ADJ', 'NOUN']\n",
      "text 3 200\n",
      "<s> Ich bin seit längerer Zeit zur zu der Behandlung verschiedenster \" Leiden \" in der Physiotherapieraxis \" Gaby Montag \" im in dem Vital Center und kann ausschließlich Positives berichten!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "['PRON', 'VERB', 'ADP', 'ADJ', 'NOUN', '_', 'ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'NOUN', 'PUNCT', 'ADP', 'DET', 'NOUN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', '_', 'ADP', 'DET', 'PROPN', 'PROPN', 'CCONJ', 'AUX', 'ADV', 'NOUN', 'VERB', 'PUNCT']\n",
      "['PRON', 'VERB', 'ADP', 'ADJ', 'NOUN', '_', 'ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'NOUN', 'PUNCT', 'ADP', 'DET', 'NOUN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', '_', 'ADP', 'DET', 'PROPN', 'PROPN', 'CCONJ', 'AUX', 'ADV', 'NOUN', 'VERB', 'PUNCT']\n",
      "text 4 200\n",
      "<s> Ob bei der Terminvergabe, den Behandlungsräumen oder den individuell zugeschnittenen Trainingsplänen sind alle Mitarbeiter äußerst kompetent und flexibel.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "['CCONJ', 'ADP', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'CCONJ', 'DET', 'ADV', 'ADJ', 'NOUN', 'AUX', 'PRON', 'NOUN', 'ADV', 'ADJ', 'CCONJ', 'ADJ', 'PUNCT']\n",
      "['CCONJ', 'ADP', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'CCONJ', 'DET', 'ADV', 'ADJ', 'NOUN', 'AUX', 'PRON', 'NOUN', 'ADV', 'ADJ', 'CCONJ', 'ADJ', 'PUNCT']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "47b6b1d3f2459c17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:21:13.952998Z",
     "start_time": "2024-12-15T21:21:13.857166Z"
    }
   },
   "source": [
    "# convert the dataset to dataloader\n",
    "train_dataset = train_dataset.map(lambda examples: tokenize_and_align_labels(examples, tokenizer), batched=True)\n",
    "valid_dataset = valid_dataset.map(lambda examples: tokenize_and_align_labels(examples, tokenizer), batched=True)\n",
    "test_datatset = test_dataset.map(lambda examples: tokenize_and_align_labels(examples, tokenizer), batched=True)\n",
    "print(train_dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 13814\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "8fdd84a0b5719d1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:24:46.947648Z",
     "start_time": "2024-12-15T21:24:46.936839Z"
    }
   },
   "source": [
    "# test if huggingface dataset is converted to torch dataset\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32)\n",
    "test_datatset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "next(iter(train_dataloader))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,  93404,  25989,  ...,      1,      1,      1],\n",
       "         [     0,    622,  30882,  ...,      1,      1,      1],\n",
       "         [     0,  43268,     13,  ...,      1,      1,      1],\n",
       "         ...,\n",
       "         [     0,  43268,      6,  ...,      1,      1,      1],\n",
       "         [     0,    563, 109833,  ...,      1,      1,      1],\n",
       "         [     0,  35302,   1649,  ...,      1,      1,      1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[-100,   14,    6,  ..., -100, -100, -100],\n",
       "         [-100,    8,    0,  ..., -100, -100, -100],\n",
       "         [-100,    6, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100,    6,    1,  ..., -100, -100, -100],\n",
       "         [-100,   16, -100,  ..., -100, -100, -100],\n",
       "         [-100,    7,   14,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "99c295ff942da356",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:21:31.587226Z",
     "start_time": "2024-12-15T21:21:31.581124Z"
    }
   },
   "source": [
    "from transformers import XLMRobertaModel\n",
    "\n",
    "\n",
    "class POSTaggingModel(torch.nn.Module):\n",
    "    def __init__(self, num_labels=18):\n",
    "        super(POSTaggingModel, self).__init__()\n",
    "\n",
    "        # load pre-trained XLM-RoBERTa model\n",
    "        self.roberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "        # freeze RoBERTa parameters\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # project 768 hidden states to 0-17 POS tags\n",
    "        self.ffn = torch.nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Shape: (batch_size, seq_length, hidden_size:768)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # feed forward layer\n",
    "        # Shape: (batch_size, seq_length, num_labels:18)\n",
    "        logits = self.ffn(hidden_states)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Extra: a more complex FFN model\n",
    "class POSTaggingProModel(torch.nn.Module):\n",
    "    def __init__(self, num_labels=18):\n",
    "        super(POSTaggingProModel, self).__init__()\n",
    "\n",
    "        # load pre-trained XLM-RoBERTa model\n",
    "        self.roberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "        # freeze RoBERTa parameters\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # feed forward layers with activation functions\n",
    "        self.ffn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 256),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Shape: (batch_size, seq_length, hidden_size:768)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # feed forward layer\n",
    "        # Shape: (batch_size, seq_length, num_labels:18)\n",
    "        logits = self.ffn(hidden_states)\n",
    "\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "a8efc9da139ce0b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:21:46.480275Z",
     "start_time": "2024-12-15T21:21:34.747072Z"
    }
   },
   "source": [
    "model = POSTaggingModel()\n",
    "data = next(iter(train_dataloader))\n",
    "x = data[\"input_ids\"]\n",
    "mask = data[\"attention_mask\"]\n",
    "\n",
    "print(model(x, mask))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WangEntang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7207, -0.6867, -0.7943,  ..., -0.9311, -0.2526,  0.3514],\n",
      "         [-0.4489, -0.3172, -0.6395,  ..., -0.2470, -0.2027,  0.1096],\n",
      "         [-0.4852, -0.2170, -0.6565,  ..., -0.2056, -0.0989,  0.0149],\n",
      "         ...,\n",
      "         [-0.4165, -0.3430, -0.5566,  ..., -0.2985, -0.1887,  0.1986],\n",
      "         [-0.4165, -0.3430, -0.5566,  ..., -0.2985, -0.1887,  0.1986],\n",
      "         [-0.4165, -0.3430, -0.5566,  ..., -0.2985, -0.1887,  0.1986]],\n",
      "\n",
      "        [[-0.6672, -0.6549, -0.6787,  ..., -0.8625, -0.2844,  0.3617],\n",
      "         [-0.4161, -0.2460, -0.6047,  ..., -0.3318, -0.2200,  0.0841],\n",
      "         [-0.5245, -0.3537, -0.5550,  ..., -0.2673, -0.1963,  0.1286],\n",
      "         ...,\n",
      "         [-0.7250, -0.6882, -0.4199,  ..., -0.7705, -0.3405,  0.4433],\n",
      "         [-0.7250, -0.6882, -0.4199,  ..., -0.7705, -0.3405,  0.4433],\n",
      "         [-0.7250, -0.6882, -0.4199,  ..., -0.7705, -0.3405,  0.4433]],\n",
      "\n",
      "        [[-0.6184, -0.5650, -0.6222,  ..., -0.6214, -0.2944,  0.3046],\n",
      "         [-0.3953, -0.4430, -0.6173,  ..., -0.4245, -0.1391,  0.1872],\n",
      "         [-0.4164, -0.3444, -0.5701,  ..., -0.3288, -0.2206,  0.1154],\n",
      "         ...,\n",
      "         [-0.6394, -0.5856, -0.5625,  ..., -0.6172, -0.3193,  0.3202],\n",
      "         [-0.6394, -0.5856, -0.5625,  ..., -0.6172, -0.3193,  0.3202],\n",
      "         [-0.6394, -0.5856, -0.5625,  ..., -0.6172, -0.3193,  0.3202]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7157, -0.7916, -0.7506,  ..., -1.0066, -0.2197,  0.3220],\n",
      "         [-0.4987, -0.4463, -0.5543,  ..., -0.3813, -0.1253,  0.2136],\n",
      "         [-0.4387, -0.4572, -0.4356,  ..., -0.3157, -0.2133,  0.1570],\n",
      "         ...,\n",
      "         [-0.4041, -0.4101, -0.5016,  ..., -0.3518, -0.1726,  0.2364],\n",
      "         [-0.4041, -0.4101, -0.5016,  ..., -0.3518, -0.1726,  0.2364],\n",
      "         [-0.4041, -0.4101, -0.5016,  ..., -0.3518, -0.1726,  0.2364]],\n",
      "\n",
      "        [[-0.6934, -0.6732, -0.7202,  ..., -0.9335, -0.2875,  0.3590],\n",
      "         [-0.3552, -0.3072, -0.5525,  ..., -0.1546, -0.2713,  0.1451],\n",
      "         [-0.3348, -0.3308, -0.5527,  ..., -0.1686, -0.2282,  0.2510],\n",
      "         ...,\n",
      "         [-0.3804, -0.3869, -0.5159,  ..., -0.3519, -0.2094,  0.2421],\n",
      "         [-0.3804, -0.3869, -0.5159,  ..., -0.3519, -0.2094,  0.2421],\n",
      "         [-0.3804, -0.3869, -0.5159,  ..., -0.3519, -0.2094,  0.2421]],\n",
      "\n",
      "        [[-0.6343, -0.6064, -0.6276,  ..., -0.6764, -0.2492,  0.3620],\n",
      "         [-0.4155, -0.2833, -0.5718,  ..., -0.3196, -0.1353,  0.1640],\n",
      "         [-0.3734, -0.3819, -0.4646,  ..., -0.2943, -0.0505,  0.1889],\n",
      "         ...,\n",
      "         [-0.4373, -0.3797, -0.4967,  ..., -0.3885, -0.1295,  0.2779],\n",
      "         [-0.4373, -0.3797, -0.4967,  ..., -0.3885, -0.1295,  0.2779],\n",
      "         [-0.4373, -0.3797, -0.4967,  ..., -0.3885, -0.1295,  0.2779]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e7ef057d58f6bf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:23:00.727500Z",
     "start_time": "2024-12-15T15:23:00.472783Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# train the model\n",
    "def train(model, train_dataloader, valid_dataset, test_dataset, device, num_epochs=3, lr=1e-4, weight_decay=1e-2):\n",
    "    # init tensorBoard path: base-base model pro-pro model\n",
    "    writer = SummaryWriter(\"./runs/base\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # ignore the padding tokens\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    print(\"batch num\", len(train_dataloader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            x = data[\"input_ids\"].to(device)\n",
    "            mask = data[\"attention_mask\"].to(device)\n",
    "            y = data[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x, mask)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # log the loss curve\n",
    "            total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch}, Iteration {i}, Loss: {loss.item()}\")\n",
    "            writer.add_scalar(\"Training Loss Iter\", loss.item(), epoch * len(train_dataloader) + i)\n",
    "\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                # test the model\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    x = test_dataset[\"input_ids\"].to(device)\n",
    "                    mask = test_dataset[\"attention_mask\"].to(device)\n",
    "                    y = test_dataset[\"labels\"].to(device)\n",
    "\n",
    "                    logits = model(x, mask)\n",
    "                    loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "                    print(f\"Epoch {epoch}, Iteration {i}, Test Loss: {loss}\")\n",
    "                    writer.add_scalar(\"Test Loss\", loss, epoch * len(train_dataloader) + i)\n",
    "                model.train()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_count = 0\n",
    "            accurate_count = 0\n",
    "            for data in valid_dataset:\n",
    "                x = data[\"input_ids\"].to(device)\n",
    "                mask = data[\"attention_mask\"].to(device)\n",
    "                y = data[\"labels\"].to(device)\n",
    "\n",
    "                logits = model(x, mask)\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                # flatten predictions and labels for comparison\n",
    "                predictions = predictions.view(-1)\n",
    "                y = y.view(-1)\n",
    "\n",
    "                for prediction, label in zip(predictions, y):\n",
    "                    # ignore following and padding tokens\n",
    "                    if label == -100:\n",
    "                        continue\n",
    "\n",
    "                    # count label tokens\n",
    "                    total_count += 1\n",
    "\n",
    "                    # compare the prediction with the ground truth and count\n",
    "                    if prediction == label:\n",
    "                        accurate_count += 1\n",
    "\n",
    "            # compute average accuracy\n",
    "            avg_acc = accurate_count / total_count\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {avg_acc}\")\n",
    "            writer.add_scalar(\"Validation Accuracy\", avg_acc, epoch)\n",
    "        model.train()\n",
    "\n",
    "    torch.save(model.state_dict(), \"pos_tagging_model_0.pth\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac125428ce5b4616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:48:24.518972Z",
     "start_time": "2024-12-15T15:23:00.742598Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch num 432\n",
      "Epoch 0, Iteration 0, Loss: 2.9078338146209717\n",
      "Epoch 0, Iteration 1, Loss: 2.8946533203125\n",
      "Epoch 0, Iteration 2, Loss: 2.8790156841278076\n",
      "Epoch 0, Iteration 3, Loss: 2.859837055206299\n",
      "Epoch 0, Iteration 4, Loss: 2.856666088104248\n",
      "Epoch 0, Iteration 5, Loss: 2.8485870361328125\n",
      "Epoch 0, Iteration 6, Loss: 2.8326451778411865\n",
      "Epoch 0, Iteration 7, Loss: 2.8118059635162354\n",
      "Epoch 0, Iteration 8, Loss: 2.8001186847686768\n",
      "Epoch 0, Iteration 9, Loss: 2.7950963973999023\n",
      "Epoch 0, Iteration 10, Loss: 2.7803945541381836\n",
      "Epoch 0, Iteration 11, Loss: 2.7646584510803223\n",
      "Epoch 0, Iteration 12, Loss: 2.737338066101074\n",
      "Epoch 0, Iteration 13, Loss: 2.7233240604400635\n",
      "Epoch 0, Iteration 14, Loss: 2.7123448848724365\n",
      "Epoch 0, Iteration 15, Loss: 2.6781132221221924\n",
      "Epoch 0, Iteration 16, Loss: 2.668684244155884\n",
      "Epoch 0, Iteration 17, Loss: 2.6365914344787598\n",
      "Epoch 0, Iteration 18, Loss: 2.6187744140625\n",
      "Epoch 0, Iteration 19, Loss: 2.6043052673339844\n",
      "Epoch 0, Iteration 20, Loss: 2.614780902862549\n",
      "Epoch 0, Iteration 21, Loss: 2.6018078327178955\n",
      "Epoch 0, Iteration 22, Loss: 2.5639097690582275\n",
      "Epoch 0, Iteration 23, Loss: 2.546950340270996\n",
      "Epoch 0, Iteration 24, Loss: 2.5249898433685303\n",
      "Epoch 0, Iteration 25, Loss: 2.505539894104004\n",
      "Epoch 0, Iteration 26, Loss: 2.484067678451538\n",
      "Epoch 0, Iteration 27, Loss: 2.5026092529296875\n",
      "Epoch 0, Iteration 28, Loss: 2.4604244232177734\n",
      "Epoch 0, Iteration 29, Loss: 2.411370277404785\n",
      "Epoch 0, Iteration 30, Loss: 2.416008710861206\n",
      "Epoch 0, Iteration 31, Loss: 2.382498025894165\n",
      "Epoch 0, Iteration 32, Loss: 2.399545669555664\n",
      "Epoch 0, Iteration 33, Loss: 2.3592567443847656\n",
      "Epoch 0, Iteration 34, Loss: 2.327575206756592\n",
      "Epoch 0, Iteration 35, Loss: 2.3172359466552734\n",
      "Epoch 0, Iteration 36, Loss: 2.3138160705566406\n",
      "Epoch 0, Iteration 37, Loss: 2.3341829776763916\n",
      "Epoch 0, Iteration 38, Loss: 2.432549476623535\n",
      "Epoch 0, Iteration 39, Loss: 2.2908263206481934\n",
      "Epoch 0, Iteration 40, Loss: 2.26682186126709\n",
      "Epoch 0, Iteration 41, Loss: 2.2683584690093994\n",
      "Epoch 0, Iteration 42, Loss: 2.2280476093292236\n",
      "Epoch 0, Iteration 43, Loss: 2.236708402633667\n",
      "Epoch 0, Iteration 44, Loss: 2.2140493392944336\n",
      "Epoch 0, Iteration 45, Loss: 2.146658182144165\n",
      "Epoch 0, Iteration 46, Loss: 2.2500786781311035\n",
      "Epoch 0, Iteration 47, Loss: 2.227473258972168\n",
      "Epoch 0, Iteration 48, Loss: 2.189368963241577\n",
      "Epoch 0, Iteration 49, Loss: 2.105635643005371\n",
      "Epoch 0, Iteration 50, Loss: 2.0980076789855957\n",
      "Epoch 0, Iteration 50, Test Loss: 2.018662691116333\n",
      "Epoch 0, Iteration 51, Loss: 2.052608013153076\n",
      "Epoch 0, Iteration 52, Loss: 2.052000045776367\n",
      "Epoch 0, Iteration 53, Loss: 2.033179759979248\n",
      "Epoch 0, Iteration 54, Loss: 2.0122976303100586\n",
      "Epoch 0, Iteration 55, Loss: 1.8975237607955933\n",
      "Epoch 0, Iteration 56, Loss: 1.9873780012130737\n",
      "Epoch 0, Iteration 57, Loss: 2.0239717960357666\n",
      "Epoch 0, Iteration 58, Loss: 1.9951533079147339\n",
      "Epoch 0, Iteration 59, Loss: 1.9001734256744385\n",
      "Epoch 0, Iteration 60, Loss: 1.9507765769958496\n",
      "Epoch 0, Iteration 61, Loss: 1.8219819068908691\n",
      "Epoch 0, Iteration 62, Loss: 1.9310582876205444\n",
      "Epoch 0, Iteration 63, Loss: 1.7761043310165405\n",
      "Epoch 0, Iteration 64, Loss: 1.8501744270324707\n",
      "Epoch 0, Iteration 65, Loss: 1.769472360610962\n",
      "Epoch 0, Iteration 66, Loss: 1.725746989250183\n",
      "Epoch 0, Iteration 67, Loss: 1.7020072937011719\n",
      "Epoch 0, Iteration 68, Loss: 1.7255295515060425\n",
      "Epoch 0, Iteration 69, Loss: 1.6981987953186035\n",
      "Epoch 0, Iteration 70, Loss: 1.677761197090149\n",
      "Epoch 0, Iteration 71, Loss: 1.70602285861969\n",
      "Epoch 0, Iteration 72, Loss: 1.6188796758651733\n",
      "Epoch 0, Iteration 73, Loss: 1.6883360147476196\n",
      "Epoch 0, Iteration 74, Loss: 1.6565556526184082\n",
      "Epoch 0, Iteration 75, Loss: 1.6223065853118896\n",
      "Epoch 0, Iteration 76, Loss: 1.5556223392486572\n",
      "Epoch 0, Iteration 77, Loss: 1.6309210062026978\n",
      "Epoch 0, Iteration 78, Loss: 1.6350041627883911\n",
      "Epoch 0, Iteration 79, Loss: 1.5451316833496094\n",
      "Epoch 0, Iteration 80, Loss: 1.5244580507278442\n",
      "Epoch 0, Iteration 81, Loss: 1.4288318157196045\n",
      "Epoch 0, Iteration 82, Loss: 1.5241142511367798\n",
      "Epoch 0, Iteration 83, Loss: 1.4441776275634766\n",
      "Epoch 0, Iteration 84, Loss: 1.5365052223205566\n",
      "Epoch 0, Iteration 85, Loss: 1.4643304347991943\n",
      "Epoch 0, Iteration 86, Loss: 1.446234107017517\n",
      "Epoch 0, Iteration 87, Loss: 1.4145586490631104\n",
      "Epoch 0, Iteration 88, Loss: 1.3397302627563477\n",
      "Epoch 0, Iteration 89, Loss: 1.4118701219558716\n",
      "Epoch 0, Iteration 90, Loss: 1.3503259420394897\n",
      "Epoch 0, Iteration 91, Loss: 1.2767945528030396\n",
      "Epoch 0, Iteration 92, Loss: 1.3666857481002808\n",
      "Epoch 0, Iteration 93, Loss: 1.295125126838684\n",
      "Epoch 0, Iteration 94, Loss: 1.3019590377807617\n",
      "Epoch 0, Iteration 95, Loss: 1.2566777467727661\n",
      "Epoch 0, Iteration 96, Loss: 1.2501403093338013\n",
      "Epoch 0, Iteration 97, Loss: 1.2108477354049683\n",
      "Epoch 0, Iteration 98, Loss: 1.1928681135177612\n",
      "Epoch 0, Iteration 99, Loss: 1.2214552164077759\n",
      "Epoch 0, Iteration 100, Loss: 1.147541880607605\n",
      "Epoch 0, Iteration 100, Test Loss: 1.3944951295852661\n",
      "Epoch 0, Iteration 101, Loss: 1.1661263704299927\n",
      "Epoch 0, Iteration 102, Loss: 1.1837571859359741\n",
      "Epoch 0, Iteration 103, Loss: 1.1494214534759521\n",
      "Epoch 0, Iteration 104, Loss: 1.1938984394073486\n",
      "Epoch 0, Iteration 105, Loss: 1.1255381107330322\n",
      "Epoch 0, Iteration 106, Loss: 1.1072574853897095\n",
      "Epoch 0, Iteration 107, Loss: 1.066240906715393\n",
      "Epoch 0, Iteration 108, Loss: 1.1669059991836548\n",
      "Epoch 0, Iteration 109, Loss: 1.0163179636001587\n",
      "Epoch 0, Iteration 110, Loss: 0.9814120531082153\n",
      "Epoch 0, Iteration 111, Loss: 1.0400938987731934\n",
      "Epoch 0, Iteration 112, Loss: 1.0191655158996582\n",
      "Epoch 0, Iteration 113, Loss: 0.969357430934906\n",
      "Epoch 0, Iteration 114, Loss: 0.9225252866744995\n",
      "Epoch 0, Iteration 115, Loss: 1.0615806579589844\n",
      "Epoch 0, Iteration 116, Loss: 1.0239981412887573\n",
      "Epoch 0, Iteration 117, Loss: 1.01291823387146\n",
      "Epoch 0, Iteration 118, Loss: 0.8442122936248779\n",
      "Epoch 0, Iteration 119, Loss: 0.9991315603256226\n",
      "Epoch 0, Iteration 120, Loss: 0.9598792195320129\n",
      "Epoch 0, Iteration 121, Loss: 0.9510788917541504\n",
      "Epoch 0, Iteration 122, Loss: 0.948252260684967\n",
      "Epoch 0, Iteration 123, Loss: 0.8312410116195679\n",
      "Epoch 0, Iteration 124, Loss: 0.8239916563034058\n",
      "Epoch 0, Iteration 125, Loss: 0.8614656329154968\n",
      "Epoch 0, Iteration 126, Loss: 0.8475839495658875\n",
      "Epoch 0, Iteration 127, Loss: 0.7974382638931274\n",
      "Epoch 0, Iteration 128, Loss: 0.8620028495788574\n",
      "Epoch 0, Iteration 129, Loss: 0.8829665780067444\n",
      "Epoch 0, Iteration 130, Loss: 0.8186985850334167\n",
      "Epoch 0, Iteration 131, Loss: 0.7551738619804382\n",
      "Epoch 0, Iteration 132, Loss: 0.7276203036308289\n",
      "Epoch 0, Iteration 133, Loss: 0.7816605567932129\n",
      "Epoch 0, Iteration 134, Loss: 0.7601654529571533\n",
      "Epoch 0, Iteration 135, Loss: 0.8149670362472534\n",
      "Epoch 0, Iteration 136, Loss: 0.7097750306129456\n",
      "Epoch 0, Iteration 137, Loss: 0.6566874980926514\n",
      "Epoch 0, Iteration 138, Loss: 0.7030441761016846\n",
      "Epoch 0, Iteration 139, Loss: 0.7232977747917175\n",
      "Epoch 0, Iteration 140, Loss: 0.6990424990653992\n",
      "Epoch 0, Iteration 141, Loss: 0.7038527727127075\n",
      "Epoch 0, Iteration 142, Loss: 0.7362452149391174\n",
      "Epoch 0, Iteration 143, Loss: 0.6597135663032532\n",
      "Epoch 0, Iteration 144, Loss: 0.648537278175354\n",
      "Epoch 0, Iteration 145, Loss: 0.7218893766403198\n",
      "Epoch 0, Iteration 146, Loss: 0.6324485540390015\n",
      "Epoch 0, Iteration 147, Loss: 0.6307884454727173\n",
      "Epoch 0, Iteration 148, Loss: 0.6449505090713501\n",
      "Epoch 0, Iteration 149, Loss: 0.7136585116386414\n",
      "Epoch 0, Iteration 150, Loss: 0.6195242404937744\n",
      "Epoch 0, Iteration 150, Test Loss: 0.7926191687583923\n",
      "Epoch 0, Iteration 151, Loss: 0.6407291889190674\n",
      "Epoch 0, Iteration 152, Loss: 0.6212723255157471\n",
      "Epoch 0, Iteration 153, Loss: 0.6026829481124878\n",
      "Epoch 0, Iteration 154, Loss: 0.5857492685317993\n",
      "Epoch 0, Iteration 155, Loss: 0.5897084474563599\n",
      "Epoch 0, Iteration 156, Loss: 0.6212924122810364\n",
      "Epoch 0, Iteration 157, Loss: 0.6135594248771667\n",
      "Epoch 0, Iteration 158, Loss: 0.618344783782959\n",
      "Epoch 0, Iteration 159, Loss: 0.5495213270187378\n",
      "Epoch 0, Iteration 160, Loss: 0.6244600415229797\n",
      "Epoch 0, Iteration 161, Loss: 0.5055873394012451\n",
      "Epoch 0, Iteration 162, Loss: 0.5318431258201599\n",
      "Epoch 0, Iteration 163, Loss: 0.5474596619606018\n",
      "Epoch 0, Iteration 164, Loss: 0.525145411491394\n",
      "Epoch 0, Iteration 165, Loss: 0.5232903957366943\n",
      "Epoch 0, Iteration 166, Loss: 0.5367723703384399\n",
      "Epoch 0, Iteration 167, Loss: 0.49544644355773926\n",
      "Epoch 0, Iteration 168, Loss: 0.5200806260108948\n",
      "Epoch 0, Iteration 169, Loss: 0.539234459400177\n",
      "Epoch 0, Iteration 170, Loss: 0.4942176043987274\n",
      "Epoch 0, Iteration 171, Loss: 0.567803680896759\n",
      "Epoch 0, Iteration 172, Loss: 0.5153486728668213\n",
      "Epoch 0, Iteration 173, Loss: 0.4968545138835907\n",
      "Epoch 0, Iteration 174, Loss: 0.4589872658252716\n",
      "Epoch 0, Iteration 175, Loss: 0.48190438747406006\n",
      "Epoch 0, Iteration 176, Loss: 0.4691406190395355\n",
      "Epoch 0, Iteration 177, Loss: 0.4534359276294708\n",
      "Epoch 0, Iteration 178, Loss: 0.4246208071708679\n",
      "Epoch 0, Iteration 179, Loss: 0.48354291915893555\n",
      "Epoch 0, Iteration 180, Loss: 0.508573055267334\n",
      "Epoch 0, Iteration 181, Loss: 0.50184565782547\n",
      "Epoch 0, Iteration 182, Loss: 0.4101703464984894\n",
      "Epoch 0, Iteration 183, Loss: 0.481234073638916\n",
      "Epoch 0, Iteration 184, Loss: 0.48479199409484863\n",
      "Epoch 0, Iteration 185, Loss: 0.42354944348335266\n",
      "Epoch 0, Iteration 186, Loss: 0.44591715931892395\n",
      "Epoch 0, Iteration 187, Loss: 0.41465866565704346\n",
      "Epoch 0, Iteration 188, Loss: 0.3915465772151947\n",
      "Epoch 0, Iteration 189, Loss: 0.39220547676086426\n",
      "Epoch 0, Iteration 190, Loss: 0.4015456438064575\n",
      "Epoch 0, Iteration 191, Loss: 0.448596328496933\n",
      "Epoch 0, Iteration 192, Loss: 0.47544193267822266\n",
      "Epoch 0, Iteration 193, Loss: 0.43732786178588867\n",
      "Epoch 0, Iteration 194, Loss: 0.4303751289844513\n",
      "Epoch 0, Iteration 195, Loss: 0.4273354709148407\n",
      "Epoch 0, Iteration 196, Loss: 0.44567006826400757\n",
      "Epoch 0, Iteration 197, Loss: 0.40276092290878296\n",
      "Epoch 0, Iteration 198, Loss: 0.3403308689594269\n",
      "Epoch 0, Iteration 199, Loss: 0.4082338213920593\n",
      "Epoch 0, Iteration 200, Loss: 0.33567774295806885\n",
      "Epoch 0, Iteration 200, Test Loss: 0.5561891198158264\n",
      "Epoch 0, Iteration 201, Loss: 0.3846457004547119\n",
      "Epoch 0, Iteration 202, Loss: 0.38256609439849854\n",
      "Epoch 0, Iteration 203, Loss: 0.34295207262039185\n",
      "Epoch 0, Iteration 204, Loss: 0.37870335578918457\n",
      "Epoch 0, Iteration 205, Loss: 0.41008874773979187\n",
      "Epoch 0, Iteration 206, Loss: 0.3775566816329956\n",
      "Epoch 0, Iteration 207, Loss: 0.3907889723777771\n",
      "Epoch 0, Iteration 208, Loss: 0.3960740566253662\n",
      "Epoch 0, Iteration 209, Loss: 0.37856224179267883\n",
      "Epoch 0, Iteration 210, Loss: 0.34989702701568604\n",
      "Epoch 0, Iteration 211, Loss: 0.37697383761405945\n",
      "Epoch 0, Iteration 212, Loss: 0.37097564339637756\n",
      "Epoch 0, Iteration 213, Loss: 0.3245365023612976\n",
      "Epoch 0, Iteration 214, Loss: 0.3568360209465027\n",
      "Epoch 0, Iteration 215, Loss: 0.33309805393218994\n",
      "Epoch 0, Iteration 216, Loss: 0.4022122621536255\n",
      "Epoch 0, Iteration 217, Loss: 0.41609078645706177\n",
      "Epoch 0, Iteration 218, Loss: 0.36943209171295166\n",
      "Epoch 0, Iteration 219, Loss: 0.3882140815258026\n",
      "Epoch 0, Iteration 220, Loss: 0.34039759635925293\n",
      "Epoch 0, Iteration 221, Loss: 0.35694754123687744\n",
      "Epoch 0, Iteration 222, Loss: 0.35541820526123047\n",
      "Epoch 0, Iteration 223, Loss: 0.361931174993515\n",
      "Epoch 0, Iteration 224, Loss: 0.31069639325141907\n",
      "Epoch 0, Iteration 225, Loss: 0.3651110529899597\n",
      "Epoch 0, Iteration 226, Loss: 0.4058157801628113\n",
      "Epoch 0, Iteration 227, Loss: 0.3532211482524872\n",
      "Epoch 0, Iteration 228, Loss: 0.5204335451126099\n",
      "Epoch 0, Iteration 229, Loss: 0.4474649727344513\n",
      "Epoch 0, Iteration 230, Loss: 0.38583019375801086\n",
      "Epoch 0, Iteration 231, Loss: 0.31673264503479004\n",
      "Epoch 0, Iteration 232, Loss: 0.35398900508880615\n",
      "Epoch 0, Iteration 233, Loss: 0.39801377058029175\n",
      "Epoch 0, Iteration 234, Loss: 0.3460923433303833\n",
      "Epoch 0, Iteration 235, Loss: 0.2936227321624756\n",
      "Epoch 0, Iteration 236, Loss: 0.3482101559638977\n",
      "Epoch 0, Iteration 237, Loss: 0.3083624541759491\n",
      "Epoch 0, Iteration 238, Loss: 0.31893977522850037\n",
      "Epoch 0, Iteration 239, Loss: 0.35220661759376526\n",
      "Epoch 0, Iteration 240, Loss: 0.33056318759918213\n",
      "Epoch 0, Iteration 241, Loss: 0.3393797278404236\n",
      "Epoch 0, Iteration 242, Loss: 0.3402898609638214\n",
      "Epoch 0, Iteration 243, Loss: 0.278947114944458\n",
      "Epoch 0, Iteration 244, Loss: 0.33925458788871765\n",
      "Epoch 0, Iteration 245, Loss: 0.31265029311180115\n",
      "Epoch 0, Iteration 246, Loss: 0.3469918668270111\n",
      "Epoch 0, Iteration 247, Loss: 0.45135244727134705\n",
      "Epoch 0, Iteration 248, Loss: 0.3673774302005768\n",
      "Epoch 0, Iteration 249, Loss: 0.419199675321579\n",
      "Epoch 0, Iteration 250, Loss: 0.4528568983078003\n",
      "Epoch 0, Iteration 250, Test Loss: 0.43338122963905334\n",
      "Epoch 0, Iteration 251, Loss: 0.40905871987342834\n",
      "Epoch 0, Iteration 252, Loss: 0.3372667729854584\n",
      "Epoch 0, Iteration 253, Loss: 0.33430835604667664\n",
      "Epoch 0, Iteration 254, Loss: 0.4884343445301056\n",
      "Epoch 0, Iteration 255, Loss: 0.3185955286026001\n",
      "Epoch 0, Iteration 256, Loss: 0.32213783264160156\n",
      "Epoch 0, Iteration 257, Loss: 0.30338284373283386\n",
      "Epoch 0, Iteration 258, Loss: 0.34217894077301025\n",
      "Epoch 0, Iteration 259, Loss: 0.35542571544647217\n",
      "Epoch 0, Iteration 260, Loss: 0.4061884582042694\n",
      "Epoch 0, Iteration 261, Loss: 0.28223326802253723\n",
      "Epoch 0, Iteration 262, Loss: 0.30411458015441895\n",
      "Epoch 0, Iteration 263, Loss: 0.3059335947036743\n",
      "Epoch 0, Iteration 264, Loss: 0.2664910554885864\n",
      "Epoch 0, Iteration 265, Loss: 0.4071238934993744\n",
      "Epoch 0, Iteration 266, Loss: 0.28646746277809143\n",
      "Epoch 0, Iteration 267, Loss: 0.30731698870658875\n",
      "Epoch 0, Iteration 268, Loss: 0.35820889472961426\n",
      "Epoch 0, Iteration 269, Loss: 0.31424975395202637\n",
      "Epoch 0, Iteration 270, Loss: 0.41000109910964966\n",
      "Epoch 0, Iteration 271, Loss: 0.2808666527271271\n",
      "Epoch 0, Iteration 272, Loss: 0.294779896736145\n",
      "Epoch 0, Iteration 273, Loss: 0.305727481842041\n",
      "Epoch 0, Iteration 274, Loss: 0.27814871072769165\n",
      "Epoch 0, Iteration 275, Loss: 0.2793927490711212\n",
      "Epoch 0, Iteration 276, Loss: 0.27859386801719666\n",
      "Epoch 0, Iteration 277, Loss: 0.2784287929534912\n",
      "Epoch 0, Iteration 278, Loss: 0.3061974346637726\n",
      "Epoch 0, Iteration 279, Loss: 0.2616479992866516\n",
      "Epoch 0, Iteration 280, Loss: 0.2777303159236908\n",
      "Epoch 0, Iteration 281, Loss: 0.26658710837364197\n",
      "Epoch 0, Iteration 282, Loss: 0.2814703583717346\n",
      "Epoch 0, Iteration 283, Loss: 0.2128346860408783\n",
      "Epoch 0, Iteration 284, Loss: 0.33057641983032227\n",
      "Epoch 0, Iteration 285, Loss: 0.28720539808273315\n",
      "Epoch 0, Iteration 286, Loss: 0.3252458870410919\n",
      "Epoch 0, Iteration 287, Loss: 0.27536889910697937\n",
      "Epoch 0, Iteration 288, Loss: 0.24902285635471344\n",
      "Epoch 0, Iteration 289, Loss: 0.3906034827232361\n",
      "Epoch 0, Iteration 290, Loss: 0.27528658509254456\n",
      "Epoch 0, Iteration 291, Loss: 0.24546785652637482\n",
      "Epoch 0, Iteration 292, Loss: 0.285177081823349\n",
      "Epoch 0, Iteration 293, Loss: 0.27620750665664673\n",
      "Epoch 0, Iteration 294, Loss: 0.26438435912132263\n",
      "Epoch 0, Iteration 295, Loss: 0.22500884532928467\n",
      "Epoch 0, Iteration 296, Loss: 0.2406185120344162\n",
      "Epoch 0, Iteration 297, Loss: 0.29616624116897583\n",
      "Epoch 0, Iteration 298, Loss: 0.23500463366508484\n",
      "Epoch 0, Iteration 299, Loss: 0.3220563232898712\n",
      "Epoch 0, Iteration 300, Loss: 0.26355594396591187\n",
      "Epoch 0, Iteration 300, Test Loss: 0.38329604268074036\n",
      "Epoch 0, Iteration 301, Loss: 0.2659137547016144\n",
      "Epoch 0, Iteration 302, Loss: 0.2352883219718933\n",
      "Epoch 0, Iteration 303, Loss: 0.2941987216472626\n",
      "Epoch 0, Iteration 304, Loss: 0.32627034187316895\n",
      "Epoch 0, Iteration 305, Loss: 0.2277553826570511\n",
      "Epoch 0, Iteration 306, Loss: 0.2728300392627716\n",
      "Epoch 0, Iteration 307, Loss: 0.24300582706928253\n",
      "Epoch 0, Iteration 308, Loss: 0.23765403032302856\n",
      "Epoch 0, Iteration 309, Loss: 0.2589096426963806\n",
      "Epoch 0, Iteration 310, Loss: 0.2916167080402374\n",
      "Epoch 0, Iteration 311, Loss: 0.22776280343532562\n",
      "Epoch 0, Iteration 312, Loss: 0.3172503113746643\n",
      "Epoch 0, Iteration 313, Loss: 0.2694337069988251\n",
      "Epoch 0, Iteration 314, Loss: 0.22932009398937225\n",
      "Epoch 0, Iteration 315, Loss: 0.25176310539245605\n",
      "Epoch 0, Iteration 316, Loss: 0.2567327618598938\n",
      "Epoch 0, Iteration 317, Loss: 0.2813679277896881\n",
      "Epoch 0, Iteration 318, Loss: 0.3411158323287964\n",
      "Epoch 0, Iteration 319, Loss: 0.2542392313480377\n",
      "Epoch 0, Iteration 320, Loss: 0.2515377998352051\n",
      "Epoch 0, Iteration 321, Loss: 0.22808638215065002\n",
      "Epoch 0, Iteration 322, Loss: 0.3174649178981781\n",
      "Epoch 0, Iteration 323, Loss: 0.2735365331172943\n",
      "Epoch 0, Iteration 324, Loss: 0.3035813271999359\n",
      "Epoch 0, Iteration 325, Loss: 0.38910871744155884\n",
      "Epoch 0, Iteration 326, Loss: 0.35992157459259033\n",
      "Epoch 0, Iteration 327, Loss: 0.2920839786529541\n",
      "Epoch 0, Iteration 328, Loss: 0.24406839907169342\n",
      "Epoch 0, Iteration 329, Loss: 0.26830923557281494\n",
      "Epoch 0, Iteration 330, Loss: 0.31282418966293335\n",
      "Epoch 0, Iteration 331, Loss: 0.2735435366630554\n",
      "Epoch 0, Iteration 332, Loss: 0.2901838421821594\n",
      "Epoch 0, Iteration 333, Loss: 0.24823522567749023\n",
      "Epoch 0, Iteration 334, Loss: 0.3626735508441925\n",
      "Epoch 0, Iteration 335, Loss: 0.300010085105896\n",
      "Epoch 0, Iteration 336, Loss: 0.2800227403640747\n",
      "Epoch 0, Iteration 337, Loss: 0.2519366443157196\n",
      "Epoch 0, Iteration 338, Loss: 0.28719034790992737\n",
      "Epoch 0, Iteration 339, Loss: 0.3430050313472748\n",
      "Epoch 0, Iteration 340, Loss: 0.3053300678730011\n",
      "Epoch 0, Iteration 341, Loss: 0.3375259041786194\n",
      "Epoch 0, Iteration 342, Loss: 0.2904762923717499\n",
      "Epoch 0, Iteration 343, Loss: 0.3020530641078949\n",
      "Epoch 0, Iteration 344, Loss: 0.2824431359767914\n",
      "Epoch 0, Iteration 345, Loss: 0.24968841671943665\n",
      "Epoch 0, Iteration 346, Loss: 0.29006052017211914\n",
      "Epoch 0, Iteration 347, Loss: 0.3085744380950928\n",
      "Epoch 0, Iteration 348, Loss: 0.30233797430992126\n",
      "Epoch 0, Iteration 349, Loss: 0.28978732228279114\n",
      "Epoch 0, Iteration 350, Loss: 0.28409525752067566\n",
      "Epoch 0, Iteration 350, Test Loss: 0.35425958037376404\n",
      "Epoch 0, Iteration 351, Loss: 0.28272515535354614\n",
      "Epoch 0, Iteration 352, Loss: 0.28911536931991577\n",
      "Epoch 0, Iteration 353, Loss: 0.2713516652584076\n",
      "Epoch 0, Iteration 354, Loss: 0.21428988873958588\n",
      "Epoch 0, Iteration 355, Loss: 0.24038930237293243\n",
      "Epoch 0, Iteration 356, Loss: 0.23293893039226532\n",
      "Epoch 0, Iteration 357, Loss: 0.3225097358226776\n",
      "Epoch 0, Iteration 358, Loss: 0.22770537436008453\n",
      "Epoch 0, Iteration 359, Loss: 0.23821236193180084\n",
      "Epoch 0, Iteration 360, Loss: 0.21549516916275024\n",
      "Epoch 0, Iteration 361, Loss: 0.26128849387168884\n",
      "Epoch 0, Iteration 362, Loss: 0.22336585819721222\n",
      "Epoch 0, Iteration 363, Loss: 0.22256769239902496\n",
      "Epoch 0, Iteration 364, Loss: 0.2658950686454773\n",
      "Epoch 0, Iteration 365, Loss: 0.21205122768878937\n",
      "Epoch 0, Iteration 366, Loss: 0.29636794328689575\n",
      "Epoch 0, Iteration 367, Loss: 0.25646138191223145\n",
      "Epoch 0, Iteration 368, Loss: 0.21657152473926544\n",
      "Epoch 0, Iteration 369, Loss: 0.22889463603496552\n",
      "Epoch 0, Iteration 370, Loss: 0.29412445425987244\n",
      "Epoch 0, Iteration 371, Loss: 0.24459250271320343\n",
      "Epoch 0, Iteration 372, Loss: 0.23761190474033356\n",
      "Epoch 0, Iteration 373, Loss: 0.2787560522556305\n",
      "Epoch 0, Iteration 374, Loss: 0.2156919538974762\n",
      "Epoch 0, Iteration 375, Loss: 0.2730311155319214\n",
      "Epoch 0, Iteration 376, Loss: 0.2557590901851654\n",
      "Epoch 0, Iteration 377, Loss: 0.2730681598186493\n",
      "Epoch 0, Iteration 378, Loss: 0.24883651733398438\n",
      "Epoch 0, Iteration 379, Loss: 0.3169134855270386\n",
      "Epoch 0, Iteration 380, Loss: 0.25361570715904236\n",
      "Epoch 0, Iteration 381, Loss: 0.24149277806282043\n",
      "Epoch 0, Iteration 382, Loss: 0.2698511779308319\n",
      "Epoch 0, Iteration 383, Loss: 0.1905837208032608\n",
      "Epoch 0, Iteration 384, Loss: 0.2696867287158966\n",
      "Epoch 0, Iteration 385, Loss: 0.22884626686573029\n",
      "Epoch 0, Iteration 386, Loss: 0.25251954793930054\n",
      "Epoch 0, Iteration 387, Loss: 0.20077943801879883\n",
      "Epoch 0, Iteration 388, Loss: 0.1823749989271164\n",
      "Epoch 0, Iteration 389, Loss: 0.24430805444717407\n",
      "Epoch 0, Iteration 390, Loss: 0.24886633455753326\n",
      "Epoch 0, Iteration 391, Loss: 0.19241021573543549\n",
      "Epoch 0, Iteration 392, Loss: 0.2887624502182007\n",
      "Epoch 0, Iteration 393, Loss: 0.28905776143074036\n",
      "Epoch 0, Iteration 394, Loss: 0.21143780648708344\n",
      "Epoch 0, Iteration 395, Loss: 0.24268043041229248\n",
      "Epoch 0, Iteration 396, Loss: 0.23171159625053406\n",
      "Epoch 0, Iteration 397, Loss: 0.246932253241539\n",
      "Epoch 0, Iteration 398, Loss: 0.4050784409046173\n",
      "Epoch 0, Iteration 399, Loss: 0.27682778239250183\n",
      "Epoch 0, Iteration 400, Loss: 0.2018045037984848\n",
      "Epoch 0, Iteration 400, Test Loss: 0.3287869989871979\n",
      "Epoch 0, Iteration 401, Loss: 0.27605196833610535\n",
      "Epoch 0, Iteration 402, Loss: 0.17959967255592346\n",
      "Epoch 0, Iteration 403, Loss: 0.23592700064182281\n",
      "Epoch 0, Iteration 404, Loss: 0.3151821196079254\n",
      "Epoch 0, Iteration 405, Loss: 0.18670280277729034\n",
      "Epoch 0, Iteration 406, Loss: 0.22349658608436584\n",
      "Epoch 0, Iteration 407, Loss: 0.23270584642887115\n",
      "Epoch 0, Iteration 408, Loss: 0.257945716381073\n",
      "Epoch 0, Iteration 409, Loss: 0.21165241301059723\n",
      "Epoch 0, Iteration 410, Loss: 0.28858765959739685\n",
      "Epoch 0, Iteration 411, Loss: 0.3016410768032074\n",
      "Epoch 0, Iteration 412, Loss: 0.21641896665096283\n",
      "Epoch 0, Iteration 413, Loss: 0.26140785217285156\n",
      "Epoch 0, Iteration 414, Loss: 0.1921842247247696\n",
      "Epoch 0, Iteration 415, Loss: 0.24855631589889526\n",
      "Epoch 0, Iteration 416, Loss: 0.24864748120307922\n",
      "Epoch 0, Iteration 417, Loss: 0.2275218367576599\n",
      "Epoch 0, Iteration 418, Loss: 0.20912659168243408\n",
      "Epoch 0, Iteration 419, Loss: 0.2302161604166031\n",
      "Epoch 0, Iteration 420, Loss: 0.2317759394645691\n",
      "Epoch 0, Iteration 421, Loss: 0.153324156999588\n",
      "Epoch 0, Iteration 422, Loss: 0.204903244972229\n",
      "Epoch 0, Iteration 423, Loss: 0.20187287032604218\n",
      "Epoch 0, Iteration 424, Loss: 0.2605496644973755\n",
      "Epoch 0, Iteration 425, Loss: 0.2508470118045807\n",
      "Epoch 0, Iteration 426, Loss: 0.19418956339359283\n",
      "Epoch 0, Iteration 427, Loss: 0.2010672688484192\n",
      "Epoch 0, Iteration 428, Loss: 0.20640431344509125\n",
      "Epoch 0, Iteration 429, Loss: 0.2044275999069214\n",
      "Epoch 0, Iteration 430, Loss: 0.22490276396274567\n",
      "Epoch 0, Iteration 431, Loss: 0.21073749661445618\n",
      "Epoch 1/15, Loss: 0.798185811240088\n",
      "Epoch 1/15, Validation Accuracy: 0.9223715415019763\n",
      "Epoch 1, Iteration 0, Loss: 0.48966869711875916\n",
      "Epoch 1, Iteration 1, Loss: 0.3531942665576935\n",
      "Epoch 1, Iteration 2, Loss: 0.48602139949798584\n",
      "Epoch 1, Iteration 3, Loss: 0.24468578398227692\n",
      "Epoch 1, Iteration 4, Loss: 0.40645578503608704\n",
      "Epoch 1, Iteration 5, Loss: 0.4093562364578247\n",
      "Epoch 1, Iteration 6, Loss: 0.3812694549560547\n",
      "Epoch 1, Iteration 7, Loss: 0.4277481436729431\n",
      "Epoch 1, Iteration 8, Loss: 0.388405442237854\n",
      "Epoch 1, Iteration 9, Loss: 0.40617096424102783\n",
      "Epoch 1, Iteration 10, Loss: 0.4649485647678375\n",
      "Epoch 1, Iteration 11, Loss: 0.34645697474479675\n",
      "Epoch 1, Iteration 12, Loss: 0.29171186685562134\n",
      "Epoch 1, Iteration 13, Loss: 0.4432889521121979\n",
      "Epoch 1, Iteration 14, Loss: 0.4318893551826477\n",
      "Epoch 1, Iteration 15, Loss: 0.27814409136772156\n",
      "Epoch 1, Iteration 16, Loss: 0.4850238859653473\n",
      "Epoch 1, Iteration 17, Loss: 0.2897970378398895\n",
      "Epoch 1, Iteration 18, Loss: 0.3446334898471832\n",
      "Epoch 1, Iteration 19, Loss: 0.3117466866970062\n",
      "Epoch 1, Iteration 20, Loss: 0.38791969418525696\n",
      "Epoch 1, Iteration 21, Loss: 0.3441488742828369\n",
      "Epoch 1, Iteration 22, Loss: 0.2923535406589508\n",
      "Epoch 1, Iteration 23, Loss: 0.31062567234039307\n",
      "Epoch 1, Iteration 24, Loss: 0.3432294726371765\n",
      "Epoch 1, Iteration 25, Loss: 0.3964226543903351\n",
      "Epoch 1, Iteration 26, Loss: 0.32466962933540344\n",
      "Epoch 1, Iteration 27, Loss: 0.2512592077255249\n",
      "Epoch 1, Iteration 28, Loss: 0.3472622334957123\n",
      "Epoch 1, Iteration 29, Loss: 0.2953493297100067\n",
      "Epoch 1, Iteration 30, Loss: 0.30619102716445923\n",
      "Epoch 1, Iteration 31, Loss: 0.3917817175388336\n",
      "Epoch 1, Iteration 32, Loss: 0.28304702043533325\n",
      "Epoch 1, Iteration 33, Loss: 0.3037247657775879\n",
      "Epoch 1, Iteration 34, Loss: 0.33952251076698303\n",
      "Epoch 1, Iteration 35, Loss: 0.2587909996509552\n",
      "Epoch 1, Iteration 36, Loss: 0.3596787750720978\n",
      "Epoch 1, Iteration 37, Loss: 0.3053901791572571\n",
      "Epoch 1, Iteration 38, Loss: 0.33786287903785706\n",
      "Epoch 1, Iteration 39, Loss: 0.3100649118423462\n",
      "Epoch 1, Iteration 40, Loss: 0.29831647872924805\n",
      "Epoch 1, Iteration 41, Loss: 0.25935354828834534\n",
      "Epoch 1, Iteration 42, Loss: 0.27217036485671997\n",
      "Epoch 1, Iteration 43, Loss: 0.36362412571907043\n",
      "Epoch 1, Iteration 44, Loss: 0.25243958830833435\n",
      "Epoch 1, Iteration 45, Loss: 0.31311509013175964\n",
      "Epoch 1, Iteration 46, Loss: 0.25964605808258057\n",
      "Epoch 1, Iteration 47, Loss: 0.3636932969093323\n",
      "Epoch 1, Iteration 48, Loss: 0.3035096526145935\n",
      "Epoch 1, Iteration 49, Loss: 0.2294967919588089\n",
      "Epoch 1, Iteration 50, Loss: 0.22195570170879364\n",
      "Epoch 1, Iteration 50, Test Loss: 0.25718408823013306\n",
      "Epoch 1, Iteration 51, Loss: 0.2268669754266739\n",
      "Epoch 1, Iteration 52, Loss: 0.30873894691467285\n",
      "Epoch 1, Iteration 53, Loss: 0.24708542227745056\n",
      "Epoch 1, Iteration 54, Loss: 0.2700881361961365\n",
      "Epoch 1, Iteration 55, Loss: 0.18991334736347198\n",
      "Epoch 1, Iteration 56, Loss: 0.23814693093299866\n",
      "Epoch 1, Iteration 57, Loss: 0.24301540851593018\n",
      "Epoch 1, Iteration 58, Loss: 0.21194051206111908\n",
      "Epoch 1, Iteration 59, Loss: 0.22616958618164062\n",
      "Epoch 1, Iteration 60, Loss: 0.24933649599552155\n",
      "Epoch 1, Iteration 61, Loss: 0.21426714956760406\n",
      "Epoch 1, Iteration 62, Loss: 0.34512582421302795\n",
      "Epoch 1, Iteration 63, Loss: 0.18565750122070312\n",
      "Epoch 1, Iteration 64, Loss: 0.21658222377300262\n",
      "Epoch 1, Iteration 65, Loss: 0.2331172525882721\n",
      "Epoch 1, Iteration 66, Loss: 0.24814604222774506\n",
      "Epoch 1, Iteration 67, Loss: 0.1740143597126007\n",
      "Epoch 1, Iteration 68, Loss: 0.20058168470859528\n",
      "Epoch 1, Iteration 69, Loss: 0.24274174869060516\n",
      "Epoch 1, Iteration 70, Loss: 0.19469712674617767\n",
      "Epoch 1, Iteration 71, Loss: 0.23977825045585632\n",
      "Epoch 1, Iteration 72, Loss: 0.2423272281885147\n",
      "Epoch 1, Iteration 73, Loss: 0.2257806956768036\n",
      "Epoch 1, Iteration 74, Loss: 0.24929851293563843\n",
      "Epoch 1, Iteration 75, Loss: 0.24059927463531494\n",
      "Epoch 1, Iteration 76, Loss: 0.22192899882793427\n",
      "Epoch 1, Iteration 77, Loss: 0.23407819867134094\n",
      "Epoch 1, Iteration 78, Loss: 0.2142561972141266\n",
      "Epoch 1, Iteration 79, Loss: 0.16021975874900818\n",
      "Epoch 1, Iteration 80, Loss: 0.2667074501514435\n",
      "Epoch 1, Iteration 81, Loss: 0.2482375055551529\n",
      "Epoch 1, Iteration 82, Loss: 0.23002026975154877\n",
      "Epoch 1, Iteration 83, Loss: 0.2066691815853119\n",
      "Epoch 1, Iteration 84, Loss: 0.20172618329524994\n",
      "Epoch 1, Iteration 85, Loss: 0.2861945331096649\n",
      "Epoch 1, Iteration 86, Loss: 0.23287013173103333\n",
      "Epoch 1, Iteration 87, Loss: 0.1895235925912857\n",
      "Epoch 1, Iteration 88, Loss: 0.20944727957248688\n",
      "Epoch 1, Iteration 89, Loss: 0.20935189723968506\n",
      "Epoch 1, Iteration 90, Loss: 0.21400903165340424\n",
      "Epoch 1, Iteration 91, Loss: 0.17359262704849243\n",
      "Epoch 1, Iteration 92, Loss: 0.20020762085914612\n",
      "Epoch 1, Iteration 93, Loss: 0.21024131774902344\n",
      "Epoch 1, Iteration 94, Loss: 0.2683451771736145\n",
      "Epoch 1, Iteration 95, Loss: 0.20879504084587097\n",
      "Epoch 1, Iteration 96, Loss: 0.19873349368572235\n",
      "Epoch 1, Iteration 97, Loss: 0.22188825905323029\n",
      "Epoch 1, Iteration 98, Loss: 0.2554239332675934\n",
      "Epoch 1, Iteration 99, Loss: 0.22131237387657166\n",
      "Epoch 1, Iteration 100, Loss: 0.20225220918655396\n",
      "Epoch 1, Iteration 100, Test Loss: 0.2888563573360443\n",
      "Epoch 1, Iteration 101, Loss: 0.2151622623205185\n",
      "Epoch 1, Iteration 102, Loss: 0.19232060015201569\n",
      "Epoch 1, Iteration 103, Loss: 0.17083020508289337\n",
      "Epoch 1, Iteration 104, Loss: 0.2146911919116974\n",
      "Epoch 1, Iteration 105, Loss: 0.22864294052124023\n",
      "Epoch 1, Iteration 106, Loss: 0.1646864265203476\n",
      "Epoch 1, Iteration 107, Loss: 0.14712122082710266\n",
      "Epoch 1, Iteration 108, Loss: 0.32822269201278687\n",
      "Epoch 1, Iteration 109, Loss: 0.28241661190986633\n",
      "Epoch 1, Iteration 110, Loss: 0.1948094666004181\n",
      "Epoch 1, Iteration 111, Loss: 0.17885445058345795\n",
      "Epoch 1, Iteration 112, Loss: 0.22410978376865387\n",
      "Epoch 1, Iteration 113, Loss: 0.16764169931411743\n",
      "Epoch 1, Iteration 114, Loss: 0.21288785338401794\n",
      "Epoch 1, Iteration 115, Loss: 0.21965545415878296\n",
      "Epoch 1, Iteration 116, Loss: 0.1883198469877243\n",
      "Epoch 1, Iteration 117, Loss: 0.25583934783935547\n",
      "Epoch 1, Iteration 118, Loss: 0.20235884189605713\n",
      "Epoch 1, Iteration 119, Loss: 0.2550341784954071\n",
      "Epoch 1, Iteration 120, Loss: 0.1735931932926178\n",
      "Epoch 1, Iteration 121, Loss: 0.2112419307231903\n",
      "Epoch 1, Iteration 122, Loss: 0.22857733070850372\n",
      "Epoch 1, Iteration 123, Loss: 0.1944146305322647\n",
      "Epoch 1, Iteration 124, Loss: 0.22556975483894348\n",
      "Epoch 1, Iteration 125, Loss: 0.25622352957725525\n",
      "Epoch 1, Iteration 126, Loss: 0.22745512425899506\n",
      "Epoch 1, Iteration 127, Loss: 0.21698810160160065\n",
      "Epoch 1, Iteration 128, Loss: 0.20369374752044678\n",
      "Epoch 1, Iteration 129, Loss: 0.24644091725349426\n",
      "Epoch 1, Iteration 130, Loss: 0.16563311219215393\n",
      "Epoch 1, Iteration 131, Loss: 0.17916905879974365\n",
      "Epoch 1, Iteration 132, Loss: 0.2368590533733368\n",
      "Epoch 1, Iteration 133, Loss: 0.28104159235954285\n",
      "Epoch 1, Iteration 134, Loss: 0.2290649712085724\n",
      "Epoch 1, Iteration 135, Loss: 0.24786879122257233\n",
      "Epoch 1, Iteration 136, Loss: 0.1533827781677246\n",
      "Epoch 1, Iteration 137, Loss: 0.1521601378917694\n",
      "Epoch 1, Iteration 138, Loss: 0.20419058203697205\n",
      "Epoch 1, Iteration 139, Loss: 0.20977503061294556\n",
      "Epoch 1, Iteration 140, Loss: 0.2313731461763382\n",
      "Epoch 1, Iteration 141, Loss: 0.21108843386173248\n",
      "Epoch 1, Iteration 142, Loss: 0.2623005509376526\n",
      "Epoch 1, Iteration 143, Loss: 0.19291256368160248\n",
      "Epoch 1, Iteration 144, Loss: 0.18479512631893158\n",
      "Epoch 1, Iteration 145, Loss: 0.21537643671035767\n",
      "Epoch 1, Iteration 146, Loss: 0.1481667459011078\n",
      "Epoch 1, Iteration 147, Loss: 0.18505476415157318\n",
      "Epoch 1, Iteration 148, Loss: 0.17970845103263855\n",
      "Epoch 1, Iteration 149, Loss: 0.19699609279632568\n",
      "Epoch 1, Iteration 150, Loss: 0.2248305082321167\n",
      "Epoch 1, Iteration 150, Test Loss: 0.2758755385875702\n",
      "Epoch 1, Iteration 151, Loss: 0.22237686812877655\n",
      "Epoch 1, Iteration 152, Loss: 0.21873946487903595\n",
      "Epoch 1, Iteration 153, Loss: 0.22888921201229095\n",
      "Epoch 1, Iteration 154, Loss: 0.17678003013134003\n",
      "Epoch 1, Iteration 155, Loss: 0.2188510298728943\n",
      "Epoch 1, Iteration 156, Loss: 0.21428516507148743\n",
      "Epoch 1, Iteration 157, Loss: 0.23781846463680267\n",
      "Epoch 1, Iteration 158, Loss: 0.28311824798583984\n",
      "Epoch 1, Iteration 159, Loss: 0.20142804086208344\n",
      "Epoch 1, Iteration 160, Loss: 0.265076607465744\n",
      "Epoch 1, Iteration 161, Loss: 0.16913649439811707\n",
      "Epoch 1, Iteration 162, Loss: 0.15884199738502502\n",
      "Epoch 1, Iteration 163, Loss: 0.20600630342960358\n",
      "Epoch 1, Iteration 164, Loss: 0.16674713790416718\n",
      "Epoch 1, Iteration 165, Loss: 0.1910904049873352\n",
      "Epoch 1, Iteration 166, Loss: 0.19592970609664917\n",
      "Epoch 1, Iteration 167, Loss: 0.18280522525310516\n",
      "Epoch 1, Iteration 168, Loss: 0.1731783002614975\n",
      "Epoch 1, Iteration 169, Loss: 0.21516385674476624\n",
      "Epoch 1, Iteration 170, Loss: 0.15217158198356628\n",
      "Epoch 1, Iteration 171, Loss: 0.26708492636680603\n",
      "Epoch 1, Iteration 172, Loss: 0.27417418360710144\n",
      "Epoch 1, Iteration 173, Loss: 0.22127830982208252\n",
      "Epoch 1, Iteration 174, Loss: 0.21391116082668304\n",
      "Epoch 1, Iteration 175, Loss: 0.1908162534236908\n",
      "Epoch 1, Iteration 176, Loss: 0.16272008419036865\n",
      "Epoch 1, Iteration 177, Loss: 0.23287741839885712\n",
      "Epoch 1, Iteration 178, Loss: 0.17472994327545166\n",
      "Epoch 1, Iteration 179, Loss: 0.19114536046981812\n",
      "Epoch 1, Iteration 180, Loss: 0.26606470346450806\n",
      "Epoch 1, Iteration 181, Loss: 0.18821923434734344\n",
      "Epoch 1, Iteration 182, Loss: 0.18489128351211548\n",
      "Epoch 1, Iteration 183, Loss: 0.20799733698368073\n",
      "Epoch 1, Iteration 184, Loss: 0.2539868652820587\n",
      "Epoch 1, Iteration 185, Loss: 0.1612807959318161\n",
      "Epoch 1, Iteration 186, Loss: 0.1968419849872589\n",
      "Epoch 1, Iteration 187, Loss: 0.17254817485809326\n",
      "Epoch 1, Iteration 188, Loss: 0.16496987640857697\n",
      "Epoch 1, Iteration 189, Loss: 0.13012969493865967\n",
      "Epoch 1, Iteration 190, Loss: 0.16103097796440125\n",
      "Epoch 1, Iteration 191, Loss: 0.1910688877105713\n",
      "Epoch 1, Iteration 192, Loss: 0.2029384970664978\n",
      "Epoch 1, Iteration 193, Loss: 0.19283993542194366\n",
      "Epoch 1, Iteration 194, Loss: 0.20299828052520752\n",
      "Epoch 1, Iteration 195, Loss: 0.20136703550815582\n",
      "Epoch 1, Iteration 196, Loss: 0.2724660336971283\n",
      "Epoch 1, Iteration 197, Loss: 0.1734750121831894\n",
      "Epoch 1, Iteration 198, Loss: 0.14616115391254425\n",
      "Epoch 1, Iteration 199, Loss: 0.19222714006900787\n",
      "Epoch 1, Iteration 200, Loss: 0.1419164538383484\n",
      "Epoch 1, Iteration 200, Test Loss: 0.278044193983078\n",
      "Epoch 1, Iteration 201, Loss: 0.16170504689216614\n",
      "Epoch 1, Iteration 202, Loss: 0.15762557089328766\n",
      "Epoch 1, Iteration 203, Loss: 0.15048843622207642\n",
      "Epoch 1, Iteration 204, Loss: 0.18988242745399475\n",
      "Epoch 1, Iteration 205, Loss: 0.2264968454837799\n",
      "Epoch 1, Iteration 206, Loss: 0.1409093588590622\n",
      "Epoch 1, Iteration 207, Loss: 0.16974502801895142\n",
      "Epoch 1, Iteration 208, Loss: 0.15177494287490845\n",
      "Epoch 1, Iteration 209, Loss: 0.1802818775177002\n",
      "Epoch 1, Iteration 210, Loss: 0.17172007262706757\n",
      "Epoch 1, Iteration 211, Loss: 0.21105346083641052\n",
      "Epoch 1, Iteration 212, Loss: 0.20258398354053497\n",
      "Epoch 1, Iteration 213, Loss: 0.15999168157577515\n",
      "Epoch 1, Iteration 214, Loss: 0.16277042031288147\n",
      "Epoch 1, Iteration 215, Loss: 0.17950858175754547\n",
      "Epoch 1, Iteration 216, Loss: 0.2058667242527008\n",
      "Epoch 1, Iteration 217, Loss: 0.24050863087177277\n",
      "Epoch 1, Iteration 218, Loss: 0.17778074741363525\n",
      "Epoch 1, Iteration 219, Loss: 0.22425054013729095\n",
      "Epoch 1, Iteration 220, Loss: 0.15648989379405975\n",
      "Epoch 1, Iteration 221, Loss: 0.2067185342311859\n",
      "Epoch 1, Iteration 222, Loss: 0.1957370936870575\n",
      "Epoch 1, Iteration 223, Loss: 0.1721523553133011\n",
      "Epoch 1, Iteration 224, Loss: 0.16041706502437592\n",
      "Epoch 1, Iteration 225, Loss: 0.1874888688325882\n",
      "Epoch 1, Iteration 226, Loss: 0.21062327921390533\n",
      "Epoch 1, Iteration 227, Loss: 0.18643973767757416\n",
      "Epoch 1, Iteration 228, Loss: 0.32844769954681396\n",
      "Epoch 1, Iteration 229, Loss: 0.2948439121246338\n",
      "Epoch 1, Iteration 230, Loss: 0.2350979745388031\n",
      "Epoch 1, Iteration 231, Loss: 0.1554575115442276\n",
      "Epoch 1, Iteration 232, Loss: 0.19428588449954987\n",
      "Epoch 1, Iteration 233, Loss: 0.2501818537712097\n",
      "Epoch 1, Iteration 234, Loss: 0.20007726550102234\n",
      "Epoch 1, Iteration 235, Loss: 0.15483999252319336\n",
      "Epoch 1, Iteration 236, Loss: 0.23130656778812408\n",
      "Epoch 1, Iteration 237, Loss: 0.18398039042949677\n",
      "Epoch 1, Iteration 238, Loss: 0.21889613568782806\n",
      "Epoch 1, Iteration 239, Loss: 0.1831686943769455\n",
      "Epoch 1, Iteration 240, Loss: 0.16533736884593964\n",
      "Epoch 1, Iteration 241, Loss: 0.18275339901447296\n",
      "Epoch 1, Iteration 242, Loss: 0.205970898270607\n",
      "Epoch 1, Iteration 243, Loss: 0.18247660994529724\n",
      "Epoch 1, Iteration 244, Loss: 0.19778983294963837\n",
      "Epoch 1, Iteration 245, Loss: 0.16692672669887543\n",
      "Epoch 1, Iteration 246, Loss: 0.209503173828125\n",
      "Epoch 1, Iteration 247, Loss: 0.32863765954971313\n",
      "Epoch 1, Iteration 248, Loss: 0.2659299075603485\n",
      "Epoch 1, Iteration 249, Loss: 0.2602998912334442\n",
      "Epoch 1, Iteration 250, Loss: 0.29769909381866455\n",
      "Epoch 1, Iteration 250, Test Loss: 0.27888983488082886\n",
      "Epoch 1, Iteration 251, Loss: 0.2447625994682312\n",
      "Epoch 1, Iteration 252, Loss: 0.17568516731262207\n",
      "Epoch 1, Iteration 253, Loss: 0.2259211242198944\n",
      "Epoch 1, Iteration 254, Loss: 0.25105273723602295\n",
      "Epoch 1, Iteration 255, Loss: 0.19346247613430023\n",
      "Epoch 1, Iteration 256, Loss: 0.19056501984596252\n",
      "Epoch 1, Iteration 257, Loss: 0.17165899276733398\n",
      "Epoch 1, Iteration 258, Loss: 0.20661239326000214\n",
      "Epoch 1, Iteration 259, Loss: 0.22244150936603546\n",
      "Epoch 1, Iteration 260, Loss: 0.26323869824409485\n",
      "Epoch 1, Iteration 261, Loss: 0.19533459842205048\n",
      "Epoch 1, Iteration 262, Loss: 0.1691020131111145\n",
      "Epoch 1, Iteration 263, Loss: 0.1707727313041687\n",
      "Epoch 1, Iteration 264, Loss: 0.16829852759838104\n",
      "Epoch 1, Iteration 265, Loss: 0.26639795303344727\n",
      "Epoch 1, Iteration 266, Loss: 0.1609966903924942\n",
      "Epoch 1, Iteration 267, Loss: 0.2117886245250702\n",
      "Epoch 1, Iteration 268, Loss: 0.23670117557048798\n",
      "Epoch 1, Iteration 269, Loss: 0.2198607176542282\n",
      "Epoch 1, Iteration 270, Loss: 0.25035205483436584\n",
      "Epoch 1, Iteration 271, Loss: 0.1347084939479828\n",
      "Epoch 1, Iteration 272, Loss: 0.18697452545166016\n",
      "Epoch 1, Iteration 273, Loss: 0.19342957437038422\n",
      "Epoch 1, Iteration 274, Loss: 0.17375843226909637\n",
      "Epoch 1, Iteration 275, Loss: 0.15290553867816925\n",
      "Epoch 1, Iteration 276, Loss: 0.15471717715263367\n",
      "Epoch 1, Iteration 277, Loss: 0.14667107164859772\n",
      "Epoch 1, Iteration 278, Loss: 0.2547496259212494\n",
      "Epoch 1, Iteration 279, Loss: 0.16035272181034088\n",
      "Epoch 1, Iteration 280, Loss: 0.1857101172208786\n",
      "Epoch 1, Iteration 281, Loss: 0.16198304295539856\n",
      "Epoch 1, Iteration 282, Loss: 0.1705171763896942\n",
      "Epoch 1, Iteration 283, Loss: 0.13334473967552185\n",
      "Epoch 1, Iteration 284, Loss: 0.186449334025383\n",
      "Epoch 1, Iteration 285, Loss: 0.16197234392166138\n",
      "Epoch 1, Iteration 286, Loss: 0.23248423635959625\n",
      "Epoch 1, Iteration 287, Loss: 0.12869860231876373\n",
      "Epoch 1, Iteration 288, Loss: 0.18269997835159302\n",
      "Epoch 1, Iteration 289, Loss: 0.2659820020198822\n",
      "Epoch 1, Iteration 290, Loss: 0.13709807395935059\n",
      "Epoch 1, Iteration 291, Loss: 0.1417064070701599\n",
      "Epoch 1, Iteration 292, Loss: 0.20701250433921814\n",
      "Epoch 1, Iteration 293, Loss: 0.20299072563648224\n",
      "Epoch 1, Iteration 294, Loss: 0.1569453477859497\n",
      "Epoch 1, Iteration 295, Loss: 0.11616597324609756\n",
      "Epoch 1, Iteration 296, Loss: 0.1457231342792511\n",
      "Epoch 1, Iteration 297, Loss: 0.18829932808876038\n",
      "Epoch 1, Iteration 298, Loss: 0.1670548915863037\n",
      "Epoch 1, Iteration 299, Loss: 0.19986863434314728\n",
      "Epoch 1, Iteration 300, Loss: 0.19875259697437286\n",
      "Epoch 1, Iteration 300, Test Loss: 0.28593364357948303\n",
      "Epoch 1, Iteration 301, Loss: 0.17495767772197723\n",
      "Epoch 1, Iteration 302, Loss: 0.16173312067985535\n",
      "Epoch 1, Iteration 303, Loss: 0.18425686657428741\n",
      "Epoch 1, Iteration 304, Loss: 0.26781919598579407\n",
      "Epoch 1, Iteration 305, Loss: 0.1793391853570938\n",
      "Epoch 1, Iteration 306, Loss: 0.1763886958360672\n",
      "Epoch 1, Iteration 307, Loss: 0.17639397084712982\n",
      "Epoch 1, Iteration 308, Loss: 0.15700261294841766\n",
      "Epoch 1, Iteration 309, Loss: 0.1582278460264206\n",
      "Epoch 1, Iteration 310, Loss: 0.2147502899169922\n",
      "Epoch 1, Iteration 311, Loss: 0.166518434882164\n",
      "Epoch 1, Iteration 312, Loss: 0.23842769861221313\n",
      "Epoch 1, Iteration 313, Loss: 0.1493702381849289\n",
      "Epoch 1, Iteration 314, Loss: 0.151626855134964\n",
      "Epoch 1, Iteration 315, Loss: 0.16485033929347992\n",
      "Epoch 1, Iteration 316, Loss: 0.16643424332141876\n",
      "Epoch 1, Iteration 317, Loss: 0.21387720108032227\n",
      "Epoch 1, Iteration 318, Loss: 0.21471796929836273\n",
      "Epoch 1, Iteration 319, Loss: 0.1613437533378601\n",
      "Epoch 1, Iteration 320, Loss: 0.19287815690040588\n",
      "Epoch 1, Iteration 321, Loss: 0.133726105093956\n",
      "Epoch 1, Iteration 322, Loss: 0.20889051258563995\n",
      "Epoch 1, Iteration 323, Loss: 0.1623252034187317\n",
      "Epoch 1, Iteration 324, Loss: 0.223030224442482\n",
      "Epoch 1, Iteration 325, Loss: 0.27193087339401245\n",
      "Epoch 1, Iteration 326, Loss: 0.24527159333229065\n",
      "Epoch 1, Iteration 327, Loss: 0.19829481840133667\n",
      "Epoch 1, Iteration 328, Loss: 0.18971925973892212\n",
      "Epoch 1, Iteration 329, Loss: 0.18701747059822083\n",
      "Epoch 1, Iteration 330, Loss: 0.21868953108787537\n",
      "Epoch 1, Iteration 331, Loss: 0.17919564247131348\n",
      "Epoch 1, Iteration 332, Loss: 0.18171462416648865\n",
      "Epoch 1, Iteration 333, Loss: 0.19291746616363525\n",
      "Epoch 1, Iteration 334, Loss: 0.27813100814819336\n",
      "Epoch 1, Iteration 335, Loss: 0.2076621800661087\n",
      "Epoch 1, Iteration 336, Loss: 0.2234029918909073\n",
      "Epoch 1, Iteration 337, Loss: 0.16969047486782074\n",
      "Epoch 1, Iteration 338, Loss: 0.1867760717868805\n",
      "Epoch 1, Iteration 339, Loss: 0.24243201315402985\n",
      "Epoch 1, Iteration 340, Loss: 0.20700055360794067\n",
      "Epoch 1, Iteration 341, Loss: 0.22797061502933502\n",
      "Epoch 1, Iteration 342, Loss: 0.21364064514636993\n",
      "Epoch 1, Iteration 343, Loss: 0.21612456440925598\n",
      "Epoch 1, Iteration 344, Loss: 0.21167297661304474\n",
      "Epoch 1, Iteration 345, Loss: 0.19112087786197662\n",
      "Epoch 1, Iteration 346, Loss: 0.1829463690519333\n",
      "Epoch 1, Iteration 347, Loss: 0.2233252376317978\n",
      "Epoch 1, Iteration 348, Loss: 0.24182839691638947\n",
      "Epoch 1, Iteration 349, Loss: 0.218904510140419\n",
      "Epoch 1, Iteration 350, Loss: 0.24504731595516205\n",
      "Epoch 1, Iteration 350, Test Loss: 0.27186429500579834\n",
      "Epoch 1, Iteration 351, Loss: 0.199324369430542\n",
      "Epoch 1, Iteration 352, Loss: 0.1958315223455429\n",
      "Epoch 1, Iteration 353, Loss: 0.21323826909065247\n",
      "Epoch 1, Iteration 354, Loss: 0.15634946525096893\n",
      "Epoch 1, Iteration 355, Loss: 0.1811983287334442\n",
      "Epoch 1, Iteration 356, Loss: 0.17973335087299347\n",
      "Epoch 1, Iteration 357, Loss: 0.2572912275791168\n",
      "Epoch 1, Iteration 358, Loss: 0.14638273417949677\n",
      "Epoch 1, Iteration 359, Loss: 0.20143871009349823\n",
      "Epoch 1, Iteration 360, Loss: 0.12685424089431763\n",
      "Epoch 1, Iteration 361, Loss: 0.20168732106685638\n",
      "Epoch 1, Iteration 362, Loss: 0.17581602931022644\n",
      "Epoch 1, Iteration 363, Loss: 0.17034024000167847\n",
      "Epoch 1, Iteration 364, Loss: 0.17150527238845825\n",
      "Epoch 1, Iteration 365, Loss: 0.15001042187213898\n",
      "Epoch 1, Iteration 366, Loss: 0.2283564805984497\n",
      "Epoch 1, Iteration 367, Loss: 0.19921933114528656\n",
      "Epoch 1, Iteration 368, Loss: 0.17536549270153046\n",
      "Epoch 1, Iteration 369, Loss: 0.18938247859477997\n",
      "Epoch 1, Iteration 370, Loss: 0.20364639163017273\n",
      "Epoch 1, Iteration 371, Loss: 0.19704119861125946\n",
      "Epoch 1, Iteration 372, Loss: 0.17235882580280304\n",
      "Epoch 1, Iteration 373, Loss: 0.22609564661979675\n",
      "Epoch 1, Iteration 374, Loss: 0.13481636345386505\n",
      "Epoch 1, Iteration 375, Loss: 0.22172951698303223\n",
      "Epoch 1, Iteration 376, Loss: 0.2123098075389862\n",
      "Epoch 1, Iteration 377, Loss: 0.1840142011642456\n",
      "Epoch 1, Iteration 378, Loss: 0.17073072493076324\n",
      "Epoch 1, Iteration 379, Loss: 0.21977195143699646\n",
      "Epoch 1, Iteration 380, Loss: 0.21607664227485657\n",
      "Epoch 1, Iteration 381, Loss: 0.189449742436409\n",
      "Epoch 1, Iteration 382, Loss: 0.17288929224014282\n",
      "Epoch 1, Iteration 383, Loss: 0.1361394226551056\n",
      "Epoch 1, Iteration 384, Loss: 0.20345532894134521\n",
      "Epoch 1, Iteration 385, Loss: 0.1844089925289154\n",
      "Epoch 1, Iteration 386, Loss: 0.16065289080142975\n",
      "Epoch 1, Iteration 387, Loss: 0.12861835956573486\n",
      "Epoch 1, Iteration 388, Loss: 0.11814989149570465\n",
      "Epoch 1, Iteration 389, Loss: 0.16955024003982544\n",
      "Epoch 1, Iteration 390, Loss: 0.1623355746269226\n",
      "Epoch 1, Iteration 391, Loss: 0.11414174735546112\n",
      "Epoch 1, Iteration 392, Loss: 0.2184426188468933\n",
      "Epoch 1, Iteration 393, Loss: 0.22310392558574677\n",
      "Epoch 1, Iteration 394, Loss: 0.14169955253601074\n",
      "Epoch 1, Iteration 395, Loss: 0.16189444065093994\n",
      "Epoch 1, Iteration 396, Loss: 0.16729475557804108\n",
      "Epoch 1, Iteration 397, Loss: 0.17125605046749115\n",
      "Epoch 1, Iteration 398, Loss: 0.3032239079475403\n",
      "Epoch 1, Iteration 399, Loss: 0.17934703826904297\n",
      "Epoch 1, Iteration 400, Loss: 0.13921970129013062\n",
      "Epoch 1, Iteration 400, Test Loss: 0.27658236026763916\n",
      "Epoch 1, Iteration 401, Loss: 0.22584077715873718\n",
      "Epoch 1, Iteration 402, Loss: 0.16430802643299103\n",
      "Epoch 1, Iteration 403, Loss: 0.15942083299160004\n",
      "Epoch 1, Iteration 404, Loss: 0.19982187449932098\n",
      "Epoch 1, Iteration 405, Loss: 0.1434154361486435\n",
      "Epoch 1, Iteration 406, Loss: 0.16365891695022583\n",
      "Epoch 1, Iteration 407, Loss: 0.18154945969581604\n",
      "Epoch 1, Iteration 408, Loss: 0.2070477157831192\n",
      "Epoch 1, Iteration 409, Loss: 0.1446438878774643\n",
      "Epoch 1, Iteration 410, Loss: 0.24055729806423187\n",
      "Epoch 1, Iteration 411, Loss: 0.21713976562023163\n",
      "Epoch 1, Iteration 412, Loss: 0.17111946642398834\n",
      "Epoch 1, Iteration 413, Loss: 0.22054411470890045\n",
      "Epoch 1, Iteration 414, Loss: 0.13430456817150116\n",
      "Epoch 1, Iteration 415, Loss: 0.1814444363117218\n",
      "Epoch 1, Iteration 416, Loss: 0.1552024781703949\n",
      "Epoch 1, Iteration 417, Loss: 0.18219701945781708\n",
      "Epoch 1, Iteration 418, Loss: 0.15305684506893158\n",
      "Epoch 1, Iteration 419, Loss: 0.19323186576366425\n",
      "Epoch 1, Iteration 420, Loss: 0.18584346771240234\n",
      "Epoch 1, Iteration 421, Loss: 0.13536015152931213\n",
      "Epoch 1, Iteration 422, Loss: 0.15498597919940948\n",
      "Epoch 1, Iteration 423, Loss: 0.16705551743507385\n",
      "Epoch 1, Iteration 424, Loss: 0.1730399876832962\n",
      "Epoch 1, Iteration 425, Loss: 0.18699069321155548\n",
      "Epoch 1, Iteration 426, Loss: 0.17754188179969788\n",
      "Epoch 1, Iteration 427, Loss: 0.17172259092330933\n",
      "Epoch 1, Iteration 428, Loss: 0.20671336352825165\n",
      "Epoch 1, Iteration 429, Loss: 0.1837487667798996\n",
      "Epoch 1, Iteration 430, Loss: 0.18581309914588928\n",
      "Epoch 1, Iteration 431, Loss: 0.17421859502792358\n",
      "Epoch 2/15, Loss: 0.21589148001469396\n",
      "Epoch 2/15, Validation Accuracy: 0.9335968379446641\n",
      "Epoch 2, Iteration 0, Loss: 0.36899900436401367\n",
      "Epoch 2, Iteration 1, Loss: 0.24980267882347107\n",
      "Epoch 2, Iteration 2, Loss: 0.3412424623966217\n",
      "Epoch 2, Iteration 3, Loss: 0.19405406713485718\n",
      "Epoch 2, Iteration 4, Loss: 0.32133322954177856\n",
      "Epoch 2, Iteration 5, Loss: 0.3685776889324188\n",
      "Epoch 2, Iteration 6, Loss: 0.25281235575675964\n",
      "Epoch 2, Iteration 7, Loss: 0.3056566119194031\n",
      "Epoch 2, Iteration 8, Loss: 0.25651830434799194\n",
      "Epoch 2, Iteration 9, Loss: 0.24901710450649261\n",
      "Epoch 2, Iteration 10, Loss: 0.2964537739753723\n",
      "Epoch 2, Iteration 11, Loss: 0.234491229057312\n",
      "Epoch 2, Iteration 12, Loss: 0.24644821882247925\n",
      "Epoch 2, Iteration 13, Loss: 0.30348148941993713\n",
      "Epoch 2, Iteration 14, Loss: 0.3293132781982422\n",
      "Epoch 2, Iteration 15, Loss: 0.1628839373588562\n",
      "Epoch 2, Iteration 16, Loss: 0.32959869503974915\n",
      "Epoch 2, Iteration 17, Loss: 0.18484845757484436\n",
      "Epoch 2, Iteration 18, Loss: 0.24018484354019165\n",
      "Epoch 2, Iteration 19, Loss: 0.18291401863098145\n",
      "Epoch 2, Iteration 20, Loss: 0.3010063171386719\n",
      "Epoch 2, Iteration 21, Loss: 0.2862190902233124\n",
      "Epoch 2, Iteration 22, Loss: 0.20825384557247162\n",
      "Epoch 2, Iteration 23, Loss: 0.25441834330558777\n",
      "Epoch 2, Iteration 24, Loss: 0.23090265691280365\n",
      "Epoch 2, Iteration 25, Loss: 0.31094756722450256\n",
      "Epoch 2, Iteration 26, Loss: 0.22349655628204346\n",
      "Epoch 2, Iteration 27, Loss: 0.2100723534822464\n",
      "Epoch 2, Iteration 28, Loss: 0.29184967279434204\n",
      "Epoch 2, Iteration 29, Loss: 0.26504525542259216\n",
      "Epoch 2, Iteration 30, Loss: 0.26504144072532654\n",
      "Epoch 2, Iteration 31, Loss: 0.2738245725631714\n",
      "Epoch 2, Iteration 32, Loss: 0.2643619179725647\n",
      "Epoch 2, Iteration 33, Loss: 0.21610508859157562\n",
      "Epoch 2, Iteration 34, Loss: 0.22703932225704193\n",
      "Epoch 2, Iteration 35, Loss: 0.18465453386306763\n",
      "Epoch 2, Iteration 36, Loss: 0.2523946166038513\n",
      "Epoch 2, Iteration 37, Loss: 0.26218676567077637\n",
      "Epoch 2, Iteration 38, Loss: 0.23992615938186646\n",
      "Epoch 2, Iteration 39, Loss: 0.24614448845386505\n",
      "Epoch 2, Iteration 40, Loss: 0.21731312572956085\n",
      "Epoch 2, Iteration 41, Loss: 0.22280408442020416\n",
      "Epoch 2, Iteration 42, Loss: 0.23046919703483582\n",
      "Epoch 2, Iteration 43, Loss: 0.2793368697166443\n",
      "Epoch 2, Iteration 44, Loss: 0.2152494490146637\n",
      "Epoch 2, Iteration 45, Loss: 0.2487655133008957\n",
      "Epoch 2, Iteration 46, Loss: 0.23105186223983765\n",
      "Epoch 2, Iteration 47, Loss: 0.2453271746635437\n",
      "Epoch 2, Iteration 48, Loss: 0.2615765929222107\n",
      "Epoch 2, Iteration 49, Loss: 0.18000152707099915\n",
      "Epoch 2, Iteration 50, Loss: 0.1827821284532547\n",
      "Epoch 2, Iteration 50, Test Loss: 0.23604384064674377\n",
      "Epoch 2, Iteration 51, Loss: 0.19241739809513092\n",
      "Epoch 2, Iteration 52, Loss: 0.22857840359210968\n",
      "Epoch 2, Iteration 53, Loss: 0.16734465956687927\n",
      "Epoch 2, Iteration 54, Loss: 0.1870623379945755\n",
      "Epoch 2, Iteration 55, Loss: 0.17154446244239807\n",
      "Epoch 2, Iteration 56, Loss: 0.2182324081659317\n",
      "Epoch 2, Iteration 57, Loss: 0.21815454959869385\n",
      "Epoch 2, Iteration 58, Loss: 0.1732846051454544\n",
      "Epoch 2, Iteration 59, Loss: 0.1552986055612564\n",
      "Epoch 2, Iteration 60, Loss: 0.17184078693389893\n",
      "Epoch 2, Iteration 61, Loss: 0.1659201681613922\n",
      "Epoch 2, Iteration 62, Loss: 0.31470155715942383\n",
      "Epoch 2, Iteration 63, Loss: 0.15234921872615814\n",
      "Epoch 2, Iteration 64, Loss: 0.16709363460540771\n",
      "Epoch 2, Iteration 65, Loss: 0.1928853839635849\n",
      "Epoch 2, Iteration 66, Loss: 0.18997307121753693\n",
      "Epoch 2, Iteration 67, Loss: 0.1412324458360672\n",
      "Epoch 2, Iteration 68, Loss: 0.18974989652633667\n",
      "Epoch 2, Iteration 69, Loss: 0.1960614174604416\n",
      "Epoch 2, Iteration 70, Loss: 0.13640640676021576\n",
      "Epoch 2, Iteration 71, Loss: 0.20128118991851807\n",
      "Epoch 2, Iteration 72, Loss: 0.18563216924667358\n",
      "Epoch 2, Iteration 73, Loss: 0.19539019465446472\n",
      "Epoch 2, Iteration 74, Loss: 0.21789051592350006\n",
      "Epoch 2, Iteration 75, Loss: 0.18977923691272736\n",
      "Epoch 2, Iteration 76, Loss: 0.16970646381378174\n",
      "Epoch 2, Iteration 77, Loss: 0.16471217572689056\n",
      "Epoch 2, Iteration 78, Loss: 0.1789424866437912\n",
      "Epoch 2, Iteration 79, Loss: 0.12184315174818039\n",
      "Epoch 2, Iteration 80, Loss: 0.21810945868492126\n",
      "Epoch 2, Iteration 81, Loss: 0.20797090232372284\n",
      "Epoch 2, Iteration 82, Loss: 0.23414424061775208\n",
      "Epoch 2, Iteration 83, Loss: 0.18002186715602875\n",
      "Epoch 2, Iteration 84, Loss: 0.17208771407604218\n",
      "Epoch 2, Iteration 85, Loss: 0.21056421101093292\n",
      "Epoch 2, Iteration 86, Loss: 0.20732295513153076\n",
      "Epoch 2, Iteration 87, Loss: 0.17988860607147217\n",
      "Epoch 2, Iteration 88, Loss: 0.1606377810239792\n",
      "Epoch 2, Iteration 89, Loss: 0.18139344453811646\n",
      "Epoch 2, Iteration 90, Loss: 0.2073165774345398\n",
      "Epoch 2, Iteration 91, Loss: 0.15723590552806854\n",
      "Epoch 2, Iteration 92, Loss: 0.17023663222789764\n",
      "Epoch 2, Iteration 93, Loss: 0.16571854054927826\n",
      "Epoch 2, Iteration 94, Loss: 0.2344731092453003\n",
      "Epoch 2, Iteration 95, Loss: 0.1630694568157196\n",
      "Epoch 2, Iteration 96, Loss: 0.11681767553091049\n",
      "Epoch 2, Iteration 97, Loss: 0.1772470474243164\n",
      "Epoch 2, Iteration 98, Loss: 0.19990278780460358\n",
      "Epoch 2, Iteration 99, Loss: 0.15134772658348083\n",
      "Epoch 2, Iteration 100, Loss: 0.15743759274482727\n",
      "Epoch 2, Iteration 100, Test Loss: 0.2615307867527008\n",
      "Epoch 2, Iteration 101, Loss: 0.1656278520822525\n",
      "Epoch 2, Iteration 102, Loss: 0.16377957165241241\n",
      "Epoch 2, Iteration 103, Loss: 0.1669435054063797\n",
      "Epoch 2, Iteration 104, Loss: 0.19612783193588257\n",
      "Epoch 2, Iteration 105, Loss: 0.17671838402748108\n",
      "Epoch 2, Iteration 106, Loss: 0.138214573264122\n",
      "Epoch 2, Iteration 107, Loss: 0.11690320819616318\n",
      "Epoch 2, Iteration 108, Loss: 0.25841331481933594\n",
      "Epoch 2, Iteration 109, Loss: 0.2146506905555725\n",
      "Epoch 2, Iteration 110, Loss: 0.1510164886713028\n",
      "Epoch 2, Iteration 111, Loss: 0.12904080748558044\n",
      "Epoch 2, Iteration 112, Loss: 0.1735467165708542\n",
      "Epoch 2, Iteration 113, Loss: 0.17591525614261627\n",
      "Epoch 2, Iteration 114, Loss: 0.15788663923740387\n",
      "Epoch 2, Iteration 115, Loss: 0.1794155240058899\n",
      "Epoch 2, Iteration 116, Loss: 0.14780816435813904\n",
      "Epoch 2, Iteration 117, Loss: 0.2377554178237915\n",
      "Epoch 2, Iteration 118, Loss: 0.18776316940784454\n",
      "Epoch 2, Iteration 119, Loss: 0.22564415633678436\n",
      "Epoch 2, Iteration 120, Loss: 0.14794526994228363\n",
      "Epoch 2, Iteration 121, Loss: 0.17093425989151\n",
      "Epoch 2, Iteration 122, Loss: 0.2058381885290146\n",
      "Epoch 2, Iteration 123, Loss: 0.2026677280664444\n",
      "Epoch 2, Iteration 124, Loss: 0.18468505144119263\n",
      "Epoch 2, Iteration 125, Loss: 0.19136923551559448\n",
      "Epoch 2, Iteration 126, Loss: 0.1771850734949112\n",
      "Epoch 2, Iteration 127, Loss: 0.17271433770656586\n",
      "Epoch 2, Iteration 128, Loss: 0.1763692945241928\n",
      "Epoch 2, Iteration 129, Loss: 0.231587752699852\n",
      "Epoch 2, Iteration 130, Loss: 0.13313497602939606\n",
      "Epoch 2, Iteration 131, Loss: 0.15961645543575287\n",
      "Epoch 2, Iteration 132, Loss: 0.2112758755683899\n",
      "Epoch 2, Iteration 133, Loss: 0.25357550382614136\n",
      "Epoch 2, Iteration 134, Loss: 0.17777730524539948\n",
      "Epoch 2, Iteration 135, Loss: 0.18562692403793335\n",
      "Epoch 2, Iteration 136, Loss: 0.15634408593177795\n",
      "Epoch 2, Iteration 137, Loss: 0.1431160420179367\n",
      "Epoch 2, Iteration 138, Loss: 0.15106356143951416\n",
      "Epoch 2, Iteration 139, Loss: 0.17818182706832886\n",
      "Epoch 2, Iteration 140, Loss: 0.20129407942295074\n",
      "Epoch 2, Iteration 141, Loss: 0.15797463059425354\n",
      "Epoch 2, Iteration 142, Loss: 0.2244025319814682\n",
      "Epoch 2, Iteration 143, Loss: 0.1752643585205078\n",
      "Epoch 2, Iteration 144, Loss: 0.13453848659992218\n",
      "Epoch 2, Iteration 145, Loss: 0.19709989428520203\n",
      "Epoch 2, Iteration 146, Loss: 0.1357981264591217\n",
      "Epoch 2, Iteration 147, Loss: 0.1542976200580597\n",
      "Epoch 2, Iteration 148, Loss: 0.139854297041893\n",
      "Epoch 2, Iteration 149, Loss: 0.1701068878173828\n",
      "Epoch 2, Iteration 150, Loss: 0.20978568494319916\n",
      "Epoch 2, Iteration 150, Test Loss: 0.2549698054790497\n",
      "Epoch 2, Iteration 151, Loss: 0.17964839935302734\n",
      "Epoch 2, Iteration 152, Loss: 0.18540647625923157\n",
      "Epoch 2, Iteration 153, Loss: 0.18285112082958221\n",
      "Epoch 2, Iteration 154, Loss: 0.16222798824310303\n",
      "Epoch 2, Iteration 155, Loss: 0.1678456962108612\n",
      "Epoch 2, Iteration 156, Loss: 0.19122150540351868\n",
      "Epoch 2, Iteration 157, Loss: 0.2222422957420349\n",
      "Epoch 2, Iteration 158, Loss: 0.263700008392334\n",
      "Epoch 2, Iteration 159, Loss: 0.16377149522304535\n",
      "Epoch 2, Iteration 160, Loss: 0.19504167139530182\n",
      "Epoch 2, Iteration 161, Loss: 0.1329086273908615\n",
      "Epoch 2, Iteration 162, Loss: 0.12393619865179062\n",
      "Epoch 2, Iteration 163, Loss: 0.17375074326992035\n",
      "Epoch 2, Iteration 164, Loss: 0.13376009464263916\n",
      "Epoch 2, Iteration 165, Loss: 0.15363840758800507\n",
      "Epoch 2, Iteration 166, Loss: 0.16789279878139496\n",
      "Epoch 2, Iteration 167, Loss: 0.15383359789848328\n",
      "Epoch 2, Iteration 168, Loss: 0.1452615112066269\n",
      "Epoch 2, Iteration 169, Loss: 0.188299760222435\n",
      "Epoch 2, Iteration 170, Loss: 0.13115207850933075\n",
      "Epoch 2, Iteration 171, Loss: 0.23538358509540558\n",
      "Epoch 2, Iteration 172, Loss: 0.22677066922187805\n",
      "Epoch 2, Iteration 173, Loss: 0.20465964078903198\n",
      "Epoch 2, Iteration 174, Loss: 0.19159449636936188\n",
      "Epoch 2, Iteration 175, Loss: 0.15223629772663116\n",
      "Epoch 2, Iteration 176, Loss: 0.1377646028995514\n",
      "Epoch 2, Iteration 177, Loss: 0.2046615481376648\n",
      "Epoch 2, Iteration 178, Loss: 0.1681346446275711\n",
      "Epoch 2, Iteration 179, Loss: 0.16424737870693207\n",
      "Epoch 2, Iteration 180, Loss: 0.2040768414735794\n",
      "Epoch 2, Iteration 181, Loss: 0.15877416729927063\n",
      "Epoch 2, Iteration 182, Loss: 0.14833760261535645\n",
      "Epoch 2, Iteration 183, Loss: 0.17468783259391785\n",
      "Epoch 2, Iteration 184, Loss: 0.24191541969776154\n",
      "Epoch 2, Iteration 185, Loss: 0.14341799914836884\n",
      "Epoch 2, Iteration 186, Loss: 0.16232885420322418\n",
      "Epoch 2, Iteration 187, Loss: 0.15549181401729584\n",
      "Epoch 2, Iteration 188, Loss: 0.12516023218631744\n",
      "Epoch 2, Iteration 189, Loss: 0.11051563173532486\n",
      "Epoch 2, Iteration 190, Loss: 0.13113033771514893\n",
      "Epoch 2, Iteration 191, Loss: 0.1191132590174675\n",
      "Epoch 2, Iteration 192, Loss: 0.16047486662864685\n",
      "Epoch 2, Iteration 193, Loss: 0.18462835252285004\n",
      "Epoch 2, Iteration 194, Loss: 0.14363224804401398\n",
      "Epoch 2, Iteration 195, Loss: 0.17395350337028503\n",
      "Epoch 2, Iteration 196, Loss: 0.2626325190067291\n",
      "Epoch 2, Iteration 197, Loss: 0.15566211938858032\n",
      "Epoch 2, Iteration 198, Loss: 0.1158357784152031\n",
      "Epoch 2, Iteration 199, Loss: 0.1505754292011261\n",
      "Epoch 2, Iteration 200, Loss: 0.1300370991230011\n",
      "Epoch 2, Iteration 200, Test Loss: 0.2607046961784363\n",
      "Epoch 2, Iteration 201, Loss: 0.14664986729621887\n",
      "Epoch 2, Iteration 202, Loss: 0.12552137672901154\n",
      "Epoch 2, Iteration 203, Loss: 0.14133718609809875\n",
      "Epoch 2, Iteration 204, Loss: 0.15685264766216278\n",
      "Epoch 2, Iteration 205, Loss: 0.19878675043582916\n",
      "Epoch 2, Iteration 206, Loss: 0.12498544901609421\n",
      "Epoch 2, Iteration 207, Loss: 0.12257319688796997\n",
      "Epoch 2, Iteration 208, Loss: 0.12863224744796753\n",
      "Epoch 2, Iteration 209, Loss: 0.16775833070278168\n",
      "Epoch 2, Iteration 210, Loss: 0.1304396539926529\n",
      "Epoch 2, Iteration 211, Loss: 0.1677367389202118\n",
      "Epoch 2, Iteration 212, Loss: 0.1993219405412674\n",
      "Epoch 2, Iteration 213, Loss: 0.15857604146003723\n",
      "Epoch 2, Iteration 214, Loss: 0.14639760553836823\n",
      "Epoch 2, Iteration 215, Loss: 0.14834904670715332\n",
      "Epoch 2, Iteration 216, Loss: 0.18394158780574799\n",
      "Epoch 2, Iteration 217, Loss: 0.23593784868717194\n",
      "Epoch 2, Iteration 218, Loss: 0.14354601502418518\n",
      "Epoch 2, Iteration 219, Loss: 0.16473881900310516\n",
      "Epoch 2, Iteration 220, Loss: 0.14756593108177185\n",
      "Epoch 2, Iteration 221, Loss: 0.18175847828388214\n",
      "Epoch 2, Iteration 222, Loss: 0.1565127670764923\n",
      "Epoch 2, Iteration 223, Loss: 0.1679496020078659\n",
      "Epoch 2, Iteration 224, Loss: 0.13699477910995483\n",
      "Epoch 2, Iteration 225, Loss: 0.16127784550189972\n",
      "Epoch 2, Iteration 226, Loss: 0.16423211991786957\n",
      "Epoch 2, Iteration 227, Loss: 0.16931872069835663\n",
      "Epoch 2, Iteration 228, Loss: 0.2977985441684723\n",
      "Epoch 2, Iteration 229, Loss: 0.2716286778450012\n",
      "Epoch 2, Iteration 230, Loss: 0.19036303460597992\n",
      "Epoch 2, Iteration 231, Loss: 0.141194686293602\n",
      "Epoch 2, Iteration 232, Loss: 0.15786097943782806\n",
      "Epoch 2, Iteration 233, Loss: 0.2091033160686493\n",
      "Epoch 2, Iteration 234, Loss: 0.14578095078468323\n",
      "Epoch 2, Iteration 235, Loss: 0.11563843488693237\n",
      "Epoch 2, Iteration 236, Loss: 0.20502619445323944\n",
      "Epoch 2, Iteration 237, Loss: 0.16149009764194489\n",
      "Epoch 2, Iteration 238, Loss: 0.16168129444122314\n",
      "Epoch 2, Iteration 239, Loss: 0.1561814248561859\n",
      "Epoch 2, Iteration 240, Loss: 0.15008008480072021\n",
      "Epoch 2, Iteration 241, Loss: 0.15498456358909607\n",
      "Epoch 2, Iteration 242, Loss: 0.17000079154968262\n",
      "Epoch 2, Iteration 243, Loss: 0.16866743564605713\n",
      "Epoch 2, Iteration 244, Loss: 0.18345563113689423\n",
      "Epoch 2, Iteration 245, Loss: 0.17263342440128326\n",
      "Epoch 2, Iteration 246, Loss: 0.188282310962677\n",
      "Epoch 2, Iteration 247, Loss: 0.3006218373775482\n",
      "Epoch 2, Iteration 248, Loss: 0.21413554251194\n",
      "Epoch 2, Iteration 249, Loss: 0.2539055347442627\n",
      "Epoch 2, Iteration 250, Loss: 0.2626337707042694\n",
      "Epoch 2, Iteration 250, Test Loss: 0.26252034306526184\n",
      "Epoch 2, Iteration 251, Loss: 0.2416069507598877\n",
      "Epoch 2, Iteration 252, Loss: 0.14712484180927277\n",
      "Epoch 2, Iteration 253, Loss: 0.18423046171665192\n",
      "Epoch 2, Iteration 254, Loss: 0.22719240188598633\n",
      "Epoch 2, Iteration 255, Loss: 0.19123071432113647\n",
      "Epoch 2, Iteration 256, Loss: 0.17455099523067474\n",
      "Epoch 2, Iteration 257, Loss: 0.13842251896858215\n",
      "Epoch 2, Iteration 258, Loss: 0.18520180881023407\n",
      "Epoch 2, Iteration 259, Loss: 0.19462892413139343\n",
      "Epoch 2, Iteration 260, Loss: 0.2167634516954422\n",
      "Epoch 2, Iteration 261, Loss: 0.15102295577526093\n",
      "Epoch 2, Iteration 262, Loss: 0.1580943614244461\n",
      "Epoch 2, Iteration 263, Loss: 0.15051928162574768\n",
      "Epoch 2, Iteration 264, Loss: 0.13299167156219482\n",
      "Epoch 2, Iteration 265, Loss: 0.24696394801139832\n",
      "Epoch 2, Iteration 266, Loss: 0.12751123309135437\n",
      "Epoch 2, Iteration 267, Loss: 0.18000741302967072\n",
      "Epoch 2, Iteration 268, Loss: 0.19599899649620056\n",
      "Epoch 2, Iteration 269, Loss: 0.18375037610530853\n",
      "Epoch 2, Iteration 270, Loss: 0.21091598272323608\n",
      "Epoch 2, Iteration 271, Loss: 0.11638843268156052\n",
      "Epoch 2, Iteration 272, Loss: 0.15869243443012238\n",
      "Epoch 2, Iteration 273, Loss: 0.1843368113040924\n",
      "Epoch 2, Iteration 274, Loss: 0.1304740458726883\n",
      "Epoch 2, Iteration 275, Loss: 0.13124337792396545\n",
      "Epoch 2, Iteration 276, Loss: 0.13077914714813232\n",
      "Epoch 2, Iteration 277, Loss: 0.1336039900779724\n",
      "Epoch 2, Iteration 278, Loss: 0.1760687381029129\n",
      "Epoch 2, Iteration 279, Loss: 0.13770578801631927\n",
      "Epoch 2, Iteration 280, Loss: 0.1738206297159195\n",
      "Epoch 2, Iteration 281, Loss: 0.13583938777446747\n",
      "Epoch 2, Iteration 282, Loss: 0.14339713752269745\n",
      "Epoch 2, Iteration 283, Loss: 0.12111404538154602\n",
      "Epoch 2, Iteration 284, Loss: 0.1608557254076004\n",
      "Epoch 2, Iteration 285, Loss: 0.1402738094329834\n",
      "Epoch 2, Iteration 286, Loss: 0.20473699271678925\n",
      "Epoch 2, Iteration 287, Loss: 0.12482966482639313\n",
      "Epoch 2, Iteration 288, Loss: 0.1647903472185135\n",
      "Epoch 2, Iteration 289, Loss: 0.23200294375419617\n",
      "Epoch 2, Iteration 290, Loss: 0.11240411549806595\n",
      "Epoch 2, Iteration 291, Loss: 0.11679849773645401\n",
      "Epoch 2, Iteration 292, Loss: 0.16284866631031036\n",
      "Epoch 2, Iteration 293, Loss: 0.19961953163146973\n",
      "Epoch 2, Iteration 294, Loss: 0.13416896760463715\n",
      "Epoch 2, Iteration 295, Loss: 0.10447730123996735\n",
      "Epoch 2, Iteration 296, Loss: 0.13510626554489136\n",
      "Epoch 2, Iteration 297, Loss: 0.13949096202850342\n",
      "Epoch 2, Iteration 298, Loss: 0.14651599526405334\n",
      "Epoch 2, Iteration 299, Loss: 0.17832516133785248\n",
      "Epoch 2, Iteration 300, Loss: 0.1393139809370041\n",
      "Epoch 2, Iteration 300, Test Loss: 0.279533714056015\n",
      "Epoch 2, Iteration 301, Loss: 0.14829480648040771\n",
      "Epoch 2, Iteration 302, Loss: 0.12821170687675476\n",
      "Epoch 2, Iteration 303, Loss: 0.1779693365097046\n",
      "Epoch 2, Iteration 304, Loss: 0.2030898630619049\n",
      "Epoch 2, Iteration 305, Loss: 0.1348850280046463\n",
      "Epoch 2, Iteration 306, Loss: 0.15530861914157867\n",
      "Epoch 2, Iteration 307, Loss: 0.12729309499263763\n",
      "Epoch 2, Iteration 308, Loss: 0.14692415297031403\n",
      "Epoch 2, Iteration 309, Loss: 0.13419680297374725\n",
      "Epoch 2, Iteration 310, Loss: 0.1872250735759735\n",
      "Epoch 2, Iteration 311, Loss: 0.12994393706321716\n",
      "Epoch 2, Iteration 312, Loss: 0.1931922882795334\n",
      "Epoch 2, Iteration 313, Loss: 0.13523519039154053\n",
      "Epoch 2, Iteration 314, Loss: 0.12462815642356873\n",
      "Epoch 2, Iteration 315, Loss: 0.13717558979988098\n",
      "Epoch 2, Iteration 316, Loss: 0.14419178664684296\n",
      "Epoch 2, Iteration 317, Loss: 0.15277381241321564\n",
      "Epoch 2, Iteration 318, Loss: 0.19637823104858398\n",
      "Epoch 2, Iteration 319, Loss: 0.12782022356987\n",
      "Epoch 2, Iteration 320, Loss: 0.18235233426094055\n",
      "Epoch 2, Iteration 321, Loss: 0.12760140001773834\n",
      "Epoch 2, Iteration 322, Loss: 0.22415393590927124\n",
      "Epoch 2, Iteration 323, Loss: 0.1325412094593048\n",
      "Epoch 2, Iteration 324, Loss: 0.18638603389263153\n",
      "Epoch 2, Iteration 325, Loss: 0.25619930028915405\n",
      "Epoch 2, Iteration 326, Loss: 0.20845448970794678\n",
      "Epoch 2, Iteration 327, Loss: 0.19322969019412994\n",
      "Epoch 2, Iteration 328, Loss: 0.17424017190933228\n",
      "Epoch 2, Iteration 329, Loss: 0.15445856750011444\n",
      "Epoch 2, Iteration 330, Loss: 0.20994769036769867\n",
      "Epoch 2, Iteration 331, Loss: 0.1738748848438263\n",
      "Epoch 2, Iteration 332, Loss: 0.19167141616344452\n",
      "Epoch 2, Iteration 333, Loss: 0.13970649242401123\n",
      "Epoch 2, Iteration 334, Loss: 0.2600961923599243\n",
      "Epoch 2, Iteration 335, Loss: 0.1701110601425171\n",
      "Epoch 2, Iteration 336, Loss: 0.17806534469127655\n",
      "Epoch 2, Iteration 337, Loss: 0.14449812471866608\n",
      "Epoch 2, Iteration 338, Loss: 0.15115344524383545\n",
      "Epoch 2, Iteration 339, Loss: 0.19910575449466705\n",
      "Epoch 2, Iteration 340, Loss: 0.16667458415031433\n",
      "Epoch 2, Iteration 341, Loss: 0.18262603878974915\n",
      "Epoch 2, Iteration 342, Loss: 0.18364021182060242\n",
      "Epoch 2, Iteration 343, Loss: 0.20597337186336517\n",
      "Epoch 2, Iteration 344, Loss: 0.19043083488941193\n",
      "Epoch 2, Iteration 345, Loss: 0.18403388559818268\n",
      "Epoch 2, Iteration 346, Loss: 0.16717347502708435\n",
      "Epoch 2, Iteration 347, Loss: 0.19547328352928162\n",
      "Epoch 2, Iteration 348, Loss: 0.21166233718395233\n",
      "Epoch 2, Iteration 349, Loss: 0.20815016329288483\n",
      "Epoch 2, Iteration 350, Loss: 0.21377050876617432\n",
      "Epoch 2, Iteration 350, Test Loss: 0.2544187903404236\n",
      "Epoch 2, Iteration 351, Loss: 0.19817137718200684\n",
      "Epoch 2, Iteration 352, Loss: 0.2096712291240692\n",
      "Epoch 2, Iteration 353, Loss: 0.2120852917432785\n",
      "Epoch 2, Iteration 354, Loss: 0.1430518478155136\n",
      "Epoch 2, Iteration 355, Loss: 0.17136737704277039\n",
      "Epoch 2, Iteration 356, Loss: 0.1560288667678833\n",
      "Epoch 2, Iteration 357, Loss: 0.24936863780021667\n",
      "Epoch 2, Iteration 358, Loss: 0.12841644883155823\n",
      "Epoch 2, Iteration 359, Loss: 0.18231089413166046\n",
      "Epoch 2, Iteration 360, Loss: 0.1207740381360054\n",
      "Epoch 2, Iteration 361, Loss: 0.17082259058952332\n",
      "Epoch 2, Iteration 362, Loss: 0.16271241009235382\n",
      "Epoch 2, Iteration 363, Loss: 0.13443192839622498\n",
      "Epoch 2, Iteration 364, Loss: 0.16193722188472748\n",
      "Epoch 2, Iteration 365, Loss: 0.14041627943515778\n",
      "Epoch 2, Iteration 366, Loss: 0.2165052592754364\n",
      "Epoch 2, Iteration 367, Loss: 0.19038943946361542\n",
      "Epoch 2, Iteration 368, Loss: 0.15861567854881287\n",
      "Epoch 2, Iteration 369, Loss: 0.1309790313243866\n",
      "Epoch 2, Iteration 370, Loss: 0.2032637745141983\n",
      "Epoch 2, Iteration 371, Loss: 0.1703978031873703\n",
      "Epoch 2, Iteration 372, Loss: 0.1583883911371231\n",
      "Epoch 2, Iteration 373, Loss: 0.20783324539661407\n",
      "Epoch 2, Iteration 374, Loss: 0.1240105926990509\n",
      "Epoch 2, Iteration 375, Loss: 0.1767650991678238\n",
      "Epoch 2, Iteration 376, Loss: 0.17828769981861115\n",
      "Epoch 2, Iteration 377, Loss: 0.17581559717655182\n",
      "Epoch 2, Iteration 378, Loss: 0.141095831990242\n",
      "Epoch 2, Iteration 379, Loss: 0.214654341340065\n",
      "Epoch 2, Iteration 380, Loss: 0.1926484853029251\n",
      "Epoch 2, Iteration 381, Loss: 0.14221803843975067\n",
      "Epoch 2, Iteration 382, Loss: 0.15327541530132294\n",
      "Epoch 2, Iteration 383, Loss: 0.13486896455287933\n",
      "Epoch 2, Iteration 384, Loss: 0.1557258814573288\n",
      "Epoch 2, Iteration 385, Loss: 0.17290790379047394\n",
      "Epoch 2, Iteration 386, Loss: 0.15170812606811523\n",
      "Epoch 2, Iteration 387, Loss: 0.10368960350751877\n",
      "Epoch 2, Iteration 388, Loss: 0.1175009235739708\n",
      "Epoch 2, Iteration 389, Loss: 0.15433675050735474\n",
      "Epoch 2, Iteration 390, Loss: 0.16038089990615845\n",
      "Epoch 2, Iteration 391, Loss: 0.12088832259178162\n",
      "Epoch 2, Iteration 392, Loss: 0.22361595928668976\n",
      "Epoch 2, Iteration 393, Loss: 0.20195451378822327\n",
      "Epoch 2, Iteration 394, Loss: 0.13795340061187744\n",
      "Epoch 2, Iteration 395, Loss: 0.1468479335308075\n",
      "Epoch 2, Iteration 396, Loss: 0.16733385622501373\n",
      "Epoch 2, Iteration 397, Loss: 0.16128414869308472\n",
      "Epoch 2, Iteration 398, Loss: 0.25347334146499634\n",
      "Epoch 2, Iteration 399, Loss: 0.1420133113861084\n",
      "Epoch 2, Iteration 400, Loss: 0.13072137534618378\n",
      "Epoch 2, Iteration 400, Test Loss: 0.2629813253879547\n",
      "Epoch 2, Iteration 401, Loss: 0.18996824324131012\n",
      "Epoch 2, Iteration 402, Loss: 0.11156895011663437\n",
      "Epoch 2, Iteration 403, Loss: 0.15716181695461273\n",
      "Epoch 2, Iteration 404, Loss: 0.16866782307624817\n",
      "Epoch 2, Iteration 405, Loss: 0.1234913021326065\n",
      "Epoch 2, Iteration 406, Loss: 0.12158896774053574\n",
      "Epoch 2, Iteration 407, Loss: 0.1522565633058548\n",
      "Epoch 2, Iteration 408, Loss: 0.19145147502422333\n",
      "Epoch 2, Iteration 409, Loss: 0.13850943744182587\n",
      "Epoch 2, Iteration 410, Loss: 0.20654670894145966\n",
      "Epoch 2, Iteration 411, Loss: 0.20790378749370575\n",
      "Epoch 2, Iteration 412, Loss: 0.17053358256816864\n",
      "Epoch 2, Iteration 413, Loss: 0.18177565932273865\n",
      "Epoch 2, Iteration 414, Loss: 0.11471138149499893\n",
      "Epoch 2, Iteration 415, Loss: 0.16465455293655396\n",
      "Epoch 2, Iteration 416, Loss: 0.14570409059524536\n",
      "Epoch 2, Iteration 417, Loss: 0.16776326298713684\n",
      "Epoch 2, Iteration 418, Loss: 0.13619397580623627\n",
      "Epoch 2, Iteration 419, Loss: 0.15531384944915771\n",
      "Epoch 2, Iteration 420, Loss: 0.1682555228471756\n",
      "Epoch 2, Iteration 421, Loss: 0.11432071775197983\n",
      "Epoch 2, Iteration 422, Loss: 0.1614903062582016\n",
      "Epoch 2, Iteration 423, Loss: 0.1503617763519287\n",
      "Epoch 2, Iteration 424, Loss: 0.16549666225910187\n",
      "Epoch 2, Iteration 425, Loss: 0.16422006487846375\n",
      "Epoch 2, Iteration 426, Loss: 0.15279854834079742\n",
      "Epoch 2, Iteration 427, Loss: 0.12764126062393188\n",
      "Epoch 2, Iteration 428, Loss: 0.14888817071914673\n",
      "Epoch 2, Iteration 429, Loss: 0.1618483066558838\n",
      "Epoch 2, Iteration 430, Loss: 0.16031785309314728\n",
      "Epoch 2, Iteration 431, Loss: 0.1352168172597885\n",
      "Epoch 3/15, Loss: 0.18076064520412022\n",
      "Epoch 3/15, Validation Accuracy: 0.9376284584980237\n",
      "Epoch 3, Iteration 0, Loss: 0.2928227186203003\n",
      "Epoch 3, Iteration 1, Loss: 0.22837023437023163\n",
      "Epoch 3, Iteration 2, Loss: 0.31200656294822693\n",
      "Epoch 3, Iteration 3, Loss: 0.19434750080108643\n",
      "Epoch 3, Iteration 4, Loss: 0.2576005756855011\n",
      "Epoch 3, Iteration 5, Loss: 0.28068166971206665\n",
      "Epoch 3, Iteration 6, Loss: 0.25748178362846375\n",
      "Epoch 3, Iteration 7, Loss: 0.31522446870803833\n",
      "Epoch 3, Iteration 8, Loss: 0.2649875283241272\n",
      "Epoch 3, Iteration 9, Loss: 0.2424677461385727\n",
      "Epoch 3, Iteration 10, Loss: 0.22692397236824036\n",
      "Epoch 3, Iteration 11, Loss: 0.19901122152805328\n",
      "Epoch 3, Iteration 12, Loss: 0.25501662492752075\n",
      "Epoch 3, Iteration 13, Loss: 0.23326939344406128\n",
      "Epoch 3, Iteration 14, Loss: 0.28644245862960815\n",
      "Epoch 3, Iteration 15, Loss: 0.12223319709300995\n",
      "Epoch 3, Iteration 16, Loss: 0.3014552593231201\n",
      "Epoch 3, Iteration 17, Loss: 0.20170463621616364\n",
      "Epoch 3, Iteration 18, Loss: 0.23315778374671936\n",
      "Epoch 3, Iteration 19, Loss: 0.18646115064620972\n",
      "Epoch 3, Iteration 20, Loss: 0.2661358118057251\n",
      "Epoch 3, Iteration 21, Loss: 0.26021888852119446\n",
      "Epoch 3, Iteration 22, Loss: 0.2148522436618805\n",
      "Epoch 3, Iteration 23, Loss: 0.2413715422153473\n",
      "Epoch 3, Iteration 24, Loss: 0.24133622646331787\n",
      "Epoch 3, Iteration 25, Loss: 0.2829189598560333\n",
      "Epoch 3, Iteration 26, Loss: 0.2476745992898941\n",
      "Epoch 3, Iteration 27, Loss: 0.188738152384758\n",
      "Epoch 3, Iteration 28, Loss: 0.2790728211402893\n",
      "Epoch 3, Iteration 29, Loss: 0.21411296725273132\n",
      "Epoch 3, Iteration 30, Loss: 0.26326170563697815\n",
      "Epoch 3, Iteration 31, Loss: 0.2704788148403168\n",
      "Epoch 3, Iteration 32, Loss: 0.21712450683116913\n",
      "Epoch 3, Iteration 33, Loss: 0.24179011583328247\n",
      "Epoch 3, Iteration 34, Loss: 0.24032214283943176\n",
      "Epoch 3, Iteration 35, Loss: 0.18443119525909424\n",
      "Epoch 3, Iteration 36, Loss: 0.23457609117031097\n",
      "Epoch 3, Iteration 37, Loss: 0.21901631355285645\n",
      "Epoch 3, Iteration 38, Loss: 0.2246689647436142\n",
      "Epoch 3, Iteration 39, Loss: 0.2575737535953522\n",
      "Epoch 3, Iteration 40, Loss: 0.20735597610473633\n",
      "Epoch 3, Iteration 41, Loss: 0.18524067103862762\n",
      "Epoch 3, Iteration 42, Loss: 0.20158074796199799\n",
      "Epoch 3, Iteration 43, Loss: 0.21935556828975677\n",
      "Epoch 3, Iteration 44, Loss: 0.18552431464195251\n",
      "Epoch 3, Iteration 45, Loss: 0.23719091713428497\n",
      "Epoch 3, Iteration 46, Loss: 0.20281022787094116\n",
      "Epoch 3, Iteration 47, Loss: 0.23855508863925934\n",
      "Epoch 3, Iteration 48, Loss: 0.224831223487854\n",
      "Epoch 3, Iteration 49, Loss: 0.14442352950572968\n",
      "Epoch 3, Iteration 50, Loss: 0.1696908175945282\n",
      "Epoch 3, Iteration 50, Test Loss: 0.23604220151901245\n",
      "Epoch 3, Iteration 51, Loss: 0.1791706681251526\n",
      "Epoch 3, Iteration 52, Loss: 0.22690390050411224\n",
      "Epoch 3, Iteration 53, Loss: 0.17441798746585846\n",
      "Epoch 3, Iteration 54, Loss: 0.20189501345157623\n",
      "Epoch 3, Iteration 55, Loss: 0.14168421924114227\n",
      "Epoch 3, Iteration 56, Loss: 0.1816924512386322\n",
      "Epoch 3, Iteration 57, Loss: 0.18365861475467682\n",
      "Epoch 3, Iteration 58, Loss: 0.1690569519996643\n",
      "Epoch 3, Iteration 59, Loss: 0.15652579069137573\n",
      "Epoch 3, Iteration 60, Loss: 0.16808080673217773\n",
      "Epoch 3, Iteration 61, Loss: 0.1442965865135193\n",
      "Epoch 3, Iteration 62, Loss: 0.26871001720428467\n",
      "Epoch 3, Iteration 63, Loss: 0.13907837867736816\n",
      "Epoch 3, Iteration 64, Loss: 0.14175774157047272\n",
      "Epoch 3, Iteration 65, Loss: 0.1661764532327652\n",
      "Epoch 3, Iteration 66, Loss: 0.1821732074022293\n",
      "Epoch 3, Iteration 67, Loss: 0.12192784994840622\n",
      "Epoch 3, Iteration 68, Loss: 0.1717819720506668\n",
      "Epoch 3, Iteration 69, Loss: 0.18031427264213562\n",
      "Epoch 3, Iteration 70, Loss: 0.15599073469638824\n",
      "Epoch 3, Iteration 71, Loss: 0.15764214098453522\n",
      "Epoch 3, Iteration 72, Loss: 0.15445156395435333\n",
      "Epoch 3, Iteration 73, Loss: 0.15231631696224213\n",
      "Epoch 3, Iteration 74, Loss: 0.1996459811925888\n",
      "Epoch 3, Iteration 75, Loss: 0.18716607987880707\n",
      "Epoch 3, Iteration 76, Loss: 0.1723523885011673\n",
      "Epoch 3, Iteration 77, Loss: 0.14501026272773743\n",
      "Epoch 3, Iteration 78, Loss: 0.18388523161411285\n",
      "Epoch 3, Iteration 79, Loss: 0.10531197488307953\n",
      "Epoch 3, Iteration 80, Loss: 0.18819555640220642\n",
      "Epoch 3, Iteration 81, Loss: 0.15624292194843292\n",
      "Epoch 3, Iteration 82, Loss: 0.19543206691741943\n",
      "Epoch 3, Iteration 83, Loss: 0.18131239712238312\n",
      "Epoch 3, Iteration 84, Loss: 0.15985068678855896\n",
      "Epoch 3, Iteration 85, Loss: 0.19259560108184814\n",
      "Epoch 3, Iteration 86, Loss: 0.2110130935907364\n",
      "Epoch 3, Iteration 87, Loss: 0.17703816294670105\n",
      "Epoch 3, Iteration 88, Loss: 0.14824625849723816\n",
      "Epoch 3, Iteration 89, Loss: 0.16196280717849731\n",
      "Epoch 3, Iteration 90, Loss: 0.16567672789096832\n",
      "Epoch 3, Iteration 91, Loss: 0.16316348314285278\n",
      "Epoch 3, Iteration 92, Loss: 0.1460205763578415\n",
      "Epoch 3, Iteration 93, Loss: 0.1663622260093689\n",
      "Epoch 3, Iteration 94, Loss: 0.2163574993610382\n",
      "Epoch 3, Iteration 95, Loss: 0.15261943638324738\n",
      "Epoch 3, Iteration 96, Loss: 0.11414215713739395\n",
      "Epoch 3, Iteration 97, Loss: 0.16667595505714417\n",
      "Epoch 3, Iteration 98, Loss: 0.18195469677448273\n",
      "Epoch 3, Iteration 99, Loss: 0.1435169279575348\n",
      "Epoch 3, Iteration 100, Loss: 0.13784059882164001\n",
      "Epoch 3, Iteration 100, Test Loss: 0.2528597116470337\n",
      "Epoch 3, Iteration 101, Loss: 0.14475411176681519\n",
      "Epoch 3, Iteration 102, Loss: 0.16065387427806854\n",
      "Epoch 3, Iteration 103, Loss: 0.15089894831180573\n",
      "Epoch 3, Iteration 104, Loss: 0.16107060015201569\n",
      "Epoch 3, Iteration 105, Loss: 0.16290803253650665\n",
      "Epoch 3, Iteration 106, Loss: 0.1407417207956314\n",
      "Epoch 3, Iteration 107, Loss: 0.09945105016231537\n",
      "Epoch 3, Iteration 108, Loss: 0.2067064493894577\n",
      "Epoch 3, Iteration 109, Loss: 0.21372194588184357\n",
      "Epoch 3, Iteration 110, Loss: 0.13689474761486053\n",
      "Epoch 3, Iteration 111, Loss: 0.10590280592441559\n",
      "Epoch 3, Iteration 112, Loss: 0.1515335738658905\n",
      "Epoch 3, Iteration 113, Loss: 0.14500658214092255\n",
      "Epoch 3, Iteration 114, Loss: 0.14287683367729187\n",
      "Epoch 3, Iteration 115, Loss: 0.16011367738246918\n",
      "Epoch 3, Iteration 116, Loss: 0.14785216748714447\n",
      "Epoch 3, Iteration 117, Loss: 0.2098710685968399\n",
      "Epoch 3, Iteration 118, Loss: 0.16018535196781158\n",
      "Epoch 3, Iteration 119, Loss: 0.22156736254692078\n",
      "Epoch 3, Iteration 120, Loss: 0.13462181389331818\n",
      "Epoch 3, Iteration 121, Loss: 0.1707330048084259\n",
      "Epoch 3, Iteration 122, Loss: 0.17546826601028442\n",
      "Epoch 3, Iteration 123, Loss: 0.14516586065292358\n",
      "Epoch 3, Iteration 124, Loss: 0.162982776761055\n",
      "Epoch 3, Iteration 125, Loss: 0.19722898304462433\n",
      "Epoch 3, Iteration 126, Loss: 0.17768532037734985\n",
      "Epoch 3, Iteration 127, Loss: 0.18623696267604828\n",
      "Epoch 3, Iteration 128, Loss: 0.1483074277639389\n",
      "Epoch 3, Iteration 129, Loss: 0.2197369784116745\n",
      "Epoch 3, Iteration 130, Loss: 0.13058404624462128\n",
      "Epoch 3, Iteration 131, Loss: 0.1336541324853897\n",
      "Epoch 3, Iteration 132, Loss: 0.1825801283121109\n",
      "Epoch 3, Iteration 133, Loss: 0.21893072128295898\n",
      "Epoch 3, Iteration 134, Loss: 0.21383674442768097\n",
      "Epoch 3, Iteration 135, Loss: 0.16823461651802063\n",
      "Epoch 3, Iteration 136, Loss: 0.12302183359861374\n",
      "Epoch 3, Iteration 137, Loss: 0.1441153585910797\n",
      "Epoch 3, Iteration 138, Loss: 0.15259654819965363\n",
      "Epoch 3, Iteration 139, Loss: 0.17858310043811798\n",
      "Epoch 3, Iteration 140, Loss: 0.20090796053409576\n",
      "Epoch 3, Iteration 141, Loss: 0.16114085912704468\n",
      "Epoch 3, Iteration 142, Loss: 0.203653946518898\n",
      "Epoch 3, Iteration 143, Loss: 0.15370547771453857\n",
      "Epoch 3, Iteration 144, Loss: 0.11746536195278168\n",
      "Epoch 3, Iteration 145, Loss: 0.16896376013755798\n",
      "Epoch 3, Iteration 146, Loss: 0.1302861124277115\n",
      "Epoch 3, Iteration 147, Loss: 0.14908181130886078\n",
      "Epoch 3, Iteration 148, Loss: 0.11694126576185226\n",
      "Epoch 3, Iteration 149, Loss: 0.1478101909160614\n",
      "Epoch 3, Iteration 150, Loss: 0.20945055782794952\n",
      "Epoch 3, Iteration 150, Test Loss: 0.2522793114185333\n",
      "Epoch 3, Iteration 151, Loss: 0.14049749076366425\n",
      "Epoch 3, Iteration 152, Loss: 0.1801009476184845\n",
      "Epoch 3, Iteration 153, Loss: 0.1693405658006668\n",
      "Epoch 3, Iteration 154, Loss: 0.15488553047180176\n",
      "Epoch 3, Iteration 155, Loss: 0.16553041338920593\n",
      "Epoch 3, Iteration 156, Loss: 0.20523491501808167\n",
      "Epoch 3, Iteration 157, Loss: 0.19954442977905273\n",
      "Epoch 3, Iteration 158, Loss: 0.2470225989818573\n",
      "Epoch 3, Iteration 159, Loss: 0.1534132957458496\n",
      "Epoch 3, Iteration 160, Loss: 0.17683671414852142\n",
      "Epoch 3, Iteration 161, Loss: 0.11108706891536713\n",
      "Epoch 3, Iteration 162, Loss: 0.11299168318510056\n",
      "Epoch 3, Iteration 163, Loss: 0.16938555240631104\n",
      "Epoch 3, Iteration 164, Loss: 0.11688949912786484\n",
      "Epoch 3, Iteration 165, Loss: 0.1459183692932129\n",
      "Epoch 3, Iteration 166, Loss: 0.17243404686450958\n",
      "Epoch 3, Iteration 167, Loss: 0.15800762176513672\n",
      "Epoch 3, Iteration 168, Loss: 0.13475246727466583\n",
      "Epoch 3, Iteration 169, Loss: 0.1642184555530548\n",
      "Epoch 3, Iteration 170, Loss: 0.12165485322475433\n",
      "Epoch 3, Iteration 171, Loss: 0.20194824039936066\n",
      "Epoch 3, Iteration 172, Loss: 0.19326721131801605\n",
      "Epoch 3, Iteration 173, Loss: 0.15123218297958374\n",
      "Epoch 3, Iteration 174, Loss: 0.16014933586120605\n",
      "Epoch 3, Iteration 175, Loss: 0.13029183447360992\n",
      "Epoch 3, Iteration 176, Loss: 0.12181384116411209\n",
      "Epoch 3, Iteration 177, Loss: 0.21419109404087067\n",
      "Epoch 3, Iteration 178, Loss: 0.13775545358657837\n",
      "Epoch 3, Iteration 179, Loss: 0.14848126471042633\n",
      "Epoch 3, Iteration 180, Loss: 0.19548887014389038\n",
      "Epoch 3, Iteration 181, Loss: 0.15905162692070007\n",
      "Epoch 3, Iteration 182, Loss: 0.12252844870090485\n",
      "Epoch 3, Iteration 183, Loss: 0.15629929304122925\n",
      "Epoch 3, Iteration 184, Loss: 0.22908273339271545\n",
      "Epoch 3, Iteration 185, Loss: 0.12277266383171082\n",
      "Epoch 3, Iteration 186, Loss: 0.14269603788852692\n",
      "Epoch 3, Iteration 187, Loss: 0.16736364364624023\n",
      "Epoch 3, Iteration 188, Loss: 0.12244801968336105\n",
      "Epoch 3, Iteration 189, Loss: 0.09465429931879044\n",
      "Epoch 3, Iteration 190, Loss: 0.12520268559455872\n",
      "Epoch 3, Iteration 191, Loss: 0.11968057602643967\n",
      "Epoch 3, Iteration 192, Loss: 0.15447422862052917\n",
      "Epoch 3, Iteration 193, Loss: 0.1652776151895523\n",
      "Epoch 3, Iteration 194, Loss: 0.1314408928155899\n",
      "Epoch 3, Iteration 195, Loss: 0.14806793630123138\n",
      "Epoch 3, Iteration 196, Loss: 0.24886870384216309\n",
      "Epoch 3, Iteration 197, Loss: 0.15060579776763916\n",
      "Epoch 3, Iteration 198, Loss: 0.12147877365350723\n",
      "Epoch 3, Iteration 199, Loss: 0.15383952856063843\n",
      "Epoch 3, Iteration 200, Loss: 0.10668104141950607\n",
      "Epoch 3, Iteration 200, Test Loss: 0.2546643912792206\n",
      "Epoch 3, Iteration 201, Loss: 0.135829895734787\n",
      "Epoch 3, Iteration 202, Loss: 0.10107285529375076\n",
      "Epoch 3, Iteration 203, Loss: 0.11388489603996277\n",
      "Epoch 3, Iteration 204, Loss: 0.1553952395915985\n",
      "Epoch 3, Iteration 205, Loss: 0.19964464008808136\n",
      "Epoch 3, Iteration 206, Loss: 0.12547320127487183\n",
      "Epoch 3, Iteration 207, Loss: 0.11875888705253601\n",
      "Epoch 3, Iteration 208, Loss: 0.12347928434610367\n",
      "Epoch 3, Iteration 209, Loss: 0.1443621665239334\n",
      "Epoch 3, Iteration 210, Loss: 0.11702839285135269\n",
      "Epoch 3, Iteration 211, Loss: 0.1525406390428543\n",
      "Epoch 3, Iteration 212, Loss: 0.18108877539634705\n",
      "Epoch 3, Iteration 213, Loss: 0.13406117260456085\n",
      "Epoch 3, Iteration 214, Loss: 0.1315458118915558\n",
      "Epoch 3, Iteration 215, Loss: 0.1551361382007599\n",
      "Epoch 3, Iteration 216, Loss: 0.1704254448413849\n",
      "Epoch 3, Iteration 217, Loss: 0.21237881481647491\n",
      "Epoch 3, Iteration 218, Loss: 0.14170899987220764\n",
      "Epoch 3, Iteration 219, Loss: 0.16838733851909637\n",
      "Epoch 3, Iteration 220, Loss: 0.11225443333387375\n",
      "Epoch 3, Iteration 221, Loss: 0.16414904594421387\n",
      "Epoch 3, Iteration 222, Loss: 0.13680577278137207\n",
      "Epoch 3, Iteration 223, Loss: 0.149524986743927\n",
      "Epoch 3, Iteration 224, Loss: 0.11838226020336151\n",
      "Epoch 3, Iteration 225, Loss: 0.14543157815933228\n",
      "Epoch 3, Iteration 226, Loss: 0.18556490540504456\n",
      "Epoch 3, Iteration 227, Loss: 0.15825647115707397\n",
      "Epoch 3, Iteration 228, Loss: 0.30227988958358765\n",
      "Epoch 3, Iteration 229, Loss: 0.2231551557779312\n",
      "Epoch 3, Iteration 230, Loss: 0.20258960127830505\n",
      "Epoch 3, Iteration 231, Loss: 0.13076117634773254\n",
      "Epoch 3, Iteration 232, Loss: 0.16675131022930145\n",
      "Epoch 3, Iteration 233, Loss: 0.20671886205673218\n",
      "Epoch 3, Iteration 234, Loss: 0.13210156559944153\n",
      "Epoch 3, Iteration 235, Loss: 0.12537354230880737\n",
      "Epoch 3, Iteration 236, Loss: 0.16538920998573303\n",
      "Epoch 3, Iteration 237, Loss: 0.14549092948436737\n",
      "Epoch 3, Iteration 238, Loss: 0.14261165261268616\n",
      "Epoch 3, Iteration 239, Loss: 0.12933319807052612\n",
      "Epoch 3, Iteration 240, Loss: 0.1499703973531723\n",
      "Epoch 3, Iteration 241, Loss: 0.13360214233398438\n",
      "Epoch 3, Iteration 242, Loss: 0.17712193727493286\n",
      "Epoch 3, Iteration 243, Loss: 0.1270686388015747\n",
      "Epoch 3, Iteration 244, Loss: 0.16683132946491241\n",
      "Epoch 3, Iteration 245, Loss: 0.1541953980922699\n",
      "Epoch 3, Iteration 246, Loss: 0.18418584764003754\n",
      "Epoch 3, Iteration 247, Loss: 0.2630251944065094\n",
      "Epoch 3, Iteration 248, Loss: 0.21725325286388397\n",
      "Epoch 3, Iteration 249, Loss: 0.24340197443962097\n",
      "Epoch 3, Iteration 250, Loss: 0.2690913677215576\n",
      "Epoch 3, Iteration 250, Test Loss: 0.2581183910369873\n",
      "Epoch 3, Iteration 251, Loss: 0.22247551381587982\n",
      "Epoch 3, Iteration 252, Loss: 0.15771600604057312\n",
      "Epoch 3, Iteration 253, Loss: 0.1819663643836975\n",
      "Epoch 3, Iteration 254, Loss: 0.21464787423610687\n",
      "Epoch 3, Iteration 255, Loss: 0.18650799989700317\n",
      "Epoch 3, Iteration 256, Loss: 0.14035344123840332\n",
      "Epoch 3, Iteration 257, Loss: 0.14377301931381226\n",
      "Epoch 3, Iteration 258, Loss: 0.1601584106683731\n",
      "Epoch 3, Iteration 259, Loss: 0.17143529653549194\n",
      "Epoch 3, Iteration 260, Loss: 0.21994777023792267\n",
      "Epoch 3, Iteration 261, Loss: 0.14953137934207916\n",
      "Epoch 3, Iteration 262, Loss: 0.12762628495693207\n",
      "Epoch 3, Iteration 263, Loss: 0.14938782155513763\n",
      "Epoch 3, Iteration 264, Loss: 0.12916523218154907\n",
      "Epoch 3, Iteration 265, Loss: 0.21934811770915985\n",
      "Epoch 3, Iteration 266, Loss: 0.12454835325479507\n",
      "Epoch 3, Iteration 267, Loss: 0.1754341423511505\n",
      "Epoch 3, Iteration 268, Loss: 0.16107498109340668\n",
      "Epoch 3, Iteration 269, Loss: 0.1748507171869278\n",
      "Epoch 3, Iteration 270, Loss: 0.20166924595832825\n",
      "Epoch 3, Iteration 271, Loss: 0.10439994931221008\n",
      "Epoch 3, Iteration 272, Loss: 0.15194225311279297\n",
      "Epoch 3, Iteration 273, Loss: 0.14559800922870636\n",
      "Epoch 3, Iteration 274, Loss: 0.1229173019528389\n",
      "Epoch 3, Iteration 275, Loss: 0.11109329760074615\n",
      "Epoch 3, Iteration 276, Loss: 0.12244265526533127\n",
      "Epoch 3, Iteration 277, Loss: 0.12504631280899048\n",
      "Epoch 3, Iteration 278, Loss: 0.16275878250598907\n",
      "Epoch 3, Iteration 279, Loss: 0.11871661990880966\n",
      "Epoch 3, Iteration 280, Loss: 0.1563175767660141\n",
      "Epoch 3, Iteration 281, Loss: 0.13898855447769165\n",
      "Epoch 3, Iteration 282, Loss: 0.12533967196941376\n",
      "Epoch 3, Iteration 283, Loss: 0.12426909059286118\n",
      "Epoch 3, Iteration 284, Loss: 0.17755678296089172\n",
      "Epoch 3, Iteration 285, Loss: 0.12352750450372696\n",
      "Epoch 3, Iteration 286, Loss: 0.17716734111309052\n",
      "Epoch 3, Iteration 287, Loss: 0.12807896733283997\n",
      "Epoch 3, Iteration 288, Loss: 0.13530844449996948\n",
      "Epoch 3, Iteration 289, Loss: 0.20162707567214966\n",
      "Epoch 3, Iteration 290, Loss: 0.10721167176961899\n",
      "Epoch 3, Iteration 291, Loss: 0.11691217869520187\n",
      "Epoch 3, Iteration 292, Loss: 0.13858233392238617\n",
      "Epoch 3, Iteration 293, Loss: 0.1985832154750824\n",
      "Epoch 3, Iteration 294, Loss: 0.11807148158550262\n",
      "Epoch 3, Iteration 295, Loss: 0.07662386447191238\n",
      "Epoch 3, Iteration 296, Loss: 0.12336644530296326\n",
      "Epoch 3, Iteration 297, Loss: 0.149045929312706\n",
      "Epoch 3, Iteration 298, Loss: 0.1205592006444931\n",
      "Epoch 3, Iteration 299, Loss: 0.18894417583942413\n",
      "Epoch 3, Iteration 300, Loss: 0.14501145482063293\n",
      "Epoch 3, Iteration 300, Test Loss: 0.26940155029296875\n",
      "Epoch 3, Iteration 301, Loss: 0.13458147644996643\n",
      "Epoch 3, Iteration 302, Loss: 0.11697221547365189\n",
      "Epoch 3, Iteration 303, Loss: 0.1621054857969284\n",
      "Epoch 3, Iteration 304, Loss: 0.1914568692445755\n",
      "Epoch 3, Iteration 305, Loss: 0.12994396686553955\n",
      "Epoch 3, Iteration 306, Loss: 0.12078419327735901\n",
      "Epoch 3, Iteration 307, Loss: 0.13246336579322815\n",
      "Epoch 3, Iteration 308, Loss: 0.12614789605140686\n",
      "Epoch 3, Iteration 309, Loss: 0.10947102308273315\n",
      "Epoch 3, Iteration 310, Loss: 0.20754610002040863\n",
      "Epoch 3, Iteration 311, Loss: 0.11042506992816925\n",
      "Epoch 3, Iteration 312, Loss: 0.15731118619441986\n",
      "Epoch 3, Iteration 313, Loss: 0.12536220252513885\n",
      "Epoch 3, Iteration 314, Loss: 0.11604887992143631\n",
      "Epoch 3, Iteration 315, Loss: 0.13469192385673523\n",
      "Epoch 3, Iteration 316, Loss: 0.1264568567276001\n",
      "Epoch 3, Iteration 317, Loss: 0.15578599274158478\n",
      "Epoch 3, Iteration 318, Loss: 0.19844184815883636\n",
      "Epoch 3, Iteration 319, Loss: 0.10700470209121704\n",
      "Epoch 3, Iteration 320, Loss: 0.1391316056251526\n",
      "Epoch 3, Iteration 321, Loss: 0.12347414344549179\n",
      "Epoch 3, Iteration 322, Loss: 0.19043712317943573\n",
      "Epoch 3, Iteration 323, Loss: 0.1370500773191452\n",
      "Epoch 3, Iteration 324, Loss: 0.17448951303958893\n",
      "Epoch 3, Iteration 325, Loss: 0.24965892732143402\n",
      "Epoch 3, Iteration 326, Loss: 0.20871348679065704\n",
      "Epoch 3, Iteration 327, Loss: 0.17822310328483582\n",
      "Epoch 3, Iteration 328, Loss: 0.15137355029582977\n",
      "Epoch 3, Iteration 329, Loss: 0.15397909283638\n",
      "Epoch 3, Iteration 330, Loss: 0.17537279427051544\n",
      "Epoch 3, Iteration 331, Loss: 0.18120813369750977\n",
      "Epoch 3, Iteration 332, Loss: 0.19732928276062012\n",
      "Epoch 3, Iteration 333, Loss: 0.14621475338935852\n",
      "Epoch 3, Iteration 334, Loss: 0.2446928769350052\n",
      "Epoch 3, Iteration 335, Loss: 0.164218470454216\n",
      "Epoch 3, Iteration 336, Loss: 0.1797047257423401\n",
      "Epoch 3, Iteration 337, Loss: 0.13571202754974365\n",
      "Epoch 3, Iteration 338, Loss: 0.14817681908607483\n",
      "Epoch 3, Iteration 339, Loss: 0.17245857417583466\n",
      "Epoch 3, Iteration 340, Loss: 0.1594158560037613\n",
      "Epoch 3, Iteration 341, Loss: 0.18863819539546967\n",
      "Epoch 3, Iteration 342, Loss: 0.16766737401485443\n",
      "Epoch 3, Iteration 343, Loss: 0.18199522793293\n",
      "Epoch 3, Iteration 344, Loss: 0.21427689492702484\n",
      "Epoch 3, Iteration 345, Loss: 0.1583847999572754\n",
      "Epoch 3, Iteration 346, Loss: 0.14329276978969574\n",
      "Epoch 3, Iteration 347, Loss: 0.19103680551052094\n",
      "Epoch 3, Iteration 348, Loss: 0.21549470722675323\n",
      "Epoch 3, Iteration 349, Loss: 0.1885613203048706\n",
      "Epoch 3, Iteration 350, Loss: 0.20500268042087555\n",
      "Epoch 3, Iteration 350, Test Loss: 0.25883564352989197\n",
      "Epoch 3, Iteration 351, Loss: 0.16685722768306732\n",
      "Epoch 3, Iteration 352, Loss: 0.1723252534866333\n",
      "Epoch 3, Iteration 353, Loss: 0.20862747728824615\n",
      "Epoch 3, Iteration 354, Loss: 0.12942549586296082\n",
      "Epoch 3, Iteration 355, Loss: 0.1516374945640564\n",
      "Epoch 3, Iteration 356, Loss: 0.1520252525806427\n",
      "Epoch 3, Iteration 357, Loss: 0.24374103546142578\n",
      "Epoch 3, Iteration 358, Loss: 0.12321211397647858\n",
      "Epoch 3, Iteration 359, Loss: 0.1681777685880661\n",
      "Epoch 3, Iteration 360, Loss: 0.10064175724983215\n",
      "Epoch 3, Iteration 361, Loss: 0.15573084354400635\n",
      "Epoch 3, Iteration 362, Loss: 0.153342604637146\n",
      "Epoch 3, Iteration 363, Loss: 0.14440931379795074\n",
      "Epoch 3, Iteration 364, Loss: 0.14253748953342438\n",
      "Epoch 3, Iteration 365, Loss: 0.1166229248046875\n",
      "Epoch 3, Iteration 366, Loss: 0.19966807961463928\n",
      "Epoch 3, Iteration 367, Loss: 0.18987266719341278\n",
      "Epoch 3, Iteration 368, Loss: 0.1504354029893875\n",
      "Epoch 3, Iteration 369, Loss: 0.1305815577507019\n",
      "Epoch 3, Iteration 370, Loss: 0.1788235455751419\n",
      "Epoch 3, Iteration 371, Loss: 0.15357349812984467\n",
      "Epoch 3, Iteration 372, Loss: 0.1279478669166565\n",
      "Epoch 3, Iteration 373, Loss: 0.17465204000473022\n",
      "Epoch 3, Iteration 374, Loss: 0.11791270971298218\n",
      "Epoch 3, Iteration 375, Loss: 0.17272116243839264\n",
      "Epoch 3, Iteration 376, Loss: 0.17333847284317017\n",
      "Epoch 3, Iteration 377, Loss: 0.14697377383708954\n",
      "Epoch 3, Iteration 378, Loss: 0.13910508155822754\n",
      "Epoch 3, Iteration 379, Loss: 0.19899111986160278\n",
      "Epoch 3, Iteration 380, Loss: 0.16109676659107208\n",
      "Epoch 3, Iteration 381, Loss: 0.1527213752269745\n",
      "Epoch 3, Iteration 382, Loss: 0.16053295135498047\n",
      "Epoch 3, Iteration 383, Loss: 0.13889092206954956\n",
      "Epoch 3, Iteration 384, Loss: 0.14174200594425201\n",
      "Epoch 3, Iteration 385, Loss: 0.13958147168159485\n",
      "Epoch 3, Iteration 386, Loss: 0.13622575998306274\n",
      "Epoch 3, Iteration 387, Loss: 0.11641283333301544\n",
      "Epoch 3, Iteration 388, Loss: 0.09922894835472107\n",
      "Epoch 3, Iteration 389, Loss: 0.1299649178981781\n",
      "Epoch 3, Iteration 390, Loss: 0.1524311751127243\n",
      "Epoch 3, Iteration 391, Loss: 0.08220574259757996\n",
      "Epoch 3, Iteration 392, Loss: 0.18021652102470398\n",
      "Epoch 3, Iteration 393, Loss: 0.19493745267391205\n",
      "Epoch 3, Iteration 394, Loss: 0.1421460211277008\n",
      "Epoch 3, Iteration 395, Loss: 0.12797687947750092\n",
      "Epoch 3, Iteration 396, Loss: 0.13208484649658203\n",
      "Epoch 3, Iteration 397, Loss: 0.1391429305076599\n",
      "Epoch 3, Iteration 398, Loss: 0.24034252762794495\n",
      "Epoch 3, Iteration 399, Loss: 0.13803373277187347\n",
      "Epoch 3, Iteration 400, Loss: 0.12606056034564972\n",
      "Epoch 3, Iteration 400, Test Loss: 0.2593507170677185\n",
      "Epoch 3, Iteration 401, Loss: 0.16263991594314575\n",
      "Epoch 3, Iteration 402, Loss: 0.1036846935749054\n",
      "Epoch 3, Iteration 403, Loss: 0.13427823781967163\n",
      "Epoch 3, Iteration 404, Loss: 0.162546768784523\n",
      "Epoch 3, Iteration 405, Loss: 0.11248388886451721\n",
      "Epoch 3, Iteration 406, Loss: 0.11776965111494064\n",
      "Epoch 3, Iteration 407, Loss: 0.16189710795879364\n",
      "Epoch 3, Iteration 408, Loss: 0.17783531546592712\n",
      "Epoch 3, Iteration 409, Loss: 0.12053065747022629\n",
      "Epoch 3, Iteration 410, Loss: 0.19234997034072876\n",
      "Epoch 3, Iteration 411, Loss: 0.1972331702709198\n",
      "Epoch 3, Iteration 412, Loss: 0.13988365232944489\n",
      "Epoch 3, Iteration 413, Loss: 0.18351639807224274\n",
      "Epoch 3, Iteration 414, Loss: 0.11478320509195328\n",
      "Epoch 3, Iteration 415, Loss: 0.14308521151542664\n",
      "Epoch 3, Iteration 416, Loss: 0.12942136824131012\n",
      "Epoch 3, Iteration 417, Loss: 0.16412590444087982\n",
      "Epoch 3, Iteration 418, Loss: 0.11807248741388321\n",
      "Epoch 3, Iteration 419, Loss: 0.1355372667312622\n",
      "Epoch 3, Iteration 420, Loss: 0.15586310625076294\n",
      "Epoch 3, Iteration 421, Loss: 0.09573153406381607\n",
      "Epoch 3, Iteration 422, Loss: 0.14241357147693634\n",
      "Epoch 3, Iteration 423, Loss: 0.1436069905757904\n",
      "Epoch 3, Iteration 424, Loss: 0.14965689182281494\n",
      "Epoch 3, Iteration 425, Loss: 0.14116916060447693\n",
      "Epoch 3, Iteration 426, Loss: 0.1359139382839203\n",
      "Epoch 3, Iteration 427, Loss: 0.12075953930616379\n",
      "Epoch 3, Iteration 428, Loss: 0.13761615753173828\n",
      "Epoch 3, Iteration 429, Loss: 0.1778119057416916\n",
      "Epoch 3, Iteration 430, Loss: 0.14241322875022888\n",
      "Epoch 3, Iteration 431, Loss: 0.1293637752532959\n",
      "Epoch 4/15, Loss: 0.16696127490312965\n",
      "Epoch 4/15, Validation Accuracy: 0.9393675889328064\n",
      "Epoch 4, Iteration 0, Loss: 0.31551897525787354\n",
      "Epoch 4, Iteration 1, Loss: 0.2565218508243561\n",
      "Epoch 4, Iteration 2, Loss: 0.24594879150390625\n",
      "Epoch 4, Iteration 3, Loss: 0.1767580509185791\n",
      "Epoch 4, Iteration 4, Loss: 0.2528447210788727\n",
      "Epoch 4, Iteration 5, Loss: 0.29046931862831116\n",
      "Epoch 4, Iteration 6, Loss: 0.2127094566822052\n",
      "Epoch 4, Iteration 7, Loss: 0.30564555525779724\n",
      "Epoch 4, Iteration 8, Loss: 0.21915848553180695\n",
      "Epoch 4, Iteration 9, Loss: 0.19230328500270844\n",
      "Epoch 4, Iteration 10, Loss: 0.21791204810142517\n",
      "Epoch 4, Iteration 11, Loss: 0.19084009528160095\n",
      "Epoch 4, Iteration 12, Loss: 0.19784194231033325\n",
      "Epoch 4, Iteration 13, Loss: 0.22179439663887024\n",
      "Epoch 4, Iteration 14, Loss: 0.2884658873081207\n",
      "Epoch 4, Iteration 15, Loss: 0.1492915153503418\n",
      "Epoch 4, Iteration 16, Loss: 0.30576667189598083\n",
      "Epoch 4, Iteration 17, Loss: 0.16451933979988098\n",
      "Epoch 4, Iteration 18, Loss: 0.22119325399398804\n",
      "Epoch 4, Iteration 19, Loss: 0.19156968593597412\n",
      "Epoch 4, Iteration 20, Loss: 0.2798032760620117\n",
      "Epoch 4, Iteration 21, Loss: 0.21535296738147736\n",
      "Epoch 4, Iteration 22, Loss: 0.21299585700035095\n",
      "Epoch 4, Iteration 23, Loss: 0.24115042388439178\n",
      "Epoch 4, Iteration 24, Loss: 0.20254382491111755\n",
      "Epoch 4, Iteration 25, Loss: 0.27597030997276306\n",
      "Epoch 4, Iteration 26, Loss: 0.203648641705513\n",
      "Epoch 4, Iteration 27, Loss: 0.20502954721450806\n",
      "Epoch 4, Iteration 28, Loss: 0.24901330471038818\n",
      "Epoch 4, Iteration 29, Loss: 0.18690362572669983\n",
      "Epoch 4, Iteration 30, Loss: 0.25737395882606506\n",
      "Epoch 4, Iteration 31, Loss: 0.2023027241230011\n",
      "Epoch 4, Iteration 32, Loss: 0.23030516505241394\n",
      "Epoch 4, Iteration 33, Loss: 0.20208102464675903\n",
      "Epoch 4, Iteration 34, Loss: 0.21775899827480316\n",
      "Epoch 4, Iteration 35, Loss: 0.1396900713443756\n",
      "Epoch 4, Iteration 36, Loss: 0.2234838902950287\n",
      "Epoch 4, Iteration 37, Loss: 0.2291678786277771\n",
      "Epoch 4, Iteration 38, Loss: 0.22156479954719543\n",
      "Epoch 4, Iteration 39, Loss: 0.21138915419578552\n",
      "Epoch 4, Iteration 40, Loss: 0.2050892412662506\n",
      "Epoch 4, Iteration 41, Loss: 0.19554077088832855\n",
      "Epoch 4, Iteration 42, Loss: 0.18954525887966156\n",
      "Epoch 4, Iteration 43, Loss: 0.20197370648384094\n",
      "Epoch 4, Iteration 44, Loss: 0.17219571769237518\n",
      "Epoch 4, Iteration 45, Loss: 0.18476463854312897\n",
      "Epoch 4, Iteration 46, Loss: 0.18574343621730804\n",
      "Epoch 4, Iteration 47, Loss: 0.21164824068546295\n",
      "Epoch 4, Iteration 48, Loss: 0.2339078187942505\n",
      "Epoch 4, Iteration 49, Loss: 0.15193377435207367\n",
      "Epoch 4, Iteration 50, Loss: 0.15617401897907257\n",
      "Epoch 4, Iteration 50, Test Loss: 0.22230273485183716\n",
      "Epoch 4, Iteration 51, Loss: 0.14047004282474518\n",
      "Epoch 4, Iteration 52, Loss: 0.17767854034900665\n",
      "Epoch 4, Iteration 53, Loss: 0.14709001779556274\n",
      "Epoch 4, Iteration 54, Loss: 0.18261191248893738\n",
      "Epoch 4, Iteration 55, Loss: 0.15557554364204407\n",
      "Epoch 4, Iteration 56, Loss: 0.15635079145431519\n",
      "Epoch 4, Iteration 57, Loss: 0.1486721783876419\n",
      "Epoch 4, Iteration 58, Loss: 0.1379738301038742\n",
      "Epoch 4, Iteration 59, Loss: 0.14585159718990326\n",
      "Epoch 4, Iteration 60, Loss: 0.16453160345554352\n",
      "Epoch 4, Iteration 61, Loss: 0.15105018019676208\n",
      "Epoch 4, Iteration 62, Loss: 0.3077930808067322\n",
      "Epoch 4, Iteration 63, Loss: 0.12832987308502197\n",
      "Epoch 4, Iteration 64, Loss: 0.14374876022338867\n",
      "Epoch 4, Iteration 65, Loss: 0.15354779362678528\n",
      "Epoch 4, Iteration 66, Loss: 0.17252497375011444\n",
      "Epoch 4, Iteration 67, Loss: 0.11402441561222076\n",
      "Epoch 4, Iteration 68, Loss: 0.15938015282154083\n",
      "Epoch 4, Iteration 69, Loss: 0.179125115275383\n",
      "Epoch 4, Iteration 70, Loss: 0.10582247376441956\n",
      "Epoch 4, Iteration 71, Loss: 0.16903086006641388\n",
      "Epoch 4, Iteration 72, Loss: 0.16474901139736176\n",
      "Epoch 4, Iteration 73, Loss: 0.14797428250312805\n",
      "Epoch 4, Iteration 74, Loss: 0.19073137640953064\n",
      "Epoch 4, Iteration 75, Loss: 0.17297007143497467\n",
      "Epoch 4, Iteration 76, Loss: 0.1641710102558136\n",
      "Epoch 4, Iteration 77, Loss: 0.15165069699287415\n",
      "Epoch 4, Iteration 78, Loss: 0.1613127738237381\n",
      "Epoch 4, Iteration 79, Loss: 0.10837292671203613\n",
      "Epoch 4, Iteration 80, Loss: 0.1851692795753479\n",
      "Epoch 4, Iteration 81, Loss: 0.14643999934196472\n",
      "Epoch 4, Iteration 82, Loss: 0.18122319877147675\n",
      "Epoch 4, Iteration 83, Loss: 0.16252832114696503\n",
      "Epoch 4, Iteration 84, Loss: 0.13409355282783508\n",
      "Epoch 4, Iteration 85, Loss: 0.18432585895061493\n",
      "Epoch 4, Iteration 86, Loss: 0.17886212468147278\n",
      "Epoch 4, Iteration 87, Loss: 0.16081081330776215\n",
      "Epoch 4, Iteration 88, Loss: 0.15027880668640137\n",
      "Epoch 4, Iteration 89, Loss: 0.16454273462295532\n",
      "Epoch 4, Iteration 90, Loss: 0.15135715901851654\n",
      "Epoch 4, Iteration 91, Loss: 0.1218532994389534\n",
      "Epoch 4, Iteration 92, Loss: 0.1278735250234604\n",
      "Epoch 4, Iteration 93, Loss: 0.15896746516227722\n",
      "Epoch 4, Iteration 94, Loss: 0.2295386642217636\n",
      "Epoch 4, Iteration 95, Loss: 0.12679466605186462\n",
      "Epoch 4, Iteration 96, Loss: 0.09150967746973038\n",
      "Epoch 4, Iteration 97, Loss: 0.15437564253807068\n",
      "Epoch 4, Iteration 98, Loss: 0.17254474759101868\n",
      "Epoch 4, Iteration 99, Loss: 0.11940152198076248\n",
      "Epoch 4, Iteration 100, Loss: 0.11044225096702576\n",
      "Epoch 4, Iteration 100, Test Loss: 0.25020265579223633\n",
      "Epoch 4, Iteration 101, Loss: 0.13048385083675385\n",
      "Epoch 4, Iteration 102, Loss: 0.1463998705148697\n",
      "Epoch 4, Iteration 103, Loss: 0.1481800377368927\n",
      "Epoch 4, Iteration 104, Loss: 0.14216147363185883\n",
      "Epoch 4, Iteration 105, Loss: 0.14080621302127838\n",
      "Epoch 4, Iteration 106, Loss: 0.11241540312767029\n",
      "Epoch 4, Iteration 107, Loss: 0.08885309100151062\n",
      "Epoch 4, Iteration 108, Loss: 0.20343826711177826\n",
      "Epoch 4, Iteration 109, Loss: 0.20090870559215546\n",
      "Epoch 4, Iteration 110, Loss: 0.12857180833816528\n",
      "Epoch 4, Iteration 111, Loss: 0.10566569119691849\n",
      "Epoch 4, Iteration 112, Loss: 0.1670222282409668\n",
      "Epoch 4, Iteration 113, Loss: 0.12970072031021118\n",
      "Epoch 4, Iteration 114, Loss: 0.13749518990516663\n",
      "Epoch 4, Iteration 115, Loss: 0.1726648509502411\n",
      "Epoch 4, Iteration 116, Loss: 0.13760729134082794\n",
      "Epoch 4, Iteration 117, Loss: 0.22995443642139435\n",
      "Epoch 4, Iteration 118, Loss: 0.15850521624088287\n",
      "Epoch 4, Iteration 119, Loss: 0.19779010117053986\n",
      "Epoch 4, Iteration 120, Loss: 0.13312891125679016\n",
      "Epoch 4, Iteration 121, Loss: 0.16426263749599457\n",
      "Epoch 4, Iteration 122, Loss: 0.1748947948217392\n",
      "Epoch 4, Iteration 123, Loss: 0.14915403723716736\n",
      "Epoch 4, Iteration 124, Loss: 0.16530804336071014\n",
      "Epoch 4, Iteration 125, Loss: 0.20013564825057983\n",
      "Epoch 4, Iteration 126, Loss: 0.1449822038412094\n",
      "Epoch 4, Iteration 127, Loss: 0.15608032047748566\n",
      "Epoch 4, Iteration 128, Loss: 0.14225688576698303\n",
      "Epoch 4, Iteration 129, Loss: 0.20754753053188324\n",
      "Epoch 4, Iteration 130, Loss: 0.1245180144906044\n",
      "Epoch 4, Iteration 131, Loss: 0.1160198524594307\n",
      "Epoch 4, Iteration 132, Loss: 0.1897430121898651\n",
      "Epoch 4, Iteration 133, Loss: 0.22662606835365295\n",
      "Epoch 4, Iteration 134, Loss: 0.17301452159881592\n",
      "Epoch 4, Iteration 135, Loss: 0.16074492037296295\n",
      "Epoch 4, Iteration 136, Loss: 0.10920773446559906\n",
      "Epoch 4, Iteration 137, Loss: 0.14564254879951477\n",
      "Epoch 4, Iteration 138, Loss: 0.17886590957641602\n",
      "Epoch 4, Iteration 139, Loss: 0.1738792508840561\n",
      "Epoch 4, Iteration 140, Loss: 0.17385688424110413\n",
      "Epoch 4, Iteration 141, Loss: 0.16432872414588928\n",
      "Epoch 4, Iteration 142, Loss: 0.1989448219537735\n",
      "Epoch 4, Iteration 143, Loss: 0.15400256216526031\n",
      "Epoch 4, Iteration 144, Loss: 0.12844112515449524\n",
      "Epoch 4, Iteration 145, Loss: 0.19059135019779205\n",
      "Epoch 4, Iteration 146, Loss: 0.1275736689567566\n",
      "Epoch 4, Iteration 147, Loss: 0.12865839898586273\n",
      "Epoch 4, Iteration 148, Loss: 0.12112850695848465\n",
      "Epoch 4, Iteration 149, Loss: 0.13001498579978943\n",
      "Epoch 4, Iteration 150, Loss: 0.16503547132015228\n",
      "Epoch 4, Iteration 150, Test Loss: 0.25013425946235657\n",
      "Epoch 4, Iteration 151, Loss: 0.15471391379833221\n",
      "Epoch 4, Iteration 152, Loss: 0.15971620380878448\n",
      "Epoch 4, Iteration 153, Loss: 0.1671462506055832\n",
      "Epoch 4, Iteration 154, Loss: 0.1389956921339035\n",
      "Epoch 4, Iteration 155, Loss: 0.15953227877616882\n",
      "Epoch 4, Iteration 156, Loss: 0.1870376616716385\n",
      "Epoch 4, Iteration 157, Loss: 0.2173604965209961\n",
      "Epoch 4, Iteration 158, Loss: 0.21569812297821045\n",
      "Epoch 4, Iteration 159, Loss: 0.1295645833015442\n",
      "Epoch 4, Iteration 160, Loss: 0.19078309834003448\n",
      "Epoch 4, Iteration 161, Loss: 0.10190222412347794\n",
      "Epoch 4, Iteration 162, Loss: 0.09948443621397018\n",
      "Epoch 4, Iteration 163, Loss: 0.16953986883163452\n",
      "Epoch 4, Iteration 164, Loss: 0.1240018978714943\n",
      "Epoch 4, Iteration 165, Loss: 0.11698392033576965\n",
      "Epoch 4, Iteration 166, Loss: 0.1654459685087204\n",
      "Epoch 4, Iteration 167, Loss: 0.15496808290481567\n",
      "Epoch 4, Iteration 168, Loss: 0.12904421985149384\n",
      "Epoch 4, Iteration 169, Loss: 0.15510408580303192\n",
      "Epoch 4, Iteration 170, Loss: 0.10361537337303162\n",
      "Epoch 4, Iteration 171, Loss: 0.18954205513000488\n",
      "Epoch 4, Iteration 172, Loss: 0.199665367603302\n",
      "Epoch 4, Iteration 173, Loss: 0.14976833760738373\n",
      "Epoch 4, Iteration 174, Loss: 0.1664886325597763\n",
      "Epoch 4, Iteration 175, Loss: 0.1351880580186844\n",
      "Epoch 4, Iteration 176, Loss: 0.11554328352212906\n",
      "Epoch 4, Iteration 177, Loss: 0.18761661648750305\n",
      "Epoch 4, Iteration 178, Loss: 0.13184668123722076\n",
      "Epoch 4, Iteration 179, Loss: 0.1398448795080185\n",
      "Epoch 4, Iteration 180, Loss: 0.17318816483020782\n",
      "Epoch 4, Iteration 181, Loss: 0.13231152296066284\n",
      "Epoch 4, Iteration 182, Loss: 0.1460876166820526\n",
      "Epoch 4, Iteration 183, Loss: 0.15060342848300934\n",
      "Epoch 4, Iteration 184, Loss: 0.21194671094417572\n",
      "Epoch 4, Iteration 185, Loss: 0.13951343297958374\n",
      "Epoch 4, Iteration 186, Loss: 0.16075368225574493\n",
      "Epoch 4, Iteration 187, Loss: 0.1512616127729416\n",
      "Epoch 4, Iteration 188, Loss: 0.11667962372303009\n",
      "Epoch 4, Iteration 189, Loss: 0.08502203226089478\n",
      "Epoch 4, Iteration 190, Loss: 0.11053763329982758\n",
      "Epoch 4, Iteration 191, Loss: 0.1171247810125351\n",
      "Epoch 4, Iteration 192, Loss: 0.13373251259326935\n",
      "Epoch 4, Iteration 193, Loss: 0.14335624873638153\n",
      "Epoch 4, Iteration 194, Loss: 0.13904611766338348\n",
      "Epoch 4, Iteration 195, Loss: 0.17322370409965515\n",
      "Epoch 4, Iteration 196, Loss: 0.2649191915988922\n",
      "Epoch 4, Iteration 197, Loss: 0.14399634301662445\n",
      "Epoch 4, Iteration 198, Loss: 0.11691019684076309\n",
      "Epoch 4, Iteration 199, Loss: 0.14655454456806183\n",
      "Epoch 4, Iteration 200, Loss: 0.1128101572394371\n",
      "Epoch 4, Iteration 200, Test Loss: 0.2535049021244049\n",
      "Epoch 4, Iteration 201, Loss: 0.13461387157440186\n",
      "Epoch 4, Iteration 202, Loss: 0.10578252375125885\n",
      "Epoch 4, Iteration 203, Loss: 0.12412356585264206\n",
      "Epoch 4, Iteration 204, Loss: 0.15168681740760803\n",
      "Epoch 4, Iteration 205, Loss: 0.17496877908706665\n",
      "Epoch 4, Iteration 206, Loss: 0.1149546355009079\n",
      "Epoch 4, Iteration 207, Loss: 0.1212104931473732\n",
      "Epoch 4, Iteration 208, Loss: 0.09410806745290756\n",
      "Epoch 4, Iteration 209, Loss: 0.13225609064102173\n",
      "Epoch 4, Iteration 210, Loss: 0.1117246001958847\n",
      "Epoch 4, Iteration 211, Loss: 0.14706096053123474\n",
      "Epoch 4, Iteration 212, Loss: 0.16954918205738068\n",
      "Epoch 4, Iteration 213, Loss: 0.14164526760578156\n",
      "Epoch 4, Iteration 214, Loss: 0.12410422414541245\n",
      "Epoch 4, Iteration 215, Loss: 0.14016978442668915\n",
      "Epoch 4, Iteration 216, Loss: 0.17585304379463196\n",
      "Epoch 4, Iteration 217, Loss: 0.1683281660079956\n",
      "Epoch 4, Iteration 218, Loss: 0.152352437376976\n",
      "Epoch 4, Iteration 219, Loss: 0.15156346559524536\n",
      "Epoch 4, Iteration 220, Loss: 0.12626159191131592\n",
      "Epoch 4, Iteration 221, Loss: 0.16213169693946838\n",
      "Epoch 4, Iteration 222, Loss: 0.16237963736057281\n",
      "Epoch 4, Iteration 223, Loss: 0.13394729793071747\n",
      "Epoch 4, Iteration 224, Loss: 0.12186523526906967\n",
      "Epoch 4, Iteration 225, Loss: 0.13605764508247375\n",
      "Epoch 4, Iteration 226, Loss: 0.1737087368965149\n",
      "Epoch 4, Iteration 227, Loss: 0.1439322680234909\n",
      "Epoch 4, Iteration 228, Loss: 0.25921082496643066\n",
      "Epoch 4, Iteration 229, Loss: 0.24618737399578094\n",
      "Epoch 4, Iteration 230, Loss: 0.16587698459625244\n",
      "Epoch 4, Iteration 231, Loss: 0.09928734600543976\n",
      "Epoch 4, Iteration 232, Loss: 0.16635897755622864\n",
      "Epoch 4, Iteration 233, Loss: 0.17790938913822174\n",
      "Epoch 4, Iteration 234, Loss: 0.11805115640163422\n",
      "Epoch 4, Iteration 235, Loss: 0.1081002727150917\n",
      "Epoch 4, Iteration 236, Loss: 0.16683200001716614\n",
      "Epoch 4, Iteration 237, Loss: 0.1384638100862503\n",
      "Epoch 4, Iteration 238, Loss: 0.15543749928474426\n",
      "Epoch 4, Iteration 239, Loss: 0.14693959057331085\n",
      "Epoch 4, Iteration 240, Loss: 0.13186903297901154\n",
      "Epoch 4, Iteration 241, Loss: 0.1678340584039688\n",
      "Epoch 4, Iteration 242, Loss: 0.1796484887599945\n",
      "Epoch 4, Iteration 243, Loss: 0.1399240791797638\n",
      "Epoch 4, Iteration 244, Loss: 0.1776140332221985\n",
      "Epoch 4, Iteration 245, Loss: 0.15188558399677277\n",
      "Epoch 4, Iteration 246, Loss: 0.16777172684669495\n",
      "Epoch 4, Iteration 247, Loss: 0.278422087430954\n",
      "Epoch 4, Iteration 248, Loss: 0.2088223546743393\n",
      "Epoch 4, Iteration 249, Loss: 0.22655180096626282\n",
      "Epoch 4, Iteration 250, Loss: 0.25181135535240173\n",
      "Epoch 4, Iteration 250, Test Loss: 0.2505961060523987\n",
      "Epoch 4, Iteration 251, Loss: 0.22832933068275452\n",
      "Epoch 4, Iteration 252, Loss: 0.15158680081367493\n",
      "Epoch 4, Iteration 253, Loss: 0.16354811191558838\n",
      "Epoch 4, Iteration 254, Loss: 0.2136479765176773\n",
      "Epoch 4, Iteration 255, Loss: 0.16612640023231506\n",
      "Epoch 4, Iteration 256, Loss: 0.16470178961753845\n",
      "Epoch 4, Iteration 257, Loss: 0.12357104569673538\n",
      "Epoch 4, Iteration 258, Loss: 0.15805652737617493\n",
      "Epoch 4, Iteration 259, Loss: 0.17161184549331665\n",
      "Epoch 4, Iteration 260, Loss: 0.20902679860591888\n",
      "Epoch 4, Iteration 261, Loss: 0.1392616331577301\n",
      "Epoch 4, Iteration 262, Loss: 0.1406547874212265\n",
      "Epoch 4, Iteration 263, Loss: 0.1305527687072754\n",
      "Epoch 4, Iteration 264, Loss: 0.1400216966867447\n",
      "Epoch 4, Iteration 265, Loss: 0.22332189977169037\n",
      "Epoch 4, Iteration 266, Loss: 0.10386282205581665\n",
      "Epoch 4, Iteration 267, Loss: 0.1535853147506714\n",
      "Epoch 4, Iteration 268, Loss: 0.16129367053508759\n",
      "Epoch 4, Iteration 269, Loss: 0.17020130157470703\n",
      "Epoch 4, Iteration 270, Loss: 0.18745088577270508\n",
      "Epoch 4, Iteration 271, Loss: 0.08817199617624283\n",
      "Epoch 4, Iteration 272, Loss: 0.15761879086494446\n",
      "Epoch 4, Iteration 273, Loss: 0.14987485110759735\n",
      "Epoch 4, Iteration 274, Loss: 0.1444646567106247\n",
      "Epoch 4, Iteration 275, Loss: 0.12110603600740433\n",
      "Epoch 4, Iteration 276, Loss: 0.1086343452334404\n",
      "Epoch 4, Iteration 277, Loss: 0.11957161873579025\n",
      "Epoch 4, Iteration 278, Loss: 0.15737667679786682\n",
      "Epoch 4, Iteration 279, Loss: 0.11698245257139206\n",
      "Epoch 4, Iteration 280, Loss: 0.1591976433992386\n",
      "Epoch 4, Iteration 281, Loss: 0.1340702623128891\n",
      "Epoch 4, Iteration 282, Loss: 0.13035744428634644\n",
      "Epoch 4, Iteration 283, Loss: 0.09234828501939774\n",
      "Epoch 4, Iteration 284, Loss: 0.14306114614009857\n",
      "Epoch 4, Iteration 285, Loss: 0.11392638832330704\n",
      "Epoch 4, Iteration 286, Loss: 0.16330882906913757\n",
      "Epoch 4, Iteration 287, Loss: 0.10709235072135925\n",
      "Epoch 4, Iteration 288, Loss: 0.12998096644878387\n",
      "Epoch 4, Iteration 289, Loss: 0.18329966068267822\n",
      "Epoch 4, Iteration 290, Loss: 0.10869345813989639\n",
      "Epoch 4, Iteration 291, Loss: 0.10798415541648865\n",
      "Epoch 4, Iteration 292, Loss: 0.1341707855463028\n",
      "Epoch 4, Iteration 293, Loss: 0.19632835686206818\n",
      "Epoch 4, Iteration 294, Loss: 0.09865334630012512\n",
      "Epoch 4, Iteration 295, Loss: 0.09340600669384003\n",
      "Epoch 4, Iteration 296, Loss: 0.11146613955497742\n",
      "Epoch 4, Iteration 297, Loss: 0.14502224326133728\n",
      "Epoch 4, Iteration 298, Loss: 0.11903899163007736\n",
      "Epoch 4, Iteration 299, Loss: 0.18206852674484253\n",
      "Epoch 4, Iteration 300, Loss: 0.12006156146526337\n",
      "Epoch 4, Iteration 300, Test Loss: 0.27368035912513733\n",
      "Epoch 4, Iteration 301, Loss: 0.09878417104482651\n",
      "Epoch 4, Iteration 302, Loss: 0.10905292630195618\n",
      "Epoch 4, Iteration 303, Loss: 0.1529293954372406\n",
      "Epoch 4, Iteration 304, Loss: 0.19504216313362122\n",
      "Epoch 4, Iteration 305, Loss: 0.10962103307247162\n",
      "Epoch 4, Iteration 306, Loss: 0.11605880409479141\n",
      "Epoch 4, Iteration 307, Loss: 0.1340285986661911\n",
      "Epoch 4, Iteration 308, Loss: 0.11932475119829178\n",
      "Epoch 4, Iteration 309, Loss: 0.1334870308637619\n",
      "Epoch 4, Iteration 310, Loss: 0.1792769432067871\n",
      "Epoch 4, Iteration 311, Loss: 0.12465488165616989\n",
      "Epoch 4, Iteration 312, Loss: 0.156488299369812\n",
      "Epoch 4, Iteration 313, Loss: 0.11712933331727982\n",
      "Epoch 4, Iteration 314, Loss: 0.13366563618183136\n",
      "Epoch 4, Iteration 315, Loss: 0.12057007104158401\n",
      "Epoch 4, Iteration 316, Loss: 0.12103313952684402\n",
      "Epoch 4, Iteration 317, Loss: 0.130479097366333\n",
      "Epoch 4, Iteration 318, Loss: 0.18910937011241913\n",
      "Epoch 4, Iteration 319, Loss: 0.10059482604265213\n",
      "Epoch 4, Iteration 320, Loss: 0.14001992344856262\n",
      "Epoch 4, Iteration 321, Loss: 0.11099116504192352\n",
      "Epoch 4, Iteration 322, Loss: 0.20439855754375458\n",
      "Epoch 4, Iteration 323, Loss: 0.10848008096218109\n",
      "Epoch 4, Iteration 324, Loss: 0.18722544610500336\n",
      "Epoch 4, Iteration 325, Loss: 0.22875939309597015\n",
      "Epoch 4, Iteration 326, Loss: 0.20329751074314117\n",
      "Epoch 4, Iteration 327, Loss: 0.16009457409381866\n",
      "Epoch 4, Iteration 328, Loss: 0.1547979861497879\n",
      "Epoch 4, Iteration 329, Loss: 0.16949094831943512\n",
      "Epoch 4, Iteration 330, Loss: 0.1822565197944641\n",
      "Epoch 4, Iteration 331, Loss: 0.16570384800434113\n",
      "Epoch 4, Iteration 332, Loss: 0.19552850723266602\n",
      "Epoch 4, Iteration 333, Loss: 0.1282360702753067\n",
      "Epoch 4, Iteration 334, Loss: 0.21606625616550446\n",
      "Epoch 4, Iteration 335, Loss: 0.14733155071735382\n",
      "Epoch 4, Iteration 336, Loss: 0.1898980587720871\n",
      "Epoch 4, Iteration 337, Loss: 0.12877768278121948\n",
      "Epoch 4, Iteration 338, Loss: 0.13595478236675262\n",
      "Epoch 4, Iteration 339, Loss: 0.1757732778787613\n",
      "Epoch 4, Iteration 340, Loss: 0.1567411571741104\n",
      "Epoch 4, Iteration 341, Loss: 0.17842116951942444\n",
      "Epoch 4, Iteration 342, Loss: 0.1700422763824463\n",
      "Epoch 4, Iteration 343, Loss: 0.199405699968338\n",
      "Epoch 4, Iteration 344, Loss: 0.21205350756645203\n",
      "Epoch 4, Iteration 345, Loss: 0.15806441009044647\n",
      "Epoch 4, Iteration 346, Loss: 0.14869797229766846\n",
      "Epoch 4, Iteration 347, Loss: 0.18248726427555084\n",
      "Epoch 4, Iteration 348, Loss: 0.19038675725460052\n",
      "Epoch 4, Iteration 349, Loss: 0.20992504060268402\n",
      "Epoch 4, Iteration 350, Loss: 0.2034023553133011\n",
      "Epoch 4, Iteration 350, Test Loss: 0.25371837615966797\n",
      "Epoch 4, Iteration 351, Loss: 0.16779282689094543\n",
      "Epoch 4, Iteration 352, Loss: 0.16039389371871948\n",
      "Epoch 4, Iteration 353, Loss: 0.19388848543167114\n",
      "Epoch 4, Iteration 354, Loss: 0.12032362073659897\n",
      "Epoch 4, Iteration 355, Loss: 0.1465328484773636\n",
      "Epoch 4, Iteration 356, Loss: 0.13622048497200012\n",
      "Epoch 4, Iteration 357, Loss: 0.21145431697368622\n",
      "Epoch 4, Iteration 358, Loss: 0.11746397614479065\n",
      "Epoch 4, Iteration 359, Loss: 0.17807064950466156\n",
      "Epoch 4, Iteration 360, Loss: 0.11147573590278625\n",
      "Epoch 4, Iteration 361, Loss: 0.15807461738586426\n",
      "Epoch 4, Iteration 362, Loss: 0.16215762495994568\n",
      "Epoch 4, Iteration 363, Loss: 0.12658274173736572\n",
      "Epoch 4, Iteration 364, Loss: 0.14395175874233246\n",
      "Epoch 4, Iteration 365, Loss: 0.10958155989646912\n",
      "Epoch 4, Iteration 366, Loss: 0.18459999561309814\n",
      "Epoch 4, Iteration 367, Loss: 0.16364794969558716\n",
      "Epoch 4, Iteration 368, Loss: 0.14520256221294403\n",
      "Epoch 4, Iteration 369, Loss: 0.12461482733488083\n",
      "Epoch 4, Iteration 370, Loss: 0.15699319541454315\n",
      "Epoch 4, Iteration 371, Loss: 0.15492011606693268\n",
      "Epoch 4, Iteration 372, Loss: 0.13565723598003387\n",
      "Epoch 4, Iteration 373, Loss: 0.16315454244613647\n",
      "Epoch 4, Iteration 374, Loss: 0.1121894046664238\n",
      "Epoch 4, Iteration 375, Loss: 0.14729434251785278\n",
      "Epoch 4, Iteration 376, Loss: 0.17812733352184296\n",
      "Epoch 4, Iteration 377, Loss: 0.15149320662021637\n",
      "Epoch 4, Iteration 378, Loss: 0.14083923399448395\n",
      "Epoch 4, Iteration 379, Loss: 0.19513070583343506\n",
      "Epoch 4, Iteration 380, Loss: 0.1713274121284485\n",
      "Epoch 4, Iteration 381, Loss: 0.1423993557691574\n",
      "Epoch 4, Iteration 382, Loss: 0.1424906849861145\n",
      "Epoch 4, Iteration 383, Loss: 0.12993918359279633\n",
      "Epoch 4, Iteration 384, Loss: 0.14759525656700134\n",
      "Epoch 4, Iteration 385, Loss: 0.1480582356452942\n",
      "Epoch 4, Iteration 386, Loss: 0.13403357565402985\n",
      "Epoch 4, Iteration 387, Loss: 0.11114339530467987\n",
      "Epoch 4, Iteration 388, Loss: 0.11531712114810944\n",
      "Epoch 4, Iteration 389, Loss: 0.1219421774148941\n",
      "Epoch 4, Iteration 390, Loss: 0.1524886041879654\n",
      "Epoch 4, Iteration 391, Loss: 0.09426410496234894\n",
      "Epoch 4, Iteration 392, Loss: 0.18034859001636505\n",
      "Epoch 4, Iteration 393, Loss: 0.20124906301498413\n",
      "Epoch 4, Iteration 394, Loss: 0.12623712420463562\n",
      "Epoch 4, Iteration 395, Loss: 0.1482134461402893\n",
      "Epoch 4, Iteration 396, Loss: 0.15024463832378387\n",
      "Epoch 4, Iteration 397, Loss: 0.1477643996477127\n",
      "Epoch 4, Iteration 398, Loss: 0.25014474987983704\n",
      "Epoch 4, Iteration 399, Loss: 0.1295701265335083\n",
      "Epoch 4, Iteration 400, Loss: 0.12994760274887085\n",
      "Epoch 4, Iteration 400, Test Loss: 0.25911301374435425\n",
      "Epoch 4, Iteration 401, Loss: 0.14956815540790558\n",
      "Epoch 4, Iteration 402, Loss: 0.12024126946926117\n",
      "Epoch 4, Iteration 403, Loss: 0.14809469878673553\n",
      "Epoch 4, Iteration 404, Loss: 0.15267159044742584\n",
      "Epoch 4, Iteration 405, Loss: 0.10533268749713898\n",
      "Epoch 4, Iteration 406, Loss: 0.10837443172931671\n",
      "Epoch 4, Iteration 407, Loss: 0.1478535234928131\n",
      "Epoch 4, Iteration 408, Loss: 0.1689130961894989\n",
      "Epoch 4, Iteration 409, Loss: 0.1071503758430481\n",
      "Epoch 4, Iteration 410, Loss: 0.19371013343334198\n",
      "Epoch 4, Iteration 411, Loss: 0.1865137219429016\n",
      "Epoch 4, Iteration 412, Loss: 0.15431003272533417\n",
      "Epoch 4, Iteration 413, Loss: 0.16344483196735382\n",
      "Epoch 4, Iteration 414, Loss: 0.09810982644557953\n",
      "Epoch 4, Iteration 415, Loss: 0.1561095416545868\n",
      "Epoch 4, Iteration 416, Loss: 0.13726148009300232\n",
      "Epoch 4, Iteration 417, Loss: 0.15658922493457794\n",
      "Epoch 4, Iteration 418, Loss: 0.11793427914381027\n",
      "Epoch 4, Iteration 419, Loss: 0.14148752391338348\n",
      "Epoch 4, Iteration 420, Loss: 0.14926619827747345\n",
      "Epoch 4, Iteration 421, Loss: 0.10877515375614166\n",
      "Epoch 4, Iteration 422, Loss: 0.13093256950378418\n",
      "Epoch 4, Iteration 423, Loss: 0.13467209041118622\n",
      "Epoch 4, Iteration 424, Loss: 0.15705347061157227\n",
      "Epoch 4, Iteration 425, Loss: 0.13788791000843048\n",
      "Epoch 4, Iteration 426, Loss: 0.13434746861457825\n",
      "Epoch 4, Iteration 427, Loss: 0.1249183788895607\n",
      "Epoch 4, Iteration 428, Loss: 0.1456371694803238\n",
      "Epoch 4, Iteration 429, Loss: 0.16427268087863922\n",
      "Epoch 4, Iteration 430, Loss: 0.14205466210842133\n",
      "Epoch 4, Iteration 431, Loss: 0.11580042541027069\n",
      "Epoch 5/15, Loss: 0.15979833956117984\n",
      "Epoch 5/15, Validation Accuracy: 0.9403162055335968\n",
      "Epoch 5, Iteration 0, Loss: 0.3017849624156952\n",
      "Epoch 5, Iteration 1, Loss: 0.20186689496040344\n",
      "Epoch 5, Iteration 2, Loss: 0.24499790370464325\n",
      "Epoch 5, Iteration 3, Loss: 0.14989693462848663\n",
      "Epoch 5, Iteration 4, Loss: 0.21692992746829987\n",
      "Epoch 5, Iteration 5, Loss: 0.2774300277233124\n",
      "Epoch 5, Iteration 6, Loss: 0.2169114202260971\n",
      "Epoch 5, Iteration 7, Loss: 0.2624509930610657\n",
      "Epoch 5, Iteration 8, Loss: 0.2567548155784607\n",
      "Epoch 5, Iteration 9, Loss: 0.19132302701473236\n",
      "Epoch 5, Iteration 10, Loss: 0.1919088363647461\n",
      "Epoch 5, Iteration 11, Loss: 0.18593472242355347\n",
      "Epoch 5, Iteration 12, Loss: 0.22085219621658325\n",
      "Epoch 5, Iteration 13, Loss: 0.1928987205028534\n",
      "Epoch 5, Iteration 14, Loss: 0.31913071870803833\n",
      "Epoch 5, Iteration 15, Loss: 0.11928702145814896\n",
      "Epoch 5, Iteration 16, Loss: 0.2582029402256012\n",
      "Epoch 5, Iteration 17, Loss: 0.1996804028749466\n",
      "Epoch 5, Iteration 18, Loss: 0.17763131856918335\n",
      "Epoch 5, Iteration 19, Loss: 0.18904641270637512\n",
      "Epoch 5, Iteration 20, Loss: 0.2521609663963318\n",
      "Epoch 5, Iteration 21, Loss: 0.2483423352241516\n",
      "Epoch 5, Iteration 22, Loss: 0.20253631472587585\n",
      "Epoch 5, Iteration 23, Loss: 0.23292464017868042\n",
      "Epoch 5, Iteration 24, Loss: 0.23057876527309418\n",
      "Epoch 5, Iteration 25, Loss: 0.2485944777727127\n",
      "Epoch 5, Iteration 26, Loss: 0.2120848298072815\n",
      "Epoch 5, Iteration 27, Loss: 0.1570713371038437\n",
      "Epoch 5, Iteration 28, Loss: 0.24910037219524384\n",
      "Epoch 5, Iteration 29, Loss: 0.1780785173177719\n",
      "Epoch 5, Iteration 30, Loss: 0.21829624474048615\n",
      "Epoch 5, Iteration 31, Loss: 0.24150493741035461\n",
      "Epoch 5, Iteration 32, Loss: 0.21479575335979462\n",
      "Epoch 5, Iteration 33, Loss: 0.182052344083786\n",
      "Epoch 5, Iteration 34, Loss: 0.21803730726242065\n",
      "Epoch 5, Iteration 35, Loss: 0.16018065810203552\n",
      "Epoch 5, Iteration 36, Loss: 0.21064642071723938\n",
      "Epoch 5, Iteration 37, Loss: 0.1874282956123352\n",
      "Epoch 5, Iteration 38, Loss: 0.21797268092632294\n",
      "Epoch 5, Iteration 39, Loss: 0.18959975242614746\n",
      "Epoch 5, Iteration 40, Loss: 0.17103180289268494\n",
      "Epoch 5, Iteration 41, Loss: 0.19190913438796997\n",
      "Epoch 5, Iteration 42, Loss: 0.18800519406795502\n",
      "Epoch 5, Iteration 43, Loss: 0.21105612814426422\n",
      "Epoch 5, Iteration 44, Loss: 0.1667812019586563\n",
      "Epoch 5, Iteration 45, Loss: 0.19543816149234772\n",
      "Epoch 5, Iteration 46, Loss: 0.2027452141046524\n",
      "Epoch 5, Iteration 47, Loss: 0.2044157236814499\n",
      "Epoch 5, Iteration 48, Loss: 0.24036741256713867\n",
      "Epoch 5, Iteration 49, Loss: 0.12827613949775696\n",
      "Epoch 5, Iteration 50, Loss: 0.14156872034072876\n",
      "Epoch 5, Iteration 50, Test Loss: 0.22062131762504578\n",
      "Epoch 5, Iteration 51, Loss: 0.14108571410179138\n",
      "Epoch 5, Iteration 52, Loss: 0.2112022042274475\n",
      "Epoch 5, Iteration 53, Loss: 0.1380075216293335\n",
      "Epoch 5, Iteration 54, Loss: 0.16589415073394775\n",
      "Epoch 5, Iteration 55, Loss: 0.13930024206638336\n",
      "Epoch 5, Iteration 56, Loss: 0.14779812097549438\n",
      "Epoch 5, Iteration 57, Loss: 0.15323829650878906\n",
      "Epoch 5, Iteration 58, Loss: 0.1430777758359909\n",
      "Epoch 5, Iteration 59, Loss: 0.1315840184688568\n",
      "Epoch 5, Iteration 60, Loss: 0.16449286043643951\n",
      "Epoch 5, Iteration 61, Loss: 0.12602247297763824\n",
      "Epoch 5, Iteration 62, Loss: 0.2513585090637207\n",
      "Epoch 5, Iteration 63, Loss: 0.11721407622098923\n",
      "Epoch 5, Iteration 64, Loss: 0.13271859288215637\n",
      "Epoch 5, Iteration 65, Loss: 0.16748443245887756\n",
      "Epoch 5, Iteration 66, Loss: 0.1725156009197235\n",
      "Epoch 5, Iteration 67, Loss: 0.12150141596794128\n",
      "Epoch 5, Iteration 68, Loss: 0.14135868847370148\n",
      "Epoch 5, Iteration 69, Loss: 0.1843119114637375\n",
      "Epoch 5, Iteration 70, Loss: 0.12009779363870621\n",
      "Epoch 5, Iteration 71, Loss: 0.14383694529533386\n",
      "Epoch 5, Iteration 72, Loss: 0.16373570263385773\n",
      "Epoch 5, Iteration 73, Loss: 0.16136150062084198\n",
      "Epoch 5, Iteration 74, Loss: 0.18360860645771027\n",
      "Epoch 5, Iteration 75, Loss: 0.18167158961296082\n",
      "Epoch 5, Iteration 76, Loss: 0.15328408777713776\n",
      "Epoch 5, Iteration 77, Loss: 0.1509803831577301\n",
      "Epoch 5, Iteration 78, Loss: 0.14916759729385376\n",
      "Epoch 5, Iteration 79, Loss: 0.11294513940811157\n",
      "Epoch 5, Iteration 80, Loss: 0.18502463400363922\n",
      "Epoch 5, Iteration 81, Loss: 0.15992029011249542\n",
      "Epoch 5, Iteration 82, Loss: 0.17518162727355957\n",
      "Epoch 5, Iteration 83, Loss: 0.15073220431804657\n",
      "Epoch 5, Iteration 84, Loss: 0.1358845829963684\n",
      "Epoch 5, Iteration 85, Loss: 0.2161785066127777\n",
      "Epoch 5, Iteration 86, Loss: 0.17029328644275665\n",
      "Epoch 5, Iteration 87, Loss: 0.16141392290592194\n",
      "Epoch 5, Iteration 88, Loss: 0.14218974113464355\n",
      "Epoch 5, Iteration 89, Loss: 0.15995794534683228\n",
      "Epoch 5, Iteration 90, Loss: 0.14226596057415009\n",
      "Epoch 5, Iteration 91, Loss: 0.12809990346431732\n",
      "Epoch 5, Iteration 92, Loss: 0.11820961534976959\n",
      "Epoch 5, Iteration 93, Loss: 0.1227986142039299\n",
      "Epoch 5, Iteration 94, Loss: 0.21817165613174438\n",
      "Epoch 5, Iteration 95, Loss: 0.1380779892206192\n",
      "Epoch 5, Iteration 96, Loss: 0.0883510410785675\n",
      "Epoch 5, Iteration 97, Loss: 0.14878368377685547\n",
      "Epoch 5, Iteration 98, Loss: 0.16927027702331543\n",
      "Epoch 5, Iteration 99, Loss: 0.13260996341705322\n",
      "Epoch 5, Iteration 100, Loss: 0.12855948507785797\n",
      "Epoch 5, Iteration 100, Test Loss: 0.24834851920604706\n",
      "Epoch 5, Iteration 101, Loss: 0.12877820432186127\n",
      "Epoch 5, Iteration 102, Loss: 0.13673755526542664\n",
      "Epoch 5, Iteration 103, Loss: 0.11852675676345825\n",
      "Epoch 5, Iteration 104, Loss: 0.14504197239875793\n",
      "Epoch 5, Iteration 105, Loss: 0.1287612020969391\n",
      "Epoch 5, Iteration 106, Loss: 0.09326557070016861\n",
      "Epoch 5, Iteration 107, Loss: 0.09792552888393402\n",
      "Epoch 5, Iteration 108, Loss: 0.19615140557289124\n",
      "Epoch 5, Iteration 109, Loss: 0.2084619253873825\n",
      "Epoch 5, Iteration 110, Loss: 0.1262817233800888\n",
      "Epoch 5, Iteration 111, Loss: 0.10416201502084732\n",
      "Epoch 5, Iteration 112, Loss: 0.1517871916294098\n",
      "Epoch 5, Iteration 113, Loss: 0.12308534979820251\n",
      "Epoch 5, Iteration 114, Loss: 0.12117723375558853\n",
      "Epoch 5, Iteration 115, Loss: 0.14892248809337616\n",
      "Epoch 5, Iteration 116, Loss: 0.16659444570541382\n",
      "Epoch 5, Iteration 117, Loss: 0.223961740732193\n",
      "Epoch 5, Iteration 118, Loss: 0.16383135318756104\n",
      "Epoch 5, Iteration 119, Loss: 0.18360471725463867\n",
      "Epoch 5, Iteration 120, Loss: 0.11588382720947266\n",
      "Epoch 5, Iteration 121, Loss: 0.17203004658222198\n",
      "Epoch 5, Iteration 122, Loss: 0.163350448012352\n",
      "Epoch 5, Iteration 123, Loss: 0.13902278244495392\n",
      "Epoch 5, Iteration 124, Loss: 0.13483071327209473\n",
      "Epoch 5, Iteration 125, Loss: 0.1887027621269226\n",
      "Epoch 5, Iteration 126, Loss: 0.15694725513458252\n",
      "Epoch 5, Iteration 127, Loss: 0.16044366359710693\n",
      "Epoch 5, Iteration 128, Loss: 0.12572385370731354\n",
      "Epoch 5, Iteration 129, Loss: 0.1930008977651596\n",
      "Epoch 5, Iteration 130, Loss: 0.10023561865091324\n",
      "Epoch 5, Iteration 131, Loss: 0.14585521817207336\n",
      "Epoch 5, Iteration 132, Loss: 0.17835481464862823\n",
      "Epoch 5, Iteration 133, Loss: 0.22231021523475647\n",
      "Epoch 5, Iteration 134, Loss: 0.1545342206954956\n",
      "Epoch 5, Iteration 135, Loss: 0.16749444603919983\n",
      "Epoch 5, Iteration 136, Loss: 0.10738522559404373\n",
      "Epoch 5, Iteration 137, Loss: 0.12399186939001083\n",
      "Epoch 5, Iteration 138, Loss: 0.155574232339859\n",
      "Epoch 5, Iteration 139, Loss: 0.14412620663642883\n",
      "Epoch 5, Iteration 140, Loss: 0.20138895511627197\n",
      "Epoch 5, Iteration 141, Loss: 0.1557682305574417\n",
      "Epoch 5, Iteration 142, Loss: 0.18224264681339264\n",
      "Epoch 5, Iteration 143, Loss: 0.15247666835784912\n",
      "Epoch 5, Iteration 144, Loss: 0.10495448857545853\n",
      "Epoch 5, Iteration 145, Loss: 0.17438514530658722\n",
      "Epoch 5, Iteration 146, Loss: 0.11862292885780334\n",
      "Epoch 5, Iteration 147, Loss: 0.12970298528671265\n",
      "Epoch 5, Iteration 148, Loss: 0.13217012584209442\n",
      "Epoch 5, Iteration 149, Loss: 0.13725543022155762\n",
      "Epoch 5, Iteration 150, Loss: 0.16370612382888794\n",
      "Epoch 5, Iteration 150, Test Loss: 0.24349790811538696\n",
      "Epoch 5, Iteration 151, Loss: 0.1529214233160019\n",
      "Epoch 5, Iteration 152, Loss: 0.16736243665218353\n",
      "Epoch 5, Iteration 153, Loss: 0.14733023941516876\n",
      "Epoch 5, Iteration 154, Loss: 0.13562431931495667\n",
      "Epoch 5, Iteration 155, Loss: 0.16050481796264648\n",
      "Epoch 5, Iteration 156, Loss: 0.1839493066072464\n",
      "Epoch 5, Iteration 157, Loss: 0.21742050349712372\n",
      "Epoch 5, Iteration 158, Loss: 0.2099483758211136\n",
      "Epoch 5, Iteration 159, Loss: 0.13463331758975983\n",
      "Epoch 5, Iteration 160, Loss: 0.1737196445465088\n",
      "Epoch 5, Iteration 161, Loss: 0.10452631115913391\n",
      "Epoch 5, Iteration 162, Loss: 0.10057087242603302\n",
      "Epoch 5, Iteration 163, Loss: 0.15099231898784637\n",
      "Epoch 5, Iteration 164, Loss: 0.10623612254858017\n",
      "Epoch 5, Iteration 165, Loss: 0.12413356453180313\n",
      "Epoch 5, Iteration 166, Loss: 0.14551839232444763\n",
      "Epoch 5, Iteration 167, Loss: 0.13493838906288147\n",
      "Epoch 5, Iteration 168, Loss: 0.12242527306079865\n",
      "Epoch 5, Iteration 169, Loss: 0.14739437401294708\n",
      "Epoch 5, Iteration 170, Loss: 0.08609197288751602\n",
      "Epoch 5, Iteration 171, Loss: 0.2030269205570221\n",
      "Epoch 5, Iteration 172, Loss: 0.19670803844928741\n",
      "Epoch 5, Iteration 173, Loss: 0.14858320355415344\n",
      "Epoch 5, Iteration 174, Loss: 0.15056967735290527\n",
      "Epoch 5, Iteration 175, Loss: 0.12368045747280121\n",
      "Epoch 5, Iteration 176, Loss: 0.1194583997130394\n",
      "Epoch 5, Iteration 177, Loss: 0.17581571638584137\n",
      "Epoch 5, Iteration 178, Loss: 0.1541813760995865\n",
      "Epoch 5, Iteration 179, Loss: 0.122020423412323\n",
      "Epoch 5, Iteration 180, Loss: 0.17463582754135132\n",
      "Epoch 5, Iteration 181, Loss: 0.12741898000240326\n",
      "Epoch 5, Iteration 182, Loss: 0.13232743740081787\n",
      "Epoch 5, Iteration 183, Loss: 0.14571884274482727\n",
      "Epoch 5, Iteration 184, Loss: 0.20564095675945282\n",
      "Epoch 5, Iteration 185, Loss: 0.11624504625797272\n",
      "Epoch 5, Iteration 186, Loss: 0.1303732842206955\n",
      "Epoch 5, Iteration 187, Loss: 0.1404973417520523\n",
      "Epoch 5, Iteration 188, Loss: 0.10562282800674438\n",
      "Epoch 5, Iteration 189, Loss: 0.0715877041220665\n",
      "Epoch 5, Iteration 190, Loss: 0.12286880612373352\n",
      "Epoch 5, Iteration 191, Loss: 0.10001696646213531\n",
      "Epoch 5, Iteration 192, Loss: 0.13860568404197693\n",
      "Epoch 5, Iteration 193, Loss: 0.1658676117658615\n",
      "Epoch 5, Iteration 194, Loss: 0.12436777353286743\n",
      "Epoch 5, Iteration 195, Loss: 0.14578211307525635\n",
      "Epoch 5, Iteration 196, Loss: 0.24208766222000122\n",
      "Epoch 5, Iteration 197, Loss: 0.12292202562093735\n",
      "Epoch 5, Iteration 198, Loss: 0.11561932414770126\n",
      "Epoch 5, Iteration 199, Loss: 0.1439463198184967\n",
      "Epoch 5, Iteration 200, Loss: 0.10770125687122345\n",
      "Epoch 5, Iteration 200, Test Loss: 0.2498648315668106\n",
      "Epoch 5, Iteration 201, Loss: 0.13444991409778595\n",
      "Epoch 5, Iteration 202, Loss: 0.09376347064971924\n",
      "Epoch 5, Iteration 203, Loss: 0.10777123272418976\n",
      "Epoch 5, Iteration 204, Loss: 0.13598822057247162\n",
      "Epoch 5, Iteration 205, Loss: 0.17732098698616028\n",
      "Epoch 5, Iteration 206, Loss: 0.11413328349590302\n",
      "Epoch 5, Iteration 207, Loss: 0.11838842928409576\n",
      "Epoch 5, Iteration 208, Loss: 0.09414810687303543\n",
      "Epoch 5, Iteration 209, Loss: 0.1547301858663559\n",
      "Epoch 5, Iteration 210, Loss: 0.12023566663265228\n",
      "Epoch 5, Iteration 211, Loss: 0.15824446082115173\n",
      "Epoch 5, Iteration 212, Loss: 0.15704166889190674\n",
      "Epoch 5, Iteration 213, Loss: 0.1331183910369873\n",
      "Epoch 5, Iteration 214, Loss: 0.11945652961730957\n",
      "Epoch 5, Iteration 215, Loss: 0.13727040588855743\n",
      "Epoch 5, Iteration 216, Loss: 0.16264139115810394\n",
      "Epoch 5, Iteration 217, Loss: 0.18195752799510956\n",
      "Epoch 5, Iteration 218, Loss: 0.1435595601797104\n",
      "Epoch 5, Iteration 219, Loss: 0.14776024222373962\n",
      "Epoch 5, Iteration 220, Loss: 0.10086584091186523\n",
      "Epoch 5, Iteration 221, Loss: 0.16285493969917297\n",
      "Epoch 5, Iteration 222, Loss: 0.1233203262090683\n",
      "Epoch 5, Iteration 223, Loss: 0.13435043394565582\n",
      "Epoch 5, Iteration 224, Loss: 0.1158931627869606\n",
      "Epoch 5, Iteration 225, Loss: 0.1436191350221634\n",
      "Epoch 5, Iteration 226, Loss: 0.15125741064548492\n",
      "Epoch 5, Iteration 227, Loss: 0.1236751526594162\n",
      "Epoch 5, Iteration 228, Loss: 0.2789704501628876\n",
      "Epoch 5, Iteration 229, Loss: 0.21396924555301666\n",
      "Epoch 5, Iteration 230, Loss: 0.16685087978839874\n",
      "Epoch 5, Iteration 231, Loss: 0.11508256942033768\n",
      "Epoch 5, Iteration 232, Loss: 0.1336742490530014\n",
      "Epoch 5, Iteration 233, Loss: 0.1739518791437149\n",
      "Epoch 5, Iteration 234, Loss: 0.10744492709636688\n",
      "Epoch 5, Iteration 235, Loss: 0.0936630591750145\n",
      "Epoch 5, Iteration 236, Loss: 0.1764139086008072\n",
      "Epoch 5, Iteration 237, Loss: 0.13062238693237305\n",
      "Epoch 5, Iteration 238, Loss: 0.14698117971420288\n",
      "Epoch 5, Iteration 239, Loss: 0.1263783872127533\n",
      "Epoch 5, Iteration 240, Loss: 0.12554433941841125\n",
      "Epoch 5, Iteration 241, Loss: 0.14874325692653656\n",
      "Epoch 5, Iteration 242, Loss: 0.17248395085334778\n",
      "Epoch 5, Iteration 243, Loss: 0.1490325629711151\n",
      "Epoch 5, Iteration 244, Loss: 0.17021974921226501\n",
      "Epoch 5, Iteration 245, Loss: 0.14976230263710022\n",
      "Epoch 5, Iteration 246, Loss: 0.16580168902873993\n",
      "Epoch 5, Iteration 247, Loss: 0.27407389879226685\n",
      "Epoch 5, Iteration 248, Loss: 0.22961358726024628\n",
      "Epoch 5, Iteration 249, Loss: 0.22615890204906464\n",
      "Epoch 5, Iteration 250, Loss: 0.23857399821281433\n",
      "Epoch 5, Iteration 250, Test Loss: 0.2530200481414795\n",
      "Epoch 5, Iteration 251, Loss: 0.20980015397071838\n",
      "Epoch 5, Iteration 252, Loss: 0.12988658249378204\n",
      "Epoch 5, Iteration 253, Loss: 0.1583876758813858\n",
      "Epoch 5, Iteration 254, Loss: 0.19459415972232819\n",
      "Epoch 5, Iteration 255, Loss: 0.17605850100517273\n",
      "Epoch 5, Iteration 256, Loss: 0.15896321833133698\n",
      "Epoch 5, Iteration 257, Loss: 0.1478099375963211\n",
      "Epoch 5, Iteration 258, Loss: 0.14736901223659515\n",
      "Epoch 5, Iteration 259, Loss: 0.1609600931406021\n",
      "Epoch 5, Iteration 260, Loss: 0.1863492727279663\n",
      "Epoch 5, Iteration 261, Loss: 0.13863688707351685\n",
      "Epoch 5, Iteration 262, Loss: 0.1135641485452652\n",
      "Epoch 5, Iteration 263, Loss: 0.14073407649993896\n",
      "Epoch 5, Iteration 264, Loss: 0.14047124981880188\n",
      "Epoch 5, Iteration 265, Loss: 0.2339634895324707\n",
      "Epoch 5, Iteration 266, Loss: 0.12174303829669952\n",
      "Epoch 5, Iteration 267, Loss: 0.16021263599395752\n",
      "Epoch 5, Iteration 268, Loss: 0.16755914688110352\n",
      "Epoch 5, Iteration 269, Loss: 0.15929964184761047\n",
      "Epoch 5, Iteration 270, Loss: 0.18005825579166412\n",
      "Epoch 5, Iteration 271, Loss: 0.09003322571516037\n",
      "Epoch 5, Iteration 272, Loss: 0.14556720852851868\n",
      "Epoch 5, Iteration 273, Loss: 0.1619734764099121\n",
      "Epoch 5, Iteration 274, Loss: 0.12329036742448807\n",
      "Epoch 5, Iteration 275, Loss: 0.11075190454721451\n",
      "Epoch 5, Iteration 276, Loss: 0.12205930054187775\n",
      "Epoch 5, Iteration 277, Loss: 0.12661446630954742\n",
      "Epoch 5, Iteration 278, Loss: 0.16451913118362427\n",
      "Epoch 5, Iteration 279, Loss: 0.09897692501544952\n",
      "Epoch 5, Iteration 280, Loss: 0.16020788252353668\n",
      "Epoch 5, Iteration 281, Loss: 0.1332184076309204\n",
      "Epoch 5, Iteration 282, Loss: 0.11899925023317337\n",
      "Epoch 5, Iteration 283, Loss: 0.09334754943847656\n",
      "Epoch 5, Iteration 284, Loss: 0.13212153315544128\n",
      "Epoch 5, Iteration 285, Loss: 0.1209515631198883\n",
      "Epoch 5, Iteration 286, Loss: 0.1660880744457245\n",
      "Epoch 5, Iteration 287, Loss: 0.08582548052072525\n",
      "Epoch 5, Iteration 288, Loss: 0.1325942873954773\n",
      "Epoch 5, Iteration 289, Loss: 0.19295531511306763\n",
      "Epoch 5, Iteration 290, Loss: 0.11048800498247147\n",
      "Epoch 5, Iteration 291, Loss: 0.10177783668041229\n",
      "Epoch 5, Iteration 292, Loss: 0.14492842555046082\n",
      "Epoch 5, Iteration 293, Loss: 0.16460645198822021\n",
      "Epoch 5, Iteration 294, Loss: 0.1170976534485817\n",
      "Epoch 5, Iteration 295, Loss: 0.08887983858585358\n",
      "Epoch 5, Iteration 296, Loss: 0.12354045361280441\n",
      "Epoch 5, Iteration 297, Loss: 0.1383705586194992\n",
      "Epoch 5, Iteration 298, Loss: 0.10270146280527115\n",
      "Epoch 5, Iteration 299, Loss: 0.1775161325931549\n",
      "Epoch 5, Iteration 300, Loss: 0.1259269416332245\n",
      "Epoch 5, Iteration 300, Test Loss: 0.27016162872314453\n",
      "Epoch 5, Iteration 301, Loss: 0.1102217361330986\n",
      "Epoch 5, Iteration 302, Loss: 0.10155410319566727\n",
      "Epoch 5, Iteration 303, Loss: 0.15735626220703125\n",
      "Epoch 5, Iteration 304, Loss: 0.18003062903881073\n",
      "Epoch 5, Iteration 305, Loss: 0.11442597955465317\n",
      "Epoch 5, Iteration 306, Loss: 0.12909214198589325\n",
      "Epoch 5, Iteration 307, Loss: 0.11692483723163605\n",
      "Epoch 5, Iteration 308, Loss: 0.11826588213443756\n",
      "Epoch 5, Iteration 309, Loss: 0.11504384875297546\n",
      "Epoch 5, Iteration 310, Loss: 0.1414753794670105\n",
      "Epoch 5, Iteration 311, Loss: 0.13058151304721832\n",
      "Epoch 5, Iteration 312, Loss: 0.1604258120059967\n",
      "Epoch 5, Iteration 313, Loss: 0.10092897713184357\n",
      "Epoch 5, Iteration 314, Loss: 0.11251882463693619\n",
      "Epoch 5, Iteration 315, Loss: 0.14203514158725739\n",
      "Epoch 5, Iteration 316, Loss: 0.10375389456748962\n",
      "Epoch 5, Iteration 317, Loss: 0.13308988511562347\n",
      "Epoch 5, Iteration 318, Loss: 0.18923284113407135\n",
      "Epoch 5, Iteration 319, Loss: 0.11116417497396469\n",
      "Epoch 5, Iteration 320, Loss: 0.14144740998744965\n",
      "Epoch 5, Iteration 321, Loss: 0.09627308696508408\n",
      "Epoch 5, Iteration 322, Loss: 0.17273543775081635\n",
      "Epoch 5, Iteration 323, Loss: 0.11410034447908401\n",
      "Epoch 5, Iteration 324, Loss: 0.16525520384311676\n",
      "Epoch 5, Iteration 325, Loss: 0.23288443684577942\n",
      "Epoch 5, Iteration 326, Loss: 0.19135728478431702\n",
      "Epoch 5, Iteration 327, Loss: 0.16586537659168243\n",
      "Epoch 5, Iteration 328, Loss: 0.13741865754127502\n",
      "Epoch 5, Iteration 329, Loss: 0.14524289965629578\n",
      "Epoch 5, Iteration 330, Loss: 0.18493512272834778\n",
      "Epoch 5, Iteration 331, Loss: 0.14677053689956665\n",
      "Epoch 5, Iteration 332, Loss: 0.16702215373516083\n",
      "Epoch 5, Iteration 333, Loss: 0.12691667675971985\n",
      "Epoch 5, Iteration 334, Loss: 0.22247043251991272\n",
      "Epoch 5, Iteration 335, Loss: 0.16166511178016663\n",
      "Epoch 5, Iteration 336, Loss: 0.194660022854805\n",
      "Epoch 5, Iteration 337, Loss: 0.1287529468536377\n",
      "Epoch 5, Iteration 338, Loss: 0.13687723875045776\n",
      "Epoch 5, Iteration 339, Loss: 0.1776074469089508\n",
      "Epoch 5, Iteration 340, Loss: 0.14575783908367157\n",
      "Epoch 5, Iteration 341, Loss: 0.16116772592067719\n",
      "Epoch 5, Iteration 342, Loss: 0.1426057517528534\n",
      "Epoch 5, Iteration 343, Loss: 0.18986776471138\n",
      "Epoch 5, Iteration 344, Loss: 0.18753544986248016\n",
      "Epoch 5, Iteration 345, Loss: 0.1565719097852707\n",
      "Epoch 5, Iteration 346, Loss: 0.14732098579406738\n",
      "Epoch 5, Iteration 347, Loss: 0.16677828133106232\n",
      "Epoch 5, Iteration 348, Loss: 0.17235150933265686\n",
      "Epoch 5, Iteration 349, Loss: 0.19109570980072021\n",
      "Epoch 5, Iteration 350, Loss: 0.2126704603433609\n",
      "Epoch 5, Iteration 350, Test Loss: 0.2529180645942688\n",
      "Epoch 5, Iteration 351, Loss: 0.14572516083717346\n",
      "Epoch 5, Iteration 352, Loss: 0.15546488761901855\n",
      "Epoch 5, Iteration 353, Loss: 0.22026193141937256\n",
      "Epoch 5, Iteration 354, Loss: 0.12249785661697388\n",
      "Epoch 5, Iteration 355, Loss: 0.14452670514583588\n",
      "Epoch 5, Iteration 356, Loss: 0.1241849735379219\n",
      "Epoch 5, Iteration 357, Loss: 0.22572550177574158\n",
      "Epoch 5, Iteration 358, Loss: 0.11681883782148361\n",
      "Epoch 5, Iteration 359, Loss: 0.1844610869884491\n",
      "Epoch 5, Iteration 360, Loss: 0.0965198278427124\n",
      "Epoch 5, Iteration 361, Loss: 0.13808013498783112\n",
      "Epoch 5, Iteration 362, Loss: 0.16438469290733337\n",
      "Epoch 5, Iteration 363, Loss: 0.1121605783700943\n",
      "Epoch 5, Iteration 364, Loss: 0.12340056896209717\n",
      "Epoch 5, Iteration 365, Loss: 0.1066834032535553\n",
      "Epoch 5, Iteration 366, Loss: 0.18539117276668549\n",
      "Epoch 5, Iteration 367, Loss: 0.15000396966934204\n",
      "Epoch 5, Iteration 368, Loss: 0.14391113817691803\n",
      "Epoch 5, Iteration 369, Loss: 0.13320952653884888\n",
      "Epoch 5, Iteration 370, Loss: 0.15806171298027039\n",
      "Epoch 5, Iteration 371, Loss: 0.15016427636146545\n",
      "Epoch 5, Iteration 372, Loss: 0.11293713748455048\n",
      "Epoch 5, Iteration 373, Loss: 0.17329157888889313\n",
      "Epoch 5, Iteration 374, Loss: 0.1269693523645401\n",
      "Epoch 5, Iteration 375, Loss: 0.16783539950847626\n",
      "Epoch 5, Iteration 376, Loss: 0.14034324884414673\n",
      "Epoch 5, Iteration 377, Loss: 0.15289795398712158\n",
      "Epoch 5, Iteration 378, Loss: 0.12590453028678894\n",
      "Epoch 5, Iteration 379, Loss: 0.17596516013145447\n",
      "Epoch 5, Iteration 380, Loss: 0.15114827454090118\n",
      "Epoch 5, Iteration 381, Loss: 0.13440753519535065\n",
      "Epoch 5, Iteration 382, Loss: 0.14584186673164368\n",
      "Epoch 5, Iteration 383, Loss: 0.12203114479780197\n",
      "Epoch 5, Iteration 384, Loss: 0.15274803340435028\n",
      "Epoch 5, Iteration 385, Loss: 0.1536979377269745\n",
      "Epoch 5, Iteration 386, Loss: 0.1275480091571808\n",
      "Epoch 5, Iteration 387, Loss: 0.1150280088186264\n",
      "Epoch 5, Iteration 388, Loss: 0.126332625746727\n",
      "Epoch 5, Iteration 389, Loss: 0.09977006167173386\n",
      "Epoch 5, Iteration 390, Loss: 0.13632838428020477\n",
      "Epoch 5, Iteration 391, Loss: 0.09005359560251236\n",
      "Epoch 5, Iteration 392, Loss: 0.18143896758556366\n",
      "Epoch 5, Iteration 393, Loss: 0.17895829677581787\n",
      "Epoch 5, Iteration 394, Loss: 0.1205277368426323\n",
      "Epoch 5, Iteration 395, Loss: 0.1406983733177185\n",
      "Epoch 5, Iteration 396, Loss: 0.13053607940673828\n",
      "Epoch 5, Iteration 397, Loss: 0.13552287220954895\n",
      "Epoch 5, Iteration 398, Loss: 0.22486668825149536\n",
      "Epoch 5, Iteration 399, Loss: 0.11847459524869919\n",
      "Epoch 5, Iteration 400, Loss: 0.1215180903673172\n",
      "Epoch 5, Iteration 400, Test Loss: 0.26025763154029846\n",
      "Epoch 5, Iteration 401, Loss: 0.16388247907161713\n",
      "Epoch 5, Iteration 402, Loss: 0.08927617222070694\n",
      "Epoch 5, Iteration 403, Loss: 0.12337163835763931\n",
      "Epoch 5, Iteration 404, Loss: 0.11909961700439453\n",
      "Epoch 5, Iteration 405, Loss: 0.09254825860261917\n",
      "Epoch 5, Iteration 406, Loss: 0.11084252595901489\n",
      "Epoch 5, Iteration 407, Loss: 0.15278388559818268\n",
      "Epoch 5, Iteration 408, Loss: 0.16167008876800537\n",
      "Epoch 5, Iteration 409, Loss: 0.09554518014192581\n",
      "Epoch 5, Iteration 410, Loss: 0.19398954510688782\n",
      "Epoch 5, Iteration 411, Loss: 0.19622032344341278\n",
      "Epoch 5, Iteration 412, Loss: 0.14416933059692383\n",
      "Epoch 5, Iteration 413, Loss: 0.1772635579109192\n",
      "Epoch 5, Iteration 414, Loss: 0.10592249035835266\n",
      "Epoch 5, Iteration 415, Loss: 0.13276581466197968\n",
      "Epoch 5, Iteration 416, Loss: 0.11610718071460724\n",
      "Epoch 5, Iteration 417, Loss: 0.14423739910125732\n",
      "Epoch 5, Iteration 418, Loss: 0.1314319372177124\n",
      "Epoch 5, Iteration 419, Loss: 0.12940433621406555\n",
      "Epoch 5, Iteration 420, Loss: 0.15052083134651184\n",
      "Epoch 5, Iteration 421, Loss: 0.10547132045030594\n",
      "Epoch 5, Iteration 422, Loss: 0.11619628965854645\n",
      "Epoch 5, Iteration 423, Loss: 0.11477906256914139\n",
      "Epoch 5, Iteration 424, Loss: 0.15527068078517914\n",
      "Epoch 5, Iteration 425, Loss: 0.15561670064926147\n",
      "Epoch 5, Iteration 426, Loss: 0.12042462080717087\n",
      "Epoch 5, Iteration 427, Loss: 0.1146521270275116\n",
      "Epoch 5, Iteration 428, Loss: 0.14397843182086945\n",
      "Epoch 5, Iteration 429, Loss: 0.16135631501674652\n",
      "Epoch 5, Iteration 430, Loss: 0.12797856330871582\n",
      "Epoch 5, Iteration 431, Loss: 0.12617114186286926\n",
      "Epoch 6/15, Loss: 0.15406363750635474\n",
      "Epoch 6/15, Validation Accuracy: 0.9416600790513834\n",
      "Epoch 6, Iteration 0, Loss: 0.26421213150024414\n",
      "Epoch 6, Iteration 1, Loss: 0.23317255079746246\n",
      "Epoch 6, Iteration 2, Loss: 0.2801108956336975\n",
      "Epoch 6, Iteration 3, Loss: 0.15500083565711975\n",
      "Epoch 6, Iteration 4, Loss: 0.2106417417526245\n",
      "Epoch 6, Iteration 5, Loss: 0.2786453366279602\n",
      "Epoch 6, Iteration 6, Loss: 0.21259531378746033\n",
      "Epoch 6, Iteration 7, Loss: 0.23666086792945862\n",
      "Epoch 6, Iteration 8, Loss: 0.20851543545722961\n",
      "Epoch 6, Iteration 9, Loss: 0.18639007210731506\n",
      "Epoch 6, Iteration 10, Loss: 0.2295556366443634\n",
      "Epoch 6, Iteration 11, Loss: 0.1710587441921234\n",
      "Epoch 6, Iteration 12, Loss: 0.19960850477218628\n",
      "Epoch 6, Iteration 13, Loss: 0.23200376331806183\n",
      "Epoch 6, Iteration 14, Loss: 0.24117699265480042\n",
      "Epoch 6, Iteration 15, Loss: 0.11322132498025894\n",
      "Epoch 6, Iteration 16, Loss: 0.3046468198299408\n",
      "Epoch 6, Iteration 17, Loss: 0.15323318541049957\n",
      "Epoch 6, Iteration 18, Loss: 0.15788233280181885\n",
      "Epoch 6, Iteration 19, Loss: 0.17232514917850494\n",
      "Epoch 6, Iteration 20, Loss: 0.2285621464252472\n",
      "Epoch 6, Iteration 21, Loss: 0.2193981111049652\n",
      "Epoch 6, Iteration 22, Loss: 0.1821499466896057\n",
      "Epoch 6, Iteration 23, Loss: 0.21210724115371704\n",
      "Epoch 6, Iteration 24, Loss: 0.17647145688533783\n",
      "Epoch 6, Iteration 25, Loss: 0.23692867159843445\n",
      "Epoch 6, Iteration 26, Loss: 0.19155922532081604\n",
      "Epoch 6, Iteration 27, Loss: 0.16049714386463165\n",
      "Epoch 6, Iteration 28, Loss: 0.2636443078517914\n",
      "Epoch 6, Iteration 29, Loss: 0.18855950236320496\n",
      "Epoch 6, Iteration 30, Loss: 0.25083762407302856\n",
      "Epoch 6, Iteration 31, Loss: 0.21159711480140686\n",
      "Epoch 6, Iteration 32, Loss: 0.20261739194393158\n",
      "Epoch 6, Iteration 33, Loss: 0.1798238307237625\n",
      "Epoch 6, Iteration 34, Loss: 0.22120589017868042\n",
      "Epoch 6, Iteration 35, Loss: 0.1798740029335022\n",
      "Epoch 6, Iteration 36, Loss: 0.2096186727285385\n",
      "Epoch 6, Iteration 37, Loss: 0.19434136152267456\n",
      "Epoch 6, Iteration 38, Loss: 0.19722110033035278\n",
      "Epoch 6, Iteration 39, Loss: 0.18794173002243042\n",
      "Epoch 6, Iteration 40, Loss: 0.15596434473991394\n",
      "Epoch 6, Iteration 41, Loss: 0.17496156692504883\n",
      "Epoch 6, Iteration 42, Loss: 0.17419616878032684\n",
      "Epoch 6, Iteration 43, Loss: 0.18253204226493835\n",
      "Epoch 6, Iteration 44, Loss: 0.17879655957221985\n",
      "Epoch 6, Iteration 45, Loss: 0.19376523792743683\n",
      "Epoch 6, Iteration 46, Loss: 0.18219244480133057\n",
      "Epoch 6, Iteration 47, Loss: 0.2052261084318161\n",
      "Epoch 6, Iteration 48, Loss: 0.21262741088867188\n",
      "Epoch 6, Iteration 49, Loss: 0.11194592714309692\n",
      "Epoch 6, Iteration 50, Loss: 0.13707461953163147\n",
      "Epoch 6, Iteration 50, Test Loss: 0.22073157131671906\n",
      "Epoch 6, Iteration 51, Loss: 0.12162376195192337\n",
      "Epoch 6, Iteration 52, Loss: 0.18884657323360443\n",
      "Epoch 6, Iteration 53, Loss: 0.13548125326633453\n",
      "Epoch 6, Iteration 54, Loss: 0.13646835088729858\n",
      "Epoch 6, Iteration 55, Loss: 0.13779723644256592\n",
      "Epoch 6, Iteration 56, Loss: 0.16224044561386108\n",
      "Epoch 6, Iteration 57, Loss: 0.15898257493972778\n",
      "Epoch 6, Iteration 58, Loss: 0.13513267040252686\n",
      "Epoch 6, Iteration 59, Loss: 0.11686767637729645\n",
      "Epoch 6, Iteration 60, Loss: 0.167989119887352\n",
      "Epoch 6, Iteration 61, Loss: 0.1263415515422821\n",
      "Epoch 6, Iteration 62, Loss: 0.24747170507907867\n",
      "Epoch 6, Iteration 63, Loss: 0.11282388120889664\n",
      "Epoch 6, Iteration 64, Loss: 0.14785929024219513\n",
      "Epoch 6, Iteration 65, Loss: 0.1414443999528885\n",
      "Epoch 6, Iteration 66, Loss: 0.1657525897026062\n",
      "Epoch 6, Iteration 67, Loss: 0.1254863142967224\n",
      "Epoch 6, Iteration 68, Loss: 0.13456037640571594\n",
      "Epoch 6, Iteration 69, Loss: 0.1534080058336258\n",
      "Epoch 6, Iteration 70, Loss: 0.11407147347927094\n",
      "Epoch 6, Iteration 71, Loss: 0.13199150562286377\n",
      "Epoch 6, Iteration 72, Loss: 0.1519644558429718\n",
      "Epoch 6, Iteration 73, Loss: 0.15047447383403778\n",
      "Epoch 6, Iteration 74, Loss: 0.1723095029592514\n",
      "Epoch 6, Iteration 75, Loss: 0.16984796524047852\n",
      "Epoch 6, Iteration 76, Loss: 0.14469720423221588\n",
      "Epoch 6, Iteration 77, Loss: 0.1552971452474594\n",
      "Epoch 6, Iteration 78, Loss: 0.15995445847511292\n",
      "Epoch 6, Iteration 79, Loss: 0.09052275866270065\n",
      "Epoch 6, Iteration 80, Loss: 0.1846846044063568\n",
      "Epoch 6, Iteration 81, Loss: 0.1670713573694229\n",
      "Epoch 6, Iteration 82, Loss: 0.17336240410804749\n",
      "Epoch 6, Iteration 83, Loss: 0.14745599031448364\n",
      "Epoch 6, Iteration 84, Loss: 0.13096876442432404\n",
      "Epoch 6, Iteration 85, Loss: 0.17799416184425354\n",
      "Epoch 6, Iteration 86, Loss: 0.18106642365455627\n",
      "Epoch 6, Iteration 87, Loss: 0.145033061504364\n",
      "Epoch 6, Iteration 88, Loss: 0.14769642055034637\n",
      "Epoch 6, Iteration 89, Loss: 0.15785087645053864\n",
      "Epoch 6, Iteration 90, Loss: 0.15462626516819\n",
      "Epoch 6, Iteration 91, Loss: 0.13462761044502258\n",
      "Epoch 6, Iteration 92, Loss: 0.13241158425807953\n",
      "Epoch 6, Iteration 93, Loss: 0.1381319910287857\n",
      "Epoch 6, Iteration 94, Loss: 0.18213175237178802\n",
      "Epoch 6, Iteration 95, Loss: 0.12411998212337494\n",
      "Epoch 6, Iteration 96, Loss: 0.0915035828948021\n",
      "Epoch 6, Iteration 97, Loss: 0.1558208018541336\n",
      "Epoch 6, Iteration 98, Loss: 0.17329341173171997\n",
      "Epoch 6, Iteration 99, Loss: 0.12358294427394867\n",
      "Epoch 6, Iteration 100, Loss: 0.1324889212846756\n",
      "Epoch 6, Iteration 100, Test Loss: 0.2531091868877411\n",
      "Epoch 6, Iteration 101, Loss: 0.14564648270606995\n",
      "Epoch 6, Iteration 102, Loss: 0.11936751753091812\n",
      "Epoch 6, Iteration 103, Loss: 0.12521928548812866\n",
      "Epoch 6, Iteration 104, Loss: 0.13246318697929382\n",
      "Epoch 6, Iteration 105, Loss: 0.13763728737831116\n",
      "Epoch 6, Iteration 106, Loss: 0.10446951538324356\n",
      "Epoch 6, Iteration 107, Loss: 0.0945591852068901\n",
      "Epoch 6, Iteration 108, Loss: 0.1904841810464859\n",
      "Epoch 6, Iteration 109, Loss: 0.21815072000026703\n",
      "Epoch 6, Iteration 110, Loss: 0.10445656627416611\n",
      "Epoch 6, Iteration 111, Loss: 0.09098467230796814\n",
      "Epoch 6, Iteration 112, Loss: 0.14526647329330444\n",
      "Epoch 6, Iteration 113, Loss: 0.1249280497431755\n",
      "Epoch 6, Iteration 114, Loss: 0.11361537873744965\n",
      "Epoch 6, Iteration 115, Loss: 0.16765724122524261\n",
      "Epoch 6, Iteration 116, Loss: 0.14754988253116608\n",
      "Epoch 6, Iteration 117, Loss: 0.2109871655702591\n",
      "Epoch 6, Iteration 118, Loss: 0.15357929468154907\n",
      "Epoch 6, Iteration 119, Loss: 0.17544296383857727\n",
      "Epoch 6, Iteration 120, Loss: 0.10733956098556519\n",
      "Epoch 6, Iteration 121, Loss: 0.15799281001091003\n",
      "Epoch 6, Iteration 122, Loss: 0.1613123118877411\n",
      "Epoch 6, Iteration 123, Loss: 0.12793926894664764\n",
      "Epoch 6, Iteration 124, Loss: 0.13299229741096497\n",
      "Epoch 6, Iteration 125, Loss: 0.1752863973379135\n",
      "Epoch 6, Iteration 126, Loss: 0.1335454136133194\n",
      "Epoch 6, Iteration 127, Loss: 0.15039552748203278\n",
      "Epoch 6, Iteration 128, Loss: 0.1228407621383667\n",
      "Epoch 6, Iteration 129, Loss: 0.176559716463089\n",
      "Epoch 6, Iteration 130, Loss: 0.102058544754982\n",
      "Epoch 6, Iteration 131, Loss: 0.12266720831394196\n",
      "Epoch 6, Iteration 132, Loss: 0.19014914333820343\n",
      "Epoch 6, Iteration 133, Loss: 0.21088658273220062\n",
      "Epoch 6, Iteration 134, Loss: 0.16358663141727448\n",
      "Epoch 6, Iteration 135, Loss: 0.16054540872573853\n",
      "Epoch 6, Iteration 136, Loss: 0.08574662357568741\n",
      "Epoch 6, Iteration 137, Loss: 0.1338123083114624\n",
      "Epoch 6, Iteration 138, Loss: 0.15192291140556335\n",
      "Epoch 6, Iteration 139, Loss: 0.1526314914226532\n",
      "Epoch 6, Iteration 140, Loss: 0.20998142659664154\n",
      "Epoch 6, Iteration 141, Loss: 0.14322800934314728\n",
      "Epoch 6, Iteration 142, Loss: 0.17337414622306824\n",
      "Epoch 6, Iteration 143, Loss: 0.14599338173866272\n",
      "Epoch 6, Iteration 144, Loss: 0.10370286554098129\n",
      "Epoch 6, Iteration 145, Loss: 0.16925664246082306\n",
      "Epoch 6, Iteration 146, Loss: 0.11453647911548615\n",
      "Epoch 6, Iteration 147, Loss: 0.12049075216054916\n",
      "Epoch 6, Iteration 148, Loss: 0.1263711154460907\n",
      "Epoch 6, Iteration 149, Loss: 0.13840952515602112\n",
      "Epoch 6, Iteration 150, Loss: 0.15856300294399261\n",
      "Epoch 6, Iteration 150, Test Loss: 0.24501048028469086\n",
      "Epoch 6, Iteration 151, Loss: 0.1428135633468628\n",
      "Epoch 6, Iteration 152, Loss: 0.1797986775636673\n",
      "Epoch 6, Iteration 153, Loss: 0.1595199555158615\n",
      "Epoch 6, Iteration 154, Loss: 0.1329750120639801\n",
      "Epoch 6, Iteration 155, Loss: 0.14840944111347198\n",
      "Epoch 6, Iteration 156, Loss: 0.1971057802438736\n",
      "Epoch 6, Iteration 157, Loss: 0.20564013719558716\n",
      "Epoch 6, Iteration 158, Loss: 0.1918165385723114\n",
      "Epoch 6, Iteration 159, Loss: 0.13764816522598267\n",
      "Epoch 6, Iteration 160, Loss: 0.1794072687625885\n",
      "Epoch 6, Iteration 161, Loss: 0.09303029626607895\n",
      "Epoch 6, Iteration 162, Loss: 0.10323626548051834\n",
      "Epoch 6, Iteration 163, Loss: 0.14214719831943512\n",
      "Epoch 6, Iteration 164, Loss: 0.10025305300951004\n",
      "Epoch 6, Iteration 165, Loss: 0.10801392793655396\n",
      "Epoch 6, Iteration 166, Loss: 0.12421675771474838\n",
      "Epoch 6, Iteration 167, Loss: 0.14491520822048187\n",
      "Epoch 6, Iteration 168, Loss: 0.10790436714887619\n",
      "Epoch 6, Iteration 169, Loss: 0.13467903435230255\n",
      "Epoch 6, Iteration 170, Loss: 0.09769095480442047\n",
      "Epoch 6, Iteration 171, Loss: 0.18108130991458893\n",
      "Epoch 6, Iteration 172, Loss: 0.1916455775499344\n",
      "Epoch 6, Iteration 173, Loss: 0.14777730405330658\n",
      "Epoch 6, Iteration 174, Loss: 0.15685032308101654\n",
      "Epoch 6, Iteration 175, Loss: 0.13721871376037598\n",
      "Epoch 6, Iteration 176, Loss: 0.11639401316642761\n",
      "Epoch 6, Iteration 177, Loss: 0.19116930663585663\n",
      "Epoch 6, Iteration 178, Loss: 0.14087919890880585\n",
      "Epoch 6, Iteration 179, Loss: 0.1344299465417862\n",
      "Epoch 6, Iteration 180, Loss: 0.15957625210285187\n",
      "Epoch 6, Iteration 181, Loss: 0.14344371855258942\n",
      "Epoch 6, Iteration 182, Loss: 0.10525301843881607\n",
      "Epoch 6, Iteration 183, Loss: 0.15314190089702606\n",
      "Epoch 6, Iteration 184, Loss: 0.2083778828382492\n",
      "Epoch 6, Iteration 185, Loss: 0.12941955029964447\n",
      "Epoch 6, Iteration 186, Loss: 0.12142670154571533\n",
      "Epoch 6, Iteration 187, Loss: 0.136864572763443\n",
      "Epoch 6, Iteration 188, Loss: 0.10286234319210052\n",
      "Epoch 6, Iteration 189, Loss: 0.0800754651427269\n",
      "Epoch 6, Iteration 190, Loss: 0.1185905784368515\n",
      "Epoch 6, Iteration 191, Loss: 0.09775146842002869\n",
      "Epoch 6, Iteration 192, Loss: 0.12372226268053055\n",
      "Epoch 6, Iteration 193, Loss: 0.13563543558120728\n",
      "Epoch 6, Iteration 194, Loss: 0.1140805184841156\n",
      "Epoch 6, Iteration 195, Loss: 0.14774160087108612\n",
      "Epoch 6, Iteration 196, Loss: 0.2347480058670044\n",
      "Epoch 6, Iteration 197, Loss: 0.13250698149204254\n",
      "Epoch 6, Iteration 198, Loss: 0.1067679151892662\n",
      "Epoch 6, Iteration 199, Loss: 0.13876663148403168\n",
      "Epoch 6, Iteration 200, Loss: 0.11366637051105499\n",
      "Epoch 6, Iteration 200, Test Loss: 0.2529844343662262\n",
      "Epoch 6, Iteration 201, Loss: 0.1279093325138092\n",
      "Epoch 6, Iteration 202, Loss: 0.0864986926317215\n",
      "Epoch 6, Iteration 203, Loss: 0.12216679751873016\n",
      "Epoch 6, Iteration 204, Loss: 0.14329613745212555\n",
      "Epoch 6, Iteration 205, Loss: 0.15695737302303314\n",
      "Epoch 6, Iteration 206, Loss: 0.10923906415700912\n",
      "Epoch 6, Iteration 207, Loss: 0.09536576271057129\n",
      "Epoch 6, Iteration 208, Loss: 0.1038515716791153\n",
      "Epoch 6, Iteration 209, Loss: 0.14118513464927673\n",
      "Epoch 6, Iteration 210, Loss: 0.10896933078765869\n",
      "Epoch 6, Iteration 211, Loss: 0.14638689160346985\n",
      "Epoch 6, Iteration 212, Loss: 0.16690030694007874\n",
      "Epoch 6, Iteration 213, Loss: 0.129098042845726\n",
      "Epoch 6, Iteration 214, Loss: 0.11210549622774124\n",
      "Epoch 6, Iteration 215, Loss: 0.12342298775911331\n",
      "Epoch 6, Iteration 216, Loss: 0.15779820084571838\n",
      "Epoch 6, Iteration 217, Loss: 0.1699693202972412\n",
      "Epoch 6, Iteration 218, Loss: 0.13068778812885284\n",
      "Epoch 6, Iteration 219, Loss: 0.13481436669826508\n",
      "Epoch 6, Iteration 220, Loss: 0.09420742094516754\n",
      "Epoch 6, Iteration 221, Loss: 0.14452454447746277\n",
      "Epoch 6, Iteration 222, Loss: 0.13842718303203583\n",
      "Epoch 6, Iteration 223, Loss: 0.1154434084892273\n",
      "Epoch 6, Iteration 224, Loss: 0.08825810998678207\n",
      "Epoch 6, Iteration 225, Loss: 0.13240523636341095\n",
      "Epoch 6, Iteration 226, Loss: 0.1410035341978073\n",
      "Epoch 6, Iteration 227, Loss: 0.12598884105682373\n",
      "Epoch 6, Iteration 228, Loss: 0.23895785212516785\n",
      "Epoch 6, Iteration 229, Loss: 0.2182278335094452\n",
      "Epoch 6, Iteration 230, Loss: 0.17293977737426758\n",
      "Epoch 6, Iteration 231, Loss: 0.10695496201515198\n",
      "Epoch 6, Iteration 232, Loss: 0.17005831003189087\n",
      "Epoch 6, Iteration 233, Loss: 0.17813898622989655\n",
      "Epoch 6, Iteration 234, Loss: 0.13387614488601685\n",
      "Epoch 6, Iteration 235, Loss: 0.11087843030691147\n",
      "Epoch 6, Iteration 236, Loss: 0.15824799239635468\n",
      "Epoch 6, Iteration 237, Loss: 0.13617286086082458\n",
      "Epoch 6, Iteration 238, Loss: 0.15986746549606323\n",
      "Epoch 6, Iteration 239, Loss: 0.13560770452022552\n",
      "Epoch 6, Iteration 240, Loss: 0.12730969488620758\n",
      "Epoch 6, Iteration 241, Loss: 0.12460510432720184\n",
      "Epoch 6, Iteration 242, Loss: 0.14432120323181152\n",
      "Epoch 6, Iteration 243, Loss: 0.13672594726085663\n",
      "Epoch 6, Iteration 244, Loss: 0.1707654893398285\n",
      "Epoch 6, Iteration 245, Loss: 0.13876137137413025\n",
      "Epoch 6, Iteration 246, Loss: 0.13976600766181946\n",
      "Epoch 6, Iteration 247, Loss: 0.24888712167739868\n",
      "Epoch 6, Iteration 248, Loss: 0.20525814592838287\n",
      "Epoch 6, Iteration 249, Loss: 0.21690517663955688\n",
      "Epoch 6, Iteration 250, Loss: 0.24559219181537628\n",
      "Epoch 6, Iteration 250, Test Loss: 0.26447197794914246\n",
      "Epoch 6, Iteration 251, Loss: 0.21467943489551544\n",
      "Epoch 6, Iteration 252, Loss: 0.13150319457054138\n",
      "Epoch 6, Iteration 253, Loss: 0.17159104347229004\n",
      "Epoch 6, Iteration 254, Loss: 0.2208164632320404\n",
      "Epoch 6, Iteration 255, Loss: 0.2020934671163559\n",
      "Epoch 6, Iteration 256, Loss: 0.15149393677711487\n",
      "Epoch 6, Iteration 257, Loss: 0.11346603184938431\n",
      "Epoch 6, Iteration 258, Loss: 0.15249525010585785\n",
      "Epoch 6, Iteration 259, Loss: 0.13867071270942688\n",
      "Epoch 6, Iteration 260, Loss: 0.19681350886821747\n",
      "Epoch 6, Iteration 261, Loss: 0.14994999766349792\n",
      "Epoch 6, Iteration 262, Loss: 0.11414789408445358\n",
      "Epoch 6, Iteration 263, Loss: 0.11529532074928284\n",
      "Epoch 6, Iteration 264, Loss: 0.12344491481781006\n",
      "Epoch 6, Iteration 265, Loss: 0.22850632667541504\n",
      "Epoch 6, Iteration 266, Loss: 0.10536674410104752\n",
      "Epoch 6, Iteration 267, Loss: 0.14933598041534424\n",
      "Epoch 6, Iteration 268, Loss: 0.14909908175468445\n",
      "Epoch 6, Iteration 269, Loss: 0.15753307938575745\n",
      "Epoch 6, Iteration 270, Loss: 0.16407929360866547\n",
      "Epoch 6, Iteration 271, Loss: 0.07709679752588272\n",
      "Epoch 6, Iteration 272, Loss: 0.13546589016914368\n",
      "Epoch 6, Iteration 273, Loss: 0.16694386303424835\n",
      "Epoch 6, Iteration 274, Loss: 0.1117272675037384\n",
      "Epoch 6, Iteration 275, Loss: 0.10984526574611664\n",
      "Epoch 6, Iteration 276, Loss: 0.11310919374227524\n",
      "Epoch 6, Iteration 277, Loss: 0.11890123784542084\n",
      "Epoch 6, Iteration 278, Loss: 0.1460833102464676\n",
      "Epoch 6, Iteration 279, Loss: 0.11603517830371857\n",
      "Epoch 6, Iteration 280, Loss: 0.16603252291679382\n",
      "Epoch 6, Iteration 281, Loss: 0.11897609382867813\n",
      "Epoch 6, Iteration 282, Loss: 0.11391573399305344\n",
      "Epoch 6, Iteration 283, Loss: 0.10610724985599518\n",
      "Epoch 6, Iteration 284, Loss: 0.12703131139278412\n",
      "Epoch 6, Iteration 285, Loss: 0.10590092837810516\n",
      "Epoch 6, Iteration 286, Loss: 0.14727161824703217\n",
      "Epoch 6, Iteration 287, Loss: 0.08966565132141113\n",
      "Epoch 6, Iteration 288, Loss: 0.11529868841171265\n",
      "Epoch 6, Iteration 289, Loss: 0.17793825268745422\n",
      "Epoch 6, Iteration 290, Loss: 0.09719356894493103\n",
      "Epoch 6, Iteration 291, Loss: 0.1008361354470253\n",
      "Epoch 6, Iteration 292, Loss: 0.14423123002052307\n",
      "Epoch 6, Iteration 293, Loss: 0.18808823823928833\n",
      "Epoch 6, Iteration 294, Loss: 0.11410202085971832\n",
      "Epoch 6, Iteration 295, Loss: 0.08366898447275162\n",
      "Epoch 6, Iteration 296, Loss: 0.10620026290416718\n",
      "Epoch 6, Iteration 297, Loss: 0.13992184400558472\n",
      "Epoch 6, Iteration 298, Loss: 0.10994407534599304\n",
      "Epoch 6, Iteration 299, Loss: 0.18128839135169983\n",
      "Epoch 6, Iteration 300, Loss: 0.12980102002620697\n",
      "Epoch 6, Iteration 300, Test Loss: 0.264313668012619\n",
      "Epoch 6, Iteration 301, Loss: 0.12112782895565033\n",
      "Epoch 6, Iteration 302, Loss: 0.07906095683574677\n",
      "Epoch 6, Iteration 303, Loss: 0.15503239631652832\n",
      "Epoch 6, Iteration 304, Loss: 0.17169484496116638\n",
      "Epoch 6, Iteration 305, Loss: 0.10647402703762054\n",
      "Epoch 6, Iteration 306, Loss: 0.10170464217662811\n",
      "Epoch 6, Iteration 307, Loss: 0.11586591601371765\n",
      "Epoch 6, Iteration 308, Loss: 0.11888927221298218\n",
      "Epoch 6, Iteration 309, Loss: 0.1023588702082634\n",
      "Epoch 6, Iteration 310, Loss: 0.14194342494010925\n",
      "Epoch 6, Iteration 311, Loss: 0.12159136682748795\n",
      "Epoch 6, Iteration 312, Loss: 0.14090010523796082\n",
      "Epoch 6, Iteration 313, Loss: 0.1174536943435669\n",
      "Epoch 6, Iteration 314, Loss: 0.11787628382444382\n",
      "Epoch 6, Iteration 315, Loss: 0.11610611528158188\n",
      "Epoch 6, Iteration 316, Loss: 0.10637066513299942\n",
      "Epoch 6, Iteration 317, Loss: 0.14662715792655945\n",
      "Epoch 6, Iteration 318, Loss: 0.17335118353366852\n",
      "Epoch 6, Iteration 319, Loss: 0.09116211533546448\n",
      "Epoch 6, Iteration 320, Loss: 0.1311492770910263\n",
      "Epoch 6, Iteration 321, Loss: 0.07823899388313293\n",
      "Epoch 6, Iteration 322, Loss: 0.17898474633693695\n",
      "Epoch 6, Iteration 323, Loss: 0.10731378197669983\n",
      "Epoch 6, Iteration 324, Loss: 0.17600512504577637\n",
      "Epoch 6, Iteration 325, Loss: 0.24079814553260803\n",
      "Epoch 6, Iteration 326, Loss: 0.17017820477485657\n",
      "Epoch 6, Iteration 327, Loss: 0.1572273075580597\n",
      "Epoch 6, Iteration 328, Loss: 0.13196684420108795\n",
      "Epoch 6, Iteration 329, Loss: 0.1393098533153534\n",
      "Epoch 6, Iteration 330, Loss: 0.19567976891994476\n",
      "Epoch 6, Iteration 331, Loss: 0.16404400765895844\n",
      "Epoch 6, Iteration 332, Loss: 0.1561548411846161\n",
      "Epoch 6, Iteration 333, Loss: 0.13605435192584991\n",
      "Epoch 6, Iteration 334, Loss: 0.2348993718624115\n",
      "Epoch 6, Iteration 335, Loss: 0.14531905949115753\n",
      "Epoch 6, Iteration 336, Loss: 0.1618012636899948\n",
      "Epoch 6, Iteration 337, Loss: 0.12136688083410263\n",
      "Epoch 6, Iteration 338, Loss: 0.13722911477088928\n",
      "Epoch 6, Iteration 339, Loss: 0.17658193409442902\n",
      "Epoch 6, Iteration 340, Loss: 0.156418114900589\n",
      "Epoch 6, Iteration 341, Loss: 0.16325248777866364\n",
      "Epoch 6, Iteration 342, Loss: 0.15735459327697754\n",
      "Epoch 6, Iteration 343, Loss: 0.203620046377182\n",
      "Epoch 6, Iteration 344, Loss: 0.1813182830810547\n",
      "Epoch 6, Iteration 345, Loss: 0.13544557988643646\n",
      "Epoch 6, Iteration 346, Loss: 0.144892618060112\n",
      "Epoch 6, Iteration 347, Loss: 0.1513187289237976\n",
      "Epoch 6, Iteration 348, Loss: 0.1707456111907959\n",
      "Epoch 6, Iteration 349, Loss: 0.18240253627300262\n",
      "Epoch 6, Iteration 350, Loss: 0.19540409743785858\n",
      "Epoch 6, Iteration 350, Test Loss: 0.24189554154872894\n",
      "Epoch 6, Iteration 351, Loss: 0.16341127455234528\n",
      "Epoch 6, Iteration 352, Loss: 0.13884525001049042\n",
      "Epoch 6, Iteration 353, Loss: 0.1875787228345871\n",
      "Epoch 6, Iteration 354, Loss: 0.11937876790761948\n",
      "Epoch 6, Iteration 355, Loss: 0.14153318107128143\n",
      "Epoch 6, Iteration 356, Loss: 0.12960512936115265\n",
      "Epoch 6, Iteration 357, Loss: 0.21629348397254944\n",
      "Epoch 6, Iteration 358, Loss: 0.11912328004837036\n",
      "Epoch 6, Iteration 359, Loss: 0.176334410905838\n",
      "Epoch 6, Iteration 360, Loss: 0.09749101102352142\n",
      "Epoch 6, Iteration 361, Loss: 0.15683238208293915\n",
      "Epoch 6, Iteration 362, Loss: 0.1376372128725052\n",
      "Epoch 6, Iteration 363, Loss: 0.1279028058052063\n",
      "Epoch 6, Iteration 364, Loss: 0.12989948689937592\n",
      "Epoch 6, Iteration 365, Loss: 0.1076258048415184\n",
      "Epoch 6, Iteration 366, Loss: 0.16895319521427155\n",
      "Epoch 6, Iteration 367, Loss: 0.1898745894432068\n",
      "Epoch 6, Iteration 368, Loss: 0.13782066106796265\n",
      "Epoch 6, Iteration 369, Loss: 0.12349833548069\n",
      "Epoch 6, Iteration 370, Loss: 0.17814455926418304\n",
      "Epoch 6, Iteration 371, Loss: 0.1269858479499817\n",
      "Epoch 6, Iteration 372, Loss: 0.1274951994419098\n",
      "Epoch 6, Iteration 373, Loss: 0.1753818243741989\n",
      "Epoch 6, Iteration 374, Loss: 0.10342159867286682\n",
      "Epoch 6, Iteration 375, Loss: 0.14349417388439178\n",
      "Epoch 6, Iteration 376, Loss: 0.14811594784259796\n",
      "Epoch 6, Iteration 377, Loss: 0.13328365981578827\n",
      "Epoch 6, Iteration 378, Loss: 0.14725369215011597\n",
      "Epoch 6, Iteration 379, Loss: 0.19612911343574524\n",
      "Epoch 6, Iteration 380, Loss: 0.17602616548538208\n",
      "Epoch 6, Iteration 381, Loss: 0.11271604895591736\n",
      "Epoch 6, Iteration 382, Loss: 0.14645041525363922\n",
      "Epoch 6, Iteration 383, Loss: 0.11705616116523743\n",
      "Epoch 6, Iteration 384, Loss: 0.14520955085754395\n",
      "Epoch 6, Iteration 385, Loss: 0.12945044040679932\n",
      "Epoch 6, Iteration 386, Loss: 0.14174403250217438\n",
      "Epoch 6, Iteration 387, Loss: 0.1055179163813591\n",
      "Epoch 6, Iteration 388, Loss: 0.08814403414726257\n",
      "Epoch 6, Iteration 389, Loss: 0.12433759868144989\n",
      "Epoch 6, Iteration 390, Loss: 0.14374497532844543\n",
      "Epoch 6, Iteration 391, Loss: 0.08497858047485352\n",
      "Epoch 6, Iteration 392, Loss: 0.16203512251377106\n",
      "Epoch 6, Iteration 393, Loss: 0.19340330362319946\n",
      "Epoch 6, Iteration 394, Loss: 0.14366063475608826\n",
      "Epoch 6, Iteration 395, Loss: 0.11362788826227188\n",
      "Epoch 6, Iteration 396, Loss: 0.1260635107755661\n",
      "Epoch 6, Iteration 397, Loss: 0.1424325853586197\n",
      "Epoch 6, Iteration 398, Loss: 0.238579660654068\n",
      "Epoch 6, Iteration 399, Loss: 0.12633705139160156\n",
      "Epoch 6, Iteration 400, Loss: 0.12351447343826294\n",
      "Epoch 6, Iteration 400, Test Loss: 0.25632017850875854\n",
      "Epoch 6, Iteration 401, Loss: 0.14719587564468384\n",
      "Epoch 6, Iteration 402, Loss: 0.09609674662351608\n",
      "Epoch 6, Iteration 403, Loss: 0.13373889029026031\n",
      "Epoch 6, Iteration 404, Loss: 0.13689462840557098\n",
      "Epoch 6, Iteration 405, Loss: 0.10119978338479996\n",
      "Epoch 6, Iteration 406, Loss: 0.11128653585910797\n",
      "Epoch 6, Iteration 407, Loss: 0.1455436497926712\n",
      "Epoch 6, Iteration 408, Loss: 0.14738258719444275\n",
      "Epoch 6, Iteration 409, Loss: 0.10345473885536194\n",
      "Epoch 6, Iteration 410, Loss: 0.18918654322624207\n",
      "Epoch 6, Iteration 411, Loss: 0.1867275983095169\n",
      "Epoch 6, Iteration 412, Loss: 0.16346074640750885\n",
      "Epoch 6, Iteration 413, Loss: 0.15424467623233795\n",
      "Epoch 6, Iteration 414, Loss: 0.11491614580154419\n",
      "Epoch 6, Iteration 415, Loss: 0.13551349937915802\n",
      "Epoch 6, Iteration 416, Loss: 0.1441873461008072\n",
      "Epoch 6, Iteration 417, Loss: 0.12257283926010132\n",
      "Epoch 6, Iteration 418, Loss: 0.1215028464794159\n",
      "Epoch 6, Iteration 419, Loss: 0.12306874990463257\n",
      "Epoch 6, Iteration 420, Loss: 0.14505578577518463\n",
      "Epoch 6, Iteration 421, Loss: 0.08945298194885254\n",
      "Epoch 6, Iteration 422, Loss: 0.11870341747999191\n",
      "Epoch 6, Iteration 423, Loss: 0.12092271447181702\n",
      "Epoch 6, Iteration 424, Loss: 0.13998939096927643\n",
      "Epoch 6, Iteration 425, Loss: 0.13942667841911316\n",
      "Epoch 6, Iteration 426, Loss: 0.10176634788513184\n",
      "Epoch 6, Iteration 427, Loss: 0.11695157736539841\n",
      "Epoch 6, Iteration 428, Loss: 0.13155658543109894\n",
      "Epoch 6, Iteration 429, Loss: 0.1635703593492508\n",
      "Epoch 6, Iteration 430, Loss: 0.11944177001714706\n",
      "Epoch 6, Iteration 431, Loss: 0.11813072115182877\n",
      "Epoch 7/15, Loss: 0.1496261390339997\n",
      "Epoch 7/15, Validation Accuracy: 0.9396837944664032\n",
      "Epoch 7, Iteration 0, Loss: 0.27630409598350525\n",
      "Epoch 7, Iteration 1, Loss: 0.223661407828331\n",
      "Epoch 7, Iteration 2, Loss: 0.21291683614253998\n",
      "Epoch 7, Iteration 3, Loss: 0.14251190423965454\n",
      "Epoch 7, Iteration 4, Loss: 0.210324227809906\n",
      "Epoch 7, Iteration 5, Loss: 0.23860377073287964\n",
      "Epoch 7, Iteration 6, Loss: 0.22921210527420044\n",
      "Epoch 7, Iteration 7, Loss: 0.23515751957893372\n",
      "Epoch 7, Iteration 8, Loss: 0.21454288065433502\n",
      "Epoch 7, Iteration 9, Loss: 0.2081567794084549\n",
      "Epoch 7, Iteration 10, Loss: 0.1974034309387207\n",
      "Epoch 7, Iteration 11, Loss: 0.16734251379966736\n",
      "Epoch 7, Iteration 12, Loss: 0.2228005975484848\n",
      "Epoch 7, Iteration 13, Loss: 0.2707862854003906\n",
      "Epoch 7, Iteration 14, Loss: 0.24730268120765686\n",
      "Epoch 7, Iteration 15, Loss: 0.13587625324726105\n",
      "Epoch 7, Iteration 16, Loss: 0.29907095432281494\n",
      "Epoch 7, Iteration 17, Loss: 0.18815815448760986\n",
      "Epoch 7, Iteration 18, Loss: 0.16264890134334564\n",
      "Epoch 7, Iteration 19, Loss: 0.1715124249458313\n",
      "Epoch 7, Iteration 20, Loss: 0.20972128212451935\n",
      "Epoch 7, Iteration 21, Loss: 0.2282729148864746\n",
      "Epoch 7, Iteration 22, Loss: 0.1862216293811798\n",
      "Epoch 7, Iteration 23, Loss: 0.21247656643390656\n",
      "Epoch 7, Iteration 24, Loss: 0.20377764105796814\n",
      "Epoch 7, Iteration 25, Loss: 0.21716178953647614\n",
      "Epoch 7, Iteration 26, Loss: 0.2136245220899582\n",
      "Epoch 7, Iteration 27, Loss: 0.15474620461463928\n",
      "Epoch 7, Iteration 28, Loss: 0.23728598654270172\n",
      "Epoch 7, Iteration 29, Loss: 0.15933629870414734\n",
      "Epoch 7, Iteration 30, Loss: 0.18838973343372345\n",
      "Epoch 7, Iteration 31, Loss: 0.22329488396644592\n",
      "Epoch 7, Iteration 32, Loss: 0.22297173738479614\n",
      "Epoch 7, Iteration 33, Loss: 0.1870063990354538\n",
      "Epoch 7, Iteration 34, Loss: 0.2196328043937683\n",
      "Epoch 7, Iteration 35, Loss: 0.13130199909210205\n",
      "Epoch 7, Iteration 36, Loss: 0.25700142979621887\n",
      "Epoch 7, Iteration 37, Loss: 0.16949161887168884\n",
      "Epoch 7, Iteration 38, Loss: 0.18426398932933807\n",
      "Epoch 7, Iteration 39, Loss: 0.18488572537899017\n",
      "Epoch 7, Iteration 40, Loss: 0.17260463535785675\n",
      "Epoch 7, Iteration 41, Loss: 0.1448773890733719\n",
      "Epoch 7, Iteration 42, Loss: 0.184212788939476\n",
      "Epoch 7, Iteration 43, Loss: 0.15664148330688477\n",
      "Epoch 7, Iteration 44, Loss: 0.14246869087219238\n",
      "Epoch 7, Iteration 45, Loss: 0.17598849534988403\n",
      "Epoch 7, Iteration 46, Loss: 0.1882942169904709\n",
      "Epoch 7, Iteration 47, Loss: 0.216876819729805\n",
      "Epoch 7, Iteration 48, Loss: 0.19072654843330383\n",
      "Epoch 7, Iteration 49, Loss: 0.12352899461984634\n",
      "Epoch 7, Iteration 50, Loss: 0.13883502781391144\n",
      "Epoch 7, Iteration 50, Test Loss: 0.22641174495220184\n",
      "Epoch 7, Iteration 51, Loss: 0.14440390467643738\n",
      "Epoch 7, Iteration 52, Loss: 0.16177797317504883\n",
      "Epoch 7, Iteration 53, Loss: 0.14347445964813232\n",
      "Epoch 7, Iteration 54, Loss: 0.1684270203113556\n",
      "Epoch 7, Iteration 55, Loss: 0.1316707730293274\n",
      "Epoch 7, Iteration 56, Loss: 0.14055310189723969\n",
      "Epoch 7, Iteration 57, Loss: 0.16390793025493622\n",
      "Epoch 7, Iteration 58, Loss: 0.12656524777412415\n",
      "Epoch 7, Iteration 59, Loss: 0.1301070749759674\n",
      "Epoch 7, Iteration 60, Loss: 0.1666872501373291\n",
      "Epoch 7, Iteration 61, Loss: 0.12537814676761627\n",
      "Epoch 7, Iteration 62, Loss: 0.22232908010482788\n",
      "Epoch 7, Iteration 63, Loss: 0.11613848060369492\n",
      "Epoch 7, Iteration 64, Loss: 0.15063458681106567\n",
      "Epoch 7, Iteration 65, Loss: 0.14544948935508728\n",
      "Epoch 7, Iteration 66, Loss: 0.14905090630054474\n",
      "Epoch 7, Iteration 67, Loss: 0.10612174868583679\n",
      "Epoch 7, Iteration 68, Loss: 0.16497665643692017\n",
      "Epoch 7, Iteration 69, Loss: 0.16751059889793396\n",
      "Epoch 7, Iteration 70, Loss: 0.11863663047552109\n",
      "Epoch 7, Iteration 71, Loss: 0.13572433590888977\n",
      "Epoch 7, Iteration 72, Loss: 0.14383283257484436\n",
      "Epoch 7, Iteration 73, Loss: 0.13952051103115082\n",
      "Epoch 7, Iteration 74, Loss: 0.17036612331867218\n",
      "Epoch 7, Iteration 75, Loss: 0.17091913521289825\n",
      "Epoch 7, Iteration 76, Loss: 0.17183570563793182\n",
      "Epoch 7, Iteration 77, Loss: 0.1364358812570572\n",
      "Epoch 7, Iteration 78, Loss: 0.16284556686878204\n",
      "Epoch 7, Iteration 79, Loss: 0.09421387314796448\n",
      "Epoch 7, Iteration 80, Loss: 0.17256350815296173\n",
      "Epoch 7, Iteration 81, Loss: 0.13194945454597473\n",
      "Epoch 7, Iteration 82, Loss: 0.1727617084980011\n",
      "Epoch 7, Iteration 83, Loss: 0.13592220842838287\n",
      "Epoch 7, Iteration 84, Loss: 0.11144087463617325\n",
      "Epoch 7, Iteration 85, Loss: 0.19522258639335632\n",
      "Epoch 7, Iteration 86, Loss: 0.17951957881450653\n",
      "Epoch 7, Iteration 87, Loss: 0.15830451250076294\n",
      "Epoch 7, Iteration 88, Loss: 0.12977933883666992\n",
      "Epoch 7, Iteration 89, Loss: 0.14809772372245789\n",
      "Epoch 7, Iteration 90, Loss: 0.15166375041007996\n",
      "Epoch 7, Iteration 91, Loss: 0.14635302126407623\n",
      "Epoch 7, Iteration 92, Loss: 0.12301956862211227\n",
      "Epoch 7, Iteration 93, Loss: 0.13906075060367584\n",
      "Epoch 7, Iteration 94, Loss: 0.19078980386257172\n",
      "Epoch 7, Iteration 95, Loss: 0.12532830238342285\n",
      "Epoch 7, Iteration 96, Loss: 0.08581017702817917\n",
      "Epoch 7, Iteration 97, Loss: 0.15916164219379425\n",
      "Epoch 7, Iteration 98, Loss: 0.1565212905406952\n",
      "Epoch 7, Iteration 99, Loss: 0.14249089360237122\n",
      "Epoch 7, Iteration 100, Loss: 0.09982519596815109\n",
      "Epoch 7, Iteration 100, Test Loss: 0.24641872942447662\n",
      "Epoch 7, Iteration 101, Loss: 0.12861663103103638\n",
      "Epoch 7, Iteration 102, Loss: 0.1240977868437767\n",
      "Epoch 7, Iteration 103, Loss: 0.1301407366991043\n",
      "Epoch 7, Iteration 104, Loss: 0.14147357642650604\n",
      "Epoch 7, Iteration 105, Loss: 0.13942208886146545\n",
      "Epoch 7, Iteration 106, Loss: 0.11152013391256332\n",
      "Epoch 7, Iteration 107, Loss: 0.08073069155216217\n",
      "Epoch 7, Iteration 108, Loss: 0.15097379684448242\n",
      "Epoch 7, Iteration 109, Loss: 0.2039763629436493\n",
      "Epoch 7, Iteration 110, Loss: 0.11756926774978638\n",
      "Epoch 7, Iteration 111, Loss: 0.08845164626836777\n",
      "Epoch 7, Iteration 112, Loss: 0.15079154074192047\n",
      "Epoch 7, Iteration 113, Loss: 0.1218351498246193\n",
      "Epoch 7, Iteration 114, Loss: 0.11354333162307739\n",
      "Epoch 7, Iteration 115, Loss: 0.1433231234550476\n",
      "Epoch 7, Iteration 116, Loss: 0.14262741804122925\n",
      "Epoch 7, Iteration 117, Loss: 0.1994556486606598\n",
      "Epoch 7, Iteration 118, Loss: 0.1489374339580536\n",
      "Epoch 7, Iteration 119, Loss: 0.1709393709897995\n",
      "Epoch 7, Iteration 120, Loss: 0.0974765345454216\n",
      "Epoch 7, Iteration 121, Loss: 0.15144306421279907\n",
      "Epoch 7, Iteration 122, Loss: 0.14540117979049683\n",
      "Epoch 7, Iteration 123, Loss: 0.12786436080932617\n",
      "Epoch 7, Iteration 124, Loss: 0.12642282247543335\n",
      "Epoch 7, Iteration 125, Loss: 0.17723579704761505\n",
      "Epoch 7, Iteration 126, Loss: 0.1402844935655594\n",
      "Epoch 7, Iteration 127, Loss: 0.14654454588890076\n",
      "Epoch 7, Iteration 128, Loss: 0.13523998856544495\n",
      "Epoch 7, Iteration 129, Loss: 0.15916064381599426\n",
      "Epoch 7, Iteration 130, Loss: 0.10254710912704468\n",
      "Epoch 7, Iteration 131, Loss: 0.11516153067350388\n",
      "Epoch 7, Iteration 132, Loss: 0.18801097571849823\n",
      "Epoch 7, Iteration 133, Loss: 0.20953625440597534\n",
      "Epoch 7, Iteration 134, Loss: 0.15009625256061554\n",
      "Epoch 7, Iteration 135, Loss: 0.15590445697307587\n",
      "Epoch 7, Iteration 136, Loss: 0.10053431987762451\n",
      "Epoch 7, Iteration 137, Loss: 0.1273082047700882\n",
      "Epoch 7, Iteration 138, Loss: 0.14896656572818756\n",
      "Epoch 7, Iteration 139, Loss: 0.15904200077056885\n",
      "Epoch 7, Iteration 140, Loss: 0.21446123719215393\n",
      "Epoch 7, Iteration 141, Loss: 0.14532817900180817\n",
      "Epoch 7, Iteration 142, Loss: 0.16686788201332092\n",
      "Epoch 7, Iteration 143, Loss: 0.162763774394989\n",
      "Epoch 7, Iteration 144, Loss: 0.10197191685438156\n",
      "Epoch 7, Iteration 145, Loss: 0.1742532104253769\n",
      "Epoch 7, Iteration 146, Loss: 0.11663801223039627\n",
      "Epoch 7, Iteration 147, Loss: 0.11694896221160889\n",
      "Epoch 7, Iteration 148, Loss: 0.1248367503285408\n",
      "Epoch 7, Iteration 149, Loss: 0.11463697999715805\n",
      "Epoch 7, Iteration 150, Loss: 0.18485082685947418\n",
      "Epoch 7, Iteration 150, Test Loss: 0.25440046191215515\n",
      "Epoch 7, Iteration 151, Loss: 0.1414610743522644\n",
      "Epoch 7, Iteration 152, Loss: 0.17777137458324432\n",
      "Epoch 7, Iteration 153, Loss: 0.15403500199317932\n",
      "Epoch 7, Iteration 154, Loss: 0.1142207607626915\n",
      "Epoch 7, Iteration 155, Loss: 0.157528817653656\n",
      "Epoch 7, Iteration 156, Loss: 0.175368994474411\n",
      "Epoch 7, Iteration 157, Loss: 0.19640246033668518\n",
      "Epoch 7, Iteration 158, Loss: 0.19496390223503113\n",
      "Epoch 7, Iteration 159, Loss: 0.12108074873685837\n",
      "Epoch 7, Iteration 160, Loss: 0.16979928314685822\n",
      "Epoch 7, Iteration 161, Loss: 0.09265647083520889\n",
      "Epoch 7, Iteration 162, Loss: 0.1004936695098877\n",
      "Epoch 7, Iteration 163, Loss: 0.15158122777938843\n",
      "Epoch 7, Iteration 164, Loss: 0.11517754197120667\n",
      "Epoch 7, Iteration 165, Loss: 0.10132840275764465\n",
      "Epoch 7, Iteration 166, Loss: 0.12735897302627563\n",
      "Epoch 7, Iteration 167, Loss: 0.16361723840236664\n",
      "Epoch 7, Iteration 168, Loss: 0.11222197860479355\n",
      "Epoch 7, Iteration 169, Loss: 0.13004852831363678\n",
      "Epoch 7, Iteration 170, Loss: 0.09531285613775253\n",
      "Epoch 7, Iteration 171, Loss: 0.19743891060352325\n",
      "Epoch 7, Iteration 172, Loss: 0.17718017101287842\n",
      "Epoch 7, Iteration 173, Loss: 0.14772988855838776\n",
      "Epoch 7, Iteration 174, Loss: 0.14785881340503693\n",
      "Epoch 7, Iteration 175, Loss: 0.14221523702144623\n",
      "Epoch 7, Iteration 176, Loss: 0.11365663260221481\n",
      "Epoch 7, Iteration 177, Loss: 0.17799799144268036\n",
      "Epoch 7, Iteration 178, Loss: 0.13272099196910858\n",
      "Epoch 7, Iteration 179, Loss: 0.11182811856269836\n",
      "Epoch 7, Iteration 180, Loss: 0.17357024550437927\n",
      "Epoch 7, Iteration 181, Loss: 0.11782880872488022\n",
      "Epoch 7, Iteration 182, Loss: 0.11529272049665451\n",
      "Epoch 7, Iteration 183, Loss: 0.13228067755699158\n",
      "Epoch 7, Iteration 184, Loss: 0.1931462585926056\n",
      "Epoch 7, Iteration 185, Loss: 0.10494675487279892\n",
      "Epoch 7, Iteration 186, Loss: 0.15011540055274963\n",
      "Epoch 7, Iteration 187, Loss: 0.13528959453105927\n",
      "Epoch 7, Iteration 188, Loss: 0.09641014039516449\n",
      "Epoch 7, Iteration 189, Loss: 0.07148697972297668\n",
      "Epoch 7, Iteration 190, Loss: 0.09743029624223709\n",
      "Epoch 7, Iteration 191, Loss: 0.090909942984581\n",
      "Epoch 7, Iteration 192, Loss: 0.12394135445356369\n",
      "Epoch 7, Iteration 193, Loss: 0.14174529910087585\n",
      "Epoch 7, Iteration 194, Loss: 0.12052369862794876\n",
      "Epoch 7, Iteration 195, Loss: 0.13969656825065613\n",
      "Epoch 7, Iteration 196, Loss: 0.22037573158740997\n",
      "Epoch 7, Iteration 197, Loss: 0.12539763748645782\n",
      "Epoch 7, Iteration 198, Loss: 0.09848422557115555\n",
      "Epoch 7, Iteration 199, Loss: 0.13057298958301544\n",
      "Epoch 7, Iteration 200, Loss: 0.1160215362906456\n",
      "Epoch 7, Iteration 200, Test Loss: 0.25836652517318726\n",
      "Epoch 7, Iteration 201, Loss: 0.1326432228088379\n",
      "Epoch 7, Iteration 202, Loss: 0.09142521023750305\n",
      "Epoch 7, Iteration 203, Loss: 0.11191615462303162\n",
      "Epoch 7, Iteration 204, Loss: 0.13052551448345184\n",
      "Epoch 7, Iteration 205, Loss: 0.15137861669063568\n",
      "Epoch 7, Iteration 206, Loss: 0.11980772763490677\n",
      "Epoch 7, Iteration 207, Loss: 0.10246572643518448\n",
      "Epoch 7, Iteration 208, Loss: 0.09803753346204758\n",
      "Epoch 7, Iteration 209, Loss: 0.13129431009292603\n",
      "Epoch 7, Iteration 210, Loss: 0.11131072044372559\n",
      "Epoch 7, Iteration 211, Loss: 0.13166864216327667\n",
      "Epoch 7, Iteration 212, Loss: 0.14557020366191864\n",
      "Epoch 7, Iteration 213, Loss: 0.11905498802661896\n",
      "Epoch 7, Iteration 214, Loss: 0.10515189915895462\n",
      "Epoch 7, Iteration 215, Loss: 0.12779325246810913\n",
      "Epoch 7, Iteration 216, Loss: 0.16411490738391876\n",
      "Epoch 7, Iteration 217, Loss: 0.1770138293504715\n",
      "Epoch 7, Iteration 218, Loss: 0.11311786621809006\n",
      "Epoch 7, Iteration 219, Loss: 0.14654169976711273\n",
      "Epoch 7, Iteration 220, Loss: 0.10357419401407242\n",
      "Epoch 7, Iteration 221, Loss: 0.14477673172950745\n",
      "Epoch 7, Iteration 222, Loss: 0.11609233170747757\n",
      "Epoch 7, Iteration 223, Loss: 0.12751467525959015\n",
      "Epoch 7, Iteration 224, Loss: 0.11609551310539246\n",
      "Epoch 7, Iteration 225, Loss: 0.1327480971813202\n",
      "Epoch 7, Iteration 226, Loss: 0.1406189501285553\n",
      "Epoch 7, Iteration 227, Loss: 0.13424423336982727\n",
      "Epoch 7, Iteration 228, Loss: 0.26663827896118164\n",
      "Epoch 7, Iteration 229, Loss: 0.22858011722564697\n",
      "Epoch 7, Iteration 230, Loss: 0.16614779829978943\n",
      "Epoch 7, Iteration 231, Loss: 0.10592874139547348\n",
      "Epoch 7, Iteration 232, Loss: 0.15618543326854706\n",
      "Epoch 7, Iteration 233, Loss: 0.18515047430992126\n",
      "Epoch 7, Iteration 234, Loss: 0.12315350770950317\n",
      "Epoch 7, Iteration 235, Loss: 0.10684085637331009\n",
      "Epoch 7, Iteration 236, Loss: 0.1666918247938156\n",
      "Epoch 7, Iteration 237, Loss: 0.1240280419588089\n",
      "Epoch 7, Iteration 238, Loss: 0.12055227905511856\n",
      "Epoch 7, Iteration 239, Loss: 0.12981297075748444\n",
      "Epoch 7, Iteration 240, Loss: 0.120604008436203\n",
      "Epoch 7, Iteration 241, Loss: 0.13717104494571686\n",
      "Epoch 7, Iteration 242, Loss: 0.15127259492874146\n",
      "Epoch 7, Iteration 243, Loss: 0.1243755891919136\n",
      "Epoch 7, Iteration 244, Loss: 0.14786942303180695\n",
      "Epoch 7, Iteration 245, Loss: 0.13857437670230865\n",
      "Epoch 7, Iteration 246, Loss: 0.17220507562160492\n",
      "Epoch 7, Iteration 247, Loss: 0.2809196412563324\n",
      "Epoch 7, Iteration 248, Loss: 0.19335752725601196\n",
      "Epoch 7, Iteration 249, Loss: 0.20171958208084106\n",
      "Epoch 7, Iteration 250, Loss: 0.21886129677295685\n",
      "Epoch 7, Iteration 250, Test Loss: 0.26777637004852295\n",
      "Epoch 7, Iteration 251, Loss: 0.20439328253269196\n",
      "Epoch 7, Iteration 252, Loss: 0.13790082931518555\n",
      "Epoch 7, Iteration 253, Loss: 0.17247578501701355\n",
      "Epoch 7, Iteration 254, Loss: 0.18687276542186737\n",
      "Epoch 7, Iteration 255, Loss: 0.16881105303764343\n",
      "Epoch 7, Iteration 256, Loss: 0.14917385578155518\n",
      "Epoch 7, Iteration 257, Loss: 0.10901876538991928\n",
      "Epoch 7, Iteration 258, Loss: 0.146178737282753\n",
      "Epoch 7, Iteration 259, Loss: 0.15730763971805573\n",
      "Epoch 7, Iteration 260, Loss: 0.1919277161359787\n",
      "Epoch 7, Iteration 261, Loss: 0.12859100103378296\n",
      "Epoch 7, Iteration 262, Loss: 0.10404197871685028\n",
      "Epoch 7, Iteration 263, Loss: 0.1414100080728531\n",
      "Epoch 7, Iteration 264, Loss: 0.12115401029586792\n",
      "Epoch 7, Iteration 265, Loss: 0.2348959892988205\n",
      "Epoch 7, Iteration 266, Loss: 0.11296630650758743\n",
      "Epoch 7, Iteration 267, Loss: 0.1478392481803894\n",
      "Epoch 7, Iteration 268, Loss: 0.13866238296031952\n",
      "Epoch 7, Iteration 269, Loss: 0.1412879377603531\n",
      "Epoch 7, Iteration 270, Loss: 0.1673353761434555\n",
      "Epoch 7, Iteration 271, Loss: 0.08759188652038574\n",
      "Epoch 7, Iteration 272, Loss: 0.1405496448278427\n",
      "Epoch 7, Iteration 273, Loss: 0.1517961323261261\n",
      "Epoch 7, Iteration 274, Loss: 0.12481976300477982\n",
      "Epoch 7, Iteration 275, Loss: 0.10938452929258347\n",
      "Epoch 7, Iteration 276, Loss: 0.10583003610372543\n",
      "Epoch 7, Iteration 277, Loss: 0.10701607912778854\n",
      "Epoch 7, Iteration 278, Loss: 0.1426130086183548\n",
      "Epoch 7, Iteration 279, Loss: 0.09572333842515945\n",
      "Epoch 7, Iteration 280, Loss: 0.12952692806720734\n",
      "Epoch 7, Iteration 281, Loss: 0.1209450364112854\n",
      "Epoch 7, Iteration 282, Loss: 0.11386463791131973\n",
      "Epoch 7, Iteration 283, Loss: 0.08522278815507889\n",
      "Epoch 7, Iteration 284, Loss: 0.13672375679016113\n",
      "Epoch 7, Iteration 285, Loss: 0.09779791533946991\n",
      "Epoch 7, Iteration 286, Loss: 0.15296180546283722\n",
      "Epoch 7, Iteration 287, Loss: 0.10651063174009323\n",
      "Epoch 7, Iteration 288, Loss: 0.10909835994243622\n",
      "Epoch 7, Iteration 289, Loss: 0.1778288334608078\n",
      "Epoch 7, Iteration 290, Loss: 0.08945988863706589\n",
      "Epoch 7, Iteration 291, Loss: 0.10624773800373077\n",
      "Epoch 7, Iteration 292, Loss: 0.1613134890794754\n",
      "Epoch 7, Iteration 293, Loss: 0.1752997785806656\n",
      "Epoch 7, Iteration 294, Loss: 0.11026039719581604\n",
      "Epoch 7, Iteration 295, Loss: 0.07553227990865707\n",
      "Epoch 7, Iteration 296, Loss: 0.11058813333511353\n",
      "Epoch 7, Iteration 297, Loss: 0.1353718340396881\n",
      "Epoch 7, Iteration 298, Loss: 0.09620106965303421\n",
      "Epoch 7, Iteration 299, Loss: 0.18978086113929749\n",
      "Epoch 7, Iteration 300, Loss: 0.10363061726093292\n",
      "Epoch 7, Iteration 300, Test Loss: 0.2735617160797119\n",
      "Epoch 7, Iteration 301, Loss: 0.10689195990562439\n",
      "Epoch 7, Iteration 302, Loss: 0.09323655068874359\n",
      "Epoch 7, Iteration 303, Loss: 0.13722114264965057\n",
      "Epoch 7, Iteration 304, Loss: 0.16493433713912964\n",
      "Epoch 7, Iteration 305, Loss: 0.10228998959064484\n",
      "Epoch 7, Iteration 306, Loss: 0.10450191795825958\n",
      "Epoch 7, Iteration 307, Loss: 0.123822420835495\n",
      "Epoch 7, Iteration 308, Loss: 0.10318591445684433\n",
      "Epoch 7, Iteration 309, Loss: 0.09857845306396484\n",
      "Epoch 7, Iteration 310, Loss: 0.14501148462295532\n",
      "Epoch 7, Iteration 311, Loss: 0.10960301011800766\n",
      "Epoch 7, Iteration 312, Loss: 0.14973071217536926\n",
      "Epoch 7, Iteration 313, Loss: 0.119029201567173\n",
      "Epoch 7, Iteration 314, Loss: 0.12824763357639313\n",
      "Epoch 7, Iteration 315, Loss: 0.13178135454654694\n",
      "Epoch 7, Iteration 316, Loss: 0.127338245511055\n",
      "Epoch 7, Iteration 317, Loss: 0.15382567048072815\n",
      "Epoch 7, Iteration 318, Loss: 0.16684922575950623\n",
      "Epoch 7, Iteration 319, Loss: 0.08503906428813934\n",
      "Epoch 7, Iteration 320, Loss: 0.13551132380962372\n",
      "Epoch 7, Iteration 321, Loss: 0.10200142115354538\n",
      "Epoch 7, Iteration 322, Loss: 0.16612230241298676\n",
      "Epoch 7, Iteration 323, Loss: 0.11489570140838623\n",
      "Epoch 7, Iteration 324, Loss: 0.17294847965240479\n",
      "Epoch 7, Iteration 325, Loss: 0.2377479523420334\n",
      "Epoch 7, Iteration 326, Loss: 0.16258926689624786\n",
      "Epoch 7, Iteration 327, Loss: 0.14821317791938782\n",
      "Epoch 7, Iteration 328, Loss: 0.14911344647407532\n",
      "Epoch 7, Iteration 329, Loss: 0.1399960070848465\n",
      "Epoch 7, Iteration 330, Loss: 0.16532790660858154\n",
      "Epoch 7, Iteration 331, Loss: 0.150914266705513\n",
      "Epoch 7, Iteration 332, Loss: 0.16384842991828918\n",
      "Epoch 7, Iteration 333, Loss: 0.11813077330589294\n",
      "Epoch 7, Iteration 334, Loss: 0.20897383987903595\n",
      "Epoch 7, Iteration 335, Loss: 0.14014264941215515\n",
      "Epoch 7, Iteration 336, Loss: 0.18286742269992828\n",
      "Epoch 7, Iteration 337, Loss: 0.11448320001363754\n",
      "Epoch 7, Iteration 338, Loss: 0.1289239525794983\n",
      "Epoch 7, Iteration 339, Loss: 0.15789572894573212\n",
      "Epoch 7, Iteration 340, Loss: 0.1415044665336609\n",
      "Epoch 7, Iteration 341, Loss: 0.14488954842090607\n",
      "Epoch 7, Iteration 342, Loss: 0.1405177265405655\n",
      "Epoch 7, Iteration 343, Loss: 0.1845695525407791\n",
      "Epoch 7, Iteration 344, Loss: 0.18593332171440125\n",
      "Epoch 7, Iteration 345, Loss: 0.15108539164066315\n",
      "Epoch 7, Iteration 346, Loss: 0.16019576787948608\n",
      "Epoch 7, Iteration 347, Loss: 0.13667938113212585\n",
      "Epoch 7, Iteration 348, Loss: 0.1700768768787384\n",
      "Epoch 7, Iteration 349, Loss: 0.19402219355106354\n",
      "Epoch 7, Iteration 350, Loss: 0.20848020911216736\n",
      "Epoch 7, Iteration 350, Test Loss: 0.24355259537696838\n",
      "Epoch 7, Iteration 351, Loss: 0.15856696665287018\n",
      "Epoch 7, Iteration 352, Loss: 0.1473957896232605\n",
      "Epoch 7, Iteration 353, Loss: 0.19541725516319275\n",
      "Epoch 7, Iteration 354, Loss: 0.12138741463422775\n",
      "Epoch 7, Iteration 355, Loss: 0.12920555472373962\n",
      "Epoch 7, Iteration 356, Loss: 0.13696226477622986\n",
      "Epoch 7, Iteration 357, Loss: 0.1981707066297531\n",
      "Epoch 7, Iteration 358, Loss: 0.10342756658792496\n",
      "Epoch 7, Iteration 359, Loss: 0.16440722346305847\n",
      "Epoch 7, Iteration 360, Loss: 0.09276051819324493\n",
      "Epoch 7, Iteration 361, Loss: 0.1623591035604477\n",
      "Epoch 7, Iteration 362, Loss: 0.13819727301597595\n",
      "Epoch 7, Iteration 363, Loss: 0.12586091458797455\n",
      "Epoch 7, Iteration 364, Loss: 0.1327883005142212\n",
      "Epoch 7, Iteration 365, Loss: 0.11702501028776169\n",
      "Epoch 7, Iteration 366, Loss: 0.17331570386886597\n",
      "Epoch 7, Iteration 367, Loss: 0.15439225733280182\n",
      "Epoch 7, Iteration 368, Loss: 0.1421586573123932\n",
      "Epoch 7, Iteration 369, Loss: 0.132597878575325\n",
      "Epoch 7, Iteration 370, Loss: 0.1684216409921646\n",
      "Epoch 7, Iteration 371, Loss: 0.14143182337284088\n",
      "Epoch 7, Iteration 372, Loss: 0.12647265195846558\n",
      "Epoch 7, Iteration 373, Loss: 0.18549714982509613\n",
      "Epoch 7, Iteration 374, Loss: 0.11122087389230728\n",
      "Epoch 7, Iteration 375, Loss: 0.14321334660053253\n",
      "Epoch 7, Iteration 376, Loss: 0.15303578972816467\n",
      "Epoch 7, Iteration 377, Loss: 0.15420329570770264\n",
      "Epoch 7, Iteration 378, Loss: 0.1279069185256958\n",
      "Epoch 7, Iteration 379, Loss: 0.18131205439567566\n",
      "Epoch 7, Iteration 380, Loss: 0.16304677724838257\n",
      "Epoch 7, Iteration 381, Loss: 0.12539106607437134\n",
      "Epoch 7, Iteration 382, Loss: 0.15448316931724548\n",
      "Epoch 7, Iteration 383, Loss: 0.10203657299280167\n",
      "Epoch 7, Iteration 384, Loss: 0.13300813734531403\n",
      "Epoch 7, Iteration 385, Loss: 0.14025317132472992\n",
      "Epoch 7, Iteration 386, Loss: 0.13205944001674652\n",
      "Epoch 7, Iteration 387, Loss: 0.10254452377557755\n",
      "Epoch 7, Iteration 388, Loss: 0.09607209265232086\n",
      "Epoch 7, Iteration 389, Loss: 0.10759027302265167\n",
      "Epoch 7, Iteration 390, Loss: 0.12767432630062103\n",
      "Epoch 7, Iteration 391, Loss: 0.1011381670832634\n",
      "Epoch 7, Iteration 392, Loss: 0.20384123921394348\n",
      "Epoch 7, Iteration 393, Loss: 0.18109337985515594\n",
      "Epoch 7, Iteration 394, Loss: 0.12032688409090042\n",
      "Epoch 7, Iteration 395, Loss: 0.10681267082691193\n",
      "Epoch 7, Iteration 396, Loss: 0.13514743745326996\n",
      "Epoch 7, Iteration 397, Loss: 0.12915652990341187\n",
      "Epoch 7, Iteration 398, Loss: 0.21630273759365082\n",
      "Epoch 7, Iteration 399, Loss: 0.12360022217035294\n",
      "Epoch 7, Iteration 400, Loss: 0.10409818589687347\n",
      "Epoch 7, Iteration 400, Test Loss: 0.2556489109992981\n",
      "Epoch 7, Iteration 401, Loss: 0.16054615378379822\n",
      "Epoch 7, Iteration 402, Loss: 0.13195577263832092\n",
      "Epoch 7, Iteration 403, Loss: 0.14180715382099152\n",
      "Epoch 7, Iteration 404, Loss: 0.13444098830223083\n",
      "Epoch 7, Iteration 405, Loss: 0.0932256430387497\n",
      "Epoch 7, Iteration 406, Loss: 0.11328813433647156\n",
      "Epoch 7, Iteration 407, Loss: 0.1463240683078766\n",
      "Epoch 7, Iteration 408, Loss: 0.15383680164813995\n",
      "Epoch 7, Iteration 409, Loss: 0.09835954755544662\n",
      "Epoch 7, Iteration 410, Loss: 0.17146220803260803\n",
      "Epoch 7, Iteration 411, Loss: 0.1839199960231781\n",
      "Epoch 7, Iteration 412, Loss: 0.11416886746883392\n",
      "Epoch 7, Iteration 413, Loss: 0.15321269631385803\n",
      "Epoch 7, Iteration 414, Loss: 0.09684621542692184\n",
      "Epoch 7, Iteration 415, Loss: 0.14523126184940338\n",
      "Epoch 7, Iteration 416, Loss: 0.11222290247678757\n",
      "Epoch 7, Iteration 417, Loss: 0.14412620663642883\n",
      "Epoch 7, Iteration 418, Loss: 0.1274179369211197\n",
      "Epoch 7, Iteration 419, Loss: 0.12351083755493164\n",
      "Epoch 7, Iteration 420, Loss: 0.1314193159341812\n",
      "Epoch 7, Iteration 421, Loss: 0.0836225226521492\n",
      "Epoch 7, Iteration 422, Loss: 0.1253909319639206\n",
      "Epoch 7, Iteration 423, Loss: 0.12331698089838028\n",
      "Epoch 7, Iteration 424, Loss: 0.14323396980762482\n",
      "Epoch 7, Iteration 425, Loss: 0.130807563662529\n",
      "Epoch 7, Iteration 426, Loss: 0.11019562184810638\n",
      "Epoch 7, Iteration 427, Loss: 0.10669877380132675\n",
      "Epoch 7, Iteration 428, Loss: 0.13289868831634521\n",
      "Epoch 7, Iteration 429, Loss: 0.1364258974790573\n",
      "Epoch 7, Iteration 430, Loss: 0.12694333493709564\n",
      "Epoch 7, Iteration 431, Loss: 0.09442286938428879\n",
      "Epoch 8/15, Loss: 0.14696445423005908\n",
      "Epoch 8/15, Validation Accuracy: 0.9423715415019763\n",
      "Epoch 8, Iteration 0, Loss: 0.2554287016391754\n",
      "Epoch 8, Iteration 1, Loss: 0.20373868942260742\n",
      "Epoch 8, Iteration 2, Loss: 0.21823519468307495\n",
      "Epoch 8, Iteration 3, Loss: 0.15832187235355377\n",
      "Epoch 8, Iteration 4, Loss: 0.1968516856431961\n",
      "Epoch 8, Iteration 5, Loss: 0.22240620851516724\n",
      "Epoch 8, Iteration 6, Loss: 0.17005719244480133\n",
      "Epoch 8, Iteration 7, Loss: 0.23122401535511017\n",
      "Epoch 8, Iteration 8, Loss: 0.18279001116752625\n",
      "Epoch 8, Iteration 9, Loss: 0.1593179702758789\n",
      "Epoch 8, Iteration 10, Loss: 0.1905786246061325\n",
      "Epoch 8, Iteration 11, Loss: 0.1646367460489273\n",
      "Epoch 8, Iteration 12, Loss: 0.1679067611694336\n",
      "Epoch 8, Iteration 13, Loss: 0.20120881497859955\n",
      "Epoch 8, Iteration 14, Loss: 0.24576975405216217\n",
      "Epoch 8, Iteration 15, Loss: 0.12795281410217285\n",
      "Epoch 8, Iteration 16, Loss: 0.28787776827812195\n",
      "Epoch 8, Iteration 17, Loss: 0.16424983739852905\n",
      "Epoch 8, Iteration 18, Loss: 0.18475428223609924\n",
      "Epoch 8, Iteration 19, Loss: 0.16933827102184296\n",
      "Epoch 8, Iteration 20, Loss: 0.2609545886516571\n",
      "Epoch 8, Iteration 21, Loss: 0.22281819581985474\n",
      "Epoch 8, Iteration 22, Loss: 0.19161014258861542\n",
      "Epoch 8, Iteration 23, Loss: 0.1769137978553772\n",
      "Epoch 8, Iteration 24, Loss: 0.19767160713672638\n",
      "Epoch 8, Iteration 25, Loss: 0.1924324333667755\n",
      "Epoch 8, Iteration 26, Loss: 0.22353297472000122\n",
      "Epoch 8, Iteration 27, Loss: 0.15096084773540497\n",
      "Epoch 8, Iteration 28, Loss: 0.2341766506433487\n",
      "Epoch 8, Iteration 29, Loss: 0.14364278316497803\n",
      "Epoch 8, Iteration 30, Loss: 0.18652015924453735\n",
      "Epoch 8, Iteration 31, Loss: 0.1991518884897232\n",
      "Epoch 8, Iteration 32, Loss: 0.21483948826789856\n",
      "Epoch 8, Iteration 33, Loss: 0.16319990158081055\n",
      "Epoch 8, Iteration 34, Loss: 0.21286296844482422\n",
      "Epoch 8, Iteration 35, Loss: 0.14572978019714355\n",
      "Epoch 8, Iteration 36, Loss: 0.14219771325588226\n",
      "Epoch 8, Iteration 37, Loss: 0.16923969984054565\n",
      "Epoch 8, Iteration 38, Loss: 0.19812136888504028\n",
      "Epoch 8, Iteration 39, Loss: 0.19595493376255035\n",
      "Epoch 8, Iteration 40, Loss: 0.1600809395313263\n",
      "Epoch 8, Iteration 41, Loss: 0.16844289004802704\n",
      "Epoch 8, Iteration 42, Loss: 0.17130626738071442\n",
      "Epoch 8, Iteration 43, Loss: 0.1885133981704712\n",
      "Epoch 8, Iteration 44, Loss: 0.14421933889389038\n",
      "Epoch 8, Iteration 45, Loss: 0.1893528550863266\n",
      "Epoch 8, Iteration 46, Loss: 0.17445284128189087\n",
      "Epoch 8, Iteration 47, Loss: 0.22068347036838531\n",
      "Epoch 8, Iteration 48, Loss: 0.2228042185306549\n",
      "Epoch 8, Iteration 49, Loss: 0.13099533319473267\n",
      "Epoch 8, Iteration 50, Loss: 0.12656942009925842\n",
      "Epoch 8, Iteration 50, Test Loss: 0.21841351687908173\n",
      "Epoch 8, Iteration 51, Loss: 0.14452484250068665\n",
      "Epoch 8, Iteration 52, Loss: 0.17818424105644226\n",
      "Epoch 8, Iteration 53, Loss: 0.15047091245651245\n",
      "Epoch 8, Iteration 54, Loss: 0.1701386570930481\n",
      "Epoch 8, Iteration 55, Loss: 0.15181982517242432\n",
      "Epoch 8, Iteration 56, Loss: 0.1527818888425827\n",
      "Epoch 8, Iteration 57, Loss: 0.17063143849372864\n",
      "Epoch 8, Iteration 58, Loss: 0.10278329998254776\n",
      "Epoch 8, Iteration 59, Loss: 0.11226432770490646\n",
      "Epoch 8, Iteration 60, Loss: 0.17554420232772827\n",
      "Epoch 8, Iteration 61, Loss: 0.11861839145421982\n",
      "Epoch 8, Iteration 62, Loss: 0.2284538298845291\n",
      "Epoch 8, Iteration 63, Loss: 0.11572104692459106\n",
      "Epoch 8, Iteration 64, Loss: 0.12078536301851273\n",
      "Epoch 8, Iteration 65, Loss: 0.12828871607780457\n",
      "Epoch 8, Iteration 66, Loss: 0.13523827493190765\n",
      "Epoch 8, Iteration 67, Loss: 0.09551161527633667\n",
      "Epoch 8, Iteration 68, Loss: 0.13630132377147675\n",
      "Epoch 8, Iteration 69, Loss: 0.16738222539424896\n",
      "Epoch 8, Iteration 70, Loss: 0.09998118877410889\n",
      "Epoch 8, Iteration 71, Loss: 0.143629789352417\n",
      "Epoch 8, Iteration 72, Loss: 0.1592053920030594\n",
      "Epoch 8, Iteration 73, Loss: 0.1387639194726944\n",
      "Epoch 8, Iteration 74, Loss: 0.15073473751544952\n",
      "Epoch 8, Iteration 75, Loss: 0.15750710666179657\n",
      "Epoch 8, Iteration 76, Loss: 0.14213764667510986\n",
      "Epoch 8, Iteration 77, Loss: 0.1386854648590088\n",
      "Epoch 8, Iteration 78, Loss: 0.18701206147670746\n",
      "Epoch 8, Iteration 79, Loss: 0.10687275230884552\n",
      "Epoch 8, Iteration 80, Loss: 0.17115943133831024\n",
      "Epoch 8, Iteration 81, Loss: 0.14083020389080048\n",
      "Epoch 8, Iteration 82, Loss: 0.1896023452281952\n",
      "Epoch 8, Iteration 83, Loss: 0.16788305342197418\n",
      "Epoch 8, Iteration 84, Loss: 0.10198609530925751\n",
      "Epoch 8, Iteration 85, Loss: 0.1725783497095108\n",
      "Epoch 8, Iteration 86, Loss: 0.17761778831481934\n",
      "Epoch 8, Iteration 87, Loss: 0.1674361526966095\n",
      "Epoch 8, Iteration 88, Loss: 0.13470537960529327\n",
      "Epoch 8, Iteration 89, Loss: 0.1288250982761383\n",
      "Epoch 8, Iteration 90, Loss: 0.13119378685951233\n",
      "Epoch 8, Iteration 91, Loss: 0.11153478175401688\n",
      "Epoch 8, Iteration 92, Loss: 0.12385927140712738\n",
      "Epoch 8, Iteration 93, Loss: 0.13862834870815277\n",
      "Epoch 8, Iteration 94, Loss: 0.19308634102344513\n",
      "Epoch 8, Iteration 95, Loss: 0.1554456204175949\n",
      "Epoch 8, Iteration 96, Loss: 0.09063145518302917\n",
      "Epoch 8, Iteration 97, Loss: 0.14207823574543\n",
      "Epoch 8, Iteration 98, Loss: 0.1516748070716858\n",
      "Epoch 8, Iteration 99, Loss: 0.11127184331417084\n",
      "Epoch 8, Iteration 100, Loss: 0.1042427346110344\n",
      "Epoch 8, Iteration 100, Test Loss: 0.24302083253860474\n",
      "Epoch 8, Iteration 101, Loss: 0.13963516056537628\n",
      "Epoch 8, Iteration 102, Loss: 0.1271783709526062\n",
      "Epoch 8, Iteration 103, Loss: 0.1068095862865448\n",
      "Epoch 8, Iteration 104, Loss: 0.14265023171901703\n",
      "Epoch 8, Iteration 105, Loss: 0.13720062375068665\n",
      "Epoch 8, Iteration 106, Loss: 0.10812763124704361\n",
      "Epoch 8, Iteration 107, Loss: 0.10184378176927567\n",
      "Epoch 8, Iteration 108, Loss: 0.16419298946857452\n",
      "Epoch 8, Iteration 109, Loss: 0.1787310093641281\n",
      "Epoch 8, Iteration 110, Loss: 0.11076667904853821\n",
      "Epoch 8, Iteration 111, Loss: 0.09916995465755463\n",
      "Epoch 8, Iteration 112, Loss: 0.1380607932806015\n",
      "Epoch 8, Iteration 113, Loss: 0.11618111282587051\n",
      "Epoch 8, Iteration 114, Loss: 0.1275814324617386\n",
      "Epoch 8, Iteration 115, Loss: 0.1660849153995514\n",
      "Epoch 8, Iteration 116, Loss: 0.15064647793769836\n",
      "Epoch 8, Iteration 117, Loss: 0.1890362948179245\n",
      "Epoch 8, Iteration 118, Loss: 0.13696981966495514\n",
      "Epoch 8, Iteration 119, Loss: 0.16266389191150665\n",
      "Epoch 8, Iteration 120, Loss: 0.10718133300542831\n",
      "Epoch 8, Iteration 121, Loss: 0.15438596904277802\n",
      "Epoch 8, Iteration 122, Loss: 0.15711857378482819\n",
      "Epoch 8, Iteration 123, Loss: 0.119249127805233\n",
      "Epoch 8, Iteration 124, Loss: 0.1532929390668869\n",
      "Epoch 8, Iteration 125, Loss: 0.1790371537208557\n",
      "Epoch 8, Iteration 126, Loss: 0.1335444301366806\n",
      "Epoch 8, Iteration 127, Loss: 0.1296779066324234\n",
      "Epoch 8, Iteration 128, Loss: 0.09989900141954422\n",
      "Epoch 8, Iteration 129, Loss: 0.16948619484901428\n",
      "Epoch 8, Iteration 130, Loss: 0.11264286190271378\n",
      "Epoch 8, Iteration 131, Loss: 0.12340035289525986\n",
      "Epoch 8, Iteration 132, Loss: 0.1940077245235443\n",
      "Epoch 8, Iteration 133, Loss: 0.21816515922546387\n",
      "Epoch 8, Iteration 134, Loss: 0.15120990574359894\n",
      "Epoch 8, Iteration 135, Loss: 0.15987101197242737\n",
      "Epoch 8, Iteration 136, Loss: 0.100496307015419\n",
      "Epoch 8, Iteration 137, Loss: 0.1193557158112526\n",
      "Epoch 8, Iteration 138, Loss: 0.14625801146030426\n",
      "Epoch 8, Iteration 139, Loss: 0.15217986702919006\n",
      "Epoch 8, Iteration 140, Loss: 0.1741626262664795\n",
      "Epoch 8, Iteration 141, Loss: 0.12133564800024033\n",
      "Epoch 8, Iteration 142, Loss: 0.17394094169139862\n",
      "Epoch 8, Iteration 143, Loss: 0.16365639865398407\n",
      "Epoch 8, Iteration 144, Loss: 0.11468980461359024\n",
      "Epoch 8, Iteration 145, Loss: 0.17688453197479248\n",
      "Epoch 8, Iteration 146, Loss: 0.11169768869876862\n",
      "Epoch 8, Iteration 147, Loss: 0.12543852627277374\n",
      "Epoch 8, Iteration 148, Loss: 0.11377596855163574\n",
      "Epoch 8, Iteration 149, Loss: 0.11011359840631485\n",
      "Epoch 8, Iteration 150, Loss: 0.17227399349212646\n",
      "Epoch 8, Iteration 150, Test Loss: 0.24133439362049103\n",
      "Epoch 8, Iteration 151, Loss: 0.14635393023490906\n",
      "Epoch 8, Iteration 152, Loss: 0.15261539816856384\n",
      "Epoch 8, Iteration 153, Loss: 0.1364193856716156\n",
      "Epoch 8, Iteration 154, Loss: 0.13861465454101562\n",
      "Epoch 8, Iteration 155, Loss: 0.16165782511234283\n",
      "Epoch 8, Iteration 156, Loss: 0.16430741548538208\n",
      "Epoch 8, Iteration 157, Loss: 0.2112538069486618\n",
      "Epoch 8, Iteration 158, Loss: 0.18782241642475128\n",
      "Epoch 8, Iteration 159, Loss: 0.1409519761800766\n",
      "Epoch 8, Iteration 160, Loss: 0.19646665453910828\n",
      "Epoch 8, Iteration 161, Loss: 0.08951017260551453\n",
      "Epoch 8, Iteration 162, Loss: 0.07641955465078354\n",
      "Epoch 8, Iteration 163, Loss: 0.1462114453315735\n",
      "Epoch 8, Iteration 164, Loss: 0.10626789927482605\n",
      "Epoch 8, Iteration 165, Loss: 0.10210250318050385\n",
      "Epoch 8, Iteration 166, Loss: 0.13325899839401245\n",
      "Epoch 8, Iteration 167, Loss: 0.1327880173921585\n",
      "Epoch 8, Iteration 168, Loss: 0.11862679570913315\n",
      "Epoch 8, Iteration 169, Loss: 0.1267136037349701\n",
      "Epoch 8, Iteration 170, Loss: 0.09734353423118591\n",
      "Epoch 8, Iteration 171, Loss: 0.17160005867481232\n",
      "Epoch 8, Iteration 172, Loss: 0.18286585807800293\n",
      "Epoch 8, Iteration 173, Loss: 0.1296834796667099\n",
      "Epoch 8, Iteration 174, Loss: 0.14618521928787231\n",
      "Epoch 8, Iteration 175, Loss: 0.13731856644153595\n",
      "Epoch 8, Iteration 176, Loss: 0.10500846803188324\n",
      "Epoch 8, Iteration 177, Loss: 0.16966412961483002\n",
      "Epoch 8, Iteration 178, Loss: 0.1393604278564453\n",
      "Epoch 8, Iteration 179, Loss: 0.12236262112855911\n",
      "Epoch 8, Iteration 180, Loss: 0.1551142781972885\n",
      "Epoch 8, Iteration 181, Loss: 0.13072170317173004\n",
      "Epoch 8, Iteration 182, Loss: 0.10600710660219193\n",
      "Epoch 8, Iteration 183, Loss: 0.1354207545518875\n",
      "Epoch 8, Iteration 184, Loss: 0.19195117056369781\n",
      "Epoch 8, Iteration 185, Loss: 0.10239242762327194\n",
      "Epoch 8, Iteration 186, Loss: 0.12094289809465408\n",
      "Epoch 8, Iteration 187, Loss: 0.1342533379793167\n",
      "Epoch 8, Iteration 188, Loss: 0.10635732859373093\n",
      "Epoch 8, Iteration 189, Loss: 0.08452858775854111\n",
      "Epoch 8, Iteration 190, Loss: 0.10251600295305252\n",
      "Epoch 8, Iteration 191, Loss: 0.10977169126272202\n",
      "Epoch 8, Iteration 192, Loss: 0.11465459316968918\n",
      "Epoch 8, Iteration 193, Loss: 0.12248358130455017\n",
      "Epoch 8, Iteration 194, Loss: 0.12432385236024857\n",
      "Epoch 8, Iteration 195, Loss: 0.12972530722618103\n",
      "Epoch 8, Iteration 196, Loss: 0.22527951002120972\n",
      "Epoch 8, Iteration 197, Loss: 0.12458284199237823\n",
      "Epoch 8, Iteration 198, Loss: 0.09366272389888763\n",
      "Epoch 8, Iteration 199, Loss: 0.1294967234134674\n",
      "Epoch 8, Iteration 200, Loss: 0.11062123626470566\n",
      "Epoch 8, Iteration 200, Test Loss: 0.25356924533843994\n",
      "Epoch 8, Iteration 201, Loss: 0.1347828209400177\n",
      "Epoch 8, Iteration 202, Loss: 0.09562330693006516\n",
      "Epoch 8, Iteration 203, Loss: 0.12833181023597717\n",
      "Epoch 8, Iteration 204, Loss: 0.12474635243415833\n",
      "Epoch 8, Iteration 205, Loss: 0.13983546197414398\n",
      "Epoch 8, Iteration 206, Loss: 0.11262527853250504\n",
      "Epoch 8, Iteration 207, Loss: 0.10124075412750244\n",
      "Epoch 8, Iteration 208, Loss: 0.09806454181671143\n",
      "Epoch 8, Iteration 209, Loss: 0.11501917243003845\n",
      "Epoch 8, Iteration 210, Loss: 0.09170515090227127\n",
      "Epoch 8, Iteration 211, Loss: 0.1322437822818756\n",
      "Epoch 8, Iteration 212, Loss: 0.15189306437969208\n",
      "Epoch 8, Iteration 213, Loss: 0.1201731339097023\n",
      "Epoch 8, Iteration 214, Loss: 0.10477228462696075\n",
      "Epoch 8, Iteration 215, Loss: 0.12829451262950897\n",
      "Epoch 8, Iteration 216, Loss: 0.16615711152553558\n",
      "Epoch 8, Iteration 217, Loss: 0.15279875695705414\n",
      "Epoch 8, Iteration 218, Loss: 0.14321035146713257\n",
      "Epoch 8, Iteration 219, Loss: 0.1341855376958847\n",
      "Epoch 8, Iteration 220, Loss: 0.10809038579463959\n",
      "Epoch 8, Iteration 221, Loss: 0.14649614691734314\n",
      "Epoch 8, Iteration 222, Loss: 0.13367512822151184\n",
      "Epoch 8, Iteration 223, Loss: 0.12337257713079453\n",
      "Epoch 8, Iteration 224, Loss: 0.1205640509724617\n",
      "Epoch 8, Iteration 225, Loss: 0.13009847700595856\n",
      "Epoch 8, Iteration 226, Loss: 0.15303154289722443\n",
      "Epoch 8, Iteration 227, Loss: 0.11248179525136948\n",
      "Epoch 8, Iteration 228, Loss: 0.2822530269622803\n",
      "Epoch 8, Iteration 229, Loss: 0.19250068068504333\n",
      "Epoch 8, Iteration 230, Loss: 0.17896662652492523\n",
      "Epoch 8, Iteration 231, Loss: 0.09672882407903671\n",
      "Epoch 8, Iteration 232, Loss: 0.14186739921569824\n",
      "Epoch 8, Iteration 233, Loss: 0.1640709787607193\n",
      "Epoch 8, Iteration 234, Loss: 0.10976682603359222\n",
      "Epoch 8, Iteration 235, Loss: 0.09742680937051773\n",
      "Epoch 8, Iteration 236, Loss: 0.1467626392841339\n",
      "Epoch 8, Iteration 237, Loss: 0.15423732995986938\n",
      "Epoch 8, Iteration 238, Loss: 0.14572888612747192\n",
      "Epoch 8, Iteration 239, Loss: 0.1163971796631813\n",
      "Epoch 8, Iteration 240, Loss: 0.1286756545305252\n",
      "Epoch 8, Iteration 241, Loss: 0.1281144917011261\n",
      "Epoch 8, Iteration 242, Loss: 0.14624904096126556\n",
      "Epoch 8, Iteration 243, Loss: 0.1321130394935608\n",
      "Epoch 8, Iteration 244, Loss: 0.14214088022708893\n",
      "Epoch 8, Iteration 245, Loss: 0.12053243070840836\n",
      "Epoch 8, Iteration 246, Loss: 0.15476928651332855\n",
      "Epoch 8, Iteration 247, Loss: 0.26568159461021423\n",
      "Epoch 8, Iteration 248, Loss: 0.20536425709724426\n",
      "Epoch 8, Iteration 249, Loss: 0.21211117506027222\n",
      "Epoch 8, Iteration 250, Loss: 0.22354082763195038\n",
      "Epoch 8, Iteration 250, Test Loss: 0.25260722637176514\n",
      "Epoch 8, Iteration 251, Loss: 0.223736971616745\n",
      "Epoch 8, Iteration 252, Loss: 0.14713750779628754\n",
      "Epoch 8, Iteration 253, Loss: 0.16819354891777039\n",
      "Epoch 8, Iteration 254, Loss: 0.19286657869815826\n",
      "Epoch 8, Iteration 255, Loss: 0.16002127528190613\n",
      "Epoch 8, Iteration 256, Loss: 0.12458285689353943\n",
      "Epoch 8, Iteration 257, Loss: 0.10977273434400558\n",
      "Epoch 8, Iteration 258, Loss: 0.14545989036560059\n",
      "Epoch 8, Iteration 259, Loss: 0.13908575475215912\n",
      "Epoch 8, Iteration 260, Loss: 0.19195657968521118\n",
      "Epoch 8, Iteration 261, Loss: 0.11716194450855255\n",
      "Epoch 8, Iteration 262, Loss: 0.10694228112697601\n",
      "Epoch 8, Iteration 263, Loss: 0.12548120319843292\n",
      "Epoch 8, Iteration 264, Loss: 0.12979374825954437\n",
      "Epoch 8, Iteration 265, Loss: 0.22824726998806\n",
      "Epoch 8, Iteration 266, Loss: 0.11436322331428528\n",
      "Epoch 8, Iteration 267, Loss: 0.14552803337574005\n",
      "Epoch 8, Iteration 268, Loss: 0.13186246156692505\n",
      "Epoch 8, Iteration 269, Loss: 0.15144138038158417\n",
      "Epoch 8, Iteration 270, Loss: 0.17708759009838104\n",
      "Epoch 8, Iteration 271, Loss: 0.08156431466341019\n",
      "Epoch 8, Iteration 272, Loss: 0.14832338690757751\n",
      "Epoch 8, Iteration 273, Loss: 0.14847591519355774\n",
      "Epoch 8, Iteration 274, Loss: 0.12089798599481583\n",
      "Epoch 8, Iteration 275, Loss: 0.10013902187347412\n",
      "Epoch 8, Iteration 276, Loss: 0.10056179016828537\n",
      "Epoch 8, Iteration 277, Loss: 0.12663134932518005\n",
      "Epoch 8, Iteration 278, Loss: 0.1459730714559555\n",
      "Epoch 8, Iteration 279, Loss: 0.0977642685174942\n",
      "Epoch 8, Iteration 280, Loss: 0.1617811769247055\n",
      "Epoch 8, Iteration 281, Loss: 0.12230896204710007\n",
      "Epoch 8, Iteration 282, Loss: 0.11808615177869797\n",
      "Epoch 8, Iteration 283, Loss: 0.09967571496963501\n",
      "Epoch 8, Iteration 284, Loss: 0.14890769124031067\n",
      "Epoch 8, Iteration 285, Loss: 0.10590989887714386\n",
      "Epoch 8, Iteration 286, Loss: 0.14441987872123718\n",
      "Epoch 8, Iteration 287, Loss: 0.08963248878717422\n",
      "Epoch 8, Iteration 288, Loss: 0.12991872429847717\n",
      "Epoch 8, Iteration 289, Loss: 0.16521646082401276\n",
      "Epoch 8, Iteration 290, Loss: 0.08373114466667175\n",
      "Epoch 8, Iteration 291, Loss: 0.11351872980594635\n",
      "Epoch 8, Iteration 292, Loss: 0.12661637365818024\n",
      "Epoch 8, Iteration 293, Loss: 0.16548795998096466\n",
      "Epoch 8, Iteration 294, Loss: 0.11720848083496094\n",
      "Epoch 8, Iteration 295, Loss: 0.10442431271076202\n",
      "Epoch 8, Iteration 296, Loss: 0.10654564946889877\n",
      "Epoch 8, Iteration 297, Loss: 0.12557944655418396\n",
      "Epoch 8, Iteration 298, Loss: 0.09881774336099625\n",
      "Epoch 8, Iteration 299, Loss: 0.16429689526557922\n",
      "Epoch 8, Iteration 300, Loss: 0.1063433438539505\n",
      "Epoch 8, Iteration 300, Test Loss: 0.26030129194259644\n",
      "Epoch 8, Iteration 301, Loss: 0.10963620990514755\n",
      "Epoch 8, Iteration 302, Loss: 0.09558818489313126\n",
      "Epoch 8, Iteration 303, Loss: 0.1455889493227005\n",
      "Epoch 8, Iteration 304, Loss: 0.15919990837574005\n",
      "Epoch 8, Iteration 305, Loss: 0.11506079882383347\n",
      "Epoch 8, Iteration 306, Loss: 0.11631873995065689\n",
      "Epoch 8, Iteration 307, Loss: 0.12466973066329956\n",
      "Epoch 8, Iteration 308, Loss: 0.11215144395828247\n",
      "Epoch 8, Iteration 309, Loss: 0.09927212446928024\n",
      "Epoch 8, Iteration 310, Loss: 0.13385914266109467\n",
      "Epoch 8, Iteration 311, Loss: 0.10318568348884583\n",
      "Epoch 8, Iteration 312, Loss: 0.16515208780765533\n",
      "Epoch 8, Iteration 313, Loss: 0.10786936432123184\n",
      "Epoch 8, Iteration 314, Loss: 0.10318818688392639\n",
      "Epoch 8, Iteration 315, Loss: 0.10977499932050705\n",
      "Epoch 8, Iteration 316, Loss: 0.10259567201137543\n",
      "Epoch 8, Iteration 317, Loss: 0.13633929193019867\n",
      "Epoch 8, Iteration 318, Loss: 0.15110169351100922\n",
      "Epoch 8, Iteration 319, Loss: 0.10454297065734863\n",
      "Epoch 8, Iteration 320, Loss: 0.1268998384475708\n",
      "Epoch 8, Iteration 321, Loss: 0.09890914708375931\n",
      "Epoch 8, Iteration 322, Loss: 0.17536821961402893\n",
      "Epoch 8, Iteration 323, Loss: 0.0879964604973793\n",
      "Epoch 8, Iteration 324, Loss: 0.15638113021850586\n",
      "Epoch 8, Iteration 325, Loss: 0.20734107494354248\n",
      "Epoch 8, Iteration 326, Loss: 0.17208723723888397\n",
      "Epoch 8, Iteration 327, Loss: 0.16025440394878387\n",
      "Epoch 8, Iteration 328, Loss: 0.14380492269992828\n",
      "Epoch 8, Iteration 329, Loss: 0.14515581727027893\n",
      "Epoch 8, Iteration 330, Loss: 0.185417041182518\n",
      "Epoch 8, Iteration 331, Loss: 0.15223151445388794\n",
      "Epoch 8, Iteration 332, Loss: 0.16999360918998718\n",
      "Epoch 8, Iteration 333, Loss: 0.1347970962524414\n",
      "Epoch 8, Iteration 334, Loss: 0.20969711244106293\n",
      "Epoch 8, Iteration 335, Loss: 0.16373087465763092\n",
      "Epoch 8, Iteration 336, Loss: 0.17113466560840607\n",
      "Epoch 8, Iteration 337, Loss: 0.11963517218828201\n",
      "Epoch 8, Iteration 338, Loss: 0.13875246047973633\n",
      "Epoch 8, Iteration 339, Loss: 0.15921646356582642\n",
      "Epoch 8, Iteration 340, Loss: 0.158263698220253\n",
      "Epoch 8, Iteration 341, Loss: 0.14557842910289764\n",
      "Epoch 8, Iteration 342, Loss: 0.12988722324371338\n",
      "Epoch 8, Iteration 343, Loss: 0.1775514781475067\n",
      "Epoch 8, Iteration 344, Loss: 0.182095468044281\n",
      "Epoch 8, Iteration 345, Loss: 0.160086527466774\n",
      "Epoch 8, Iteration 346, Loss: 0.14232154190540314\n",
      "Epoch 8, Iteration 347, Loss: 0.16149181127548218\n",
      "Epoch 8, Iteration 348, Loss: 0.21092095971107483\n",
      "Epoch 8, Iteration 349, Loss: 0.19238123297691345\n",
      "Epoch 8, Iteration 350, Loss: 0.19836680591106415\n",
      "Epoch 8, Iteration 350, Test Loss: 0.24068120121955872\n",
      "Epoch 8, Iteration 351, Loss: 0.15224167704582214\n",
      "Epoch 8, Iteration 352, Loss: 0.152016282081604\n",
      "Epoch 8, Iteration 353, Loss: 0.20994772017002106\n",
      "Epoch 8, Iteration 354, Loss: 0.12410526722669601\n",
      "Epoch 8, Iteration 355, Loss: 0.1315355747938156\n",
      "Epoch 8, Iteration 356, Loss: 0.13246862590312958\n",
      "Epoch 8, Iteration 357, Loss: 0.2152845710515976\n",
      "Epoch 8, Iteration 358, Loss: 0.11125734448432922\n",
      "Epoch 8, Iteration 359, Loss: 0.1763557493686676\n",
      "Epoch 8, Iteration 360, Loss: 0.09608587622642517\n",
      "Epoch 8, Iteration 361, Loss: 0.16273730993270874\n",
      "Epoch 8, Iteration 362, Loss: 0.12960760295391083\n",
      "Epoch 8, Iteration 363, Loss: 0.1079402044415474\n",
      "Epoch 8, Iteration 364, Loss: 0.12222251296043396\n",
      "Epoch 8, Iteration 365, Loss: 0.10294713079929352\n",
      "Epoch 8, Iteration 366, Loss: 0.15961292386054993\n",
      "Epoch 8, Iteration 367, Loss: 0.15800757706165314\n",
      "Epoch 8, Iteration 368, Loss: 0.1189727857708931\n",
      "Epoch 8, Iteration 369, Loss: 0.13155464828014374\n",
      "Epoch 8, Iteration 370, Loss: 0.14733563363552094\n",
      "Epoch 8, Iteration 371, Loss: 0.12514346837997437\n",
      "Epoch 8, Iteration 372, Loss: 0.11083769053220749\n",
      "Epoch 8, Iteration 373, Loss: 0.17165271937847137\n",
      "Epoch 8, Iteration 374, Loss: 0.1111808493733406\n",
      "Epoch 8, Iteration 375, Loss: 0.13944238424301147\n",
      "Epoch 8, Iteration 376, Loss: 0.1635536253452301\n",
      "Epoch 8, Iteration 377, Loss: 0.13490146398544312\n",
      "Epoch 8, Iteration 378, Loss: 0.1256592571735382\n",
      "Epoch 8, Iteration 379, Loss: 0.16830414533615112\n",
      "Epoch 8, Iteration 380, Loss: 0.16021892428398132\n",
      "Epoch 8, Iteration 381, Loss: 0.11901714652776718\n",
      "Epoch 8, Iteration 382, Loss: 0.13366052508354187\n",
      "Epoch 8, Iteration 383, Loss: 0.10017207264900208\n",
      "Epoch 8, Iteration 384, Loss: 0.13865795731544495\n",
      "Epoch 8, Iteration 385, Loss: 0.12691712379455566\n",
      "Epoch 8, Iteration 386, Loss: 0.13926155865192413\n",
      "Epoch 8, Iteration 387, Loss: 0.09765435010194778\n",
      "Epoch 8, Iteration 388, Loss: 0.11224353313446045\n",
      "Epoch 8, Iteration 389, Loss: 0.10255709290504456\n",
      "Epoch 8, Iteration 390, Loss: 0.12001923471689224\n",
      "Epoch 8, Iteration 391, Loss: 0.07064884901046753\n",
      "Epoch 8, Iteration 392, Loss: 0.19400842487812042\n",
      "Epoch 8, Iteration 393, Loss: 0.20099608600139618\n",
      "Epoch 8, Iteration 394, Loss: 0.12574750185012817\n",
      "Epoch 8, Iteration 395, Loss: 0.11298119276762009\n",
      "Epoch 8, Iteration 396, Loss: 0.15600237250328064\n",
      "Epoch 8, Iteration 397, Loss: 0.1192924901843071\n",
      "Epoch 8, Iteration 398, Loss: 0.1944119930267334\n",
      "Epoch 8, Iteration 399, Loss: 0.11076760292053223\n",
      "Epoch 8, Iteration 400, Loss: 0.10350918024778366\n",
      "Epoch 8, Iteration 400, Test Loss: 0.25480467081069946\n",
      "Epoch 8, Iteration 401, Loss: 0.14420926570892334\n",
      "Epoch 8, Iteration 402, Loss: 0.09320687502622604\n",
      "Epoch 8, Iteration 403, Loss: 0.12826061248779297\n",
      "Epoch 8, Iteration 404, Loss: 0.14685291051864624\n",
      "Epoch 8, Iteration 405, Loss: 0.09082959592342377\n",
      "Epoch 8, Iteration 406, Loss: 0.11805307120084763\n",
      "Epoch 8, Iteration 407, Loss: 0.12691070139408112\n",
      "Epoch 8, Iteration 408, Loss: 0.14729063212871552\n",
      "Epoch 8, Iteration 409, Loss: 0.10470084846019745\n",
      "Epoch 8, Iteration 410, Loss: 0.1867242008447647\n",
      "Epoch 8, Iteration 411, Loss: 0.17850780487060547\n",
      "Epoch 8, Iteration 412, Loss: 0.12183777987957001\n",
      "Epoch 8, Iteration 413, Loss: 0.16102777421474457\n",
      "Epoch 8, Iteration 414, Loss: 0.09761399030685425\n",
      "Epoch 8, Iteration 415, Loss: 0.12935395538806915\n",
      "Epoch 8, Iteration 416, Loss: 0.11406620591878891\n",
      "Epoch 8, Iteration 417, Loss: 0.13424575328826904\n",
      "Epoch 8, Iteration 418, Loss: 0.11556872725486755\n",
      "Epoch 8, Iteration 419, Loss: 0.12364732474088669\n",
      "Epoch 8, Iteration 420, Loss: 0.14330434799194336\n",
      "Epoch 8, Iteration 421, Loss: 0.09971547871828079\n",
      "Epoch 8, Iteration 422, Loss: 0.12954774498939514\n",
      "Epoch 8, Iteration 423, Loss: 0.11945950984954834\n",
      "Epoch 8, Iteration 424, Loss: 0.13195668160915375\n",
      "Epoch 8, Iteration 425, Loss: 0.1254972517490387\n",
      "Epoch 8, Iteration 426, Loss: 0.09993037581443787\n",
      "Epoch 8, Iteration 427, Loss: 0.11795172840356827\n",
      "Epoch 8, Iteration 428, Loss: 0.13275544345378876\n",
      "Epoch 8, Iteration 429, Loss: 0.14261537790298462\n",
      "Epoch 8, Iteration 430, Loss: 0.12399420142173767\n",
      "Epoch 8, Iteration 431, Loss: 0.11460357159376144\n",
      "Epoch 9/15, Loss: 0.1443821823189932\n",
      "Epoch 9/15, Validation Accuracy: 0.9426877470355731\n",
      "Epoch 9, Iteration 0, Loss: 0.30776095390319824\n",
      "Epoch 9, Iteration 1, Loss: 0.20650440454483032\n",
      "Epoch 9, Iteration 2, Loss: 0.24164451658725739\n",
      "Epoch 9, Iteration 3, Loss: 0.15079396963119507\n",
      "Epoch 9, Iteration 4, Loss: 0.20219174027442932\n",
      "Epoch 9, Iteration 5, Loss: 0.25044357776641846\n",
      "Epoch 9, Iteration 6, Loss: 0.17389710247516632\n",
      "Epoch 9, Iteration 7, Loss: 0.24291618168354034\n",
      "Epoch 9, Iteration 8, Loss: 0.18799123167991638\n",
      "Epoch 9, Iteration 9, Loss: 0.1835104078054428\n",
      "Epoch 9, Iteration 10, Loss: 0.20313261449337006\n",
      "Epoch 9, Iteration 11, Loss: 0.13869701325893402\n",
      "Epoch 9, Iteration 12, Loss: 0.19002094864845276\n",
      "Epoch 9, Iteration 13, Loss: 0.22643794119358063\n",
      "Epoch 9, Iteration 14, Loss: 0.20395244657993317\n",
      "Epoch 9, Iteration 15, Loss: 0.08334391564130783\n",
      "Epoch 9, Iteration 16, Loss: 0.23650160431861877\n",
      "Epoch 9, Iteration 17, Loss: 0.18376396596431732\n",
      "Epoch 9, Iteration 18, Loss: 0.18359464406967163\n",
      "Epoch 9, Iteration 19, Loss: 0.15476848185062408\n",
      "Epoch 9, Iteration 20, Loss: 0.20527233183383942\n",
      "Epoch 9, Iteration 21, Loss: 0.2035193145275116\n",
      "Epoch 9, Iteration 22, Loss: 0.18859481811523438\n",
      "Epoch 9, Iteration 23, Loss: 0.21518126130104065\n",
      "Epoch 9, Iteration 24, Loss: 0.14686718583106995\n",
      "Epoch 9, Iteration 25, Loss: 0.23782122135162354\n",
      "Epoch 9, Iteration 26, Loss: 0.18958014249801636\n",
      "Epoch 9, Iteration 27, Loss: 0.14892254769802094\n",
      "Epoch 9, Iteration 28, Loss: 0.22687235474586487\n",
      "Epoch 9, Iteration 29, Loss: 0.1527029275894165\n",
      "Epoch 9, Iteration 30, Loss: 0.21953743696212769\n",
      "Epoch 9, Iteration 31, Loss: 0.1842324286699295\n",
      "Epoch 9, Iteration 32, Loss: 0.20984914898872375\n",
      "Epoch 9, Iteration 33, Loss: 0.16103091835975647\n",
      "Epoch 9, Iteration 34, Loss: 0.1786998212337494\n",
      "Epoch 9, Iteration 35, Loss: 0.14426392316818237\n",
      "Epoch 9, Iteration 36, Loss: 0.1617342084646225\n",
      "Epoch 9, Iteration 37, Loss: 0.1560267060995102\n",
      "Epoch 9, Iteration 38, Loss: 0.17340616881847382\n",
      "Epoch 9, Iteration 39, Loss: 0.1748308539390564\n",
      "Epoch 9, Iteration 40, Loss: 0.15945692360401154\n",
      "Epoch 9, Iteration 41, Loss: 0.1547057032585144\n",
      "Epoch 9, Iteration 42, Loss: 0.15441322326660156\n",
      "Epoch 9, Iteration 43, Loss: 0.15897499024868011\n",
      "Epoch 9, Iteration 44, Loss: 0.1371765434741974\n",
      "Epoch 9, Iteration 45, Loss: 0.15835505723953247\n",
      "Epoch 9, Iteration 46, Loss: 0.16377943754196167\n",
      "Epoch 9, Iteration 47, Loss: 0.20932932198047638\n",
      "Epoch 9, Iteration 48, Loss: 0.22968138754367828\n",
      "Epoch 9, Iteration 49, Loss: 0.1265464574098587\n",
      "Epoch 9, Iteration 50, Loss: 0.1143912672996521\n",
      "Epoch 9, Iteration 50, Test Loss: 0.2237909436225891\n",
      "Epoch 9, Iteration 51, Loss: 0.1465604305267334\n",
      "Epoch 9, Iteration 52, Loss: 0.1655743420124054\n",
      "Epoch 9, Iteration 53, Loss: 0.14402873814105988\n",
      "Epoch 9, Iteration 54, Loss: 0.1597183346748352\n",
      "Epoch 9, Iteration 55, Loss: 0.13173645734786987\n",
      "Epoch 9, Iteration 56, Loss: 0.15248370170593262\n",
      "Epoch 9, Iteration 57, Loss: 0.13448111712932587\n",
      "Epoch 9, Iteration 58, Loss: 0.12129056453704834\n",
      "Epoch 9, Iteration 59, Loss: 0.11373040825128555\n",
      "Epoch 9, Iteration 60, Loss: 0.17247824370861053\n",
      "Epoch 9, Iteration 61, Loss: 0.10816439986228943\n",
      "Epoch 9, Iteration 62, Loss: 0.22819465398788452\n",
      "Epoch 9, Iteration 63, Loss: 0.09655425697565079\n",
      "Epoch 9, Iteration 64, Loss: 0.11912790685892105\n",
      "Epoch 9, Iteration 65, Loss: 0.14517347514629364\n",
      "Epoch 9, Iteration 66, Loss: 0.12373196333646774\n",
      "Epoch 9, Iteration 67, Loss: 0.09661678224802017\n",
      "Epoch 9, Iteration 68, Loss: 0.14364087581634521\n",
      "Epoch 9, Iteration 69, Loss: 0.15698476135730743\n",
      "Epoch 9, Iteration 70, Loss: 0.10822919756174088\n",
      "Epoch 9, Iteration 71, Loss: 0.13803040981292725\n",
      "Epoch 9, Iteration 72, Loss: 0.16810110211372375\n",
      "Epoch 9, Iteration 73, Loss: 0.13783417642116547\n",
      "Epoch 9, Iteration 74, Loss: 0.1649523675441742\n",
      "Epoch 9, Iteration 75, Loss: 0.14833039045333862\n",
      "Epoch 9, Iteration 76, Loss: 0.14116418361663818\n",
      "Epoch 9, Iteration 77, Loss: 0.14412163197994232\n",
      "Epoch 9, Iteration 78, Loss: 0.1517927497625351\n",
      "Epoch 9, Iteration 79, Loss: 0.10888654738664627\n",
      "Epoch 9, Iteration 80, Loss: 0.15158624947071075\n",
      "Epoch 9, Iteration 81, Loss: 0.14735642075538635\n",
      "Epoch 9, Iteration 82, Loss: 0.17911376059055328\n",
      "Epoch 9, Iteration 83, Loss: 0.14015483856201172\n",
      "Epoch 9, Iteration 84, Loss: 0.12051035463809967\n",
      "Epoch 9, Iteration 85, Loss: 0.1587907373905182\n",
      "Epoch 9, Iteration 86, Loss: 0.17596128582954407\n",
      "Epoch 9, Iteration 87, Loss: 0.14031678438186646\n",
      "Epoch 9, Iteration 88, Loss: 0.15425342321395874\n",
      "Epoch 9, Iteration 89, Loss: 0.14240899682044983\n",
      "Epoch 9, Iteration 90, Loss: 0.15166142582893372\n",
      "Epoch 9, Iteration 91, Loss: 0.11743856966495514\n",
      "Epoch 9, Iteration 92, Loss: 0.11151324957609177\n",
      "Epoch 9, Iteration 93, Loss: 0.11395640671253204\n",
      "Epoch 9, Iteration 94, Loss: 0.17231838405132294\n",
      "Epoch 9, Iteration 95, Loss: 0.11549565196037292\n",
      "Epoch 9, Iteration 96, Loss: 0.08751432597637177\n",
      "Epoch 9, Iteration 97, Loss: 0.13807746767997742\n",
      "Epoch 9, Iteration 98, Loss: 0.15921162068843842\n",
      "Epoch 9, Iteration 99, Loss: 0.12274157255887985\n",
      "Epoch 9, Iteration 100, Loss: 0.10682950168848038\n",
      "Epoch 9, Iteration 100, Test Loss: 0.2543884813785553\n",
      "Epoch 9, Iteration 101, Loss: 0.13477763533592224\n",
      "Epoch 9, Iteration 102, Loss: 0.120689757168293\n",
      "Epoch 9, Iteration 103, Loss: 0.12077546864748001\n",
      "Epoch 9, Iteration 104, Loss: 0.1367342621088028\n",
      "Epoch 9, Iteration 105, Loss: 0.14102724194526672\n",
      "Epoch 9, Iteration 106, Loss: 0.1004336029291153\n",
      "Epoch 9, Iteration 107, Loss: 0.09133253246545792\n",
      "Epoch 9, Iteration 108, Loss: 0.1444937139749527\n",
      "Epoch 9, Iteration 109, Loss: 0.23847199976444244\n",
      "Epoch 9, Iteration 110, Loss: 0.10984285920858383\n",
      "Epoch 9, Iteration 111, Loss: 0.10817449539899826\n",
      "Epoch 9, Iteration 112, Loss: 0.14156962931156158\n",
      "Epoch 9, Iteration 113, Loss: 0.12771742045879364\n",
      "Epoch 9, Iteration 114, Loss: 0.12563396990299225\n",
      "Epoch 9, Iteration 115, Loss: 0.14411470293998718\n",
      "Epoch 9, Iteration 116, Loss: 0.13090918958187103\n",
      "Epoch 9, Iteration 117, Loss: 0.17518435418605804\n",
      "Epoch 9, Iteration 118, Loss: 0.14850331842899323\n",
      "Epoch 9, Iteration 119, Loss: 0.15473610162734985\n",
      "Epoch 9, Iteration 120, Loss: 0.10752802342176437\n",
      "Epoch 9, Iteration 121, Loss: 0.13952568173408508\n",
      "Epoch 9, Iteration 122, Loss: 0.1615847498178482\n",
      "Epoch 9, Iteration 123, Loss: 0.1275670975446701\n",
      "Epoch 9, Iteration 124, Loss: 0.1336110681295395\n",
      "Epoch 9, Iteration 125, Loss: 0.16898378729820251\n",
      "Epoch 9, Iteration 126, Loss: 0.11808022856712341\n",
      "Epoch 9, Iteration 127, Loss: 0.15105195343494415\n",
      "Epoch 9, Iteration 128, Loss: 0.13513970375061035\n",
      "Epoch 9, Iteration 129, Loss: 0.20396453142166138\n",
      "Epoch 9, Iteration 130, Loss: 0.10353941470384598\n",
      "Epoch 9, Iteration 131, Loss: 0.11829745769500732\n",
      "Epoch 9, Iteration 132, Loss: 0.15808364748954773\n",
      "Epoch 9, Iteration 133, Loss: 0.19740895926952362\n",
      "Epoch 9, Iteration 134, Loss: 0.1405860036611557\n",
      "Epoch 9, Iteration 135, Loss: 0.15456074476242065\n",
      "Epoch 9, Iteration 136, Loss: 0.09887760132551193\n",
      "Epoch 9, Iteration 137, Loss: 0.12192507088184357\n",
      "Epoch 9, Iteration 138, Loss: 0.1328044831752777\n",
      "Epoch 9, Iteration 139, Loss: 0.14950791001319885\n",
      "Epoch 9, Iteration 140, Loss: 0.19246616959571838\n",
      "Epoch 9, Iteration 141, Loss: 0.13964086771011353\n",
      "Epoch 9, Iteration 142, Loss: 0.16696026921272278\n",
      "Epoch 9, Iteration 143, Loss: 0.1386210024356842\n",
      "Epoch 9, Iteration 144, Loss: 0.09714533388614655\n",
      "Epoch 9, Iteration 145, Loss: 0.17892423272132874\n",
      "Epoch 9, Iteration 146, Loss: 0.11256379634141922\n",
      "Epoch 9, Iteration 147, Loss: 0.11113187670707703\n",
      "Epoch 9, Iteration 148, Loss: 0.12145792692899704\n",
      "Epoch 9, Iteration 149, Loss: 0.10770491510629654\n",
      "Epoch 9, Iteration 150, Loss: 0.173211008310318\n",
      "Epoch 9, Iteration 150, Test Loss: 0.24874675273895264\n",
      "Epoch 9, Iteration 151, Loss: 0.13300874829292297\n",
      "Epoch 9, Iteration 152, Loss: 0.1420726329088211\n",
      "Epoch 9, Iteration 153, Loss: 0.13515223562717438\n",
      "Epoch 9, Iteration 154, Loss: 0.12047039717435837\n",
      "Epoch 9, Iteration 155, Loss: 0.1543087512254715\n",
      "Epoch 9, Iteration 156, Loss: 0.14817991852760315\n",
      "Epoch 9, Iteration 157, Loss: 0.19647011160850525\n",
      "Epoch 9, Iteration 158, Loss: 0.18599194288253784\n",
      "Epoch 9, Iteration 159, Loss: 0.13392049074172974\n",
      "Epoch 9, Iteration 160, Loss: 0.16336631774902344\n",
      "Epoch 9, Iteration 161, Loss: 0.08393127471208572\n",
      "Epoch 9, Iteration 162, Loss: 0.09469762444496155\n",
      "Epoch 9, Iteration 163, Loss: 0.13430851697921753\n",
      "Epoch 9, Iteration 164, Loss: 0.10431092232465744\n",
      "Epoch 9, Iteration 165, Loss: 0.1280054897069931\n",
      "Epoch 9, Iteration 166, Loss: 0.1363181471824646\n",
      "Epoch 9, Iteration 167, Loss: 0.12487548589706421\n",
      "Epoch 9, Iteration 168, Loss: 0.10321441292762756\n",
      "Epoch 9, Iteration 169, Loss: 0.1490543782711029\n",
      "Epoch 9, Iteration 170, Loss: 0.08489667624235153\n",
      "Epoch 9, Iteration 171, Loss: 0.16039693355560303\n",
      "Epoch 9, Iteration 172, Loss: 0.18287232518196106\n",
      "Epoch 9, Iteration 173, Loss: 0.13676869869232178\n",
      "Epoch 9, Iteration 174, Loss: 0.13514064252376556\n",
      "Epoch 9, Iteration 175, Loss: 0.12116166949272156\n",
      "Epoch 9, Iteration 176, Loss: 0.09868264943361282\n",
      "Epoch 9, Iteration 177, Loss: 0.17514783143997192\n",
      "Epoch 9, Iteration 178, Loss: 0.12599562108516693\n",
      "Epoch 9, Iteration 179, Loss: 0.12367305159568787\n",
      "Epoch 9, Iteration 180, Loss: 0.15917573869228363\n",
      "Epoch 9, Iteration 181, Loss: 0.11211545765399933\n",
      "Epoch 9, Iteration 182, Loss: 0.1017942726612091\n",
      "Epoch 9, Iteration 183, Loss: 0.12793689966201782\n",
      "Epoch 9, Iteration 184, Loss: 0.18786758184432983\n",
      "Epoch 9, Iteration 185, Loss: 0.11319869011640549\n",
      "Epoch 9, Iteration 186, Loss: 0.1375357061624527\n",
      "Epoch 9, Iteration 187, Loss: 0.13832899928092957\n",
      "Epoch 9, Iteration 188, Loss: 0.09803344309329987\n",
      "Epoch 9, Iteration 189, Loss: 0.08489283919334412\n",
      "Epoch 9, Iteration 190, Loss: 0.10502808541059494\n",
      "Epoch 9, Iteration 191, Loss: 0.09887555241584778\n",
      "Epoch 9, Iteration 192, Loss: 0.12052048742771149\n",
      "Epoch 9, Iteration 193, Loss: 0.16693565249443054\n",
      "Epoch 9, Iteration 194, Loss: 0.11309690028429031\n",
      "Epoch 9, Iteration 195, Loss: 0.13772572576999664\n",
      "Epoch 9, Iteration 196, Loss: 0.24688118696212769\n",
      "Epoch 9, Iteration 197, Loss: 0.11954737454652786\n",
      "Epoch 9, Iteration 198, Loss: 0.12123929709196091\n",
      "Epoch 9, Iteration 199, Loss: 0.1428336650133133\n",
      "Epoch 9, Iteration 200, Loss: 0.10360219329595566\n",
      "Epoch 9, Iteration 200, Test Loss: 0.250786691904068\n",
      "Epoch 9, Iteration 201, Loss: 0.12758314609527588\n",
      "Epoch 9, Iteration 202, Loss: 0.07963114231824875\n",
      "Epoch 9, Iteration 203, Loss: 0.12585148215293884\n",
      "Epoch 9, Iteration 204, Loss: 0.12341603636741638\n",
      "Epoch 9, Iteration 205, Loss: 0.14749889075756073\n",
      "Epoch 9, Iteration 206, Loss: 0.10223939269781113\n",
      "Epoch 9, Iteration 207, Loss: 0.09702174365520477\n",
      "Epoch 9, Iteration 208, Loss: 0.10045526921749115\n",
      "Epoch 9, Iteration 209, Loss: 0.12272915989160538\n",
      "Epoch 9, Iteration 210, Loss: 0.0912095159292221\n",
      "Epoch 9, Iteration 211, Loss: 0.13451433181762695\n",
      "Epoch 9, Iteration 212, Loss: 0.1503198891878128\n",
      "Epoch 9, Iteration 213, Loss: 0.10882794857025146\n",
      "Epoch 9, Iteration 214, Loss: 0.11052899807691574\n",
      "Epoch 9, Iteration 215, Loss: 0.1362151801586151\n",
      "Epoch 9, Iteration 216, Loss: 0.15207628905773163\n",
      "Epoch 9, Iteration 217, Loss: 0.14836108684539795\n",
      "Epoch 9, Iteration 218, Loss: 0.14211305975914001\n",
      "Epoch 9, Iteration 219, Loss: 0.13922013342380524\n",
      "Epoch 9, Iteration 220, Loss: 0.09172121435403824\n",
      "Epoch 9, Iteration 221, Loss: 0.1425323337316513\n",
      "Epoch 9, Iteration 222, Loss: 0.11865294724702835\n",
      "Epoch 9, Iteration 223, Loss: 0.1127956211566925\n",
      "Epoch 9, Iteration 224, Loss: 0.11361415684223175\n",
      "Epoch 9, Iteration 225, Loss: 0.12754125893115997\n",
      "Epoch 9, Iteration 226, Loss: 0.14949266612529755\n",
      "Epoch 9, Iteration 227, Loss: 0.1128464788198471\n",
      "Epoch 9, Iteration 228, Loss: 0.2503904700279236\n",
      "Epoch 9, Iteration 229, Loss: 0.20402131974697113\n",
      "Epoch 9, Iteration 230, Loss: 0.14213965833187103\n",
      "Epoch 9, Iteration 231, Loss: 0.1180889680981636\n",
      "Epoch 9, Iteration 232, Loss: 0.11484716832637787\n",
      "Epoch 9, Iteration 233, Loss: 0.1879451870918274\n",
      "Epoch 9, Iteration 234, Loss: 0.1014583632349968\n",
      "Epoch 9, Iteration 235, Loss: 0.09749259054660797\n",
      "Epoch 9, Iteration 236, Loss: 0.17344148457050323\n",
      "Epoch 9, Iteration 237, Loss: 0.13434256613254547\n",
      "Epoch 9, Iteration 238, Loss: 0.1286853700876236\n",
      "Epoch 9, Iteration 239, Loss: 0.12576904892921448\n",
      "Epoch 9, Iteration 240, Loss: 0.12419702857732773\n",
      "Epoch 9, Iteration 241, Loss: 0.11256859451532364\n",
      "Epoch 9, Iteration 242, Loss: 0.13681653141975403\n",
      "Epoch 9, Iteration 243, Loss: 0.11710610240697861\n",
      "Epoch 9, Iteration 244, Loss: 0.13769575953483582\n",
      "Epoch 9, Iteration 245, Loss: 0.12357308715581894\n",
      "Epoch 9, Iteration 246, Loss: 0.1495247483253479\n",
      "Epoch 9, Iteration 247, Loss: 0.24749276041984558\n",
      "Epoch 9, Iteration 248, Loss: 0.20968756079673767\n",
      "Epoch 9, Iteration 249, Loss: 0.19647055864334106\n",
      "Epoch 9, Iteration 250, Loss: 0.22765986621379852\n",
      "Epoch 9, Iteration 250, Test Loss: 0.2563844323158264\n",
      "Epoch 9, Iteration 251, Loss: 0.19311508536338806\n",
      "Epoch 9, Iteration 252, Loss: 0.13665950298309326\n",
      "Epoch 9, Iteration 253, Loss: 0.16086134314537048\n",
      "Epoch 9, Iteration 254, Loss: 0.1990528702735901\n",
      "Epoch 9, Iteration 255, Loss: 0.1515674889087677\n",
      "Epoch 9, Iteration 256, Loss: 0.14365476369857788\n",
      "Epoch 9, Iteration 257, Loss: 0.11717170476913452\n",
      "Epoch 9, Iteration 258, Loss: 0.1479945331811905\n",
      "Epoch 9, Iteration 259, Loss: 0.1444467306137085\n",
      "Epoch 9, Iteration 260, Loss: 0.17356990277767181\n",
      "Epoch 9, Iteration 261, Loss: 0.12797275185585022\n",
      "Epoch 9, Iteration 262, Loss: 0.1007736474275589\n",
      "Epoch 9, Iteration 263, Loss: 0.11707121878862381\n",
      "Epoch 9, Iteration 264, Loss: 0.1295318454504013\n",
      "Epoch 9, Iteration 265, Loss: 0.20904502272605896\n",
      "Epoch 9, Iteration 266, Loss: 0.10588742792606354\n",
      "Epoch 9, Iteration 267, Loss: 0.13552166521549225\n",
      "Epoch 9, Iteration 268, Loss: 0.15588249266147614\n",
      "Epoch 9, Iteration 269, Loss: 0.13865116238594055\n",
      "Epoch 9, Iteration 270, Loss: 0.17556683719158173\n",
      "Epoch 9, Iteration 271, Loss: 0.07968135923147202\n",
      "Epoch 9, Iteration 272, Loss: 0.12971720099449158\n",
      "Epoch 9, Iteration 273, Loss: 0.15134473145008087\n",
      "Epoch 9, Iteration 274, Loss: 0.11210811883211136\n",
      "Epoch 9, Iteration 275, Loss: 0.10914156585931778\n",
      "Epoch 9, Iteration 276, Loss: 0.10018517822027206\n",
      "Epoch 9, Iteration 277, Loss: 0.11356177926063538\n",
      "Epoch 9, Iteration 278, Loss: 0.15830828249454498\n",
      "Epoch 9, Iteration 279, Loss: 0.11409316956996918\n",
      "Epoch 9, Iteration 280, Loss: 0.13629966974258423\n",
      "Epoch 9, Iteration 281, Loss: 0.1153155192732811\n",
      "Epoch 9, Iteration 282, Loss: 0.10972350835800171\n",
      "Epoch 9, Iteration 283, Loss: 0.07952728122472763\n",
      "Epoch 9, Iteration 284, Loss: 0.14779742062091827\n",
      "Epoch 9, Iteration 285, Loss: 0.10518866032361984\n",
      "Epoch 9, Iteration 286, Loss: 0.15788674354553223\n",
      "Epoch 9, Iteration 287, Loss: 0.08758970350027084\n",
      "Epoch 9, Iteration 288, Loss: 0.10226760804653168\n",
      "Epoch 9, Iteration 289, Loss: 0.17236647009849548\n",
      "Epoch 9, Iteration 290, Loss: 0.11178799718618393\n",
      "Epoch 9, Iteration 291, Loss: 0.08978118747472763\n",
      "Epoch 9, Iteration 292, Loss: 0.13468444347381592\n",
      "Epoch 9, Iteration 293, Loss: 0.18514549732208252\n",
      "Epoch 9, Iteration 294, Loss: 0.11666473001241684\n",
      "Epoch 9, Iteration 295, Loss: 0.07709304988384247\n",
      "Epoch 9, Iteration 296, Loss: 0.09389504045248032\n",
      "Epoch 9, Iteration 297, Loss: 0.14573203027248383\n",
      "Epoch 9, Iteration 298, Loss: 0.09508264064788818\n",
      "Epoch 9, Iteration 299, Loss: 0.1656010001897812\n",
      "Epoch 9, Iteration 300, Loss: 0.1078636422753334\n",
      "Epoch 9, Iteration 300, Test Loss: 0.26670655608177185\n",
      "Epoch 9, Iteration 301, Loss: 0.10349573940038681\n",
      "Epoch 9, Iteration 302, Loss: 0.10259970277547836\n",
      "Epoch 9, Iteration 303, Loss: 0.13907890021800995\n",
      "Epoch 9, Iteration 304, Loss: 0.163826122879982\n",
      "Epoch 9, Iteration 305, Loss: 0.0990646630525589\n",
      "Epoch 9, Iteration 306, Loss: 0.0905056744813919\n",
      "Epoch 9, Iteration 307, Loss: 0.12257647514343262\n",
      "Epoch 9, Iteration 308, Loss: 0.1029709205031395\n",
      "Epoch 9, Iteration 309, Loss: 0.1241520419716835\n",
      "Epoch 9, Iteration 310, Loss: 0.13960692286491394\n",
      "Epoch 9, Iteration 311, Loss: 0.09959390014410019\n",
      "Epoch 9, Iteration 312, Loss: 0.1390342265367508\n",
      "Epoch 9, Iteration 313, Loss: 0.11500491946935654\n",
      "Epoch 9, Iteration 314, Loss: 0.10685942322015762\n",
      "Epoch 9, Iteration 315, Loss: 0.11409984529018402\n",
      "Epoch 9, Iteration 316, Loss: 0.09184806048870087\n",
      "Epoch 9, Iteration 317, Loss: 0.1447719931602478\n",
      "Epoch 9, Iteration 318, Loss: 0.1635022908449173\n",
      "Epoch 9, Iteration 319, Loss: 0.08829047530889511\n",
      "Epoch 9, Iteration 320, Loss: 0.11908872425556183\n",
      "Epoch 9, Iteration 321, Loss: 0.0982007086277008\n",
      "Epoch 9, Iteration 322, Loss: 0.18264825642108917\n",
      "Epoch 9, Iteration 323, Loss: 0.1141364574432373\n",
      "Epoch 9, Iteration 324, Loss: 0.16358238458633423\n",
      "Epoch 9, Iteration 325, Loss: 0.2552426755428314\n",
      "Epoch 9, Iteration 326, Loss: 0.1487211287021637\n",
      "Epoch 9, Iteration 327, Loss: 0.13518071174621582\n",
      "Epoch 9, Iteration 328, Loss: 0.11024106293916702\n",
      "Epoch 9, Iteration 329, Loss: 0.12833106517791748\n",
      "Epoch 9, Iteration 330, Loss: 0.17540360987186432\n",
      "Epoch 9, Iteration 331, Loss: 0.1592012345790863\n",
      "Epoch 9, Iteration 332, Loss: 0.14599481225013733\n",
      "Epoch 9, Iteration 333, Loss: 0.12498054653406143\n",
      "Epoch 9, Iteration 334, Loss: 0.20554998517036438\n",
      "Epoch 9, Iteration 335, Loss: 0.1368342638015747\n",
      "Epoch 9, Iteration 336, Loss: 0.16540342569351196\n",
      "Epoch 9, Iteration 337, Loss: 0.12697802484035492\n",
      "Epoch 9, Iteration 338, Loss: 0.1383555680513382\n",
      "Epoch 9, Iteration 339, Loss: 0.1652173101902008\n",
      "Epoch 9, Iteration 340, Loss: 0.15160273015499115\n",
      "Epoch 9, Iteration 341, Loss: 0.12790560722351074\n",
      "Epoch 9, Iteration 342, Loss: 0.15442848205566406\n",
      "Epoch 9, Iteration 343, Loss: 0.18547667562961578\n",
      "Epoch 9, Iteration 344, Loss: 0.17474256455898285\n",
      "Epoch 9, Iteration 345, Loss: 0.18038323521614075\n",
      "Epoch 9, Iteration 346, Loss: 0.12052284181118011\n",
      "Epoch 9, Iteration 347, Loss: 0.14172625541687012\n",
      "Epoch 9, Iteration 348, Loss: 0.17068205773830414\n",
      "Epoch 9, Iteration 349, Loss: 0.1791987419128418\n",
      "Epoch 9, Iteration 350, Loss: 0.199534609913826\n",
      "Epoch 9, Iteration 350, Test Loss: 0.24176931381225586\n",
      "Epoch 9, Iteration 351, Loss: 0.1493634283542633\n",
      "Epoch 9, Iteration 352, Loss: 0.13710559904575348\n",
      "Epoch 9, Iteration 353, Loss: 0.20079421997070312\n",
      "Epoch 9, Iteration 354, Loss: 0.1233372762799263\n",
      "Epoch 9, Iteration 355, Loss: 0.13697007298469543\n",
      "Epoch 9, Iteration 356, Loss: 0.11841088533401489\n",
      "Epoch 9, Iteration 357, Loss: 0.20863007009029388\n",
      "Epoch 9, Iteration 358, Loss: 0.10866575688123703\n",
      "Epoch 9, Iteration 359, Loss: 0.15578676760196686\n",
      "Epoch 9, Iteration 360, Loss: 0.09231237322092056\n",
      "Epoch 9, Iteration 361, Loss: 0.1418343484401703\n",
      "Epoch 9, Iteration 362, Loss: 0.1290261149406433\n",
      "Epoch 9, Iteration 363, Loss: 0.11967282742261887\n",
      "Epoch 9, Iteration 364, Loss: 0.11078830808401108\n",
      "Epoch 9, Iteration 365, Loss: 0.10730870068073273\n",
      "Epoch 9, Iteration 366, Loss: 0.1724134385585785\n",
      "Epoch 9, Iteration 367, Loss: 0.16581355035305023\n",
      "Epoch 9, Iteration 368, Loss: 0.12811490893363953\n",
      "Epoch 9, Iteration 369, Loss: 0.12959688901901245\n",
      "Epoch 9, Iteration 370, Loss: 0.1559508889913559\n",
      "Epoch 9, Iteration 371, Loss: 0.12205129861831665\n",
      "Epoch 9, Iteration 372, Loss: 0.10712124407291412\n",
      "Epoch 9, Iteration 373, Loss: 0.1557496190071106\n",
      "Epoch 9, Iteration 374, Loss: 0.09893567115068436\n",
      "Epoch 9, Iteration 375, Loss: 0.15046638250350952\n",
      "Epoch 9, Iteration 376, Loss: 0.1394803524017334\n",
      "Epoch 9, Iteration 377, Loss: 0.14738473296165466\n",
      "Epoch 9, Iteration 378, Loss: 0.14718744158744812\n",
      "Epoch 9, Iteration 379, Loss: 0.15439839661121368\n",
      "Epoch 9, Iteration 380, Loss: 0.1496831327676773\n",
      "Epoch 9, Iteration 381, Loss: 0.1371604949235916\n",
      "Epoch 9, Iteration 382, Loss: 0.13149654865264893\n",
      "Epoch 9, Iteration 383, Loss: 0.10651672631502151\n",
      "Epoch 9, Iteration 384, Loss: 0.12584742903709412\n",
      "Epoch 9, Iteration 385, Loss: 0.13776269555091858\n",
      "Epoch 9, Iteration 386, Loss: 0.12167664617300034\n",
      "Epoch 9, Iteration 387, Loss: 0.09115529805421829\n",
      "Epoch 9, Iteration 388, Loss: 0.09898042678833008\n",
      "Epoch 9, Iteration 389, Loss: 0.09670697897672653\n",
      "Epoch 9, Iteration 390, Loss: 0.12106193602085114\n",
      "Epoch 9, Iteration 391, Loss: 0.08189765363931656\n",
      "Epoch 9, Iteration 392, Loss: 0.17514464259147644\n",
      "Epoch 9, Iteration 393, Loss: 0.1758391410112381\n",
      "Epoch 9, Iteration 394, Loss: 0.11501388251781464\n",
      "Epoch 9, Iteration 395, Loss: 0.11182024329900742\n",
      "Epoch 9, Iteration 396, Loss: 0.13396117091178894\n",
      "Epoch 9, Iteration 397, Loss: 0.1161126121878624\n",
      "Epoch 9, Iteration 398, Loss: 0.2338762730360031\n",
      "Epoch 9, Iteration 399, Loss: 0.1076473668217659\n",
      "Epoch 9, Iteration 400, Loss: 0.1000475063920021\n",
      "Epoch 9, Iteration 400, Test Loss: 0.25608596205711365\n",
      "Epoch 9, Iteration 401, Loss: 0.14424562454223633\n",
      "Epoch 9, Iteration 402, Loss: 0.0711621567606926\n",
      "Epoch 9, Iteration 403, Loss: 0.12385505437850952\n",
      "Epoch 9, Iteration 404, Loss: 0.13242435455322266\n",
      "Epoch 9, Iteration 405, Loss: 0.08854429423809052\n",
      "Epoch 9, Iteration 406, Loss: 0.09825463593006134\n",
      "Epoch 9, Iteration 407, Loss: 0.14214880764484406\n",
      "Epoch 9, Iteration 408, Loss: 0.15318386256694794\n",
      "Epoch 9, Iteration 409, Loss: 0.09091440588235855\n",
      "Epoch 9, Iteration 410, Loss: 0.15487486124038696\n",
      "Epoch 9, Iteration 411, Loss: 0.19628795981407166\n",
      "Epoch 9, Iteration 412, Loss: 0.13411058485507965\n",
      "Epoch 9, Iteration 413, Loss: 0.15829366445541382\n",
      "Epoch 9, Iteration 414, Loss: 0.09745631366968155\n",
      "Epoch 9, Iteration 415, Loss: 0.12770012021064758\n",
      "Epoch 9, Iteration 416, Loss: 0.09738427400588989\n",
      "Epoch 9, Iteration 417, Loss: 0.14348585903644562\n",
      "Epoch 9, Iteration 418, Loss: 0.10519099235534668\n",
      "Epoch 9, Iteration 419, Loss: 0.1309792399406433\n",
      "Epoch 9, Iteration 420, Loss: 0.13654620945453644\n",
      "Epoch 9, Iteration 421, Loss: 0.08627507090568542\n",
      "Epoch 9, Iteration 422, Loss: 0.13923421502113342\n",
      "Epoch 9, Iteration 423, Loss: 0.12028868496417999\n",
      "Epoch 9, Iteration 424, Loss: 0.13568837940692902\n",
      "Epoch 9, Iteration 425, Loss: 0.14023710787296295\n",
      "Epoch 9, Iteration 426, Loss: 0.11077145487070084\n",
      "Epoch 9, Iteration 427, Loss: 0.11807949095964432\n",
      "Epoch 9, Iteration 428, Loss: 0.12365438044071198\n",
      "Epoch 9, Iteration 429, Loss: 0.16630864143371582\n",
      "Epoch 9, Iteration 430, Loss: 0.12948447465896606\n",
      "Epoch 9, Iteration 431, Loss: 0.1507512927055359\n",
      "Epoch 10/15, Loss: 0.1414255060221034\n",
      "Epoch 10/15, Validation Accuracy: 0.9435573122529645\n",
      "Epoch 10, Iteration 0, Loss: 0.27375826239585876\n",
      "Epoch 10, Iteration 1, Loss: 0.20967988669872284\n",
      "Epoch 10, Iteration 2, Loss: 0.2094254046678543\n",
      "Epoch 10, Iteration 3, Loss: 0.15146955847740173\n",
      "Epoch 10, Iteration 4, Loss: 0.21202702820301056\n",
      "Epoch 10, Iteration 5, Loss: 0.27919843792915344\n",
      "Epoch 10, Iteration 6, Loss: 0.21062135696411133\n",
      "Epoch 10, Iteration 7, Loss: 0.23377202451229095\n",
      "Epoch 10, Iteration 8, Loss: 0.2075619101524353\n",
      "Epoch 10, Iteration 9, Loss: 0.19851233065128326\n",
      "Epoch 10, Iteration 10, Loss: 0.20116163790225983\n",
      "Epoch 10, Iteration 11, Loss: 0.15245124697685242\n",
      "Epoch 10, Iteration 12, Loss: 0.19407381117343903\n",
      "Epoch 10, Iteration 13, Loss: 0.2223511040210724\n",
      "Epoch 10, Iteration 14, Loss: 0.20061230659484863\n",
      "Epoch 10, Iteration 15, Loss: 0.11473376303911209\n",
      "Epoch 10, Iteration 16, Loss: 0.23970763385295868\n",
      "Epoch 10, Iteration 17, Loss: 0.14703762531280518\n",
      "Epoch 10, Iteration 18, Loss: 0.1888236254453659\n",
      "Epoch 10, Iteration 19, Loss: 0.17611201107501984\n",
      "Epoch 10, Iteration 20, Loss: 0.20242345333099365\n",
      "Epoch 10, Iteration 21, Loss: 0.24799318611621857\n",
      "Epoch 10, Iteration 22, Loss: 0.16592653095722198\n",
      "Epoch 10, Iteration 23, Loss: 0.2016431838274002\n",
      "Epoch 10, Iteration 24, Loss: 0.17609184980392456\n",
      "Epoch 10, Iteration 25, Loss: 0.2235184758901596\n",
      "Epoch 10, Iteration 26, Loss: 0.13622525334358215\n",
      "Epoch 10, Iteration 27, Loss: 0.13533467054367065\n",
      "Epoch 10, Iteration 28, Loss: 0.2290966808795929\n",
      "Epoch 10, Iteration 29, Loss: 0.1638323813676834\n",
      "Epoch 10, Iteration 30, Loss: 0.206334188580513\n",
      "Epoch 10, Iteration 31, Loss: 0.19506880640983582\n",
      "Epoch 10, Iteration 32, Loss: 0.17972324788570404\n",
      "Epoch 10, Iteration 33, Loss: 0.16334936022758484\n",
      "Epoch 10, Iteration 34, Loss: 0.18528605997562408\n",
      "Epoch 10, Iteration 35, Loss: 0.14392143487930298\n",
      "Epoch 10, Iteration 36, Loss: 0.1762143075466156\n",
      "Epoch 10, Iteration 37, Loss: 0.18472598493099213\n",
      "Epoch 10, Iteration 38, Loss: 0.18613706529140472\n",
      "Epoch 10, Iteration 39, Loss: 0.18894736468791962\n",
      "Epoch 10, Iteration 40, Loss: 0.1415059119462967\n",
      "Epoch 10, Iteration 41, Loss: 0.14532963931560516\n",
      "Epoch 10, Iteration 42, Loss: 0.15454372763633728\n",
      "Epoch 10, Iteration 43, Loss: 0.16146141290664673\n",
      "Epoch 10, Iteration 44, Loss: 0.16942065954208374\n",
      "Epoch 10, Iteration 45, Loss: 0.17200084030628204\n",
      "Epoch 10, Iteration 46, Loss: 0.17894482612609863\n",
      "Epoch 10, Iteration 47, Loss: 0.18881438672542572\n",
      "Epoch 10, Iteration 48, Loss: 0.22658219933509827\n",
      "Epoch 10, Iteration 49, Loss: 0.12513591349124908\n",
      "Epoch 10, Iteration 50, Loss: 0.13895751535892487\n",
      "Epoch 10, Iteration 50, Test Loss: 0.21150913834571838\n",
      "Epoch 10, Iteration 51, Loss: 0.13136394321918488\n",
      "Epoch 10, Iteration 52, Loss: 0.1824479103088379\n",
      "Epoch 10, Iteration 53, Loss: 0.14003263413906097\n",
      "Epoch 10, Iteration 54, Loss: 0.14660117030143738\n",
      "Epoch 10, Iteration 55, Loss: 0.1325308084487915\n",
      "Epoch 10, Iteration 56, Loss: 0.14052987098693848\n",
      "Epoch 10, Iteration 57, Loss: 0.1374024599790573\n",
      "Epoch 10, Iteration 58, Loss: 0.11425971984863281\n",
      "Epoch 10, Iteration 59, Loss: 0.1346936672925949\n",
      "Epoch 10, Iteration 60, Loss: 0.1676383763551712\n",
      "Epoch 10, Iteration 61, Loss: 0.09579407423734665\n",
      "Epoch 10, Iteration 62, Loss: 0.21749867498874664\n",
      "Epoch 10, Iteration 63, Loss: 0.09822908788919449\n",
      "Epoch 10, Iteration 64, Loss: 0.1220034807920456\n",
      "Epoch 10, Iteration 65, Loss: 0.1438695341348648\n",
      "Epoch 10, Iteration 66, Loss: 0.15082822740077972\n",
      "Epoch 10, Iteration 67, Loss: 0.09845245629549026\n",
      "Epoch 10, Iteration 68, Loss: 0.1546921730041504\n",
      "Epoch 10, Iteration 69, Loss: 0.15513387322425842\n",
      "Epoch 10, Iteration 70, Loss: 0.10062547028064728\n",
      "Epoch 10, Iteration 71, Loss: 0.13565121591091156\n",
      "Epoch 10, Iteration 72, Loss: 0.14359337091445923\n",
      "Epoch 10, Iteration 73, Loss: 0.15039214491844177\n",
      "Epoch 10, Iteration 74, Loss: 0.1585291177034378\n",
      "Epoch 10, Iteration 75, Loss: 0.1588825285434723\n",
      "Epoch 10, Iteration 76, Loss: 0.14514662325382233\n",
      "Epoch 10, Iteration 77, Loss: 0.11477340757846832\n",
      "Epoch 10, Iteration 78, Loss: 0.16107885539531708\n",
      "Epoch 10, Iteration 79, Loss: 0.09856820851564407\n",
      "Epoch 10, Iteration 80, Loss: 0.15934805572032928\n",
      "Epoch 10, Iteration 81, Loss: 0.13300062716007233\n",
      "Epoch 10, Iteration 82, Loss: 0.16829849779605865\n",
      "Epoch 10, Iteration 83, Loss: 0.13119740784168243\n",
      "Epoch 10, Iteration 84, Loss: 0.10190126299858093\n",
      "Epoch 10, Iteration 85, Loss: 0.15819348394870758\n",
      "Epoch 10, Iteration 86, Loss: 0.18054671585559845\n",
      "Epoch 10, Iteration 87, Loss: 0.16102328896522522\n",
      "Epoch 10, Iteration 88, Loss: 0.1300949901342392\n",
      "Epoch 10, Iteration 89, Loss: 0.15445730090141296\n",
      "Epoch 10, Iteration 90, Loss: 0.14136728644371033\n",
      "Epoch 10, Iteration 91, Loss: 0.12946465611457825\n",
      "Epoch 10, Iteration 92, Loss: 0.11535337567329407\n",
      "Epoch 10, Iteration 93, Loss: 0.12351559102535248\n",
      "Epoch 10, Iteration 94, Loss: 0.17011409997940063\n",
      "Epoch 10, Iteration 95, Loss: 0.11237082630395889\n",
      "Epoch 10, Iteration 96, Loss: 0.09010330587625504\n",
      "Epoch 10, Iteration 97, Loss: 0.13662582635879517\n",
      "Epoch 10, Iteration 98, Loss: 0.15449757874011993\n",
      "Epoch 10, Iteration 99, Loss: 0.12141498923301697\n",
      "Epoch 10, Iteration 100, Loss: 0.10506045818328857\n",
      "Epoch 10, Iteration 100, Test Loss: 0.24637435376644135\n",
      "Epoch 10, Iteration 101, Loss: 0.1281384825706482\n",
      "Epoch 10, Iteration 102, Loss: 0.14259666204452515\n",
      "Epoch 10, Iteration 103, Loss: 0.11593944579362869\n",
      "Epoch 10, Iteration 104, Loss: 0.1390025019645691\n",
      "Epoch 10, Iteration 105, Loss: 0.13084881007671356\n",
      "Epoch 10, Iteration 106, Loss: 0.10102663189172745\n",
      "Epoch 10, Iteration 107, Loss: 0.09284874051809311\n",
      "Epoch 10, Iteration 108, Loss: 0.14571473002433777\n",
      "Epoch 10, Iteration 109, Loss: 0.2013649046421051\n",
      "Epoch 10, Iteration 110, Loss: 0.10774318128824234\n",
      "Epoch 10, Iteration 111, Loss: 0.0767286941409111\n",
      "Epoch 10, Iteration 112, Loss: 0.12737473845481873\n",
      "Epoch 10, Iteration 113, Loss: 0.1394195556640625\n",
      "Epoch 10, Iteration 114, Loss: 0.12791769206523895\n",
      "Epoch 10, Iteration 115, Loss: 0.1300891637802124\n",
      "Epoch 10, Iteration 116, Loss: 0.135740727186203\n",
      "Epoch 10, Iteration 117, Loss: 0.18906418979167938\n",
      "Epoch 10, Iteration 118, Loss: 0.14214970171451569\n",
      "Epoch 10, Iteration 119, Loss: 0.17681224644184113\n",
      "Epoch 10, Iteration 120, Loss: 0.1057654470205307\n",
      "Epoch 10, Iteration 121, Loss: 0.15684393048286438\n",
      "Epoch 10, Iteration 122, Loss: 0.16085241734981537\n",
      "Epoch 10, Iteration 123, Loss: 0.13984297215938568\n",
      "Epoch 10, Iteration 124, Loss: 0.13783979415893555\n",
      "Epoch 10, Iteration 125, Loss: 0.1674465537071228\n",
      "Epoch 10, Iteration 126, Loss: 0.1262105107307434\n",
      "Epoch 10, Iteration 127, Loss: 0.14930160343647003\n",
      "Epoch 10, Iteration 128, Loss: 0.12150543928146362\n",
      "Epoch 10, Iteration 129, Loss: 0.18482600152492523\n",
      "Epoch 10, Iteration 130, Loss: 0.0943201556801796\n",
      "Epoch 10, Iteration 131, Loss: 0.11878589540719986\n",
      "Epoch 10, Iteration 132, Loss: 0.18185339868068695\n",
      "Epoch 10, Iteration 133, Loss: 0.21479828655719757\n",
      "Epoch 10, Iteration 134, Loss: 0.14681801199913025\n",
      "Epoch 10, Iteration 135, Loss: 0.18147514760494232\n",
      "Epoch 10, Iteration 136, Loss: 0.09988947957754135\n",
      "Epoch 10, Iteration 137, Loss: 0.1093323826789856\n",
      "Epoch 10, Iteration 138, Loss: 0.15395130217075348\n",
      "Epoch 10, Iteration 139, Loss: 0.15911176800727844\n",
      "Epoch 10, Iteration 140, Loss: 0.16220003366470337\n",
      "Epoch 10, Iteration 141, Loss: 0.14466632902622223\n",
      "Epoch 10, Iteration 142, Loss: 0.17492611706256866\n",
      "Epoch 10, Iteration 143, Loss: 0.15071408450603485\n",
      "Epoch 10, Iteration 144, Loss: 0.10175849497318268\n",
      "Epoch 10, Iteration 145, Loss: 0.18260425329208374\n",
      "Epoch 10, Iteration 146, Loss: 0.1104944720864296\n",
      "Epoch 10, Iteration 147, Loss: 0.11057215183973312\n",
      "Epoch 10, Iteration 148, Loss: 0.10693148523569107\n",
      "Epoch 10, Iteration 149, Loss: 0.1259509027004242\n",
      "Epoch 10, Iteration 150, Loss: 0.15081153810024261\n",
      "Epoch 10, Iteration 150, Test Loss: 0.2455475926399231\n",
      "Epoch 10, Iteration 151, Loss: 0.15325617790222168\n",
      "Epoch 10, Iteration 152, Loss: 0.1584816724061966\n",
      "Epoch 10, Iteration 153, Loss: 0.15587614476680756\n",
      "Epoch 10, Iteration 154, Loss: 0.13406801223754883\n",
      "Epoch 10, Iteration 155, Loss: 0.15948893129825592\n",
      "Epoch 10, Iteration 156, Loss: 0.1424643099308014\n",
      "Epoch 10, Iteration 157, Loss: 0.19208769500255585\n",
      "Epoch 10, Iteration 158, Loss: 0.19377240538597107\n",
      "Epoch 10, Iteration 159, Loss: 0.13575606048107147\n",
      "Epoch 10, Iteration 160, Loss: 0.15497203171253204\n",
      "Epoch 10, Iteration 161, Loss: 0.09121214598417282\n",
      "Epoch 10, Iteration 162, Loss: 0.08487353473901749\n",
      "Epoch 10, Iteration 163, Loss: 0.12453028559684753\n",
      "Epoch 10, Iteration 164, Loss: 0.08522137254476547\n",
      "Epoch 10, Iteration 165, Loss: 0.10835012048482895\n",
      "Epoch 10, Iteration 166, Loss: 0.12973137199878693\n",
      "Epoch 10, Iteration 167, Loss: 0.13092176616191864\n",
      "Epoch 10, Iteration 168, Loss: 0.10990199446678162\n",
      "Epoch 10, Iteration 169, Loss: 0.12776492536067963\n",
      "Epoch 10, Iteration 170, Loss: 0.08579086512327194\n",
      "Epoch 10, Iteration 171, Loss: 0.17733058333396912\n",
      "Epoch 10, Iteration 172, Loss: 0.1859763115644455\n",
      "Epoch 10, Iteration 173, Loss: 0.14595356583595276\n",
      "Epoch 10, Iteration 174, Loss: 0.13521942496299744\n",
      "Epoch 10, Iteration 175, Loss: 0.13549143075942993\n",
      "Epoch 10, Iteration 176, Loss: 0.10849250853061676\n",
      "Epoch 10, Iteration 177, Loss: 0.17108550667762756\n",
      "Epoch 10, Iteration 178, Loss: 0.12079211324453354\n",
      "Epoch 10, Iteration 179, Loss: 0.11608507484197617\n",
      "Epoch 10, Iteration 180, Loss: 0.1576455980539322\n",
      "Epoch 10, Iteration 181, Loss: 0.11559592932462692\n",
      "Epoch 10, Iteration 182, Loss: 0.11379600316286087\n",
      "Epoch 10, Iteration 183, Loss: 0.14310042560100555\n",
      "Epoch 10, Iteration 184, Loss: 0.2267790585756302\n",
      "Epoch 10, Iteration 185, Loss: 0.10330802947282791\n",
      "Epoch 10, Iteration 186, Loss: 0.1271762102842331\n",
      "Epoch 10, Iteration 187, Loss: 0.1188867911696434\n",
      "Epoch 10, Iteration 188, Loss: 0.097336046397686\n",
      "Epoch 10, Iteration 189, Loss: 0.0781051516532898\n",
      "Epoch 10, Iteration 190, Loss: 0.09460709244012833\n",
      "Epoch 10, Iteration 191, Loss: 0.08548295497894287\n",
      "Epoch 10, Iteration 192, Loss: 0.1199750304222107\n",
      "Epoch 10, Iteration 193, Loss: 0.15202555060386658\n",
      "Epoch 10, Iteration 194, Loss: 0.12677501142024994\n",
      "Epoch 10, Iteration 195, Loss: 0.1191449984908104\n",
      "Epoch 10, Iteration 196, Loss: 0.2379685491323471\n",
      "Epoch 10, Iteration 197, Loss: 0.12441569566726685\n",
      "Epoch 10, Iteration 198, Loss: 0.1097288429737091\n",
      "Epoch 10, Iteration 199, Loss: 0.1304524540901184\n",
      "Epoch 10, Iteration 200, Loss: 0.09907537698745728\n",
      "Epoch 10, Iteration 200, Test Loss: 0.25864923000335693\n",
      "Epoch 10, Iteration 201, Loss: 0.12580640614032745\n",
      "Epoch 10, Iteration 202, Loss: 0.09335340559482574\n",
      "Epoch 10, Iteration 203, Loss: 0.10881195217370987\n",
      "Epoch 10, Iteration 204, Loss: 0.12649725377559662\n",
      "Epoch 10, Iteration 205, Loss: 0.15205706655979156\n",
      "Epoch 10, Iteration 206, Loss: 0.10753004252910614\n",
      "Epoch 10, Iteration 207, Loss: 0.09644512832164764\n",
      "Epoch 10, Iteration 208, Loss: 0.10156998783349991\n",
      "Epoch 10, Iteration 209, Loss: 0.13192744553089142\n",
      "Epoch 10, Iteration 210, Loss: 0.08885286748409271\n",
      "Epoch 10, Iteration 211, Loss: 0.1316295564174652\n",
      "Epoch 10, Iteration 212, Loss: 0.16248442232608795\n",
      "Epoch 10, Iteration 213, Loss: 0.10962849110364914\n",
      "Epoch 10, Iteration 214, Loss: 0.12063437700271606\n",
      "Epoch 10, Iteration 215, Loss: 0.1047680675983429\n",
      "Epoch 10, Iteration 216, Loss: 0.14939549565315247\n",
      "Epoch 10, Iteration 217, Loss: 0.17058777809143066\n",
      "Epoch 10, Iteration 218, Loss: 0.13685615360736847\n",
      "Epoch 10, Iteration 219, Loss: 0.15378595888614655\n",
      "Epoch 10, Iteration 220, Loss: 0.09921597689390182\n",
      "Epoch 10, Iteration 221, Loss: 0.16506733000278473\n",
      "Epoch 10, Iteration 222, Loss: 0.10617808252573013\n",
      "Epoch 10, Iteration 223, Loss: 0.12749801576137543\n",
      "Epoch 10, Iteration 224, Loss: 0.10437189787626266\n",
      "Epoch 10, Iteration 225, Loss: 0.1245805025100708\n",
      "Epoch 10, Iteration 226, Loss: 0.14565810561180115\n",
      "Epoch 10, Iteration 227, Loss: 0.12554016709327698\n",
      "Epoch 10, Iteration 228, Loss: 0.2464795559644699\n",
      "Epoch 10, Iteration 229, Loss: 0.19702856242656708\n",
      "Epoch 10, Iteration 230, Loss: 0.16185536980628967\n",
      "Epoch 10, Iteration 231, Loss: 0.12251776456832886\n",
      "Epoch 10, Iteration 232, Loss: 0.12483334541320801\n",
      "Epoch 10, Iteration 233, Loss: 0.16371862590312958\n",
      "Epoch 10, Iteration 234, Loss: 0.11214854568243027\n",
      "Epoch 10, Iteration 235, Loss: 0.09316428005695343\n",
      "Epoch 10, Iteration 236, Loss: 0.15948915481567383\n",
      "Epoch 10, Iteration 237, Loss: 0.13077479600906372\n",
      "Epoch 10, Iteration 238, Loss: 0.13606677949428558\n",
      "Epoch 10, Iteration 239, Loss: 0.13843485713005066\n",
      "Epoch 10, Iteration 240, Loss: 0.1257781833410263\n",
      "Epoch 10, Iteration 241, Loss: 0.10084028542041779\n",
      "Epoch 10, Iteration 242, Loss: 0.14107313752174377\n",
      "Epoch 10, Iteration 243, Loss: 0.10713788121938705\n",
      "Epoch 10, Iteration 244, Loss: 0.1346662938594818\n",
      "Epoch 10, Iteration 245, Loss: 0.12202579528093338\n",
      "Epoch 10, Iteration 246, Loss: 0.15785019099712372\n",
      "Epoch 10, Iteration 247, Loss: 0.23021453619003296\n",
      "Epoch 10, Iteration 248, Loss: 0.20178677141666412\n",
      "Epoch 10, Iteration 249, Loss: 0.20712921023368835\n",
      "Epoch 10, Iteration 250, Loss: 0.20910479128360748\n",
      "Epoch 10, Iteration 250, Test Loss: 0.25165989995002747\n",
      "Epoch 10, Iteration 251, Loss: 0.2015475332736969\n",
      "Epoch 10, Iteration 252, Loss: 0.1374114602804184\n",
      "Epoch 10, Iteration 253, Loss: 0.15898087620735168\n",
      "Epoch 10, Iteration 254, Loss: 0.19004987180233002\n",
      "Epoch 10, Iteration 255, Loss: 0.14089453220367432\n",
      "Epoch 10, Iteration 256, Loss: 0.13626021146774292\n",
      "Epoch 10, Iteration 257, Loss: 0.12221928685903549\n",
      "Epoch 10, Iteration 258, Loss: 0.14867548644542694\n",
      "Epoch 10, Iteration 259, Loss: 0.15152376890182495\n",
      "Epoch 10, Iteration 260, Loss: 0.1783214956521988\n",
      "Epoch 10, Iteration 261, Loss: 0.11313356459140778\n",
      "Epoch 10, Iteration 262, Loss: 0.10465753823518753\n",
      "Epoch 10, Iteration 263, Loss: 0.11254754662513733\n",
      "Epoch 10, Iteration 264, Loss: 0.12070561945438385\n",
      "Epoch 10, Iteration 265, Loss: 0.20571063458919525\n",
      "Epoch 10, Iteration 266, Loss: 0.09325508028268814\n",
      "Epoch 10, Iteration 267, Loss: 0.13663917779922485\n",
      "Epoch 10, Iteration 268, Loss: 0.14975281059741974\n",
      "Epoch 10, Iteration 269, Loss: 0.13948357105255127\n",
      "Epoch 10, Iteration 270, Loss: 0.15698418021202087\n",
      "Epoch 10, Iteration 271, Loss: 0.07792041450738907\n",
      "Epoch 10, Iteration 272, Loss: 0.1284988969564438\n",
      "Epoch 10, Iteration 273, Loss: 0.15391302108764648\n",
      "Epoch 10, Iteration 274, Loss: 0.10984473675489426\n",
      "Epoch 10, Iteration 275, Loss: 0.09539252519607544\n",
      "Epoch 10, Iteration 276, Loss: 0.10717877000570297\n",
      "Epoch 10, Iteration 277, Loss: 0.12779466807842255\n",
      "Epoch 10, Iteration 278, Loss: 0.1509280502796173\n",
      "Epoch 10, Iteration 279, Loss: 0.09463242441415787\n",
      "Epoch 10, Iteration 280, Loss: 0.14822986721992493\n",
      "Epoch 10, Iteration 281, Loss: 0.11715937405824661\n",
      "Epoch 10, Iteration 282, Loss: 0.11370868980884552\n",
      "Epoch 10, Iteration 283, Loss: 0.09011808037757874\n",
      "Epoch 10, Iteration 284, Loss: 0.12351229786872864\n",
      "Epoch 10, Iteration 285, Loss: 0.0967082604765892\n",
      "Epoch 10, Iteration 286, Loss: 0.16892114281654358\n",
      "Epoch 10, Iteration 287, Loss: 0.09218655526638031\n",
      "Epoch 10, Iteration 288, Loss: 0.11405301094055176\n",
      "Epoch 10, Iteration 289, Loss: 0.17288009822368622\n",
      "Epoch 10, Iteration 290, Loss: 0.10119090229272842\n",
      "Epoch 10, Iteration 291, Loss: 0.11177406460046768\n",
      "Epoch 10, Iteration 292, Loss: 0.11895103007555008\n",
      "Epoch 10, Iteration 293, Loss: 0.15186013281345367\n",
      "Epoch 10, Iteration 294, Loss: 0.10427885502576828\n",
      "Epoch 10, Iteration 295, Loss: 0.07326336205005646\n",
      "Epoch 10, Iteration 296, Loss: 0.09873663634061813\n",
      "Epoch 10, Iteration 297, Loss: 0.13698360323905945\n",
      "Epoch 10, Iteration 298, Loss: 0.10009554028511047\n",
      "Epoch 10, Iteration 299, Loss: 0.17586670815944672\n",
      "Epoch 10, Iteration 300, Loss: 0.10613872855901718\n",
      "Epoch 10, Iteration 300, Test Loss: 0.25952425599098206\n",
      "Epoch 10, Iteration 301, Loss: 0.11535775661468506\n",
      "Epoch 10, Iteration 302, Loss: 0.08754820376634598\n",
      "Epoch 10, Iteration 303, Loss: 0.14919431507587433\n",
      "Epoch 10, Iteration 304, Loss: 0.16082197427749634\n",
      "Epoch 10, Iteration 305, Loss: 0.11477728933095932\n",
      "Epoch 10, Iteration 306, Loss: 0.09624283015727997\n",
      "Epoch 10, Iteration 307, Loss: 0.10885044932365417\n",
      "Epoch 10, Iteration 308, Loss: 0.11147168278694153\n",
      "Epoch 10, Iteration 309, Loss: 0.1120276227593422\n",
      "Epoch 10, Iteration 310, Loss: 0.12935231626033783\n",
      "Epoch 10, Iteration 311, Loss: 0.10698400437831879\n",
      "Epoch 10, Iteration 312, Loss: 0.13509488105773926\n",
      "Epoch 10, Iteration 313, Loss: 0.10367961972951889\n",
      "Epoch 10, Iteration 314, Loss: 0.11224354058504105\n",
      "Epoch 10, Iteration 315, Loss: 0.12159537523984909\n",
      "Epoch 10, Iteration 316, Loss: 0.10001359134912491\n",
      "Epoch 10, Iteration 317, Loss: 0.14582590758800507\n",
      "Epoch 10, Iteration 318, Loss: 0.14273370802402496\n",
      "Epoch 10, Iteration 319, Loss: 0.08930307626724243\n",
      "Epoch 10, Iteration 320, Loss: 0.1268555372953415\n",
      "Epoch 10, Iteration 321, Loss: 0.09513556212186813\n",
      "Epoch 10, Iteration 322, Loss: 0.14334701001644135\n",
      "Epoch 10, Iteration 323, Loss: 0.10747965425252914\n",
      "Epoch 10, Iteration 324, Loss: 0.17238925397396088\n",
      "Epoch 10, Iteration 325, Loss: 0.23833104968070984\n",
      "Epoch 10, Iteration 326, Loss: 0.17160774767398834\n",
      "Epoch 10, Iteration 327, Loss: 0.13064458966255188\n",
      "Epoch 10, Iteration 328, Loss: 0.12839071452617645\n",
      "Epoch 10, Iteration 329, Loss: 0.13286234438419342\n",
      "Epoch 10, Iteration 330, Loss: 0.17418882250785828\n",
      "Epoch 10, Iteration 331, Loss: 0.14680293202400208\n",
      "Epoch 10, Iteration 332, Loss: 0.15721052885055542\n",
      "Epoch 10, Iteration 333, Loss: 0.1229194849729538\n",
      "Epoch 10, Iteration 334, Loss: 0.18748049437999725\n",
      "Epoch 10, Iteration 335, Loss: 0.1467178463935852\n",
      "Epoch 10, Iteration 336, Loss: 0.18422648310661316\n",
      "Epoch 10, Iteration 337, Loss: 0.10904695838689804\n",
      "Epoch 10, Iteration 338, Loss: 0.12004703283309937\n",
      "Epoch 10, Iteration 339, Loss: 0.155720055103302\n",
      "Epoch 10, Iteration 340, Loss: 0.15743671357631683\n",
      "Epoch 10, Iteration 341, Loss: 0.11985364556312561\n",
      "Epoch 10, Iteration 342, Loss: 0.13929404318332672\n",
      "Epoch 10, Iteration 343, Loss: 0.18252840638160706\n",
      "Epoch 10, Iteration 344, Loss: 0.1804107427597046\n",
      "Epoch 10, Iteration 345, Loss: 0.15564590692520142\n",
      "Epoch 10, Iteration 346, Loss: 0.11460436880588531\n",
      "Epoch 10, Iteration 347, Loss: 0.1580708622932434\n",
      "Epoch 10, Iteration 348, Loss: 0.17173661291599274\n",
      "Epoch 10, Iteration 349, Loss: 0.18605799973011017\n",
      "Epoch 10, Iteration 350, Loss: 0.1774902492761612\n",
      "Epoch 10, Iteration 350, Test Loss: 0.24358779191970825\n",
      "Epoch 10, Iteration 351, Loss: 0.14067304134368896\n",
      "Epoch 10, Iteration 352, Loss: 0.1277882307767868\n",
      "Epoch 10, Iteration 353, Loss: 0.1887173354625702\n",
      "Epoch 10, Iteration 354, Loss: 0.1284533441066742\n",
      "Epoch 10, Iteration 355, Loss: 0.13083209097385406\n",
      "Epoch 10, Iteration 356, Loss: 0.10827357321977615\n",
      "Epoch 10, Iteration 357, Loss: 0.21639271080493927\n",
      "Epoch 10, Iteration 358, Loss: 0.10661125928163528\n",
      "Epoch 10, Iteration 359, Loss: 0.18084464967250824\n",
      "Epoch 10, Iteration 360, Loss: 0.0973018929362297\n",
      "Epoch 10, Iteration 361, Loss: 0.1431403011083603\n",
      "Epoch 10, Iteration 362, Loss: 0.1461770385503769\n",
      "Epoch 10, Iteration 363, Loss: 0.10654185712337494\n",
      "Epoch 10, Iteration 364, Loss: 0.11469779908657074\n",
      "Epoch 10, Iteration 365, Loss: 0.10079970210790634\n",
      "Epoch 10, Iteration 366, Loss: 0.15034516155719757\n",
      "Epoch 10, Iteration 367, Loss: 0.15615960955619812\n",
      "Epoch 10, Iteration 368, Loss: 0.13291813433170319\n",
      "Epoch 10, Iteration 369, Loss: 0.11694588512182236\n",
      "Epoch 10, Iteration 370, Loss: 0.211610808968544\n",
      "Epoch 10, Iteration 371, Loss: 0.13309818506240845\n",
      "Epoch 10, Iteration 372, Loss: 0.12767267227172852\n",
      "Epoch 10, Iteration 373, Loss: 0.15150174498558044\n",
      "Epoch 10, Iteration 374, Loss: 0.10805138200521469\n",
      "Epoch 10, Iteration 375, Loss: 0.151826873421669\n",
      "Epoch 10, Iteration 376, Loss: 0.14735819399356842\n",
      "Epoch 10, Iteration 377, Loss: 0.145083487033844\n",
      "Epoch 10, Iteration 378, Loss: 0.13389787077903748\n",
      "Epoch 10, Iteration 379, Loss: 0.17164833843708038\n",
      "Epoch 10, Iteration 380, Loss: 0.15017065405845642\n",
      "Epoch 10, Iteration 381, Loss: 0.1344338357448578\n",
      "Epoch 10, Iteration 382, Loss: 0.13805437088012695\n",
      "Epoch 10, Iteration 383, Loss: 0.10628129541873932\n",
      "Epoch 10, Iteration 384, Loss: 0.13944333791732788\n",
      "Epoch 10, Iteration 385, Loss: 0.125580832362175\n",
      "Epoch 10, Iteration 386, Loss: 0.12071467936038971\n",
      "Epoch 10, Iteration 387, Loss: 0.08692608028650284\n",
      "Epoch 10, Iteration 388, Loss: 0.09302330762147903\n",
      "Epoch 10, Iteration 389, Loss: 0.10877625644207001\n",
      "Epoch 10, Iteration 390, Loss: 0.12899313867092133\n",
      "Epoch 10, Iteration 391, Loss: 0.0881725400686264\n",
      "Epoch 10, Iteration 392, Loss: 0.19012980163097382\n",
      "Epoch 10, Iteration 393, Loss: 0.1904769241809845\n",
      "Epoch 10, Iteration 394, Loss: 0.1253204643726349\n",
      "Epoch 10, Iteration 395, Loss: 0.12378916144371033\n",
      "Epoch 10, Iteration 396, Loss: 0.13213972747325897\n",
      "Epoch 10, Iteration 397, Loss: 0.1075621172785759\n",
      "Epoch 10, Iteration 398, Loss: 0.2334458976984024\n",
      "Epoch 10, Iteration 399, Loss: 0.09920088201761246\n",
      "Epoch 10, Iteration 400, Loss: 0.10433877259492874\n",
      "Epoch 10, Iteration 400, Test Loss: 0.24823269248008728\n",
      "Epoch 10, Iteration 401, Loss: 0.15282392501831055\n",
      "Epoch 10, Iteration 402, Loss: 0.08666427433490753\n",
      "Epoch 10, Iteration 403, Loss: 0.12719257175922394\n",
      "Epoch 10, Iteration 404, Loss: 0.1361934095621109\n",
      "Epoch 10, Iteration 405, Loss: 0.09811976552009583\n",
      "Epoch 10, Iteration 406, Loss: 0.10273493826389313\n",
      "Epoch 10, Iteration 407, Loss: 0.13629019260406494\n",
      "Epoch 10, Iteration 408, Loss: 0.14160120487213135\n",
      "Epoch 10, Iteration 409, Loss: 0.08934909105300903\n",
      "Epoch 10, Iteration 410, Loss: 0.16346734762191772\n",
      "Epoch 10, Iteration 411, Loss: 0.15760360658168793\n",
      "Epoch 10, Iteration 412, Loss: 0.11224257200956345\n",
      "Epoch 10, Iteration 413, Loss: 0.17349889874458313\n",
      "Epoch 10, Iteration 414, Loss: 0.08770816028118134\n",
      "Epoch 10, Iteration 415, Loss: 0.13358533382415771\n",
      "Epoch 10, Iteration 416, Loss: 0.0954938530921936\n",
      "Epoch 10, Iteration 417, Loss: 0.11978078633546829\n",
      "Epoch 10, Iteration 418, Loss: 0.11427424848079681\n",
      "Epoch 10, Iteration 419, Loss: 0.11207318305969238\n",
      "Epoch 10, Iteration 420, Loss: 0.13958372175693512\n",
      "Epoch 10, Iteration 421, Loss: 0.10226795077323914\n",
      "Epoch 10, Iteration 422, Loss: 0.11667539179325104\n",
      "Epoch 10, Iteration 423, Loss: 0.13970986008644104\n",
      "Epoch 10, Iteration 424, Loss: 0.12566213309764862\n",
      "Epoch 10, Iteration 425, Loss: 0.14898280799388885\n",
      "Epoch 10, Iteration 426, Loss: 0.11648397892713547\n",
      "Epoch 10, Iteration 427, Loss: 0.10722185671329498\n",
      "Epoch 10, Iteration 428, Loss: 0.12117287516593933\n",
      "Epoch 10, Iteration 429, Loss: 0.1338043510913849\n",
      "Epoch 10, Iteration 430, Loss: 0.13024243712425232\n",
      "Epoch 10, Iteration 431, Loss: 0.1201750636100769\n",
      "Epoch 11/15, Loss: 0.14116902473486131\n",
      "Epoch 11/15, Validation Accuracy: 0.9430830039525692\n",
      "Epoch 11, Iteration 0, Loss: 0.24767853319644928\n",
      "Epoch 11, Iteration 1, Loss: 0.18476039171218872\n",
      "Epoch 11, Iteration 2, Loss: 0.21348336338996887\n",
      "Epoch 11, Iteration 3, Loss: 0.1367761790752411\n",
      "Epoch 11, Iteration 4, Loss: 0.2296408712863922\n",
      "Epoch 11, Iteration 5, Loss: 0.25862371921539307\n",
      "Epoch 11, Iteration 6, Loss: 0.23627687990665436\n",
      "Epoch 11, Iteration 7, Loss: 0.24370142817497253\n",
      "Epoch 11, Iteration 8, Loss: 0.20055687427520752\n",
      "Epoch 11, Iteration 9, Loss: 0.14768335223197937\n",
      "Epoch 11, Iteration 10, Loss: 0.1957932710647583\n",
      "Epoch 11, Iteration 11, Loss: 0.1474786251783371\n",
      "Epoch 11, Iteration 12, Loss: 0.1575498878955841\n",
      "Epoch 11, Iteration 13, Loss: 0.19748453795909882\n",
      "Epoch 11, Iteration 14, Loss: 0.21698187291622162\n",
      "Epoch 11, Iteration 15, Loss: 0.10658431798219681\n",
      "Epoch 11, Iteration 16, Loss: 0.2387017160654068\n",
      "Epoch 11, Iteration 17, Loss: 0.15739479660987854\n",
      "Epoch 11, Iteration 18, Loss: 0.17013274133205414\n",
      "Epoch 11, Iteration 19, Loss: 0.18645954132080078\n",
      "Epoch 11, Iteration 20, Loss: 0.2036782056093216\n",
      "Epoch 11, Iteration 21, Loss: 0.2077428549528122\n",
      "Epoch 11, Iteration 22, Loss: 0.1452943980693817\n",
      "Epoch 11, Iteration 23, Loss: 0.1912878453731537\n",
      "Epoch 11, Iteration 24, Loss: 0.17201238870620728\n",
      "Epoch 11, Iteration 25, Loss: 0.21266606450080872\n",
      "Epoch 11, Iteration 26, Loss: 0.1608082354068756\n",
      "Epoch 11, Iteration 27, Loss: 0.14969989657402039\n",
      "Epoch 11, Iteration 28, Loss: 0.20657563209533691\n",
      "Epoch 11, Iteration 29, Loss: 0.13385994732379913\n",
      "Epoch 11, Iteration 30, Loss: 0.18356211483478546\n",
      "Epoch 11, Iteration 31, Loss: 0.18521955609321594\n",
      "Epoch 11, Iteration 32, Loss: 0.21008309721946716\n",
      "Epoch 11, Iteration 33, Loss: 0.16548095643520355\n",
      "Epoch 11, Iteration 34, Loss: 0.18113481998443604\n",
      "Epoch 11, Iteration 35, Loss: 0.10748950392007828\n",
      "Epoch 11, Iteration 36, Loss: 0.16220436990261078\n",
      "Epoch 11, Iteration 37, Loss: 0.15331073105335236\n",
      "Epoch 11, Iteration 38, Loss: 0.17815589904785156\n",
      "Epoch 11, Iteration 39, Loss: 0.177842378616333\n",
      "Epoch 11, Iteration 40, Loss: 0.1741158664226532\n",
      "Epoch 11, Iteration 41, Loss: 0.15555331110954285\n",
      "Epoch 11, Iteration 42, Loss: 0.16738829016685486\n",
      "Epoch 11, Iteration 43, Loss: 0.15794260799884796\n",
      "Epoch 11, Iteration 44, Loss: 0.13964411616325378\n",
      "Epoch 11, Iteration 45, Loss: 0.15565378963947296\n",
      "Epoch 11, Iteration 46, Loss: 0.19126470386981964\n",
      "Epoch 11, Iteration 47, Loss: 0.1851779967546463\n",
      "Epoch 11, Iteration 48, Loss: 0.21080154180526733\n",
      "Epoch 11, Iteration 49, Loss: 0.12381037324666977\n",
      "Epoch 11, Iteration 50, Loss: 0.14764246344566345\n",
      "Epoch 11, Iteration 50, Test Loss: 0.22209113836288452\n",
      "Epoch 11, Iteration 51, Loss: 0.10449977964162827\n",
      "Epoch 11, Iteration 52, Loss: 0.16382330656051636\n",
      "Epoch 11, Iteration 53, Loss: 0.1335403174161911\n",
      "Epoch 11, Iteration 54, Loss: 0.1518193632364273\n",
      "Epoch 11, Iteration 55, Loss: 0.13042788207530975\n",
      "Epoch 11, Iteration 56, Loss: 0.1477283388376236\n",
      "Epoch 11, Iteration 57, Loss: 0.13145673274993896\n",
      "Epoch 11, Iteration 58, Loss: 0.11234370619058609\n",
      "Epoch 11, Iteration 59, Loss: 0.1084623634815216\n",
      "Epoch 11, Iteration 60, Loss: 0.13953068852424622\n",
      "Epoch 11, Iteration 61, Loss: 0.12617528438568115\n",
      "Epoch 11, Iteration 62, Loss: 0.254100501537323\n",
      "Epoch 11, Iteration 63, Loss: 0.09438527375459671\n",
      "Epoch 11, Iteration 64, Loss: 0.11736597120761871\n",
      "Epoch 11, Iteration 65, Loss: 0.14675067365169525\n",
      "Epoch 11, Iteration 66, Loss: 0.14585578441619873\n",
      "Epoch 11, Iteration 67, Loss: 0.10380105674266815\n",
      "Epoch 11, Iteration 68, Loss: 0.1633041799068451\n",
      "Epoch 11, Iteration 69, Loss: 0.17543499171733856\n",
      "Epoch 11, Iteration 70, Loss: 0.10225266963243484\n",
      "Epoch 11, Iteration 71, Loss: 0.1404711902141571\n",
      "Epoch 11, Iteration 72, Loss: 0.14103512465953827\n",
      "Epoch 11, Iteration 73, Loss: 0.1478421837091446\n",
      "Epoch 11, Iteration 74, Loss: 0.13666318356990814\n",
      "Epoch 11, Iteration 75, Loss: 0.15785802900791168\n",
      "Epoch 11, Iteration 76, Loss: 0.15697050094604492\n",
      "Epoch 11, Iteration 77, Loss: 0.13824526965618134\n",
      "Epoch 11, Iteration 78, Loss: 0.1454167366027832\n",
      "Epoch 11, Iteration 79, Loss: 0.09159371256828308\n",
      "Epoch 11, Iteration 80, Loss: 0.15504387021064758\n",
      "Epoch 11, Iteration 81, Loss: 0.1415160745382309\n",
      "Epoch 11, Iteration 82, Loss: 0.1896408200263977\n",
      "Epoch 11, Iteration 83, Loss: 0.13111205399036407\n",
      "Epoch 11, Iteration 84, Loss: 0.11804484575986862\n",
      "Epoch 11, Iteration 85, Loss: 0.17144720256328583\n",
      "Epoch 11, Iteration 86, Loss: 0.1625184565782547\n",
      "Epoch 11, Iteration 87, Loss: 0.13607890903949738\n",
      "Epoch 11, Iteration 88, Loss: 0.1301279515028\n",
      "Epoch 11, Iteration 89, Loss: 0.17002254724502563\n",
      "Epoch 11, Iteration 90, Loss: 0.1366787999868393\n",
      "Epoch 11, Iteration 91, Loss: 0.13421036303043365\n",
      "Epoch 11, Iteration 92, Loss: 0.10963394492864609\n",
      "Epoch 11, Iteration 93, Loss: 0.1104402095079422\n",
      "Epoch 11, Iteration 94, Loss: 0.16644276678562164\n",
      "Epoch 11, Iteration 95, Loss: 0.11793845891952515\n",
      "Epoch 11, Iteration 96, Loss: 0.0873323380947113\n",
      "Epoch 11, Iteration 97, Loss: 0.1489267647266388\n",
      "Epoch 11, Iteration 98, Loss: 0.15266378223896027\n",
      "Epoch 11, Iteration 99, Loss: 0.10130167007446289\n",
      "Epoch 11, Iteration 100, Loss: 0.11424826085567474\n",
      "Epoch 11, Iteration 100, Test Loss: 0.24697792530059814\n",
      "Epoch 11, Iteration 101, Loss: 0.11933383345603943\n",
      "Epoch 11, Iteration 102, Loss: 0.13967277109622955\n",
      "Epoch 11, Iteration 103, Loss: 0.1092534065246582\n",
      "Epoch 11, Iteration 104, Loss: 0.13240544497966766\n",
      "Epoch 11, Iteration 105, Loss: 0.14296825230121613\n",
      "Epoch 11, Iteration 106, Loss: 0.10190772265195847\n",
      "Epoch 11, Iteration 107, Loss: 0.10331904143095016\n",
      "Epoch 11, Iteration 108, Loss: 0.15692131221294403\n",
      "Epoch 11, Iteration 109, Loss: 0.17718200385570526\n",
      "Epoch 11, Iteration 110, Loss: 0.10344832390546799\n",
      "Epoch 11, Iteration 111, Loss: 0.0841546580195427\n",
      "Epoch 11, Iteration 112, Loss: 0.12112656980752945\n",
      "Epoch 11, Iteration 113, Loss: 0.11333050578832626\n",
      "Epoch 11, Iteration 114, Loss: 0.1059621199965477\n",
      "Epoch 11, Iteration 115, Loss: 0.1317121833562851\n",
      "Epoch 11, Iteration 116, Loss: 0.13517779111862183\n",
      "Epoch 11, Iteration 117, Loss: 0.20432303845882416\n",
      "Epoch 11, Iteration 118, Loss: 0.15477417409420013\n",
      "Epoch 11, Iteration 119, Loss: 0.1611296385526657\n",
      "Epoch 11, Iteration 120, Loss: 0.09474222362041473\n",
      "Epoch 11, Iteration 121, Loss: 0.13187389075756073\n",
      "Epoch 11, Iteration 122, Loss: 0.13477852940559387\n",
      "Epoch 11, Iteration 123, Loss: 0.12791547179222107\n",
      "Epoch 11, Iteration 124, Loss: 0.12777520716190338\n",
      "Epoch 11, Iteration 125, Loss: 0.1616620272397995\n",
      "Epoch 11, Iteration 126, Loss: 0.13585852086544037\n",
      "Epoch 11, Iteration 127, Loss: 0.13754433393478394\n",
      "Epoch 11, Iteration 128, Loss: 0.1023811623454094\n",
      "Epoch 11, Iteration 129, Loss: 0.1818309873342514\n",
      "Epoch 11, Iteration 130, Loss: 0.09522873163223267\n",
      "Epoch 11, Iteration 131, Loss: 0.11446212232112885\n",
      "Epoch 11, Iteration 132, Loss: 0.18562550842761993\n",
      "Epoch 11, Iteration 133, Loss: 0.21317727863788605\n",
      "Epoch 11, Iteration 134, Loss: 0.13973428308963776\n",
      "Epoch 11, Iteration 135, Loss: 0.14713077247142792\n",
      "Epoch 11, Iteration 136, Loss: 0.1042376384139061\n",
      "Epoch 11, Iteration 137, Loss: 0.12600569427013397\n",
      "Epoch 11, Iteration 138, Loss: 0.14020349085330963\n",
      "Epoch 11, Iteration 139, Loss: 0.15038272738456726\n",
      "Epoch 11, Iteration 140, Loss: 0.18318860232830048\n",
      "Epoch 11, Iteration 141, Loss: 0.1525033861398697\n",
      "Epoch 11, Iteration 142, Loss: 0.1634911447763443\n",
      "Epoch 11, Iteration 143, Loss: 0.14668796956539154\n",
      "Epoch 11, Iteration 144, Loss: 0.08785546571016312\n",
      "Epoch 11, Iteration 145, Loss: 0.14857593178749084\n",
      "Epoch 11, Iteration 146, Loss: 0.08877754956483841\n",
      "Epoch 11, Iteration 147, Loss: 0.11655081063508987\n",
      "Epoch 11, Iteration 148, Loss: 0.08802492171525955\n",
      "Epoch 11, Iteration 149, Loss: 0.10973460972309113\n",
      "Epoch 11, Iteration 150, Loss: 0.17115528881549835\n",
      "Epoch 11, Iteration 150, Test Loss: 0.24371105432510376\n",
      "Epoch 11, Iteration 151, Loss: 0.14083148539066315\n",
      "Epoch 11, Iteration 152, Loss: 0.13999560475349426\n",
      "Epoch 11, Iteration 153, Loss: 0.1474657654762268\n",
      "Epoch 11, Iteration 154, Loss: 0.1250692456960678\n",
      "Epoch 11, Iteration 155, Loss: 0.1460524946451187\n",
      "Epoch 11, Iteration 156, Loss: 0.16293752193450928\n",
      "Epoch 11, Iteration 157, Loss: 0.1865845024585724\n",
      "Epoch 11, Iteration 158, Loss: 0.16621850430965424\n",
      "Epoch 11, Iteration 159, Loss: 0.12791581451892853\n",
      "Epoch 11, Iteration 160, Loss: 0.15948861837387085\n",
      "Epoch 11, Iteration 161, Loss: 0.08516349643468857\n",
      "Epoch 11, Iteration 162, Loss: 0.0798427164554596\n",
      "Epoch 11, Iteration 163, Loss: 0.13422785699367523\n",
      "Epoch 11, Iteration 164, Loss: 0.09501445293426514\n",
      "Epoch 11, Iteration 165, Loss: 0.11464609205722809\n",
      "Epoch 11, Iteration 166, Loss: 0.12100765109062195\n",
      "Epoch 11, Iteration 167, Loss: 0.14431515336036682\n",
      "Epoch 11, Iteration 168, Loss: 0.10719306021928787\n",
      "Epoch 11, Iteration 169, Loss: 0.13031093776226044\n",
      "Epoch 11, Iteration 170, Loss: 0.09741166234016418\n",
      "Epoch 11, Iteration 171, Loss: 0.16011472046375275\n",
      "Epoch 11, Iteration 172, Loss: 0.1905428171157837\n",
      "Epoch 11, Iteration 173, Loss: 0.1167440339922905\n",
      "Epoch 11, Iteration 174, Loss: 0.13699305057525635\n",
      "Epoch 11, Iteration 175, Loss: 0.1211780235171318\n",
      "Epoch 11, Iteration 176, Loss: 0.10487750172615051\n",
      "Epoch 11, Iteration 177, Loss: 0.190703883767128\n",
      "Epoch 11, Iteration 178, Loss: 0.14751453697681427\n",
      "Epoch 11, Iteration 179, Loss: 0.12468259781599045\n",
      "Epoch 11, Iteration 180, Loss: 0.16534419357776642\n",
      "Epoch 11, Iteration 181, Loss: 0.13119694590568542\n",
      "Epoch 11, Iteration 182, Loss: 0.09758411347866058\n",
      "Epoch 11, Iteration 183, Loss: 0.13465788960456848\n",
      "Epoch 11, Iteration 184, Loss: 0.20877818763256073\n",
      "Epoch 11, Iteration 185, Loss: 0.11524006724357605\n",
      "Epoch 11, Iteration 186, Loss: 0.11734910309314728\n",
      "Epoch 11, Iteration 187, Loss: 0.11685945838689804\n",
      "Epoch 11, Iteration 188, Loss: 0.09882382303476334\n",
      "Epoch 11, Iteration 189, Loss: 0.08772290498018265\n",
      "Epoch 11, Iteration 190, Loss: 0.11736495792865753\n",
      "Epoch 11, Iteration 191, Loss: 0.09108320623636246\n",
      "Epoch 11, Iteration 192, Loss: 0.12436288595199585\n",
      "Epoch 11, Iteration 193, Loss: 0.1247587725520134\n",
      "Epoch 11, Iteration 194, Loss: 0.12387647479772568\n",
      "Epoch 11, Iteration 195, Loss: 0.15538381040096283\n",
      "Epoch 11, Iteration 196, Loss: 0.2352321594953537\n",
      "Epoch 11, Iteration 197, Loss: 0.11933283507823944\n",
      "Epoch 11, Iteration 198, Loss: 0.1001434251666069\n",
      "Epoch 11, Iteration 199, Loss: 0.12884429097175598\n",
      "Epoch 11, Iteration 200, Loss: 0.10759967565536499\n",
      "Epoch 11, Iteration 200, Test Loss: 0.24729681015014648\n",
      "Epoch 11, Iteration 201, Loss: 0.11517307162284851\n",
      "Epoch 11, Iteration 202, Loss: 0.08634479343891144\n",
      "Epoch 11, Iteration 203, Loss: 0.11914778500795364\n",
      "Epoch 11, Iteration 204, Loss: 0.11887198686599731\n",
      "Epoch 11, Iteration 205, Loss: 0.13892509043216705\n",
      "Epoch 11, Iteration 206, Loss: 0.10590089857578278\n",
      "Epoch 11, Iteration 207, Loss: 0.09130731970071793\n",
      "Epoch 11, Iteration 208, Loss: 0.1091049313545227\n",
      "Epoch 11, Iteration 209, Loss: 0.1168658658862114\n",
      "Epoch 11, Iteration 210, Loss: 0.08531661331653595\n",
      "Epoch 11, Iteration 211, Loss: 0.11587908118963242\n",
      "Epoch 11, Iteration 212, Loss: 0.13302171230316162\n",
      "Epoch 11, Iteration 213, Loss: 0.10469621419906616\n",
      "Epoch 11, Iteration 214, Loss: 0.11768289655447006\n",
      "Epoch 11, Iteration 215, Loss: 0.1103815957903862\n",
      "Epoch 11, Iteration 216, Loss: 0.1492721438407898\n",
      "Epoch 11, Iteration 217, Loss: 0.155198872089386\n",
      "Epoch 11, Iteration 218, Loss: 0.1307409703731537\n",
      "Epoch 11, Iteration 219, Loss: 0.14880958199501038\n",
      "Epoch 11, Iteration 220, Loss: 0.10747400671243668\n",
      "Epoch 11, Iteration 221, Loss: 0.15034271776676178\n",
      "Epoch 11, Iteration 222, Loss: 0.12256766110658646\n",
      "Epoch 11, Iteration 223, Loss: 0.1289931982755661\n",
      "Epoch 11, Iteration 224, Loss: 0.0925191268324852\n",
      "Epoch 11, Iteration 225, Loss: 0.13377149403095245\n",
      "Epoch 11, Iteration 226, Loss: 0.16826605796813965\n",
      "Epoch 11, Iteration 227, Loss: 0.123380146920681\n",
      "Epoch 11, Iteration 228, Loss: 0.23052625358104706\n",
      "Epoch 11, Iteration 229, Loss: 0.21521809697151184\n",
      "Epoch 11, Iteration 230, Loss: 0.1735047996044159\n",
      "Epoch 11, Iteration 231, Loss: 0.11323454976081848\n",
      "Epoch 11, Iteration 232, Loss: 0.13413545489311218\n",
      "Epoch 11, Iteration 233, Loss: 0.17006666958332062\n",
      "Epoch 11, Iteration 234, Loss: 0.11467549204826355\n",
      "Epoch 11, Iteration 235, Loss: 0.09097027033567429\n",
      "Epoch 11, Iteration 236, Loss: 0.1511843055486679\n",
      "Epoch 11, Iteration 237, Loss: 0.12221337854862213\n",
      "Epoch 11, Iteration 238, Loss: 0.1267966777086258\n",
      "Epoch 11, Iteration 239, Loss: 0.11639651656150818\n",
      "Epoch 11, Iteration 240, Loss: 0.11879328638315201\n",
      "Epoch 11, Iteration 241, Loss: 0.1312340646982193\n",
      "Epoch 11, Iteration 242, Loss: 0.13702483475208282\n",
      "Epoch 11, Iteration 243, Loss: 0.11302456259727478\n",
      "Epoch 11, Iteration 244, Loss: 0.1516217142343521\n",
      "Epoch 11, Iteration 245, Loss: 0.12811598181724548\n",
      "Epoch 11, Iteration 246, Loss: 0.14889183640480042\n",
      "Epoch 11, Iteration 247, Loss: 0.2615010440349579\n",
      "Epoch 11, Iteration 248, Loss: 0.17353981733322144\n",
      "Epoch 11, Iteration 249, Loss: 0.21408696472644806\n",
      "Epoch 11, Iteration 250, Loss: 0.21274447441101074\n",
      "Epoch 11, Iteration 250, Test Loss: 0.2603324353694916\n",
      "Epoch 11, Iteration 251, Loss: 0.2037830501794815\n",
      "Epoch 11, Iteration 252, Loss: 0.1395711898803711\n",
      "Epoch 11, Iteration 253, Loss: 0.1522873193025589\n",
      "Epoch 11, Iteration 254, Loss: 0.1987149715423584\n",
      "Epoch 11, Iteration 255, Loss: 0.16697391867637634\n",
      "Epoch 11, Iteration 256, Loss: 0.13742245733737946\n",
      "Epoch 11, Iteration 257, Loss: 0.11014024913311005\n",
      "Epoch 11, Iteration 258, Loss: 0.15010935068130493\n",
      "Epoch 11, Iteration 259, Loss: 0.13449420034885406\n",
      "Epoch 11, Iteration 260, Loss: 0.17302151024341583\n",
      "Epoch 11, Iteration 261, Loss: 0.13463883101940155\n",
      "Epoch 11, Iteration 262, Loss: 0.10875583440065384\n",
      "Epoch 11, Iteration 263, Loss: 0.11764277517795563\n",
      "Epoch 11, Iteration 264, Loss: 0.10744398832321167\n",
      "Epoch 11, Iteration 265, Loss: 0.1773936003446579\n",
      "Epoch 11, Iteration 266, Loss: 0.09818749129772186\n",
      "Epoch 11, Iteration 267, Loss: 0.14236973226070404\n",
      "Epoch 11, Iteration 268, Loss: 0.12745629251003265\n",
      "Epoch 11, Iteration 269, Loss: 0.1185249462723732\n",
      "Epoch 11, Iteration 270, Loss: 0.1419871747493744\n",
      "Epoch 11, Iteration 271, Loss: 0.07922901958227158\n",
      "Epoch 11, Iteration 272, Loss: 0.14922279119491577\n",
      "Epoch 11, Iteration 273, Loss: 0.14018523693084717\n",
      "Epoch 11, Iteration 274, Loss: 0.1005733534693718\n",
      "Epoch 11, Iteration 275, Loss: 0.10040883719921112\n",
      "Epoch 11, Iteration 276, Loss: 0.10668379813432693\n",
      "Epoch 11, Iteration 277, Loss: 0.11079121381044388\n",
      "Epoch 11, Iteration 278, Loss: 0.14493481814861298\n",
      "Epoch 11, Iteration 279, Loss: 0.10443731397390366\n",
      "Epoch 11, Iteration 280, Loss: 0.1256527453660965\n",
      "Epoch 11, Iteration 281, Loss: 0.11209768801927567\n",
      "Epoch 11, Iteration 282, Loss: 0.11172530055046082\n",
      "Epoch 11, Iteration 283, Loss: 0.0999714806675911\n",
      "Epoch 11, Iteration 284, Loss: 0.13157488405704498\n",
      "Epoch 11, Iteration 285, Loss: 0.09241814911365509\n",
      "Epoch 11, Iteration 286, Loss: 0.1390514075756073\n",
      "Epoch 11, Iteration 287, Loss: 0.07680892944335938\n",
      "Epoch 11, Iteration 288, Loss: 0.09691299498081207\n",
      "Epoch 11, Iteration 289, Loss: 0.16922329366207123\n",
      "Epoch 11, Iteration 290, Loss: 0.10025596618652344\n",
      "Epoch 11, Iteration 291, Loss: 0.10335864871740341\n",
      "Epoch 11, Iteration 292, Loss: 0.1314239352941513\n",
      "Epoch 11, Iteration 293, Loss: 0.15956668555736542\n",
      "Epoch 11, Iteration 294, Loss: 0.10258039832115173\n",
      "Epoch 11, Iteration 295, Loss: 0.07982325553894043\n",
      "Epoch 11, Iteration 296, Loss: 0.10812384635210037\n",
      "Epoch 11, Iteration 297, Loss: 0.10915803909301758\n",
      "Epoch 11, Iteration 298, Loss: 0.10614633560180664\n",
      "Epoch 11, Iteration 299, Loss: 0.19418396055698395\n",
      "Epoch 11, Iteration 300, Loss: 0.10432914644479752\n",
      "Epoch 11, Iteration 300, Test Loss: 0.2625791132450104\n",
      "Epoch 11, Iteration 301, Loss: 0.10653286427259445\n",
      "Epoch 11, Iteration 302, Loss: 0.07636590301990509\n",
      "Epoch 11, Iteration 303, Loss: 0.1310093253850937\n",
      "Epoch 11, Iteration 304, Loss: 0.15536054968833923\n",
      "Epoch 11, Iteration 305, Loss: 0.11422871798276901\n",
      "Epoch 11, Iteration 306, Loss: 0.09210091084241867\n",
      "Epoch 11, Iteration 307, Loss: 0.11042847484350204\n",
      "Epoch 11, Iteration 308, Loss: 0.11567138880491257\n",
      "Epoch 11, Iteration 309, Loss: 0.09160757064819336\n",
      "Epoch 11, Iteration 310, Loss: 0.13297191262245178\n",
      "Epoch 11, Iteration 311, Loss: 0.08948110789060593\n",
      "Epoch 11, Iteration 312, Loss: 0.14118988811969757\n",
      "Epoch 11, Iteration 313, Loss: 0.10286276787519455\n",
      "Epoch 11, Iteration 314, Loss: 0.11199717223644257\n",
      "Epoch 11, Iteration 315, Loss: 0.10206729918718338\n",
      "Epoch 11, Iteration 316, Loss: 0.09434132277965546\n",
      "Epoch 11, Iteration 317, Loss: 0.1355213224887848\n",
      "Epoch 11, Iteration 318, Loss: 0.14583534002304077\n",
      "Epoch 11, Iteration 319, Loss: 0.0953252911567688\n",
      "Epoch 11, Iteration 320, Loss: 0.12180459499359131\n",
      "Epoch 11, Iteration 321, Loss: 0.09941323846578598\n",
      "Epoch 11, Iteration 322, Loss: 0.14176803827285767\n",
      "Epoch 11, Iteration 323, Loss: 0.09331945329904556\n",
      "Epoch 11, Iteration 324, Loss: 0.1593201458454132\n",
      "Epoch 11, Iteration 325, Loss: 0.2366422414779663\n",
      "Epoch 11, Iteration 326, Loss: 0.1592225730419159\n",
      "Epoch 11, Iteration 327, Loss: 0.1340845674276352\n",
      "Epoch 11, Iteration 328, Loss: 0.13843540847301483\n",
      "Epoch 11, Iteration 329, Loss: 0.14110134541988373\n",
      "Epoch 11, Iteration 330, Loss: 0.15963178873062134\n",
      "Epoch 11, Iteration 331, Loss: 0.14213094115257263\n",
      "Epoch 11, Iteration 332, Loss: 0.13696397840976715\n",
      "Epoch 11, Iteration 333, Loss: 0.1300698071718216\n",
      "Epoch 11, Iteration 334, Loss: 0.19542160630226135\n",
      "Epoch 11, Iteration 335, Loss: 0.12610004842281342\n",
      "Epoch 11, Iteration 336, Loss: 0.15268641710281372\n",
      "Epoch 11, Iteration 337, Loss: 0.11742069572210312\n",
      "Epoch 11, Iteration 338, Loss: 0.1256086379289627\n",
      "Epoch 11, Iteration 339, Loss: 0.16384218633174896\n",
      "Epoch 11, Iteration 340, Loss: 0.1428443193435669\n",
      "Epoch 11, Iteration 341, Loss: 0.13672083616256714\n",
      "Epoch 11, Iteration 342, Loss: 0.12901760637760162\n",
      "Epoch 11, Iteration 343, Loss: 0.18485523760318756\n",
      "Epoch 11, Iteration 344, Loss: 0.17755162715911865\n",
      "Epoch 11, Iteration 345, Loss: 0.1490737646818161\n",
      "Epoch 11, Iteration 346, Loss: 0.14652900397777557\n",
      "Epoch 11, Iteration 347, Loss: 0.1318528801202774\n",
      "Epoch 11, Iteration 348, Loss: 0.177734836935997\n",
      "Epoch 11, Iteration 349, Loss: 0.15151947736740112\n",
      "Epoch 11, Iteration 350, Loss: 0.19620996713638306\n",
      "Epoch 11, Iteration 350, Test Loss: 0.23171716928482056\n",
      "Epoch 11, Iteration 351, Loss: 0.16265647113323212\n",
      "Epoch 11, Iteration 352, Loss: 0.14006313681602478\n",
      "Epoch 11, Iteration 353, Loss: 0.1649746298789978\n",
      "Epoch 11, Iteration 354, Loss: 0.11683503538370132\n",
      "Epoch 11, Iteration 355, Loss: 0.13396018743515015\n",
      "Epoch 11, Iteration 356, Loss: 0.1057807058095932\n",
      "Epoch 11, Iteration 357, Loss: 0.17329809069633484\n",
      "Epoch 11, Iteration 358, Loss: 0.10631418973207474\n",
      "Epoch 11, Iteration 359, Loss: 0.15017454326152802\n",
      "Epoch 11, Iteration 360, Loss: 0.09053447842597961\n",
      "Epoch 11, Iteration 361, Loss: 0.1490357667207718\n",
      "Epoch 11, Iteration 362, Loss: 0.15823118388652802\n",
      "Epoch 11, Iteration 363, Loss: 0.10131686925888062\n",
      "Epoch 11, Iteration 364, Loss: 0.1208270937204361\n",
      "Epoch 11, Iteration 365, Loss: 0.0914577916264534\n",
      "Epoch 11, Iteration 366, Loss: 0.1617249995470047\n",
      "Epoch 11, Iteration 367, Loss: 0.16161853075027466\n",
      "Epoch 11, Iteration 368, Loss: 0.12727507948875427\n",
      "Epoch 11, Iteration 369, Loss: 0.11711365729570389\n",
      "Epoch 11, Iteration 370, Loss: 0.17366008460521698\n",
      "Epoch 11, Iteration 371, Loss: 0.13391521573066711\n",
      "Epoch 11, Iteration 372, Loss: 0.11281397938728333\n",
      "Epoch 11, Iteration 373, Loss: 0.18156753480434418\n",
      "Epoch 11, Iteration 374, Loss: 0.09507878124713898\n",
      "Epoch 11, Iteration 375, Loss: 0.13465428352355957\n",
      "Epoch 11, Iteration 376, Loss: 0.15836897492408752\n",
      "Epoch 11, Iteration 377, Loss: 0.1418626457452774\n",
      "Epoch 11, Iteration 378, Loss: 0.13260500133037567\n",
      "Epoch 11, Iteration 379, Loss: 0.16851212084293365\n",
      "Epoch 11, Iteration 380, Loss: 0.16704729199409485\n",
      "Epoch 11, Iteration 381, Loss: 0.13588866591453552\n",
      "Epoch 11, Iteration 382, Loss: 0.135725736618042\n",
      "Epoch 11, Iteration 383, Loss: 0.09070716053247452\n",
      "Epoch 11, Iteration 384, Loss: 0.12766113877296448\n",
      "Epoch 11, Iteration 385, Loss: 0.12155015021562576\n",
      "Epoch 11, Iteration 386, Loss: 0.11897606402635574\n",
      "Epoch 11, Iteration 387, Loss: 0.09574425965547562\n",
      "Epoch 11, Iteration 388, Loss: 0.08279271423816681\n",
      "Epoch 11, Iteration 389, Loss: 0.10711652785539627\n",
      "Epoch 11, Iteration 390, Loss: 0.1198168620467186\n",
      "Epoch 11, Iteration 391, Loss: 0.08439135551452637\n",
      "Epoch 11, Iteration 392, Loss: 0.17557579278945923\n",
      "Epoch 11, Iteration 393, Loss: 0.17615804076194763\n",
      "Epoch 11, Iteration 394, Loss: 0.12340870499610901\n",
      "Epoch 11, Iteration 395, Loss: 0.11031653732061386\n",
      "Epoch 11, Iteration 396, Loss: 0.13882912695407867\n",
      "Epoch 11, Iteration 397, Loss: 0.10253390669822693\n",
      "Epoch 11, Iteration 398, Loss: 0.2009025365114212\n",
      "Epoch 11, Iteration 399, Loss: 0.10582076758146286\n",
      "Epoch 11, Iteration 400, Loss: 0.1091374009847641\n",
      "Epoch 11, Iteration 400, Test Loss: 0.2511507272720337\n",
      "Epoch 11, Iteration 401, Loss: 0.12694045901298523\n",
      "Epoch 11, Iteration 402, Loss: 0.0962243303656578\n",
      "Epoch 11, Iteration 403, Loss: 0.10925780236721039\n",
      "Epoch 11, Iteration 404, Loss: 0.1393127739429474\n",
      "Epoch 11, Iteration 405, Loss: 0.07403907924890518\n",
      "Epoch 11, Iteration 406, Loss: 0.11252804100513458\n",
      "Epoch 11, Iteration 407, Loss: 0.14664766192436218\n",
      "Epoch 11, Iteration 408, Loss: 0.15976117551326752\n",
      "Epoch 11, Iteration 409, Loss: 0.09242570400238037\n",
      "Epoch 11, Iteration 410, Loss: 0.15410275757312775\n",
      "Epoch 11, Iteration 411, Loss: 0.15711034834384918\n",
      "Epoch 11, Iteration 412, Loss: 0.1381157487630844\n",
      "Epoch 11, Iteration 413, Loss: 0.15115973353385925\n",
      "Epoch 11, Iteration 414, Loss: 0.10604935139417648\n",
      "Epoch 11, Iteration 415, Loss: 0.12739984691143036\n",
      "Epoch 11, Iteration 416, Loss: 0.10995262116193771\n",
      "Epoch 11, Iteration 417, Loss: 0.1308325082063675\n",
      "Epoch 11, Iteration 418, Loss: 0.10299021005630493\n",
      "Epoch 11, Iteration 419, Loss: 0.1209685355424881\n",
      "Epoch 11, Iteration 420, Loss: 0.128453329205513\n",
      "Epoch 11, Iteration 421, Loss: 0.0815119817852974\n",
      "Epoch 11, Iteration 422, Loss: 0.12979523837566376\n",
      "Epoch 11, Iteration 423, Loss: 0.10817217081785202\n",
      "Epoch 11, Iteration 424, Loss: 0.13484343886375427\n",
      "Epoch 11, Iteration 425, Loss: 0.1302168071269989\n",
      "Epoch 11, Iteration 426, Loss: 0.12507329881191254\n",
      "Epoch 11, Iteration 427, Loss: 0.11618070304393768\n",
      "Epoch 11, Iteration 428, Loss: 0.12388899177312851\n",
      "Epoch 11, Iteration 429, Loss: 0.14533017575740814\n",
      "Epoch 11, Iteration 430, Loss: 0.11545632034540176\n",
      "Epoch 11, Iteration 431, Loss: 0.1172565445303917\n",
      "Epoch 12/15, Loss: 0.13817710401835265\n",
      "Epoch 12/15, Validation Accuracy: 0.9431620553359684\n",
      "Epoch 12, Iteration 0, Loss: 0.24826321005821228\n",
      "Epoch 12, Iteration 1, Loss: 0.2295079529285431\n",
      "Epoch 12, Iteration 2, Loss: 0.21072010695934296\n",
      "Epoch 12, Iteration 3, Loss: 0.1535620391368866\n",
      "Epoch 12, Iteration 4, Loss: 0.21580317616462708\n",
      "Epoch 12, Iteration 5, Loss: 0.2023809552192688\n",
      "Epoch 12, Iteration 6, Loss: 0.1795680969953537\n",
      "Epoch 12, Iteration 7, Loss: 0.22854779660701752\n",
      "Epoch 12, Iteration 8, Loss: 0.20693208277225494\n",
      "Epoch 12, Iteration 9, Loss: 0.1845996379852295\n",
      "Epoch 12, Iteration 10, Loss: 0.1707889884710312\n",
      "Epoch 12, Iteration 11, Loss: 0.12804734706878662\n",
      "Epoch 12, Iteration 12, Loss: 0.18159645795822144\n",
      "Epoch 12, Iteration 13, Loss: 0.1892312914133072\n",
      "Epoch 12, Iteration 14, Loss: 0.2129676342010498\n",
      "Epoch 12, Iteration 15, Loss: 0.08309245854616165\n",
      "Epoch 12, Iteration 16, Loss: 0.24126878380775452\n",
      "Epoch 12, Iteration 17, Loss: 0.16309291124343872\n",
      "Epoch 12, Iteration 18, Loss: 0.1498812586069107\n",
      "Epoch 12, Iteration 19, Loss: 0.1663636416196823\n",
      "Epoch 12, Iteration 20, Loss: 0.18406414985656738\n",
      "Epoch 12, Iteration 21, Loss: 0.22044572234153748\n",
      "Epoch 12, Iteration 22, Loss: 0.1675185263156891\n",
      "Epoch 12, Iteration 23, Loss: 0.160082146525383\n",
      "Epoch 12, Iteration 24, Loss: 0.1367126852273941\n",
      "Epoch 12, Iteration 25, Loss: 0.2832639813423157\n",
      "Epoch 12, Iteration 26, Loss: 0.1891963928937912\n",
      "Epoch 12, Iteration 27, Loss: 0.14477241039276123\n",
      "Epoch 12, Iteration 28, Loss: 0.2266911417245865\n",
      "Epoch 12, Iteration 29, Loss: 0.18109561502933502\n",
      "Epoch 12, Iteration 30, Loss: 0.19811081886291504\n",
      "Epoch 12, Iteration 31, Loss: 0.17471012473106384\n",
      "Epoch 12, Iteration 32, Loss: 0.1934332251548767\n",
      "Epoch 12, Iteration 33, Loss: 0.16347721219062805\n",
      "Epoch 12, Iteration 34, Loss: 0.1890604943037033\n",
      "Epoch 12, Iteration 35, Loss: 0.11692236363887787\n",
      "Epoch 12, Iteration 36, Loss: 0.1452881246805191\n",
      "Epoch 12, Iteration 37, Loss: 0.15061810612678528\n",
      "Epoch 12, Iteration 38, Loss: 0.16980135440826416\n",
      "Epoch 12, Iteration 39, Loss: 0.16661621630191803\n",
      "Epoch 12, Iteration 40, Loss: 0.1517256200313568\n",
      "Epoch 12, Iteration 41, Loss: 0.14512145519256592\n",
      "Epoch 12, Iteration 42, Loss: 0.14580166339874268\n",
      "Epoch 12, Iteration 43, Loss: 0.1520056128501892\n",
      "Epoch 12, Iteration 44, Loss: 0.18070921301841736\n",
      "Epoch 12, Iteration 45, Loss: 0.19047367572784424\n",
      "Epoch 12, Iteration 46, Loss: 0.14826929569244385\n",
      "Epoch 12, Iteration 47, Loss: 0.1642688363790512\n",
      "Epoch 12, Iteration 48, Loss: 0.21244113147258759\n",
      "Epoch 12, Iteration 49, Loss: 0.15611550211906433\n",
      "Epoch 12, Iteration 50, Loss: 0.12852820754051208\n",
      "Epoch 12, Iteration 50, Test Loss: 0.21666187047958374\n",
      "Epoch 12, Iteration 51, Loss: 0.11863958835601807\n",
      "Epoch 12, Iteration 52, Loss: 0.1831856518983841\n",
      "Epoch 12, Iteration 53, Loss: 0.1275513470172882\n",
      "Epoch 12, Iteration 54, Loss: 0.13431796431541443\n",
      "Epoch 12, Iteration 55, Loss: 0.1064189225435257\n",
      "Epoch 12, Iteration 56, Loss: 0.15267495810985565\n",
      "Epoch 12, Iteration 57, Loss: 0.1354975551366806\n",
      "Epoch 12, Iteration 58, Loss: 0.1211298406124115\n",
      "Epoch 12, Iteration 59, Loss: 0.09276580065488815\n",
      "Epoch 12, Iteration 60, Loss: 0.15075913071632385\n",
      "Epoch 12, Iteration 61, Loss: 0.0965351089835167\n",
      "Epoch 12, Iteration 62, Loss: 0.23284214735031128\n",
      "Epoch 12, Iteration 63, Loss: 0.0948122963309288\n",
      "Epoch 12, Iteration 64, Loss: 0.13067643344402313\n",
      "Epoch 12, Iteration 65, Loss: 0.13583534955978394\n",
      "Epoch 12, Iteration 66, Loss: 0.12137238681316376\n",
      "Epoch 12, Iteration 67, Loss: 0.10124683380126953\n",
      "Epoch 12, Iteration 68, Loss: 0.13742095232009888\n",
      "Epoch 12, Iteration 69, Loss: 0.16495446860790253\n",
      "Epoch 12, Iteration 70, Loss: 0.11904961615800858\n",
      "Epoch 12, Iteration 71, Loss: 0.1291745901107788\n",
      "Epoch 12, Iteration 72, Loss: 0.14447489380836487\n",
      "Epoch 12, Iteration 73, Loss: 0.1426669955253601\n",
      "Epoch 12, Iteration 74, Loss: 0.14624108374118805\n",
      "Epoch 12, Iteration 75, Loss: 0.14368416368961334\n",
      "Epoch 12, Iteration 76, Loss: 0.13028044998645782\n",
      "Epoch 12, Iteration 77, Loss: 0.15901772677898407\n",
      "Epoch 12, Iteration 78, Loss: 0.15599505603313446\n",
      "Epoch 12, Iteration 79, Loss: 0.09633368998765945\n",
      "Epoch 12, Iteration 80, Loss: 0.17223942279815674\n",
      "Epoch 12, Iteration 81, Loss: 0.12977644801139832\n",
      "Epoch 12, Iteration 82, Loss: 0.1694846749305725\n",
      "Epoch 12, Iteration 83, Loss: 0.12196934223175049\n",
      "Epoch 12, Iteration 84, Loss: 0.1166129931807518\n",
      "Epoch 12, Iteration 85, Loss: 0.17109347879886627\n",
      "Epoch 12, Iteration 86, Loss: 0.1683991700410843\n",
      "Epoch 12, Iteration 87, Loss: 0.15954835712909698\n",
      "Epoch 12, Iteration 88, Loss: 0.1416555941104889\n",
      "Epoch 12, Iteration 89, Loss: 0.12640325725078583\n",
      "Epoch 12, Iteration 90, Loss: 0.15534135699272156\n",
      "Epoch 12, Iteration 91, Loss: 0.13505041599273682\n",
      "Epoch 12, Iteration 92, Loss: 0.11516401171684265\n",
      "Epoch 12, Iteration 93, Loss: 0.14044596254825592\n",
      "Epoch 12, Iteration 94, Loss: 0.1753300130367279\n",
      "Epoch 12, Iteration 95, Loss: 0.1224367544054985\n",
      "Epoch 12, Iteration 96, Loss: 0.08875130116939545\n",
      "Epoch 12, Iteration 97, Loss: 0.1427297592163086\n",
      "Epoch 12, Iteration 98, Loss: 0.16407892107963562\n",
      "Epoch 12, Iteration 99, Loss: 0.10022743791341782\n",
      "Epoch 12, Iteration 100, Loss: 0.09793052822351456\n",
      "Epoch 12, Iteration 100, Test Loss: 0.23375515639781952\n",
      "Epoch 12, Iteration 101, Loss: 0.12727339565753937\n",
      "Epoch 12, Iteration 102, Loss: 0.11929834634065628\n",
      "Epoch 12, Iteration 103, Loss: 0.0956687331199646\n",
      "Epoch 12, Iteration 104, Loss: 0.1311989724636078\n",
      "Epoch 12, Iteration 105, Loss: 0.12277757376432419\n",
      "Epoch 12, Iteration 106, Loss: 0.09877899289131165\n",
      "Epoch 12, Iteration 107, Loss: 0.10324741899967194\n",
      "Epoch 12, Iteration 108, Loss: 0.14320246875286102\n",
      "Epoch 12, Iteration 109, Loss: 0.20140515267848969\n",
      "Epoch 12, Iteration 110, Loss: 0.11014297604560852\n",
      "Epoch 12, Iteration 111, Loss: 0.07870391011238098\n",
      "Epoch 12, Iteration 112, Loss: 0.14494138956069946\n",
      "Epoch 12, Iteration 113, Loss: 0.13544760644435883\n",
      "Epoch 12, Iteration 114, Loss: 0.10912266373634338\n",
      "Epoch 12, Iteration 115, Loss: 0.169870525598526\n",
      "Epoch 12, Iteration 116, Loss: 0.15921804308891296\n",
      "Epoch 12, Iteration 117, Loss: 0.2347780466079712\n",
      "Epoch 12, Iteration 118, Loss: 0.153172567486763\n",
      "Epoch 12, Iteration 119, Loss: 0.15319451689720154\n",
      "Epoch 12, Iteration 120, Loss: 0.09700383245944977\n",
      "Epoch 12, Iteration 121, Loss: 0.14950165152549744\n",
      "Epoch 12, Iteration 122, Loss: 0.15819497406482697\n",
      "Epoch 12, Iteration 123, Loss: 0.10962725430727005\n",
      "Epoch 12, Iteration 124, Loss: 0.1400936245918274\n",
      "Epoch 12, Iteration 125, Loss: 0.16348110139369965\n",
      "Epoch 12, Iteration 126, Loss: 0.115913026034832\n",
      "Epoch 12, Iteration 127, Loss: 0.13991308212280273\n",
      "Epoch 12, Iteration 128, Loss: 0.11690692603588104\n",
      "Epoch 12, Iteration 129, Loss: 0.1725599765777588\n",
      "Epoch 12, Iteration 130, Loss: 0.09867554903030396\n",
      "Epoch 12, Iteration 131, Loss: 0.11067772656679153\n",
      "Epoch 12, Iteration 132, Loss: 0.1921483725309372\n",
      "Epoch 12, Iteration 133, Loss: 0.2270520031452179\n",
      "Epoch 12, Iteration 134, Loss: 0.13591918349266052\n",
      "Epoch 12, Iteration 135, Loss: 0.15398544073104858\n",
      "Epoch 12, Iteration 136, Loss: 0.09711078554391861\n",
      "Epoch 12, Iteration 137, Loss: 0.10528699308633804\n",
      "Epoch 12, Iteration 138, Loss: 0.1343751847743988\n",
      "Epoch 12, Iteration 139, Loss: 0.15408238768577576\n",
      "Epoch 12, Iteration 140, Loss: 0.17490838468074799\n",
      "Epoch 12, Iteration 141, Loss: 0.1382323056459427\n",
      "Epoch 12, Iteration 142, Loss: 0.16094596683979034\n",
      "Epoch 12, Iteration 143, Loss: 0.14288213849067688\n",
      "Epoch 12, Iteration 144, Loss: 0.10051038861274719\n",
      "Epoch 12, Iteration 145, Loss: 0.16094249486923218\n",
      "Epoch 12, Iteration 146, Loss: 0.10896065086126328\n",
      "Epoch 12, Iteration 147, Loss: 0.10374852269887924\n",
      "Epoch 12, Iteration 148, Loss: 0.10534076392650604\n",
      "Epoch 12, Iteration 149, Loss: 0.1097128614783287\n",
      "Epoch 12, Iteration 150, Loss: 0.1488637924194336\n",
      "Epoch 12, Iteration 150, Test Loss: 0.24757391214370728\n",
      "Epoch 12, Iteration 151, Loss: 0.11608663201332092\n",
      "Epoch 12, Iteration 152, Loss: 0.1468048095703125\n",
      "Epoch 12, Iteration 153, Loss: 0.14902620017528534\n",
      "Epoch 12, Iteration 154, Loss: 0.11669470369815826\n",
      "Epoch 12, Iteration 155, Loss: 0.14840590953826904\n",
      "Epoch 12, Iteration 156, Loss: 0.17730002105236053\n",
      "Epoch 12, Iteration 157, Loss: 0.1876717358827591\n",
      "Epoch 12, Iteration 158, Loss: 0.18347729742527008\n",
      "Epoch 12, Iteration 159, Loss: 0.11132767051458359\n",
      "Epoch 12, Iteration 160, Loss: 0.16655714809894562\n",
      "Epoch 12, Iteration 161, Loss: 0.08603186160326004\n",
      "Epoch 12, Iteration 162, Loss: 0.08707568049430847\n",
      "Epoch 12, Iteration 163, Loss: 0.1471685767173767\n",
      "Epoch 12, Iteration 164, Loss: 0.08835095167160034\n",
      "Epoch 12, Iteration 165, Loss: 0.1104108989238739\n",
      "Epoch 12, Iteration 166, Loss: 0.12622755765914917\n",
      "Epoch 12, Iteration 167, Loss: 0.12789341807365417\n",
      "Epoch 12, Iteration 168, Loss: 0.08725173771381378\n",
      "Epoch 12, Iteration 169, Loss: 0.13264340162277222\n",
      "Epoch 12, Iteration 170, Loss: 0.08709243685007095\n",
      "Epoch 12, Iteration 171, Loss: 0.16802576184272766\n",
      "Epoch 12, Iteration 172, Loss: 0.20620203018188477\n",
      "Epoch 12, Iteration 173, Loss: 0.14630791544914246\n",
      "Epoch 12, Iteration 174, Loss: 0.14025725424289703\n",
      "Epoch 12, Iteration 175, Loss: 0.12475351989269257\n",
      "Epoch 12, Iteration 176, Loss: 0.11290391534566879\n",
      "Epoch 12, Iteration 177, Loss: 0.1918390393257141\n",
      "Epoch 12, Iteration 178, Loss: 0.12367583811283112\n",
      "Epoch 12, Iteration 179, Loss: 0.11689586937427521\n",
      "Epoch 12, Iteration 180, Loss: 0.1628025472164154\n",
      "Epoch 12, Iteration 181, Loss: 0.12367279827594757\n",
      "Epoch 12, Iteration 182, Loss: 0.12057552486658096\n",
      "Epoch 12, Iteration 183, Loss: 0.13689295947551727\n",
      "Epoch 12, Iteration 184, Loss: 0.2001827359199524\n",
      "Epoch 12, Iteration 185, Loss: 0.10467121005058289\n",
      "Epoch 12, Iteration 186, Loss: 0.11689507961273193\n",
      "Epoch 12, Iteration 187, Loss: 0.12566260993480682\n",
      "Epoch 12, Iteration 188, Loss: 0.11990078538656235\n",
      "Epoch 12, Iteration 189, Loss: 0.07849981635808945\n",
      "Epoch 12, Iteration 190, Loss: 0.0922975167632103\n",
      "Epoch 12, Iteration 191, Loss: 0.10634350031614304\n",
      "Epoch 12, Iteration 192, Loss: 0.1120063066482544\n",
      "Epoch 12, Iteration 193, Loss: 0.11258071660995483\n",
      "Epoch 12, Iteration 194, Loss: 0.10632413625717163\n",
      "Epoch 12, Iteration 195, Loss: 0.1298488974571228\n",
      "Epoch 12, Iteration 196, Loss: 0.22728610038757324\n",
      "Epoch 12, Iteration 197, Loss: 0.10079838335514069\n",
      "Epoch 12, Iteration 198, Loss: 0.09609381854534149\n",
      "Epoch 12, Iteration 199, Loss: 0.13596168160438538\n",
      "Epoch 12, Iteration 200, Loss: 0.10286839306354523\n",
      "Epoch 12, Iteration 200, Test Loss: 0.24568302929401398\n",
      "Epoch 12, Iteration 201, Loss: 0.10057466477155685\n",
      "Epoch 12, Iteration 202, Loss: 0.07964177429676056\n",
      "Epoch 12, Iteration 203, Loss: 0.11996600776910782\n",
      "Epoch 12, Iteration 204, Loss: 0.10529394447803497\n",
      "Epoch 12, Iteration 205, Loss: 0.13457663357257843\n",
      "Epoch 12, Iteration 206, Loss: 0.09804438799619675\n",
      "Epoch 12, Iteration 207, Loss: 0.100161612033844\n",
      "Epoch 12, Iteration 208, Loss: 0.10240412503480911\n",
      "Epoch 12, Iteration 209, Loss: 0.12651292979717255\n",
      "Epoch 12, Iteration 210, Loss: 0.08694726973772049\n",
      "Epoch 12, Iteration 211, Loss: 0.12336099147796631\n",
      "Epoch 12, Iteration 212, Loss: 0.14989812672138214\n",
      "Epoch 12, Iteration 213, Loss: 0.11708864569664001\n",
      "Epoch 12, Iteration 214, Loss: 0.10433536767959595\n",
      "Epoch 12, Iteration 215, Loss: 0.11209053546190262\n",
      "Epoch 12, Iteration 216, Loss: 0.1198706328868866\n",
      "Epoch 12, Iteration 217, Loss: 0.13542677462100983\n",
      "Epoch 12, Iteration 218, Loss: 0.1469336450099945\n",
      "Epoch 12, Iteration 219, Loss: 0.13339419662952423\n",
      "Epoch 12, Iteration 220, Loss: 0.10711773484945297\n",
      "Epoch 12, Iteration 221, Loss: 0.12816853821277618\n",
      "Epoch 12, Iteration 222, Loss: 0.1203291267156601\n",
      "Epoch 12, Iteration 223, Loss: 0.14779633283615112\n",
      "Epoch 12, Iteration 224, Loss: 0.1103396937251091\n",
      "Epoch 12, Iteration 225, Loss: 0.12947580218315125\n",
      "Epoch 12, Iteration 226, Loss: 0.1250879168510437\n",
      "Epoch 12, Iteration 227, Loss: 0.12483622878789902\n",
      "Epoch 12, Iteration 228, Loss: 0.23229669034481049\n",
      "Epoch 12, Iteration 229, Loss: 0.22691963613033295\n",
      "Epoch 12, Iteration 230, Loss: 0.15218594670295715\n",
      "Epoch 12, Iteration 231, Loss: 0.11934451758861542\n",
      "Epoch 12, Iteration 232, Loss: 0.1263686716556549\n",
      "Epoch 12, Iteration 233, Loss: 0.16728077828884125\n",
      "Epoch 12, Iteration 234, Loss: 0.09049751609563828\n",
      "Epoch 12, Iteration 235, Loss: 0.08685530722141266\n",
      "Epoch 12, Iteration 236, Loss: 0.14828336238861084\n",
      "Epoch 12, Iteration 237, Loss: 0.13124553859233856\n",
      "Epoch 12, Iteration 238, Loss: 0.1255168616771698\n",
      "Epoch 12, Iteration 239, Loss: 0.11872129142284393\n",
      "Epoch 12, Iteration 240, Loss: 0.11905593425035477\n",
      "Epoch 12, Iteration 241, Loss: 0.11993751674890518\n",
      "Epoch 12, Iteration 242, Loss: 0.13057903945446014\n",
      "Epoch 12, Iteration 243, Loss: 0.09895718097686768\n",
      "Epoch 12, Iteration 244, Loss: 0.13705813884735107\n",
      "Epoch 12, Iteration 245, Loss: 0.1484585851430893\n",
      "Epoch 12, Iteration 246, Loss: 0.16481944918632507\n",
      "Epoch 12, Iteration 247, Loss: 0.24553832411766052\n",
      "Epoch 12, Iteration 248, Loss: 0.20568738877773285\n",
      "Epoch 12, Iteration 249, Loss: 0.21001094579696655\n",
      "Epoch 12, Iteration 250, Loss: 0.20387552678585052\n",
      "Epoch 12, Iteration 250, Test Loss: 0.2476281076669693\n",
      "Epoch 12, Iteration 251, Loss: 0.20563949644565582\n",
      "Epoch 12, Iteration 252, Loss: 0.12014032155275345\n",
      "Epoch 12, Iteration 253, Loss: 0.15754112601280212\n",
      "Epoch 12, Iteration 254, Loss: 0.1979149878025055\n",
      "Epoch 12, Iteration 255, Loss: 0.14345020055770874\n",
      "Epoch 12, Iteration 256, Loss: 0.12620005011558533\n",
      "Epoch 12, Iteration 257, Loss: 0.11594792455434799\n",
      "Epoch 12, Iteration 258, Loss: 0.142532080411911\n",
      "Epoch 12, Iteration 259, Loss: 0.1365416944026947\n",
      "Epoch 12, Iteration 260, Loss: 0.19438132643699646\n",
      "Epoch 12, Iteration 261, Loss: 0.12316535413265228\n",
      "Epoch 12, Iteration 262, Loss: 0.11115548014640808\n",
      "Epoch 12, Iteration 263, Loss: 0.09682174026966095\n",
      "Epoch 12, Iteration 264, Loss: 0.12071593105792999\n",
      "Epoch 12, Iteration 265, Loss: 0.186628058552742\n",
      "Epoch 12, Iteration 266, Loss: 0.09750760346651077\n",
      "Epoch 12, Iteration 267, Loss: 0.12589459121227264\n",
      "Epoch 12, Iteration 268, Loss: 0.1350865215063095\n",
      "Epoch 12, Iteration 269, Loss: 0.12226644158363342\n",
      "Epoch 12, Iteration 270, Loss: 0.14368484914302826\n",
      "Epoch 12, Iteration 271, Loss: 0.06615447998046875\n",
      "Epoch 12, Iteration 272, Loss: 0.13921993970870972\n",
      "Epoch 12, Iteration 273, Loss: 0.1341315656900406\n",
      "Epoch 12, Iteration 274, Loss: 0.10498391091823578\n",
      "Epoch 12, Iteration 275, Loss: 0.11228586733341217\n",
      "Epoch 12, Iteration 276, Loss: 0.10005703568458557\n",
      "Epoch 12, Iteration 277, Loss: 0.11971887946128845\n",
      "Epoch 12, Iteration 278, Loss: 0.14126701653003693\n",
      "Epoch 12, Iteration 279, Loss: 0.09448281675577164\n",
      "Epoch 12, Iteration 280, Loss: 0.14285919070243835\n",
      "Epoch 12, Iteration 281, Loss: 0.09321744740009308\n",
      "Epoch 12, Iteration 282, Loss: 0.11700484901666641\n",
      "Epoch 12, Iteration 283, Loss: 0.07377903908491135\n",
      "Epoch 12, Iteration 284, Loss: 0.12803314626216888\n",
      "Epoch 12, Iteration 285, Loss: 0.09113945811986923\n",
      "Epoch 12, Iteration 286, Loss: 0.15662480890750885\n",
      "Epoch 12, Iteration 287, Loss: 0.08488459140062332\n",
      "Epoch 12, Iteration 288, Loss: 0.09429931640625\n",
      "Epoch 12, Iteration 289, Loss: 0.1694960743188858\n",
      "Epoch 12, Iteration 290, Loss: 0.09298519045114517\n",
      "Epoch 12, Iteration 291, Loss: 0.11589346081018448\n",
      "Epoch 12, Iteration 292, Loss: 0.13483378291130066\n",
      "Epoch 12, Iteration 293, Loss: 0.16807928681373596\n",
      "Epoch 12, Iteration 294, Loss: 0.09398984163999557\n",
      "Epoch 12, Iteration 295, Loss: 0.0777292475104332\n",
      "Epoch 12, Iteration 296, Loss: 0.0851183533668518\n",
      "Epoch 12, Iteration 297, Loss: 0.10888919234275818\n",
      "Epoch 12, Iteration 298, Loss: 0.099761962890625\n",
      "Epoch 12, Iteration 299, Loss: 0.160848006606102\n",
      "Epoch 12, Iteration 300, Loss: 0.12116444110870361\n",
      "Epoch 12, Iteration 300, Test Loss: 0.2567494511604309\n",
      "Epoch 12, Iteration 301, Loss: 0.10164429247379303\n",
      "Epoch 12, Iteration 302, Loss: 0.07725024968385696\n",
      "Epoch 12, Iteration 303, Loss: 0.14697138965129852\n",
      "Epoch 12, Iteration 304, Loss: 0.14975082874298096\n",
      "Epoch 12, Iteration 305, Loss: 0.1033926010131836\n",
      "Epoch 12, Iteration 306, Loss: 0.0988302007317543\n",
      "Epoch 12, Iteration 307, Loss: 0.1175156980752945\n",
      "Epoch 12, Iteration 308, Loss: 0.11174877732992172\n",
      "Epoch 12, Iteration 309, Loss: 0.11506039649248123\n",
      "Epoch 12, Iteration 310, Loss: 0.13124506175518036\n",
      "Epoch 12, Iteration 311, Loss: 0.10906994342803955\n",
      "Epoch 12, Iteration 312, Loss: 0.12478026002645493\n",
      "Epoch 12, Iteration 313, Loss: 0.08496574312448502\n",
      "Epoch 12, Iteration 314, Loss: 0.1153503879904747\n",
      "Epoch 12, Iteration 315, Loss: 0.11693315207958221\n",
      "Epoch 12, Iteration 316, Loss: 0.11844366788864136\n",
      "Epoch 12, Iteration 317, Loss: 0.13347944617271423\n",
      "Epoch 12, Iteration 318, Loss: 0.1464320719242096\n",
      "Epoch 12, Iteration 319, Loss: 0.10220359265804291\n",
      "Epoch 12, Iteration 320, Loss: 0.126628577709198\n",
      "Epoch 12, Iteration 321, Loss: 0.09656204283237457\n",
      "Epoch 12, Iteration 322, Loss: 0.14902304112911224\n",
      "Epoch 12, Iteration 323, Loss: 0.10790279507637024\n",
      "Epoch 12, Iteration 324, Loss: 0.14342385530471802\n",
      "Epoch 12, Iteration 325, Loss: 0.2325628697872162\n",
      "Epoch 12, Iteration 326, Loss: 0.16926082968711853\n",
      "Epoch 12, Iteration 327, Loss: 0.12408286333084106\n",
      "Epoch 12, Iteration 328, Loss: 0.13726386427879333\n",
      "Epoch 12, Iteration 329, Loss: 0.11455430835485458\n",
      "Epoch 12, Iteration 330, Loss: 0.15578244626522064\n",
      "Epoch 12, Iteration 331, Loss: 0.14048171043395996\n",
      "Epoch 12, Iteration 332, Loss: 0.12571628391742706\n",
      "Epoch 12, Iteration 333, Loss: 0.11663922667503357\n",
      "Epoch 12, Iteration 334, Loss: 0.21295294165611267\n",
      "Epoch 12, Iteration 335, Loss: 0.14439108967781067\n",
      "Epoch 12, Iteration 336, Loss: 0.18136313557624817\n",
      "Epoch 12, Iteration 337, Loss: 0.12528672814369202\n",
      "Epoch 12, Iteration 338, Loss: 0.125094473361969\n",
      "Epoch 12, Iteration 339, Loss: 0.1670904904603958\n",
      "Epoch 12, Iteration 340, Loss: 0.13915041089057922\n",
      "Epoch 12, Iteration 341, Loss: 0.1270999312400818\n",
      "Epoch 12, Iteration 342, Loss: 0.12597763538360596\n",
      "Epoch 12, Iteration 343, Loss: 0.1751086562871933\n",
      "Epoch 12, Iteration 344, Loss: 0.18520651757717133\n",
      "Epoch 12, Iteration 345, Loss: 0.14540886878967285\n",
      "Epoch 12, Iteration 346, Loss: 0.13358494639396667\n",
      "Epoch 12, Iteration 347, Loss: 0.1587572544813156\n",
      "Epoch 12, Iteration 348, Loss: 0.1661461442708969\n",
      "Epoch 12, Iteration 349, Loss: 0.17825762927532196\n",
      "Epoch 12, Iteration 350, Loss: 0.17157414555549622\n",
      "Epoch 12, Iteration 350, Test Loss: 0.2313358038663864\n",
      "Epoch 12, Iteration 351, Loss: 0.16347108781337738\n",
      "Epoch 12, Iteration 352, Loss: 0.1451830118894577\n",
      "Epoch 12, Iteration 353, Loss: 0.1811353862285614\n",
      "Epoch 12, Iteration 354, Loss: 0.11028184741735458\n",
      "Epoch 12, Iteration 355, Loss: 0.13231459259986877\n",
      "Epoch 12, Iteration 356, Loss: 0.1199297085404396\n",
      "Epoch 12, Iteration 357, Loss: 0.1853724718093872\n",
      "Epoch 12, Iteration 358, Loss: 0.11319755017757416\n",
      "Epoch 12, Iteration 359, Loss: 0.18890869617462158\n",
      "Epoch 12, Iteration 360, Loss: 0.08247384428977966\n",
      "Epoch 12, Iteration 361, Loss: 0.14137426018714905\n",
      "Epoch 12, Iteration 362, Loss: 0.1218092292547226\n",
      "Epoch 12, Iteration 363, Loss: 0.10123741626739502\n",
      "Epoch 12, Iteration 364, Loss: 0.1268530786037445\n",
      "Epoch 12, Iteration 365, Loss: 0.1053222045302391\n",
      "Epoch 12, Iteration 366, Loss: 0.17629525065422058\n",
      "Epoch 12, Iteration 367, Loss: 0.16324953734874725\n",
      "Epoch 12, Iteration 368, Loss: 0.1265782117843628\n",
      "Epoch 12, Iteration 369, Loss: 0.1173577755689621\n",
      "Epoch 12, Iteration 370, Loss: 0.1334107518196106\n",
      "Epoch 12, Iteration 371, Loss: 0.13179750740528107\n",
      "Epoch 12, Iteration 372, Loss: 0.1177443340420723\n",
      "Epoch 12, Iteration 373, Loss: 0.1583574414253235\n",
      "Epoch 12, Iteration 374, Loss: 0.10432779788970947\n",
      "Epoch 12, Iteration 375, Loss: 0.140290305018425\n",
      "Epoch 12, Iteration 376, Loss: 0.13991627097129822\n",
      "Epoch 12, Iteration 377, Loss: 0.15156352519989014\n",
      "Epoch 12, Iteration 378, Loss: 0.1256798505783081\n",
      "Epoch 12, Iteration 379, Loss: 0.16507156193256378\n",
      "Epoch 12, Iteration 380, Loss: 0.14308682084083557\n",
      "Epoch 12, Iteration 381, Loss: 0.12511742115020752\n",
      "Epoch 12, Iteration 382, Loss: 0.1144634559750557\n",
      "Epoch 12, Iteration 383, Loss: 0.10442812740802765\n",
      "Epoch 12, Iteration 384, Loss: 0.11509567499160767\n",
      "Epoch 12, Iteration 385, Loss: 0.12550847232341766\n",
      "Epoch 12, Iteration 386, Loss: 0.12095041573047638\n",
      "Epoch 12, Iteration 387, Loss: 0.09683002531528473\n",
      "Epoch 12, Iteration 388, Loss: 0.08313098549842834\n",
      "Epoch 12, Iteration 389, Loss: 0.09778008610010147\n",
      "Epoch 12, Iteration 390, Loss: 0.12885938584804535\n",
      "Epoch 12, Iteration 391, Loss: 0.08358625322580338\n",
      "Epoch 12, Iteration 392, Loss: 0.1755698174238205\n",
      "Epoch 12, Iteration 393, Loss: 0.18207132816314697\n",
      "Epoch 12, Iteration 394, Loss: 0.12493728846311569\n",
      "Epoch 12, Iteration 395, Loss: 0.10355491936206818\n",
      "Epoch 12, Iteration 396, Loss: 0.11085694283246994\n",
      "Epoch 12, Iteration 397, Loss: 0.1051398292183876\n",
      "Epoch 12, Iteration 398, Loss: 0.1974949836730957\n",
      "Epoch 12, Iteration 399, Loss: 0.1085357517004013\n",
      "Epoch 12, Iteration 400, Loss: 0.09952946752309799\n",
      "Epoch 12, Iteration 400, Test Loss: 0.248513862490654\n",
      "Epoch 12, Iteration 401, Loss: 0.12821313738822937\n",
      "Epoch 12, Iteration 402, Loss: 0.08324947208166122\n",
      "Epoch 12, Iteration 403, Loss: 0.12137246131896973\n",
      "Epoch 12, Iteration 404, Loss: 0.11930815130472183\n",
      "Epoch 12, Iteration 405, Loss: 0.08885853737592697\n",
      "Epoch 12, Iteration 406, Loss: 0.10557638108730316\n",
      "Epoch 12, Iteration 407, Loss: 0.1483125239610672\n",
      "Epoch 12, Iteration 408, Loss: 0.12732446193695068\n",
      "Epoch 12, Iteration 409, Loss: 0.09847957640886307\n",
      "Epoch 12, Iteration 410, Loss: 0.1714669167995453\n",
      "Epoch 12, Iteration 411, Loss: 0.15690326690673828\n",
      "Epoch 12, Iteration 412, Loss: 0.12456052005290985\n",
      "Epoch 12, Iteration 413, Loss: 0.14389078319072723\n",
      "Epoch 12, Iteration 414, Loss: 0.08304616063833237\n",
      "Epoch 12, Iteration 415, Loss: 0.1283465325832367\n",
      "Epoch 12, Iteration 416, Loss: 0.11620256304740906\n",
      "Epoch 12, Iteration 417, Loss: 0.1407717764377594\n",
      "Epoch 12, Iteration 418, Loss: 0.1069948747754097\n",
      "Epoch 12, Iteration 419, Loss: 0.10792767256498337\n",
      "Epoch 12, Iteration 420, Loss: 0.12684819102287292\n",
      "Epoch 12, Iteration 421, Loss: 0.09606495499610901\n",
      "Epoch 12, Iteration 422, Loss: 0.1261620819568634\n",
      "Epoch 12, Iteration 423, Loss: 0.12778009474277496\n",
      "Epoch 12, Iteration 424, Loss: 0.13831527531147003\n",
      "Epoch 12, Iteration 425, Loss: 0.13601195812225342\n",
      "Epoch 12, Iteration 426, Loss: 0.10753604769706726\n",
      "Epoch 12, Iteration 427, Loss: 0.09577955305576324\n",
      "Epoch 12, Iteration 428, Loss: 0.12574809789657593\n",
      "Epoch 12, Iteration 429, Loss: 0.140823632478714\n",
      "Epoch 12, Iteration 430, Loss: 0.11469969898462296\n",
      "Epoch 12, Iteration 431, Loss: 0.12326095998287201\n",
      "Epoch 13/15, Loss: 0.13717336697434937\n",
      "Epoch 13/15, Validation Accuracy: 0.9429249011857708\n",
      "Epoch 13, Iteration 0, Loss: 0.22627748548984528\n",
      "Epoch 13, Iteration 1, Loss: 0.23167555034160614\n",
      "Epoch 13, Iteration 2, Loss: 0.2503454089164734\n",
      "Epoch 13, Iteration 3, Loss: 0.13122409582138062\n",
      "Epoch 13, Iteration 4, Loss: 0.20756526291370392\n",
      "Epoch 13, Iteration 5, Loss: 0.2413095086812973\n",
      "Epoch 13, Iteration 6, Loss: 0.17469879984855652\n",
      "Epoch 13, Iteration 7, Loss: 0.2306855171918869\n",
      "Epoch 13, Iteration 8, Loss: 0.18778865039348602\n",
      "Epoch 13, Iteration 9, Loss: 0.16921280324459076\n",
      "Epoch 13, Iteration 10, Loss: 0.1604822874069214\n",
      "Epoch 13, Iteration 11, Loss: 0.15532681345939636\n",
      "Epoch 13, Iteration 12, Loss: 0.1990620493888855\n",
      "Epoch 13, Iteration 13, Loss: 0.1826900690793991\n",
      "Epoch 13, Iteration 14, Loss: 0.21472926437854767\n",
      "Epoch 13, Iteration 15, Loss: 0.11306802928447723\n",
      "Epoch 13, Iteration 16, Loss: 0.2272651046514511\n",
      "Epoch 13, Iteration 17, Loss: 0.17817360162734985\n",
      "Epoch 13, Iteration 18, Loss: 0.15455779433250427\n",
      "Epoch 13, Iteration 19, Loss: 0.1406945437192917\n",
      "Epoch 13, Iteration 20, Loss: 0.197068452835083\n",
      "Epoch 13, Iteration 21, Loss: 0.21776822209358215\n",
      "Epoch 13, Iteration 22, Loss: 0.1527387946844101\n",
      "Epoch 13, Iteration 23, Loss: 0.1804739087820053\n",
      "Epoch 13, Iteration 24, Loss: 0.16624151170253754\n",
      "Epoch 13, Iteration 25, Loss: 0.25328096747398376\n",
      "Epoch 13, Iteration 26, Loss: 0.18277223408222198\n",
      "Epoch 13, Iteration 27, Loss: 0.1762964427471161\n",
      "Epoch 13, Iteration 28, Loss: 0.2457292079925537\n",
      "Epoch 13, Iteration 29, Loss: 0.15972189605236053\n",
      "Epoch 13, Iteration 30, Loss: 0.19862869381904602\n",
      "Epoch 13, Iteration 31, Loss: 0.1840861737728119\n",
      "Epoch 13, Iteration 32, Loss: 0.18424467742443085\n",
      "Epoch 13, Iteration 33, Loss: 0.16161862015724182\n",
      "Epoch 13, Iteration 34, Loss: 0.17625530064105988\n",
      "Epoch 13, Iteration 35, Loss: 0.12550827860832214\n",
      "Epoch 13, Iteration 36, Loss: 0.18019455671310425\n",
      "Epoch 13, Iteration 37, Loss: 0.15807950496673584\n",
      "Epoch 13, Iteration 38, Loss: 0.16514287889003754\n",
      "Epoch 13, Iteration 39, Loss: 0.18643982708454132\n",
      "Epoch 13, Iteration 40, Loss: 0.13775795698165894\n",
      "Epoch 13, Iteration 41, Loss: 0.12025368958711624\n",
      "Epoch 13, Iteration 42, Loss: 0.1325807273387909\n",
      "Epoch 13, Iteration 43, Loss: 0.19401732087135315\n",
      "Epoch 13, Iteration 44, Loss: 0.13109426200389862\n",
      "Epoch 13, Iteration 45, Loss: 0.18951790034770966\n",
      "Epoch 13, Iteration 46, Loss: 0.14641734957695007\n",
      "Epoch 13, Iteration 47, Loss: 0.19978660345077515\n",
      "Epoch 13, Iteration 48, Loss: 0.21893447637557983\n",
      "Epoch 13, Iteration 49, Loss: 0.12277702242136002\n",
      "Epoch 13, Iteration 50, Loss: 0.11976369470357895\n",
      "Epoch 13, Iteration 50, Test Loss: 0.21569374203681946\n",
      "Epoch 13, Iteration 51, Loss: 0.10915227979421616\n",
      "Epoch 13, Iteration 52, Loss: 0.1923852413892746\n",
      "Epoch 13, Iteration 53, Loss: 0.1487724334001541\n",
      "Epoch 13, Iteration 54, Loss: 0.1605188548564911\n",
      "Epoch 13, Iteration 55, Loss: 0.11592207103967667\n",
      "Epoch 13, Iteration 56, Loss: 0.14158578217029572\n",
      "Epoch 13, Iteration 57, Loss: 0.13937576115131378\n",
      "Epoch 13, Iteration 58, Loss: 0.10535422712564468\n",
      "Epoch 13, Iteration 59, Loss: 0.09519660472869873\n",
      "Epoch 13, Iteration 60, Loss: 0.14476679265499115\n",
      "Epoch 13, Iteration 61, Loss: 0.104067362844944\n",
      "Epoch 13, Iteration 62, Loss: 0.21887364983558655\n",
      "Epoch 13, Iteration 63, Loss: 0.09882199019193649\n",
      "Epoch 13, Iteration 64, Loss: 0.11515485495328903\n",
      "Epoch 13, Iteration 65, Loss: 0.14199703931808472\n",
      "Epoch 13, Iteration 66, Loss: 0.1347350776195526\n",
      "Epoch 13, Iteration 67, Loss: 0.09842578321695328\n",
      "Epoch 13, Iteration 68, Loss: 0.1297529935836792\n",
      "Epoch 13, Iteration 69, Loss: 0.12788452208042145\n",
      "Epoch 13, Iteration 70, Loss: 0.10960196703672409\n",
      "Epoch 13, Iteration 71, Loss: 0.12680917978286743\n",
      "Epoch 13, Iteration 72, Loss: 0.13860772550106049\n",
      "Epoch 13, Iteration 73, Loss: 0.13817566633224487\n",
      "Epoch 13, Iteration 74, Loss: 0.14455071091651917\n",
      "Epoch 13, Iteration 75, Loss: 0.17011480033397675\n",
      "Epoch 13, Iteration 76, Loss: 0.15495848655700684\n",
      "Epoch 13, Iteration 77, Loss: 0.13498073816299438\n",
      "Epoch 13, Iteration 78, Loss: 0.15155167877674103\n",
      "Epoch 13, Iteration 79, Loss: 0.11782693862915039\n",
      "Epoch 13, Iteration 80, Loss: 0.1671823263168335\n",
      "Epoch 13, Iteration 81, Loss: 0.11833319813013077\n",
      "Epoch 13, Iteration 82, Loss: 0.1578553169965744\n",
      "Epoch 13, Iteration 83, Loss: 0.12108135968446732\n",
      "Epoch 13, Iteration 84, Loss: 0.10114304721355438\n",
      "Epoch 13, Iteration 85, Loss: 0.14296214282512665\n",
      "Epoch 13, Iteration 86, Loss: 0.16648049652576447\n",
      "Epoch 13, Iteration 87, Loss: 0.1352568417787552\n",
      "Epoch 13, Iteration 88, Loss: 0.13584023714065552\n",
      "Epoch 13, Iteration 89, Loss: 0.12917761504650116\n",
      "Epoch 13, Iteration 90, Loss: 0.1349010169506073\n",
      "Epoch 13, Iteration 91, Loss: 0.12905274331569672\n",
      "Epoch 13, Iteration 92, Loss: 0.10828737914562225\n",
      "Epoch 13, Iteration 93, Loss: 0.142154261469841\n",
      "Epoch 13, Iteration 94, Loss: 0.17565688490867615\n",
      "Epoch 13, Iteration 95, Loss: 0.13371820747852325\n",
      "Epoch 13, Iteration 96, Loss: 0.07744553685188293\n",
      "Epoch 13, Iteration 97, Loss: 0.12943054735660553\n",
      "Epoch 13, Iteration 98, Loss: 0.1462572067975998\n",
      "Epoch 13, Iteration 99, Loss: 0.11221723258495331\n",
      "Epoch 13, Iteration 100, Loss: 0.09991439431905746\n",
      "Epoch 13, Iteration 100, Test Loss: 0.23555158078670502\n",
      "Epoch 13, Iteration 101, Loss: 0.11198420822620392\n",
      "Epoch 13, Iteration 102, Loss: 0.11185568571090698\n",
      "Epoch 13, Iteration 103, Loss: 0.10561081767082214\n",
      "Epoch 13, Iteration 104, Loss: 0.1279122680425644\n",
      "Epoch 13, Iteration 105, Loss: 0.12950961291790009\n",
      "Epoch 13, Iteration 106, Loss: 0.09413384646177292\n",
      "Epoch 13, Iteration 107, Loss: 0.10595882683992386\n",
      "Epoch 13, Iteration 108, Loss: 0.15333308279514313\n",
      "Epoch 13, Iteration 109, Loss: 0.19787335395812988\n",
      "Epoch 13, Iteration 110, Loss: 0.1210203543305397\n",
      "Epoch 13, Iteration 111, Loss: 0.08269192278385162\n",
      "Epoch 13, Iteration 112, Loss: 0.12534648180007935\n",
      "Epoch 13, Iteration 113, Loss: 0.11606134474277496\n",
      "Epoch 13, Iteration 114, Loss: 0.11258934438228607\n",
      "Epoch 13, Iteration 115, Loss: 0.14299334585666656\n",
      "Epoch 13, Iteration 116, Loss: 0.1336829513311386\n",
      "Epoch 13, Iteration 117, Loss: 0.20403943955898285\n",
      "Epoch 13, Iteration 118, Loss: 0.14667898416519165\n",
      "Epoch 13, Iteration 119, Loss: 0.1489291489124298\n",
      "Epoch 13, Iteration 120, Loss: 0.10109464079141617\n",
      "Epoch 13, Iteration 121, Loss: 0.13841493427753448\n",
      "Epoch 13, Iteration 122, Loss: 0.15488699078559875\n",
      "Epoch 13, Iteration 123, Loss: 0.10646849125623703\n",
      "Epoch 13, Iteration 124, Loss: 0.12525629997253418\n",
      "Epoch 13, Iteration 125, Loss: 0.15109577775001526\n",
      "Epoch 13, Iteration 126, Loss: 0.13378138840198517\n",
      "Epoch 13, Iteration 127, Loss: 0.14276264607906342\n",
      "Epoch 13, Iteration 128, Loss: 0.1129406988620758\n",
      "Epoch 13, Iteration 129, Loss: 0.16138525307178497\n",
      "Epoch 13, Iteration 130, Loss: 0.10436877608299255\n",
      "Epoch 13, Iteration 131, Loss: 0.09756596386432648\n",
      "Epoch 13, Iteration 132, Loss: 0.17447681725025177\n",
      "Epoch 13, Iteration 133, Loss: 0.1845911294221878\n",
      "Epoch 13, Iteration 134, Loss: 0.1673891693353653\n",
      "Epoch 13, Iteration 135, Loss: 0.17578592896461487\n",
      "Epoch 13, Iteration 136, Loss: 0.11590003222227097\n",
      "Epoch 13, Iteration 137, Loss: 0.1172880232334137\n",
      "Epoch 13, Iteration 138, Loss: 0.1517120897769928\n",
      "Epoch 13, Iteration 139, Loss: 0.15303465723991394\n",
      "Epoch 13, Iteration 140, Loss: 0.19865527749061584\n",
      "Epoch 13, Iteration 141, Loss: 0.13117524981498718\n",
      "Epoch 13, Iteration 142, Loss: 0.15605102479457855\n",
      "Epoch 13, Iteration 143, Loss: 0.13719172775745392\n",
      "Epoch 13, Iteration 144, Loss: 0.08173904567956924\n",
      "Epoch 13, Iteration 145, Loss: 0.16228817403316498\n",
      "Epoch 13, Iteration 146, Loss: 0.09160348027944565\n",
      "Epoch 13, Iteration 147, Loss: 0.13260722160339355\n",
      "Epoch 13, Iteration 148, Loss: 0.10819830745458603\n",
      "Epoch 13, Iteration 149, Loss: 0.09940733760595322\n",
      "Epoch 13, Iteration 150, Loss: 0.15156859159469604\n",
      "Epoch 13, Iteration 150, Test Loss: 0.24327881634235382\n",
      "Epoch 13, Iteration 151, Loss: 0.1377803087234497\n",
      "Epoch 13, Iteration 152, Loss: 0.12072784453630447\n",
      "Epoch 13, Iteration 153, Loss: 0.14229896664619446\n",
      "Epoch 13, Iteration 154, Loss: 0.11645127087831497\n",
      "Epoch 13, Iteration 155, Loss: 0.14335604012012482\n",
      "Epoch 13, Iteration 156, Loss: 0.14850902557373047\n",
      "Epoch 13, Iteration 157, Loss: 0.17427946627140045\n",
      "Epoch 13, Iteration 158, Loss: 0.19109030067920685\n",
      "Epoch 13, Iteration 159, Loss: 0.12095135450363159\n",
      "Epoch 13, Iteration 160, Loss: 0.16659273207187653\n",
      "Epoch 13, Iteration 161, Loss: 0.08730033040046692\n",
      "Epoch 13, Iteration 162, Loss: 0.08561180531978607\n",
      "Epoch 13, Iteration 163, Loss: 0.14397244155406952\n",
      "Epoch 13, Iteration 164, Loss: 0.09004282206296921\n",
      "Epoch 13, Iteration 165, Loss: 0.10577689111232758\n",
      "Epoch 13, Iteration 166, Loss: 0.11858997493982315\n",
      "Epoch 13, Iteration 167, Loss: 0.1608833372592926\n",
      "Epoch 13, Iteration 168, Loss: 0.09588455408811569\n",
      "Epoch 13, Iteration 169, Loss: 0.13919346034526825\n",
      "Epoch 13, Iteration 170, Loss: 0.08526473492383957\n",
      "Epoch 13, Iteration 171, Loss: 0.16911515593528748\n",
      "Epoch 13, Iteration 172, Loss: 0.19393160939216614\n",
      "Epoch 13, Iteration 173, Loss: 0.1439422369003296\n",
      "Epoch 13, Iteration 174, Loss: 0.13701197504997253\n",
      "Epoch 13, Iteration 175, Loss: 0.11675579845905304\n",
      "Epoch 13, Iteration 176, Loss: 0.10732142627239227\n",
      "Epoch 13, Iteration 177, Loss: 0.17938540875911713\n",
      "Epoch 13, Iteration 178, Loss: 0.09676261991262436\n",
      "Epoch 13, Iteration 179, Loss: 0.12729796767234802\n",
      "Epoch 13, Iteration 180, Loss: 0.1738971322774887\n",
      "Epoch 13, Iteration 181, Loss: 0.13629552721977234\n",
      "Epoch 13, Iteration 182, Loss: 0.10925742983818054\n",
      "Epoch 13, Iteration 183, Loss: 0.14396192133426666\n",
      "Epoch 13, Iteration 184, Loss: 0.1893800050020218\n",
      "Epoch 13, Iteration 185, Loss: 0.10653772950172424\n",
      "Epoch 13, Iteration 186, Loss: 0.10146670788526535\n",
      "Epoch 13, Iteration 187, Loss: 0.1167069599032402\n",
      "Epoch 13, Iteration 188, Loss: 0.1002700999379158\n",
      "Epoch 13, Iteration 189, Loss: 0.06096905842423439\n",
      "Epoch 13, Iteration 190, Loss: 0.1046653538942337\n",
      "Epoch 13, Iteration 191, Loss: 0.10536684095859528\n",
      "Epoch 13, Iteration 192, Loss: 0.10822049528360367\n",
      "Epoch 13, Iteration 193, Loss: 0.12043553590774536\n",
      "Epoch 13, Iteration 194, Loss: 0.12333918362855911\n",
      "Epoch 13, Iteration 195, Loss: 0.12664048373699188\n",
      "Epoch 13, Iteration 196, Loss: 0.23911061882972717\n",
      "Epoch 13, Iteration 197, Loss: 0.1051226258277893\n",
      "Epoch 13, Iteration 198, Loss: 0.09356456995010376\n",
      "Epoch 13, Iteration 199, Loss: 0.12598788738250732\n",
      "Epoch 13, Iteration 200, Loss: 0.09061863273382187\n",
      "Epoch 13, Iteration 200, Test Loss: 0.24122503399848938\n",
      "Epoch 13, Iteration 201, Loss: 0.12399710714817047\n",
      "Epoch 13, Iteration 202, Loss: 0.09132187068462372\n",
      "Epoch 13, Iteration 203, Loss: 0.11009220778942108\n",
      "Epoch 13, Iteration 204, Loss: 0.1270669847726822\n",
      "Epoch 13, Iteration 205, Loss: 0.14062796533107758\n",
      "Epoch 13, Iteration 206, Loss: 0.09368931502103806\n",
      "Epoch 13, Iteration 207, Loss: 0.09171098470687866\n",
      "Epoch 13, Iteration 208, Loss: 0.09542684257030487\n",
      "Epoch 13, Iteration 209, Loss: 0.12789921462535858\n",
      "Epoch 13, Iteration 210, Loss: 0.09436115622520447\n",
      "Epoch 13, Iteration 211, Loss: 0.11254540830850601\n",
      "Epoch 13, Iteration 212, Loss: 0.16017073392868042\n",
      "Epoch 13, Iteration 213, Loss: 0.09491328150033951\n",
      "Epoch 13, Iteration 214, Loss: 0.12122221291065216\n",
      "Epoch 13, Iteration 215, Loss: 0.11660552024841309\n",
      "Epoch 13, Iteration 216, Loss: 0.13102515041828156\n",
      "Epoch 13, Iteration 217, Loss: 0.13916923105716705\n",
      "Epoch 13, Iteration 218, Loss: 0.1009431704878807\n",
      "Epoch 13, Iteration 219, Loss: 0.12935838103294373\n",
      "Epoch 13, Iteration 220, Loss: 0.08880490809679031\n",
      "Epoch 13, Iteration 221, Loss: 0.13515065610408783\n",
      "Epoch 13, Iteration 222, Loss: 0.11695555597543716\n",
      "Epoch 13, Iteration 223, Loss: 0.12318344414234161\n",
      "Epoch 13, Iteration 224, Loss: 0.09536223858594894\n",
      "Epoch 13, Iteration 225, Loss: 0.12492980808019638\n",
      "Epoch 13, Iteration 226, Loss: 0.1541397124528885\n",
      "Epoch 13, Iteration 227, Loss: 0.12231133878231049\n",
      "Epoch 13, Iteration 228, Loss: 0.23742364346981049\n",
      "Epoch 13, Iteration 229, Loss: 0.22799281775951385\n",
      "Epoch 13, Iteration 230, Loss: 0.13935799896717072\n",
      "Epoch 13, Iteration 231, Loss: 0.09753106534481049\n",
      "Epoch 13, Iteration 232, Loss: 0.14190153777599335\n",
      "Epoch 13, Iteration 233, Loss: 0.17982161045074463\n",
      "Epoch 13, Iteration 234, Loss: 0.10111900418996811\n",
      "Epoch 13, Iteration 235, Loss: 0.08753762394189835\n",
      "Epoch 13, Iteration 236, Loss: 0.15822550654411316\n",
      "Epoch 13, Iteration 237, Loss: 0.14914552867412567\n",
      "Epoch 13, Iteration 238, Loss: 0.13106851279735565\n",
      "Epoch 13, Iteration 239, Loss: 0.12185788154602051\n",
      "Epoch 13, Iteration 240, Loss: 0.11104533076286316\n",
      "Epoch 13, Iteration 241, Loss: 0.11995449662208557\n",
      "Epoch 13, Iteration 242, Loss: 0.1534358561038971\n",
      "Epoch 13, Iteration 243, Loss: 0.11889851093292236\n",
      "Epoch 13, Iteration 244, Loss: 0.13301604986190796\n",
      "Epoch 13, Iteration 245, Loss: 0.1093730479478836\n",
      "Epoch 13, Iteration 246, Loss: 0.1670927107334137\n",
      "Epoch 13, Iteration 247, Loss: 0.2326253205537796\n",
      "Epoch 13, Iteration 248, Loss: 0.18048764765262604\n",
      "Epoch 13, Iteration 249, Loss: 0.19474844634532928\n",
      "Epoch 13, Iteration 250, Loss: 0.21024414896965027\n",
      "Epoch 13, Iteration 250, Test Loss: 0.25014349818229675\n",
      "Epoch 13, Iteration 251, Loss: 0.21376870572566986\n",
      "Epoch 13, Iteration 252, Loss: 0.12025952339172363\n",
      "Epoch 13, Iteration 253, Loss: 0.15545225143432617\n",
      "Epoch 13, Iteration 254, Loss: 0.17952832579612732\n",
      "Epoch 13, Iteration 255, Loss: 0.1412813663482666\n",
      "Epoch 13, Iteration 256, Loss: 0.14642146229743958\n",
      "Epoch 13, Iteration 257, Loss: 0.10837139189243317\n",
      "Epoch 13, Iteration 258, Loss: 0.14010611176490784\n",
      "Epoch 13, Iteration 259, Loss: 0.13710494339466095\n",
      "Epoch 13, Iteration 260, Loss: 0.1613834649324417\n",
      "Epoch 13, Iteration 261, Loss: 0.11585576832294464\n",
      "Epoch 13, Iteration 262, Loss: 0.09446951746940613\n",
      "Epoch 13, Iteration 263, Loss: 0.12308911979198456\n",
      "Epoch 13, Iteration 264, Loss: 0.12260037660598755\n",
      "Epoch 13, Iteration 265, Loss: 0.1854810267686844\n",
      "Epoch 13, Iteration 266, Loss: 0.12071509659290314\n",
      "Epoch 13, Iteration 267, Loss: 0.1324460804462433\n",
      "Epoch 13, Iteration 268, Loss: 0.1203380897641182\n",
      "Epoch 13, Iteration 269, Loss: 0.14879684150218964\n",
      "Epoch 13, Iteration 270, Loss: 0.14285030961036682\n",
      "Epoch 13, Iteration 271, Loss: 0.07960473746061325\n",
      "Epoch 13, Iteration 272, Loss: 0.1453201323747635\n",
      "Epoch 13, Iteration 273, Loss: 0.14634177088737488\n",
      "Epoch 13, Iteration 274, Loss: 0.11172360181808472\n",
      "Epoch 13, Iteration 275, Loss: 0.09633616358041763\n",
      "Epoch 13, Iteration 276, Loss: 0.08801160752773285\n",
      "Epoch 13, Iteration 277, Loss: 0.11151640117168427\n",
      "Epoch 13, Iteration 278, Loss: 0.15853406488895416\n",
      "Epoch 13, Iteration 279, Loss: 0.08173185586929321\n",
      "Epoch 13, Iteration 280, Loss: 0.14938890933990479\n",
      "Epoch 13, Iteration 281, Loss: 0.10919985920190811\n",
      "Epoch 13, Iteration 282, Loss: 0.1015654057264328\n",
      "Epoch 13, Iteration 283, Loss: 0.06768866628408432\n",
      "Epoch 13, Iteration 284, Loss: 0.13014638423919678\n",
      "Epoch 13, Iteration 285, Loss: 0.10465126484632492\n",
      "Epoch 13, Iteration 286, Loss: 0.14972586929798126\n",
      "Epoch 13, Iteration 287, Loss: 0.08064202964305878\n",
      "Epoch 13, Iteration 288, Loss: 0.11881235986948013\n",
      "Epoch 13, Iteration 289, Loss: 0.15856042504310608\n",
      "Epoch 13, Iteration 290, Loss: 0.09068404138088226\n",
      "Epoch 13, Iteration 291, Loss: 0.09467756748199463\n",
      "Epoch 13, Iteration 292, Loss: 0.11366541683673859\n",
      "Epoch 13, Iteration 293, Loss: 0.16367529332637787\n",
      "Epoch 13, Iteration 294, Loss: 0.08870887756347656\n",
      "Epoch 13, Iteration 295, Loss: 0.08046033978462219\n",
      "Epoch 13, Iteration 296, Loss: 0.10277021676301956\n",
      "Epoch 13, Iteration 297, Loss: 0.11311416327953339\n",
      "Epoch 13, Iteration 298, Loss: 0.095456562936306\n",
      "Epoch 13, Iteration 299, Loss: 0.16433802247047424\n",
      "Epoch 13, Iteration 300, Loss: 0.12426112592220306\n",
      "Epoch 13, Iteration 300, Test Loss: 0.2580685317516327\n",
      "Epoch 13, Iteration 301, Loss: 0.10425087064504623\n",
      "Epoch 13, Iteration 302, Loss: 0.07695242762565613\n",
      "Epoch 13, Iteration 303, Loss: 0.14697225391864777\n",
      "Epoch 13, Iteration 304, Loss: 0.15722239017486572\n",
      "Epoch 13, Iteration 305, Loss: 0.10298989713191986\n",
      "Epoch 13, Iteration 306, Loss: 0.10545029491186142\n",
      "Epoch 13, Iteration 307, Loss: 0.11262085288763046\n",
      "Epoch 13, Iteration 308, Loss: 0.10847631841897964\n",
      "Epoch 13, Iteration 309, Loss: 0.11803794652223587\n",
      "Epoch 13, Iteration 310, Loss: 0.12147875130176544\n",
      "Epoch 13, Iteration 311, Loss: 0.09419053047895432\n",
      "Epoch 13, Iteration 312, Loss: 0.15148387849330902\n",
      "Epoch 13, Iteration 313, Loss: 0.09074212610721588\n",
      "Epoch 13, Iteration 314, Loss: 0.10704851150512695\n",
      "Epoch 13, Iteration 315, Loss: 0.11072098463773727\n",
      "Epoch 13, Iteration 316, Loss: 0.09949015080928802\n",
      "Epoch 13, Iteration 317, Loss: 0.13626116514205933\n",
      "Epoch 13, Iteration 318, Loss: 0.15289834141731262\n",
      "Epoch 13, Iteration 319, Loss: 0.09237459301948547\n",
      "Epoch 13, Iteration 320, Loss: 0.16018441319465637\n",
      "Epoch 13, Iteration 321, Loss: 0.09732108563184738\n",
      "Epoch 13, Iteration 322, Loss: 0.15759539604187012\n",
      "Epoch 13, Iteration 323, Loss: 0.11261986941099167\n",
      "Epoch 13, Iteration 324, Loss: 0.16762712597846985\n",
      "Epoch 13, Iteration 325, Loss: 0.21113860607147217\n",
      "Epoch 13, Iteration 326, Loss: 0.16464389860630035\n",
      "Epoch 13, Iteration 327, Loss: 0.1403478980064392\n",
      "Epoch 13, Iteration 328, Loss: 0.13836856186389923\n",
      "Epoch 13, Iteration 329, Loss: 0.11193635314702988\n",
      "Epoch 13, Iteration 330, Loss: 0.14238131046295166\n",
      "Epoch 13, Iteration 331, Loss: 0.14173084497451782\n",
      "Epoch 13, Iteration 332, Loss: 0.1215178593993187\n",
      "Epoch 13, Iteration 333, Loss: 0.10087952017784119\n",
      "Epoch 13, Iteration 334, Loss: 0.20791861414909363\n",
      "Epoch 13, Iteration 335, Loss: 0.17354659736156464\n",
      "Epoch 13, Iteration 336, Loss: 0.1776430755853653\n",
      "Epoch 13, Iteration 337, Loss: 0.1044946014881134\n",
      "Epoch 13, Iteration 338, Loss: 0.12293124943971634\n",
      "Epoch 13, Iteration 339, Loss: 0.16306138038635254\n",
      "Epoch 13, Iteration 340, Loss: 0.14069069921970367\n",
      "Epoch 13, Iteration 341, Loss: 0.13521847128868103\n",
      "Epoch 13, Iteration 342, Loss: 0.1221720427274704\n",
      "Epoch 13, Iteration 343, Loss: 0.17100249230861664\n",
      "Epoch 13, Iteration 344, Loss: 0.16449591517448425\n",
      "Epoch 13, Iteration 345, Loss: 0.1316978931427002\n",
      "Epoch 13, Iteration 346, Loss: 0.12873107194900513\n",
      "Epoch 13, Iteration 347, Loss: 0.14793118834495544\n",
      "Epoch 13, Iteration 348, Loss: 0.16104499995708466\n",
      "Epoch 13, Iteration 349, Loss: 0.17864714562892914\n",
      "Epoch 13, Iteration 350, Loss: 0.18892662227153778\n",
      "Epoch 13, Iteration 350, Test Loss: 0.2300431877374649\n",
      "Epoch 13, Iteration 351, Loss: 0.1530354917049408\n",
      "Epoch 13, Iteration 352, Loss: 0.1437915563583374\n",
      "Epoch 13, Iteration 353, Loss: 0.20795439183712006\n",
      "Epoch 13, Iteration 354, Loss: 0.12481799721717834\n",
      "Epoch 13, Iteration 355, Loss: 0.13180579245090485\n",
      "Epoch 13, Iteration 356, Loss: 0.13285063207149506\n",
      "Epoch 13, Iteration 357, Loss: 0.17517822980880737\n",
      "Epoch 13, Iteration 358, Loss: 0.1129136011004448\n",
      "Epoch 13, Iteration 359, Loss: 0.17551419138908386\n",
      "Epoch 13, Iteration 360, Loss: 0.08359940350055695\n",
      "Epoch 13, Iteration 361, Loss: 0.13308125734329224\n",
      "Epoch 13, Iteration 362, Loss: 0.12119974195957184\n",
      "Epoch 13, Iteration 363, Loss: 0.11384622752666473\n",
      "Epoch 13, Iteration 364, Loss: 0.12108965218067169\n",
      "Epoch 13, Iteration 365, Loss: 0.10295885801315308\n",
      "Epoch 13, Iteration 366, Loss: 0.15520964562892914\n",
      "Epoch 13, Iteration 367, Loss: 0.16006094217300415\n",
      "Epoch 13, Iteration 368, Loss: 0.1293099820613861\n",
      "Epoch 13, Iteration 369, Loss: 0.10727725923061371\n",
      "Epoch 13, Iteration 370, Loss: 0.13580608367919922\n",
      "Epoch 13, Iteration 371, Loss: 0.12603750824928284\n",
      "Epoch 13, Iteration 372, Loss: 0.11787237226963043\n",
      "Epoch 13, Iteration 373, Loss: 0.1698661893606186\n",
      "Epoch 13, Iteration 374, Loss: 0.09875471889972687\n",
      "Epoch 13, Iteration 375, Loss: 0.1298178732395172\n",
      "Epoch 13, Iteration 376, Loss: 0.12045764923095703\n",
      "Epoch 13, Iteration 377, Loss: 0.1363578736782074\n",
      "Epoch 13, Iteration 378, Loss: 0.12432429939508438\n",
      "Epoch 13, Iteration 379, Loss: 0.16067025065422058\n",
      "Epoch 13, Iteration 380, Loss: 0.15273284912109375\n",
      "Epoch 13, Iteration 381, Loss: 0.12749755382537842\n",
      "Epoch 13, Iteration 382, Loss: 0.13728761672973633\n",
      "Epoch 13, Iteration 383, Loss: 0.09817081689834595\n",
      "Epoch 13, Iteration 384, Loss: 0.12350384145975113\n",
      "Epoch 13, Iteration 385, Loss: 0.12851473689079285\n",
      "Epoch 13, Iteration 386, Loss: 0.11078966408967972\n",
      "Epoch 13, Iteration 387, Loss: 0.09058330953121185\n",
      "Epoch 13, Iteration 388, Loss: 0.10068744421005249\n",
      "Epoch 13, Iteration 389, Loss: 0.10257421433925629\n",
      "Epoch 13, Iteration 390, Loss: 0.12167299538850784\n",
      "Epoch 13, Iteration 391, Loss: 0.08635251969099045\n",
      "Epoch 13, Iteration 392, Loss: 0.1767595112323761\n",
      "Epoch 13, Iteration 393, Loss: 0.1747717410326004\n",
      "Epoch 13, Iteration 394, Loss: 0.11819380521774292\n",
      "Epoch 13, Iteration 395, Loss: 0.1106080636382103\n",
      "Epoch 13, Iteration 396, Loss: 0.1418694108724594\n",
      "Epoch 13, Iteration 397, Loss: 0.1217760294675827\n",
      "Epoch 13, Iteration 398, Loss: 0.23013269901275635\n",
      "Epoch 13, Iteration 399, Loss: 0.10843367129564285\n",
      "Epoch 13, Iteration 400, Loss: 0.09862923622131348\n",
      "Epoch 13, Iteration 400, Test Loss: 0.24464724957942963\n",
      "Epoch 13, Iteration 401, Loss: 0.12208416312932968\n",
      "Epoch 13, Iteration 402, Loss: 0.092717744410038\n",
      "Epoch 13, Iteration 403, Loss: 0.10829484462738037\n",
      "Epoch 13, Iteration 404, Loss: 0.13177262246608734\n",
      "Epoch 13, Iteration 405, Loss: 0.10045916587114334\n",
      "Epoch 13, Iteration 406, Loss: 0.10430908203125\n",
      "Epoch 13, Iteration 407, Loss: 0.14087317883968353\n",
      "Epoch 13, Iteration 408, Loss: 0.159860298037529\n",
      "Epoch 13, Iteration 409, Loss: 0.07899697870016098\n",
      "Epoch 13, Iteration 410, Loss: 0.17688824236392975\n",
      "Epoch 13, Iteration 411, Loss: 0.16888846457004547\n",
      "Epoch 13, Iteration 412, Loss: 0.11302056163549423\n",
      "Epoch 13, Iteration 413, Loss: 0.1607174128293991\n",
      "Epoch 13, Iteration 414, Loss: 0.08839654922485352\n",
      "Epoch 13, Iteration 415, Loss: 0.11415386945009232\n",
      "Epoch 13, Iteration 416, Loss: 0.10212700814008713\n",
      "Epoch 13, Iteration 417, Loss: 0.14510007202625275\n",
      "Epoch 13, Iteration 418, Loss: 0.10205014795064926\n",
      "Epoch 13, Iteration 419, Loss: 0.13091258704662323\n",
      "Epoch 13, Iteration 420, Loss: 0.12825456261634827\n",
      "Epoch 13, Iteration 421, Loss: 0.07070504128932953\n",
      "Epoch 13, Iteration 422, Loss: 0.11900380998849869\n",
      "Epoch 13, Iteration 423, Loss: 0.11142454296350479\n",
      "Epoch 13, Iteration 424, Loss: 0.14318645000457764\n",
      "Epoch 13, Iteration 425, Loss: 0.1211264505982399\n",
      "Epoch 13, Iteration 426, Loss: 0.12490455061197281\n",
      "Epoch 13, Iteration 427, Loss: 0.10504763573408127\n",
      "Epoch 13, Iteration 428, Loss: 0.13465172052383423\n",
      "Epoch 13, Iteration 429, Loss: 0.14946219325065613\n",
      "Epoch 13, Iteration 430, Loss: 0.11413738876581192\n",
      "Epoch 13, Iteration 431, Loss: 0.10922736674547195\n",
      "Epoch 14/15, Loss: 0.13646635795184583\n",
      "Epoch 14/15, Validation Accuracy: 0.9440316205533597\n",
      "Epoch 14, Iteration 0, Loss: 0.21169285476207733\n",
      "Epoch 14, Iteration 1, Loss: 0.2011169195175171\n",
      "Epoch 14, Iteration 2, Loss: 0.18408845365047455\n",
      "Epoch 14, Iteration 3, Loss: 0.13270136713981628\n",
      "Epoch 14, Iteration 4, Loss: 0.21634741127490997\n",
      "Epoch 14, Iteration 5, Loss: 0.22648610174655914\n",
      "Epoch 14, Iteration 6, Loss: 0.20157553255558014\n",
      "Epoch 14, Iteration 7, Loss: 0.21675746142864227\n",
      "Epoch 14, Iteration 8, Loss: 0.17689114809036255\n",
      "Epoch 14, Iteration 9, Loss: 0.16023889183998108\n",
      "Epoch 14, Iteration 10, Loss: 0.1693374663591385\n",
      "Epoch 14, Iteration 11, Loss: 0.14277952909469604\n",
      "Epoch 14, Iteration 12, Loss: 0.2107977420091629\n",
      "Epoch 14, Iteration 13, Loss: 0.16909001767635345\n",
      "Epoch 14, Iteration 14, Loss: 0.23618537187576294\n",
      "Epoch 14, Iteration 15, Loss: 0.11257373541593552\n",
      "Epoch 14, Iteration 16, Loss: 0.23537060618400574\n",
      "Epoch 14, Iteration 17, Loss: 0.1629249006509781\n",
      "Epoch 14, Iteration 18, Loss: 0.1583653688430786\n",
      "Epoch 14, Iteration 19, Loss: 0.12947149574756622\n",
      "Epoch 14, Iteration 20, Loss: 0.2016136348247528\n",
      "Epoch 14, Iteration 21, Loss: 0.2249901443719864\n",
      "Epoch 14, Iteration 22, Loss: 0.15724706649780273\n",
      "Epoch 14, Iteration 23, Loss: 0.20361842215061188\n",
      "Epoch 14, Iteration 24, Loss: 0.16953587532043457\n",
      "Epoch 14, Iteration 25, Loss: 0.21472696959972382\n",
      "Epoch 14, Iteration 26, Loss: 0.1460692286491394\n",
      "Epoch 14, Iteration 27, Loss: 0.16289322078227997\n",
      "Epoch 14, Iteration 28, Loss: 0.22830022871494293\n",
      "Epoch 14, Iteration 29, Loss: 0.16384263336658478\n",
      "Epoch 14, Iteration 30, Loss: 0.20430243015289307\n",
      "Epoch 14, Iteration 31, Loss: 0.21156539022922516\n",
      "Epoch 14, Iteration 32, Loss: 0.17449727654457092\n",
      "Epoch 14, Iteration 33, Loss: 0.1519814431667328\n",
      "Epoch 14, Iteration 34, Loss: 0.16796430945396423\n",
      "Epoch 14, Iteration 35, Loss: 0.14027227461338043\n",
      "Epoch 14, Iteration 36, Loss: 0.17133624851703644\n",
      "Epoch 14, Iteration 37, Loss: 0.13645796477794647\n",
      "Epoch 14, Iteration 38, Loss: 0.16693562269210815\n",
      "Epoch 14, Iteration 39, Loss: 0.17602141201496124\n",
      "Epoch 14, Iteration 40, Loss: 0.13547657430171967\n",
      "Epoch 14, Iteration 41, Loss: 0.14015740156173706\n",
      "Epoch 14, Iteration 42, Loss: 0.15083815157413483\n",
      "Epoch 14, Iteration 43, Loss: 0.1415519267320633\n",
      "Epoch 14, Iteration 44, Loss: 0.15254995226860046\n",
      "Epoch 14, Iteration 45, Loss: 0.16675445437431335\n",
      "Epoch 14, Iteration 46, Loss: 0.17751741409301758\n",
      "Epoch 14, Iteration 47, Loss: 0.19148388504981995\n",
      "Epoch 14, Iteration 48, Loss: 0.2159663736820221\n",
      "Epoch 14, Iteration 49, Loss: 0.11378307640552521\n",
      "Epoch 14, Iteration 50, Loss: 0.13086506724357605\n",
      "Epoch 14, Iteration 50, Test Loss: 0.2116388976573944\n",
      "Epoch 14, Iteration 51, Loss: 0.1247212290763855\n",
      "Epoch 14, Iteration 52, Loss: 0.161481112241745\n",
      "Epoch 14, Iteration 53, Loss: 0.1387787163257599\n",
      "Epoch 14, Iteration 54, Loss: 0.15092825889587402\n",
      "Epoch 14, Iteration 55, Loss: 0.1323014199733734\n",
      "Epoch 14, Iteration 56, Loss: 0.1644439399242401\n",
      "Epoch 14, Iteration 57, Loss: 0.13095775246620178\n",
      "Epoch 14, Iteration 58, Loss: 0.1586093157529831\n",
      "Epoch 14, Iteration 59, Loss: 0.10431598126888275\n",
      "Epoch 14, Iteration 60, Loss: 0.1494881957769394\n",
      "Epoch 14, Iteration 61, Loss: 0.10159796476364136\n",
      "Epoch 14, Iteration 62, Loss: 0.19219970703125\n",
      "Epoch 14, Iteration 63, Loss: 0.08650153130292892\n",
      "Epoch 14, Iteration 64, Loss: 0.13507726788520813\n",
      "Epoch 14, Iteration 65, Loss: 0.1318969428539276\n",
      "Epoch 14, Iteration 66, Loss: 0.13714487850666046\n",
      "Epoch 14, Iteration 67, Loss: 0.09854235500097275\n",
      "Epoch 14, Iteration 68, Loss: 0.14476096630096436\n",
      "Epoch 14, Iteration 69, Loss: 0.15045128762722015\n",
      "Epoch 14, Iteration 70, Loss: 0.11063990741968155\n",
      "Epoch 14, Iteration 71, Loss: 0.1470414251089096\n",
      "Epoch 14, Iteration 72, Loss: 0.13577589392662048\n",
      "Epoch 14, Iteration 73, Loss: 0.13808679580688477\n",
      "Epoch 14, Iteration 74, Loss: 0.16271264851093292\n",
      "Epoch 14, Iteration 75, Loss: 0.13836969435214996\n",
      "Epoch 14, Iteration 76, Loss: 0.15099291503429413\n",
      "Epoch 14, Iteration 77, Loss: 0.14387662708759308\n",
      "Epoch 14, Iteration 78, Loss: 0.1397770643234253\n",
      "Epoch 14, Iteration 79, Loss: 0.10264970362186432\n",
      "Epoch 14, Iteration 80, Loss: 0.14903542399406433\n",
      "Epoch 14, Iteration 81, Loss: 0.14598844945430756\n",
      "Epoch 14, Iteration 82, Loss: 0.16409797966480255\n",
      "Epoch 14, Iteration 83, Loss: 0.14204074442386627\n",
      "Epoch 14, Iteration 84, Loss: 0.09480451047420502\n",
      "Epoch 14, Iteration 85, Loss: 0.1893967092037201\n",
      "Epoch 14, Iteration 86, Loss: 0.17517097294330597\n",
      "Epoch 14, Iteration 87, Loss: 0.13566173613071442\n",
      "Epoch 14, Iteration 88, Loss: 0.13376964628696442\n",
      "Epoch 14, Iteration 89, Loss: 0.15373164415359497\n",
      "Epoch 14, Iteration 90, Loss: 0.1254836916923523\n",
      "Epoch 14, Iteration 91, Loss: 0.1348162144422531\n",
      "Epoch 14, Iteration 92, Loss: 0.11085334420204163\n",
      "Epoch 14, Iteration 93, Loss: 0.12580280005931854\n",
      "Epoch 14, Iteration 94, Loss: 0.1740114986896515\n",
      "Epoch 14, Iteration 95, Loss: 0.12713918089866638\n",
      "Epoch 14, Iteration 96, Loss: 0.08221851289272308\n",
      "Epoch 14, Iteration 97, Loss: 0.13915219902992249\n",
      "Epoch 14, Iteration 98, Loss: 0.16152139008045197\n",
      "Epoch 14, Iteration 99, Loss: 0.11155785620212555\n",
      "Epoch 14, Iteration 100, Loss: 0.10899742692708969\n",
      "Epoch 14, Iteration 100, Test Loss: 0.2339164912700653\n",
      "Epoch 14, Iteration 101, Loss: 0.14209802448749542\n",
      "Epoch 14, Iteration 102, Loss: 0.11738937348127365\n",
      "Epoch 14, Iteration 103, Loss: 0.11658796668052673\n",
      "Epoch 14, Iteration 104, Loss: 0.12591597437858582\n",
      "Epoch 14, Iteration 105, Loss: 0.12669731676578522\n",
      "Epoch 14, Iteration 106, Loss: 0.09995119273662567\n",
      "Epoch 14, Iteration 107, Loss: 0.07310209423303604\n",
      "Epoch 14, Iteration 108, Loss: 0.15409404039382935\n",
      "Epoch 14, Iteration 109, Loss: 0.18092982470989227\n",
      "Epoch 14, Iteration 110, Loss: 0.13714522123336792\n",
      "Epoch 14, Iteration 111, Loss: 0.07985065877437592\n",
      "Epoch 14, Iteration 112, Loss: 0.11982876807451248\n",
      "Epoch 14, Iteration 113, Loss: 0.11508047580718994\n",
      "Epoch 14, Iteration 114, Loss: 0.1281292736530304\n",
      "Epoch 14, Iteration 115, Loss: 0.13571082055568695\n",
      "Epoch 14, Iteration 116, Loss: 0.12885677814483643\n",
      "Epoch 14, Iteration 117, Loss: 0.18525204062461853\n",
      "Epoch 14, Iteration 118, Loss: 0.1523117572069168\n",
      "Epoch 14, Iteration 119, Loss: 0.15966646373271942\n",
      "Epoch 14, Iteration 120, Loss: 0.08921290934085846\n",
      "Epoch 14, Iteration 121, Loss: 0.14362193644046783\n",
      "Epoch 14, Iteration 122, Loss: 0.15354909002780914\n",
      "Epoch 14, Iteration 123, Loss: 0.11755246669054031\n",
      "Epoch 14, Iteration 124, Loss: 0.12423156201839447\n",
      "Epoch 14, Iteration 125, Loss: 0.1677410751581192\n",
      "Epoch 14, Iteration 126, Loss: 0.13296401500701904\n",
      "Epoch 14, Iteration 127, Loss: 0.1318272352218628\n",
      "Epoch 14, Iteration 128, Loss: 0.10937236249446869\n",
      "Epoch 14, Iteration 129, Loss: 0.17547529935836792\n",
      "Epoch 14, Iteration 130, Loss: 0.09772752970457077\n",
      "Epoch 14, Iteration 131, Loss: 0.11848515272140503\n",
      "Epoch 14, Iteration 132, Loss: 0.18425986170768738\n",
      "Epoch 14, Iteration 133, Loss: 0.21076443791389465\n",
      "Epoch 14, Iteration 134, Loss: 0.1548454314470291\n",
      "Epoch 14, Iteration 135, Loss: 0.1657055765390396\n",
      "Epoch 14, Iteration 136, Loss: 0.09104236215353012\n",
      "Epoch 14, Iteration 137, Loss: 0.118756502866745\n",
      "Epoch 14, Iteration 138, Loss: 0.14296959340572357\n",
      "Epoch 14, Iteration 139, Loss: 0.15049101412296295\n",
      "Epoch 14, Iteration 140, Loss: 0.17592798173427582\n",
      "Epoch 14, Iteration 141, Loss: 0.1236984059214592\n",
      "Epoch 14, Iteration 142, Loss: 0.19205355644226074\n",
      "Epoch 14, Iteration 143, Loss: 0.125374436378479\n",
      "Epoch 14, Iteration 144, Loss: 0.10494586825370789\n",
      "Epoch 14, Iteration 145, Loss: 0.16095954179763794\n",
      "Epoch 14, Iteration 146, Loss: 0.10044346749782562\n",
      "Epoch 14, Iteration 147, Loss: 0.11569283157587051\n",
      "Epoch 14, Iteration 148, Loss: 0.1159312054514885\n",
      "Epoch 14, Iteration 149, Loss: 0.11787666380405426\n",
      "Epoch 14, Iteration 150, Loss: 0.18022145330905914\n",
      "Epoch 14, Iteration 150, Test Loss: 0.23772424459457397\n",
      "Epoch 14, Iteration 151, Loss: 0.12124660611152649\n",
      "Epoch 14, Iteration 152, Loss: 0.14559081196784973\n",
      "Epoch 14, Iteration 153, Loss: 0.12602582573890686\n",
      "Epoch 14, Iteration 154, Loss: 0.10424431413412094\n",
      "Epoch 14, Iteration 155, Loss: 0.14570677280426025\n",
      "Epoch 14, Iteration 156, Loss: 0.14488065242767334\n",
      "Epoch 14, Iteration 157, Loss: 0.17543146014213562\n",
      "Epoch 14, Iteration 158, Loss: 0.1661900132894516\n",
      "Epoch 14, Iteration 159, Loss: 0.12005293369293213\n",
      "Epoch 14, Iteration 160, Loss: 0.1716870665550232\n",
      "Epoch 14, Iteration 161, Loss: 0.0927225649356842\n",
      "Epoch 14, Iteration 162, Loss: 0.07646793872117996\n",
      "Epoch 14, Iteration 163, Loss: 0.12733595073223114\n",
      "Epoch 14, Iteration 164, Loss: 0.1145477443933487\n",
      "Epoch 14, Iteration 165, Loss: 0.11806783080101013\n",
      "Epoch 14, Iteration 166, Loss: 0.1257084608078003\n",
      "Epoch 14, Iteration 167, Loss: 0.1388678401708603\n",
      "Epoch 14, Iteration 168, Loss: 0.11254841834306717\n",
      "Epoch 14, Iteration 169, Loss: 0.12123771756887436\n",
      "Epoch 14, Iteration 170, Loss: 0.08074643462896347\n",
      "Epoch 14, Iteration 171, Loss: 0.16696378588676453\n",
      "Epoch 14, Iteration 172, Loss: 0.18468190729618073\n",
      "Epoch 14, Iteration 173, Loss: 0.1269455999135971\n",
      "Epoch 14, Iteration 174, Loss: 0.13801327347755432\n",
      "Epoch 14, Iteration 175, Loss: 0.1270959973335266\n",
      "Epoch 14, Iteration 176, Loss: 0.11508065462112427\n",
      "Epoch 14, Iteration 177, Loss: 0.17851166427135468\n",
      "Epoch 14, Iteration 178, Loss: 0.10553223639726639\n",
      "Epoch 14, Iteration 179, Loss: 0.13793089985847473\n",
      "Epoch 14, Iteration 180, Loss: 0.1471564918756485\n",
      "Epoch 14, Iteration 181, Loss: 0.10755700618028641\n",
      "Epoch 14, Iteration 182, Loss: 0.1324896216392517\n",
      "Epoch 14, Iteration 183, Loss: 0.12783849239349365\n",
      "Epoch 14, Iteration 184, Loss: 0.1950337439775467\n",
      "Epoch 14, Iteration 185, Loss: 0.08446487784385681\n",
      "Epoch 14, Iteration 186, Loss: 0.12485704571008682\n",
      "Epoch 14, Iteration 187, Loss: 0.13977913558483124\n",
      "Epoch 14, Iteration 188, Loss: 0.08741317689418793\n",
      "Epoch 14, Iteration 189, Loss: 0.07707597315311432\n",
      "Epoch 14, Iteration 190, Loss: 0.09180309623479843\n",
      "Epoch 14, Iteration 191, Loss: 0.09316768497228622\n",
      "Epoch 14, Iteration 192, Loss: 0.10216525197029114\n",
      "Epoch 14, Iteration 193, Loss: 0.1291166990995407\n",
      "Epoch 14, Iteration 194, Loss: 0.10329040884971619\n",
      "Epoch 14, Iteration 195, Loss: 0.1342189908027649\n",
      "Epoch 14, Iteration 196, Loss: 0.24301576614379883\n",
      "Epoch 14, Iteration 197, Loss: 0.10196205973625183\n",
      "Epoch 14, Iteration 198, Loss: 0.08582053333520889\n",
      "Epoch 14, Iteration 199, Loss: 0.13104122877120972\n",
      "Epoch 14, Iteration 200, Loss: 0.09812113642692566\n",
      "Epoch 14, Iteration 200, Test Loss: 0.23914724588394165\n",
      "Epoch 14, Iteration 201, Loss: 0.11023043096065521\n",
      "Epoch 14, Iteration 202, Loss: 0.07035219669342041\n",
      "Epoch 14, Iteration 203, Loss: 0.11735179275274277\n",
      "Epoch 14, Iteration 204, Loss: 0.11004941165447235\n",
      "Epoch 14, Iteration 205, Loss: 0.1280113011598587\n",
      "Epoch 14, Iteration 206, Loss: 0.0867537334561348\n",
      "Epoch 14, Iteration 207, Loss: 0.10010603815317154\n",
      "Epoch 14, Iteration 208, Loss: 0.09442558884620667\n",
      "Epoch 14, Iteration 209, Loss: 0.11375731974840164\n",
      "Epoch 14, Iteration 210, Loss: 0.08414100855588913\n",
      "Epoch 14, Iteration 211, Loss: 0.11849338561296463\n",
      "Epoch 14, Iteration 212, Loss: 0.13481050729751587\n",
      "Epoch 14, Iteration 213, Loss: 0.10220135748386383\n",
      "Epoch 14, Iteration 214, Loss: 0.11189717799425125\n",
      "Epoch 14, Iteration 215, Loss: 0.11144591122865677\n",
      "Epoch 14, Iteration 216, Loss: 0.13075043261051178\n",
      "Epoch 14, Iteration 217, Loss: 0.13253167271614075\n",
      "Epoch 14, Iteration 218, Loss: 0.13972638547420502\n",
      "Epoch 14, Iteration 219, Loss: 0.14570820331573486\n",
      "Epoch 14, Iteration 220, Loss: 0.08953756093978882\n",
      "Epoch 14, Iteration 221, Loss: 0.15812203288078308\n",
      "Epoch 14, Iteration 222, Loss: 0.11673514544963837\n",
      "Epoch 14, Iteration 223, Loss: 0.11837532371282578\n",
      "Epoch 14, Iteration 224, Loss: 0.10769416391849518\n",
      "Epoch 14, Iteration 225, Loss: 0.12296847999095917\n",
      "Epoch 14, Iteration 226, Loss: 0.1529911756515503\n",
      "Epoch 14, Iteration 227, Loss: 0.13330624997615814\n",
      "Epoch 14, Iteration 228, Loss: 0.26952841877937317\n",
      "Epoch 14, Iteration 229, Loss: 0.21099743247032166\n",
      "Epoch 14, Iteration 230, Loss: 0.1467566341161728\n",
      "Epoch 14, Iteration 231, Loss: 0.1132754385471344\n",
      "Epoch 14, Iteration 232, Loss: 0.13503390550613403\n",
      "Epoch 14, Iteration 233, Loss: 0.16569726169109344\n",
      "Epoch 14, Iteration 234, Loss: 0.10577254742383957\n",
      "Epoch 14, Iteration 235, Loss: 0.08981736749410629\n",
      "Epoch 14, Iteration 236, Loss: 0.14209474623203278\n",
      "Epoch 14, Iteration 237, Loss: 0.14021427929401398\n",
      "Epoch 14, Iteration 238, Loss: 0.13458900153636932\n",
      "Epoch 14, Iteration 239, Loss: 0.11245551705360413\n",
      "Epoch 14, Iteration 240, Loss: 0.10880901664495468\n",
      "Epoch 14, Iteration 241, Loss: 0.11878181993961334\n",
      "Epoch 14, Iteration 242, Loss: 0.14546535909175873\n",
      "Epoch 14, Iteration 243, Loss: 0.1122274100780487\n",
      "Epoch 14, Iteration 244, Loss: 0.150142103433609\n",
      "Epoch 14, Iteration 245, Loss: 0.12194216251373291\n",
      "Epoch 14, Iteration 246, Loss: 0.13977684080600739\n",
      "Epoch 14, Iteration 247, Loss: 0.2460309863090515\n",
      "Epoch 14, Iteration 248, Loss: 0.17322516441345215\n",
      "Epoch 14, Iteration 249, Loss: 0.18430937826633453\n",
      "Epoch 14, Iteration 250, Loss: 0.19951945543289185\n",
      "Epoch 14, Iteration 250, Test Loss: 0.23717111349105835\n",
      "Epoch 14, Iteration 251, Loss: 0.18998195230960846\n",
      "Epoch 14, Iteration 252, Loss: 0.13060016930103302\n",
      "Epoch 14, Iteration 253, Loss: 0.15869751572608948\n",
      "Epoch 14, Iteration 254, Loss: 0.1845604032278061\n",
      "Epoch 14, Iteration 255, Loss: 0.1476450264453888\n",
      "Epoch 14, Iteration 256, Loss: 0.13532230257987976\n",
      "Epoch 14, Iteration 257, Loss: 0.13102802634239197\n",
      "Epoch 14, Iteration 258, Loss: 0.14317214488983154\n",
      "Epoch 14, Iteration 259, Loss: 0.14867565035820007\n",
      "Epoch 14, Iteration 260, Loss: 0.17222128808498383\n",
      "Epoch 14, Iteration 261, Loss: 0.11637301743030548\n",
      "Epoch 14, Iteration 262, Loss: 0.09626974910497665\n",
      "Epoch 14, Iteration 263, Loss: 0.11403775215148926\n",
      "Epoch 14, Iteration 264, Loss: 0.1061144471168518\n",
      "Epoch 14, Iteration 265, Loss: 0.17879636585712433\n",
      "Epoch 14, Iteration 266, Loss: 0.10634972900152206\n",
      "Epoch 14, Iteration 267, Loss: 0.1336243897676468\n",
      "Epoch 14, Iteration 268, Loss: 0.15800218284130096\n",
      "Epoch 14, Iteration 269, Loss: 0.11628254503011703\n",
      "Epoch 14, Iteration 270, Loss: 0.14142154157161713\n",
      "Epoch 14, Iteration 271, Loss: 0.08790824562311172\n",
      "Epoch 14, Iteration 272, Loss: 0.12111201137304306\n",
      "Epoch 14, Iteration 273, Loss: 0.16427797079086304\n",
      "Epoch 14, Iteration 274, Loss: 0.10564226657152176\n",
      "Epoch 14, Iteration 275, Loss: 0.09385472536087036\n",
      "Epoch 14, Iteration 276, Loss: 0.09133689850568771\n",
      "Epoch 14, Iteration 277, Loss: 0.10679609328508377\n",
      "Epoch 14, Iteration 278, Loss: 0.13150009512901306\n",
      "Epoch 14, Iteration 279, Loss: 0.08794960379600525\n",
      "Epoch 14, Iteration 280, Loss: 0.15964218974113464\n",
      "Epoch 14, Iteration 281, Loss: 0.1029934212565422\n",
      "Epoch 14, Iteration 282, Loss: 0.09432985633611679\n",
      "Epoch 14, Iteration 283, Loss: 0.09697853773832321\n",
      "Epoch 14, Iteration 284, Loss: 0.10450832545757294\n",
      "Epoch 14, Iteration 285, Loss: 0.08149148523807526\n",
      "Epoch 14, Iteration 286, Loss: 0.13970793783664703\n",
      "Epoch 14, Iteration 287, Loss: 0.08728806674480438\n",
      "Epoch 14, Iteration 288, Loss: 0.11902755498886108\n",
      "Epoch 14, Iteration 289, Loss: 0.16796685755252838\n",
      "Epoch 14, Iteration 290, Loss: 0.1073426827788353\n",
      "Epoch 14, Iteration 291, Loss: 0.11019770801067352\n",
      "Epoch 14, Iteration 292, Loss: 0.11782398819923401\n",
      "Epoch 14, Iteration 293, Loss: 0.16290542483329773\n",
      "Epoch 14, Iteration 294, Loss: 0.10142774134874344\n",
      "Epoch 14, Iteration 295, Loss: 0.07947240024805069\n",
      "Epoch 14, Iteration 296, Loss: 0.08432081341743469\n",
      "Epoch 14, Iteration 297, Loss: 0.10835187137126923\n",
      "Epoch 14, Iteration 298, Loss: 0.09762021154165268\n",
      "Epoch 14, Iteration 299, Loss: 0.1827530413866043\n",
      "Epoch 14, Iteration 300, Loss: 0.10159413516521454\n",
      "Epoch 14, Iteration 300, Test Loss: 0.26823318004608154\n",
      "Epoch 14, Iteration 301, Loss: 0.10886596143245697\n",
      "Epoch 14, Iteration 302, Loss: 0.0728263333439827\n",
      "Epoch 14, Iteration 303, Loss: 0.13051193952560425\n",
      "Epoch 14, Iteration 304, Loss: 0.14303910732269287\n",
      "Epoch 14, Iteration 305, Loss: 0.10173320025205612\n",
      "Epoch 14, Iteration 306, Loss: 0.09721371531486511\n",
      "Epoch 14, Iteration 307, Loss: 0.11709298938512802\n",
      "Epoch 14, Iteration 308, Loss: 0.12621760368347168\n",
      "Epoch 14, Iteration 309, Loss: 0.10097505897283554\n",
      "Epoch 14, Iteration 310, Loss: 0.11698313802480698\n",
      "Epoch 14, Iteration 311, Loss: 0.0996059998869896\n",
      "Epoch 14, Iteration 312, Loss: 0.13107645511627197\n",
      "Epoch 14, Iteration 313, Loss: 0.09322408586740494\n",
      "Epoch 14, Iteration 314, Loss: 0.11396972835063934\n",
      "Epoch 14, Iteration 315, Loss: 0.09355022013187408\n",
      "Epoch 14, Iteration 316, Loss: 0.08121614158153534\n",
      "Epoch 14, Iteration 317, Loss: 0.12569287419319153\n",
      "Epoch 14, Iteration 318, Loss: 0.1350729763507843\n",
      "Epoch 14, Iteration 319, Loss: 0.09065615385770798\n",
      "Epoch 14, Iteration 320, Loss: 0.11664332449436188\n",
      "Epoch 14, Iteration 321, Loss: 0.08463602513074875\n",
      "Epoch 14, Iteration 322, Loss: 0.1269369274377823\n",
      "Epoch 14, Iteration 323, Loss: 0.12154621630907059\n",
      "Epoch 14, Iteration 324, Loss: 0.1449042707681656\n",
      "Epoch 14, Iteration 325, Loss: 0.2531580626964569\n",
      "Epoch 14, Iteration 326, Loss: 0.17434494197368622\n",
      "Epoch 14, Iteration 327, Loss: 0.13956241309642792\n",
      "Epoch 14, Iteration 328, Loss: 0.13213256001472473\n",
      "Epoch 14, Iteration 329, Loss: 0.1351689100265503\n",
      "Epoch 14, Iteration 330, Loss: 0.1671300083398819\n",
      "Epoch 14, Iteration 331, Loss: 0.15176892280578613\n",
      "Epoch 14, Iteration 332, Loss: 0.14883609116077423\n",
      "Epoch 14, Iteration 333, Loss: 0.10470433533191681\n",
      "Epoch 14, Iteration 334, Loss: 0.20516452193260193\n",
      "Epoch 14, Iteration 335, Loss: 0.1348741054534912\n",
      "Epoch 14, Iteration 336, Loss: 0.1574583798646927\n",
      "Epoch 14, Iteration 337, Loss: 0.11371789127588272\n",
      "Epoch 14, Iteration 338, Loss: 0.11454134434461594\n",
      "Epoch 14, Iteration 339, Loss: 0.15586841106414795\n",
      "Epoch 14, Iteration 340, Loss: 0.15027908980846405\n",
      "Epoch 14, Iteration 341, Loss: 0.15052145719528198\n",
      "Epoch 14, Iteration 342, Loss: 0.13029931485652924\n",
      "Epoch 14, Iteration 343, Loss: 0.18810483813285828\n",
      "Epoch 14, Iteration 344, Loss: 0.18561014533042908\n",
      "Epoch 14, Iteration 345, Loss: 0.14046305418014526\n",
      "Epoch 14, Iteration 346, Loss: 0.13331131637096405\n",
      "Epoch 14, Iteration 347, Loss: 0.14873440563678741\n",
      "Epoch 14, Iteration 348, Loss: 0.16451352834701538\n",
      "Epoch 14, Iteration 349, Loss: 0.1655249297618866\n",
      "Epoch 14, Iteration 350, Loss: 0.16932637989521027\n",
      "Epoch 14, Iteration 350, Test Loss: 0.22489365935325623\n",
      "Epoch 14, Iteration 351, Loss: 0.15019409358501434\n",
      "Epoch 14, Iteration 352, Loss: 0.1173270121216774\n",
      "Epoch 14, Iteration 353, Loss: 0.16967642307281494\n",
      "Epoch 14, Iteration 354, Loss: 0.1140449270606041\n",
      "Epoch 14, Iteration 355, Loss: 0.12183405458927155\n",
      "Epoch 14, Iteration 356, Loss: 0.11761464923620224\n",
      "Epoch 14, Iteration 357, Loss: 0.18286055326461792\n",
      "Epoch 14, Iteration 358, Loss: 0.11267884075641632\n",
      "Epoch 14, Iteration 359, Loss: 0.1398410052061081\n",
      "Epoch 14, Iteration 360, Loss: 0.08367040753364563\n",
      "Epoch 14, Iteration 361, Loss: 0.1500176340341568\n",
      "Epoch 14, Iteration 362, Loss: 0.12311895936727524\n",
      "Epoch 14, Iteration 363, Loss: 0.11168292909860611\n",
      "Epoch 14, Iteration 364, Loss: 0.10960642248392105\n",
      "Epoch 14, Iteration 365, Loss: 0.10863520950078964\n",
      "Epoch 14, Iteration 366, Loss: 0.1588541567325592\n",
      "Epoch 14, Iteration 367, Loss: 0.14564351737499237\n",
      "Epoch 14, Iteration 368, Loss: 0.12754468619823456\n",
      "Epoch 14, Iteration 369, Loss: 0.11679138243198395\n",
      "Epoch 14, Iteration 370, Loss: 0.14216887950897217\n",
      "Epoch 14, Iteration 371, Loss: 0.1428718864917755\n",
      "Epoch 14, Iteration 372, Loss: 0.1182650476694107\n",
      "Epoch 14, Iteration 373, Loss: 0.13887837529182434\n",
      "Epoch 14, Iteration 374, Loss: 0.10234448313713074\n",
      "Epoch 14, Iteration 375, Loss: 0.14382340013980865\n",
      "Epoch 14, Iteration 376, Loss: 0.1481918841600418\n",
      "Epoch 14, Iteration 377, Loss: 0.1382855772972107\n",
      "Epoch 14, Iteration 378, Loss: 0.12512370944023132\n",
      "Epoch 14, Iteration 379, Loss: 0.17000871896743774\n",
      "Epoch 14, Iteration 380, Loss: 0.15837299823760986\n",
      "Epoch 14, Iteration 381, Loss: 0.10540909320116043\n",
      "Epoch 14, Iteration 382, Loss: 0.12029246240854263\n",
      "Epoch 14, Iteration 383, Loss: 0.10792072117328644\n",
      "Epoch 14, Iteration 384, Loss: 0.12888973951339722\n",
      "Epoch 14, Iteration 385, Loss: 0.12662440538406372\n",
      "Epoch 14, Iteration 386, Loss: 0.1180584505200386\n",
      "Epoch 14, Iteration 387, Loss: 0.08730453252792358\n",
      "Epoch 14, Iteration 388, Loss: 0.10017538070678711\n",
      "Epoch 14, Iteration 389, Loss: 0.12270424515008926\n",
      "Epoch 14, Iteration 390, Loss: 0.11844171583652496\n",
      "Epoch 14, Iteration 391, Loss: 0.07504457235336304\n",
      "Epoch 14, Iteration 392, Loss: 0.17477378249168396\n",
      "Epoch 14, Iteration 393, Loss: 0.17403073608875275\n",
      "Epoch 14, Iteration 394, Loss: 0.10203228890895844\n",
      "Epoch 14, Iteration 395, Loss: 0.10880622267723083\n",
      "Epoch 14, Iteration 396, Loss: 0.12996922433376312\n",
      "Epoch 14, Iteration 397, Loss: 0.12173278629779816\n",
      "Epoch 14, Iteration 398, Loss: 0.21445852518081665\n",
      "Epoch 14, Iteration 399, Loss: 0.09229089319705963\n",
      "Epoch 14, Iteration 400, Loss: 0.10544870793819427\n",
      "Epoch 14, Iteration 400, Test Loss: 0.24744153022766113\n",
      "Epoch 14, Iteration 401, Loss: 0.12204337120056152\n",
      "Epoch 14, Iteration 402, Loss: 0.08436664938926697\n",
      "Epoch 14, Iteration 403, Loss: 0.12266918271780014\n",
      "Epoch 14, Iteration 404, Loss: 0.12828367948532104\n",
      "Epoch 14, Iteration 405, Loss: 0.09323710948228836\n",
      "Epoch 14, Iteration 406, Loss: 0.09533079713582993\n",
      "Epoch 14, Iteration 407, Loss: 0.1484990119934082\n",
      "Epoch 14, Iteration 408, Loss: 0.1386387199163437\n",
      "Epoch 14, Iteration 409, Loss: 0.09443391114473343\n",
      "Epoch 14, Iteration 410, Loss: 0.16960668563842773\n",
      "Epoch 14, Iteration 411, Loss: 0.1815001666545868\n",
      "Epoch 14, Iteration 412, Loss: 0.11398909240961075\n",
      "Epoch 14, Iteration 413, Loss: 0.1584368199110031\n",
      "Epoch 14, Iteration 414, Loss: 0.08611129969358444\n",
      "Epoch 14, Iteration 415, Loss: 0.12853942811489105\n",
      "Epoch 14, Iteration 416, Loss: 0.11171908676624298\n",
      "Epoch 14, Iteration 417, Loss: 0.11732958257198334\n",
      "Epoch 14, Iteration 418, Loss: 0.11022267490625381\n",
      "Epoch 14, Iteration 419, Loss: 0.10222826898097992\n",
      "Epoch 14, Iteration 420, Loss: 0.12556853890419006\n",
      "Epoch 14, Iteration 421, Loss: 0.07825545966625214\n",
      "Epoch 14, Iteration 422, Loss: 0.12183889746665955\n",
      "Epoch 14, Iteration 423, Loss: 0.1129932627081871\n",
      "Epoch 14, Iteration 424, Loss: 0.11473924666643143\n",
      "Epoch 14, Iteration 425, Loss: 0.1272776573896408\n",
      "Epoch 14, Iteration 426, Loss: 0.10361962020397186\n",
      "Epoch 14, Iteration 427, Loss: 0.08766701817512512\n",
      "Epoch 14, Iteration 428, Loss: 0.12941546738147736\n",
      "Epoch 14, Iteration 429, Loss: 0.1401391625404358\n",
      "Epoch 14, Iteration 430, Loss: 0.11849924921989441\n",
      "Epoch 14, Iteration 431, Loss: 0.12016372382640839\n",
      "Epoch 15/15, Loss: 0.13549152806539227\n",
      "Epoch 15/15, Validation Accuracy: 0.9434782608695652\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train(model, train_dataloader, valid_dataloader, test_datatset[:32], device, lr=1e-3, num_epochs=5)\n",
    "\n",
    "# Extra: train the model with a more complex feed forward network\n",
    "# model = POSTaggingProModel()\n",
    "# train(model, train_dataloader, valid_dataloader, test_datatset[:32], device, lr=3e-4, num_epochs=15)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T21:29:26.234127400Z",
     "start_time": "2024-12-15T21:29:23.799998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# evaluate the trained model\n",
    "# to run this, please ignore the two training cells above\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = POSTaggingProModel().to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"pos_tagging_model_0.pth\"))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_count = 0\n",
    "    accurate_count = 0\n",
    "    for data in valid_dataloader:\n",
    "        x = data[\"input_ids\"].to(device)\n",
    "        mask = data[\"attention_mask\"].to(device)\n",
    "        y = data[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(x, mask)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # flatten predictions and labels for comparison\n",
    "        predictions = predictions.view(-1)\n",
    "        y = y.view(-1)\n",
    "\n",
    "        for prediction, label in zip(predictions, y):\n",
    "            # ignore following and padding tokens\n",
    "            if label == -100:\n",
    "                continue\n",
    "\n",
    "            # count label tokens\n",
    "            total_count += 1\n",
    "\n",
    "            # compare the prediction with the ground truth and count\n",
    "            if prediction == label:\n",
    "                accurate_count += 1\n",
    "\n",
    "    # compute average accuracy\n",
    "    avg_acc = accurate_count / total_count\n",
    "\n",
    "print(f\"Validation Accuracy: {avg_acc}\")"
   ],
   "id": "c9d938f64e13c610",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# evaluate the trained model\u001B[39;00m\n\u001B[0;32m      2\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mPOSTaggingProModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpos_tagging_model_0.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m      8\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1171\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m-> 1174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    801\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    802\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    803\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 805\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    806\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    808\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1153\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1154\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[0;32m   1155\u001B[0m             device,\n\u001B[0;32m   1156\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1157\u001B[0m             non_blocking,\n\u001B[0;32m   1158\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[0;32m   1159\u001B[0m         )\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1163\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1164\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Observation\n",
    "I trained 4 neural network models with different hyperparameters and architectures. \n",
    "The models are:\n",
    "- Base Model: RoBERTa + Linear Layer\n",
    "    - Learning Rate: 1e-3\n",
    "    - Epochs: 5     \n",
    "    - Final Validation Accuracy: 0.9356\n",
    "- Pro Model1: RoBERTa + 3 Linear Layers with GELU activation functions\n",
    "    - Learning Rate: 1e-3\n",
    "    - Epochs: 5\n",
    "    - Final Validation Accuracy: 0.9377\n",
    "- Pro Model2: RoBERTa + 3 Linear Layers with GELU activation functions\n",
    "    - Learning Rate: 6e-4\n",
    "    - Epochs: 7\n",
    "    - Final Validation Accuracy: 0.9382\n",
    "- Pro Model3: RoBERTa + 3 Linear Layers with GELU activation functions\n",
    "    - Learning Rate: 3e-4\n",
    "    - Epochs: 15\n",
    "    - Final Validation Accuracy: 0.9435\n",
    "\n",
    "All of the models are trained with a 32 batch size. Compared to the HMM model whose valid accuracy is 91%, all the neural network models have a higher accuracy rate. The best model is Pro Model3 with a 94.35% validation accuracy. The model has a more complex feed forward network with 3 linear layers and GELU activation functions.\n",
    "\n",
    "Compared to the HMM model, the neural network model has a higher accuracy score. However, it also takes a longer time to be trained. And the inference time is also longer than the HMM model. \n",
    "In summary, the neural network model is more complex and requires more computational resources. The HMM model is simpler and faster but less accurate.  \n",
    "\n"
   ],
   "id": "f6d12990827a7b92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "af3deba0de2b15a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
