<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>dependency_parsing</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iLTEwIC0xMCAxMzEuMTYxMzYxNjk0MzM1OTQgMTMyLjM4ODk5OTkzODk2NDg0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzA2OTk4IiBkPSJNIDU0LjkxODc4NSw5LjE5Mjc0MjFlLTQgQyA1MC4zMzUxMzIsMC4wMjIyMTcyNyA0NS45NTc4NDYsMC40MTMxMzY5NyA0Mi4xMDYyODUsMS4wOTQ2NjkzIDMwLjc2MDA2OSwzLjA5OTE3MzEgMjguNzAwMDM2LDcuMjk0NzcxNCAyOC43MDAwMzUsMTUuMDMyMTY5IHYgMTAuMjE4NzUgaCAyNi44MTI1IHYgMy40MDYyNSBoIC0yNi44MTI1IC0xMC4wNjI1IGMgLTcuNzkyNDU5LDAgLTE0LjYxNTc1ODgsNC42ODM3MTcgLTE2Ljc0OTk5OTgsMTMuNTkzNzUgLTIuNDYxODE5OTgsMTAuMjEyOTY2IC0yLjU3MTAxNTA4LDE2LjU4NjAyMyAwLDI3LjI1IDEuOTA1OTI4Myw3LjkzNzg1MiA2LjQ1NzU0MzIsMTMuNTkzNzQ4IDE0LjI0OTk5OTgsMTMuNTkzNzUgaCA5LjIxODc1IHYgLTEyLjI1IGMgMCwtOC44NDk5MDIgNy42NTcxNDQsLTE2LjY1NjI0OCAxNi43NSwtMTYuNjU2MjUgaCAyNi43ODEyNSBjIDcuNDU0OTUxLDAgMTMuNDA2MjUzLC02LjEzODE2NCAxMy40MDYyNSwtMTMuNjI1IHYgLTI1LjUzMTI1IGMgMCwtNy4yNjYzMzg2IC02LjEyOTk4LC0xMi43MjQ3NzcxIC0xMy40MDYyNSwtMTMuOTM3NDk5NyBDIDY0LjI4MTU0OCwwLjMyNzk0Mzk3IDU5LjUwMjQzOCwtMC4wMjAzNzkwMyA1NC45MTg3ODUsOS4xOTI3NDIxZS00IFogbSAtMTQuNSw4LjIxODc1MDEyNTc5IGMgMi43Njk1NDcsMCA1LjAzMTI1LDIuMjk4NjQ1NiA1LjAzMTI1LDUuMTI0OTk5NiAtMmUtNiwyLjgxNjMzNiAtMi4yNjE3MDMsNS4wOTM3NSAtNS4wMzEyNSw1LjA5Mzc1IC0yLjc3OTQ3NiwtMWUtNiAtNS4wMzEyNSwtMi4yNzc0MTUgLTUuMDMxMjUsLTUuMDkzNzUgLTEwZS03LC0yLjgyNjM1MyAyLjI1MTc3NCwtNS4xMjQ5OTk2IDUuMDMxMjUsLTUuMTI0OTk5NiB6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2ZmZDQzYiIgZD0ibSA4NS42Mzc1MzUsMjguNjU3MTY5IHYgMTEuOTA2MjUgYyAwLDkuMjMwNzU1IC03LjgyNTg5NSwxNi45OTk5OTkgLTE2Ljc1LDE3IGggLTI2Ljc4MTI1IGMgLTcuMzM1ODMzLDAgLTEzLjQwNjI0OSw2LjI3ODQ4MyAtMTMuNDA2MjUsMTMuNjI1IHYgMjUuNTMxMjQ3IGMgMCw3LjI2NjM0NCA2LjMxODU4OCwxMS41NDAzMjQgMTMuNDA2MjUsMTMuNjI1MDA0IDguNDg3MzMxLDIuNDk1NjEgMTYuNjI2MjM3LDIuOTQ2NjMgMjYuNzgxMjUsMCA2Ljc1MDE1NSwtMS45NTQzOSAxMy40MDYyNTMsLTUuODg3NjEgMTMuNDA2MjUsLTEzLjYyNTAwNCBWIDg2LjUwMDkxOSBoIC0yNi43ODEyNSB2IC0zLjQwNjI1IGggMjYuNzgxMjUgMTMuNDA2MjU0IGMgNy43OTI0NjEsMCAxMC42OTYyNTEsLTUuNDM1NDA4IDEzLjQwNjI0MSwtMTMuNTkzNzUgMi43OTkzMywtOC4zOTg4ODYgMi42ODAyMiwtMTYuNDc1Nzc2IDAsLTI3LjI1IC0xLjkyNTc4LC03Ljc1NzQ0MSAtNS42MDM4NywtMTMuNTkzNzUgLTEzLjQwNjI0MSwtMTMuNTkzNzUgeiBtIC0xNS4wNjI1LDY0LjY1NjI1IGMgMi43Nzk0NzgsM2UtNiA1LjAzMTI1LDIuMjc3NDE3IDUuMDMxMjUsNS4wOTM3NDcgLTJlLTYsMi44MjYzNTQgLTIuMjUxNzc1LDUuMTI1MDA0IC01LjAzMTI1LDUuMTI1MDA0IC0yLjc2OTU1LDAgLTUuMDMxMjUsLTIuMjk4NjUgLTUuMDMxMjUsLTUuMTI1MDA0IDJlLTYsLTIuODE2MzMgMi4yNjE2OTcsLTUuMDkzNzQ3IDUuMDMxMjUsLTUuMDkzNzQ3IHoiLz4KPC9zdmc+Cg==);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=initial_id">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">datasets</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">XLMRobertaModel</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=246be7d70e3db82">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># load the dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s2">"universal_dependencies"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"en_ewt"</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"validation"</span><span class="p">]</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">[</span><span class="s2">"text"</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>DatasetDict({
    train: Dataset({
        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],
        num_rows: 12543
    })
    validation: Dataset({
        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],
        num_rows: 2002
    })
    test: Dataset({
        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],
        num_rows: 2077
    })
})
['Al-Zaman : American forces killed Shaikh Abdullah al-Ani, the preacher at the mosque in the town of Qaim, near the Syrian border.', '[This killing of a respected cleric will be causing us trouble for years to come.]', 'DPA: Iraqi authorities announced that they had busted up 3 terrorist cells operating in Baghdad.', 'Two of them were being run by 2 officials of the Ministry of the Interior!', 'The MoI in Iraq is equivalent to the US FBI, so this would be like having J. Edgar Hoover unwittingly employ at a high level members of the Weathermen bombers back in the 1960s.', 'The third was being run by the head of an investment firm.', 'You wonder if he was manipulating the market with his bombing targets.', 'The cells were operating in the Ghazaliyah and al-Jihad districts of the capital.', "Although the announcement was probably made to show progress in identifying and breaking up terror cells, I don't find the news that the Baathists continue to penetrate the Iraqi government very hopeful.", 'It reminds me too much of the ARVN officers who were secretly working for the other side in Vietnam.']
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=907365ee16f491c5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">all_deprels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># these are the default UD dependency relations according to https://universaldependencies.org/u/dep/</span>
    <span class="s2">"acl"</span><span class="p">,</span> <span class="s2">"acl:relcl"</span><span class="p">,</span> <span class="s2">"advcl"</span><span class="p">,</span> <span class="s2">"advcl:relcl"</span><span class="p">,</span> <span class="s2">"advmod"</span><span class="p">,</span> <span class="s2">"advmod:emph"</span><span class="p">,</span> <span class="s2">"advmod:lmod"</span><span class="p">,</span> <span class="s2">"amod"</span><span class="p">,</span> <span class="s2">"appos"</span><span class="p">,</span>
    <span class="s2">"aux"</span><span class="p">,</span> <span class="s2">"aux:pass"</span><span class="p">,</span> <span class="s2">"case"</span><span class="p">,</span> <span class="s2">"cc"</span><span class="p">,</span> <span class="s2">"cc:preconj"</span><span class="p">,</span> <span class="s2">"ccomp"</span><span class="p">,</span> <span class="s2">"clf"</span><span class="p">,</span> <span class="s2">"compound"</span><span class="p">,</span> <span class="s2">"compound:lvc"</span><span class="p">,</span>
    <span class="s2">"compound:prt"</span><span class="p">,</span> <span class="s2">"compound:redup"</span><span class="p">,</span> <span class="s2">"compound:svc"</span><span class="p">,</span> <span class="s2">"conj"</span><span class="p">,</span> <span class="s2">"cop"</span><span class="p">,</span> <span class="s2">"csubj"</span><span class="p">,</span> <span class="s2">"csubj:outer"</span><span class="p">,</span>
    <span class="s2">"csubj:pass"</span><span class="p">,</span> <span class="s2">"dep"</span><span class="p">,</span> <span class="s2">"det"</span><span class="p">,</span> <span class="s2">"det:numgov"</span><span class="p">,</span> <span class="s2">"det:nummod"</span><span class="p">,</span> <span class="s2">"det:poss"</span><span class="p">,</span> <span class="s2">"discourse"</span><span class="p">,</span> <span class="s2">"dislocated"</span><span class="p">,</span>
    <span class="s2">"expl"</span><span class="p">,</span> <span class="s2">"expl:impers"</span><span class="p">,</span> <span class="s2">"expl:pass"</span><span class="p">,</span> <span class="s2">"expl:pv"</span><span class="p">,</span> <span class="s2">"fixed"</span><span class="p">,</span> <span class="s2">"flat"</span><span class="p">,</span> <span class="s2">"flat:foreign"</span><span class="p">,</span> <span class="s2">"flat:name"</span><span class="p">,</span>
    <span class="s2">"goeswith"</span><span class="p">,</span> <span class="s2">"iobj"</span><span class="p">,</span> <span class="s2">"list"</span><span class="p">,</span> <span class="s2">"mark"</span><span class="p">,</span> <span class="s2">"nmod"</span><span class="p">,</span> <span class="s2">"nmod:poss"</span><span class="p">,</span> <span class="s2">"nmod:tmod"</span><span class="p">,</span> <span class="s2">"nsubj"</span><span class="p">,</span> <span class="s2">"nsubj:outer"</span><span class="p">,</span>
    <span class="s2">"nsubj:pass"</span><span class="p">,</span> <span class="s2">"nummod"</span><span class="p">,</span> <span class="s2">"nummod:gov"</span><span class="p">,</span> <span class="s2">"obj"</span><span class="p">,</span> <span class="s2">"obl"</span><span class="p">,</span> <span class="s2">"obl:agent"</span><span class="p">,</span> <span class="s2">"obl:arg"</span><span class="p">,</span> <span class="s2">"obl:lmod"</span><span class="p">,</span>
    <span class="s2">"obl:tmod"</span><span class="p">,</span> <span class="s2">"orphan"</span><span class="p">,</span> <span class="s2">"parataxis"</span><span class="p">,</span> <span class="s2">"punct"</span><span class="p">,</span> <span class="s2">"reparandum"</span><span class="p">,</span> <span class="s2">"root"</span><span class="p">,</span> <span class="s2">"vocative"</span><span class="p">,</span> <span class="s2">"xcomp"</span><span class="p">,</span>

    <span class="c1"># we need some more for en_ewt</span>
    <span class="s2">"det:predet"</span><span class="p">,</span> <span class="s2">"obl:npmod"</span><span class="p">,</span> <span class="s2">"nmod:npmod"</span>
<span class="p">]</span>

<span class="c1"># construct deprel to ID mapping</span>
<span class="n">deprel_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">rel</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">rel</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_deprels</span><span class="p">)}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=fd3b37577b6ef9f5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Code for the assignment in https://github.com/coli-saar/cl/wiki/Assignment:-Dependency-parsing</span>
<span class="c1"># Alexander Koller, December 2023</span>

<span class="k">def</span> <span class="nf">strip_none_heads</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s2">"tokens"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="n">heads</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s2">"head"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="n">deprels</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s2">"deprel"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>

    <span class="n">non_none</span> <span class="o">=</span> <span class="p">[(</span><span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">heads</span><span class="p">,</span> <span class="n">deprels</span><span class="p">)</span> <span class="k">if</span> <span class="n">h</span> <span class="o">!=</span> <span class="s2">"None"</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">non_none</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">map_first_occurrence</span><span class="p">(</span><span class="n">nums</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Maps a list of numbers to a dictionary that assigns each unique number the position of its first occurrence.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt; map_first_occurrence([0,1,2,3,3,3,4])</span>
<span class="sd">    {0: 0, 1: 1, 2: 2, 3: 3, 4: 6}</span>

<span class="sd">    :param nums:</span>
<span class="sd">    :return:</span>
<span class="sd">    """</span>
    <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">num</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nums</span><span class="p">)</span> <span class="k">if</span> <span class="n">num</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">num</span><span class="p">)}</span>


<span class="k">def</span> <span class="nf">pad_to_same_size</span><span class="p">(</span><span class="n">lists</span><span class="p">,</span> <span class="n">padding_symbol</span><span class="p">):</span>
    <span class="n">maxlen</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lists</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">l</span> <span class="o">+</span> <span class="p">(</span><span class="n">padding_symbol</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="n">maxlen</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">))</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lists</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">tokenize_and_align_labels</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">deprel_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">skip_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># delete tokens with "None" head and their annotations</span>
    <span class="n">examples_tokens</span><span class="p">,</span> <span class="n">examples_heads</span><span class="p">,</span> <span class="n">examples_deprels</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sentence_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"tokens"</span><span class="p">])):</span>
        <span class="n">tt</span><span class="p">,</span> <span class="n">hh</span><span class="p">,</span> <span class="n">dd</span> <span class="o">=</span> <span class="n">strip_none_heads</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">sentence_id</span><span class="p">)</span>
        <span class="n">examples_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tt</span><span class="p">)</span>
        <span class="n">examples_heads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hh</span><span class="p">)</span>
        <span class="n">examples_deprels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dd</span><span class="p">)</span>

    <span class="n">tokenized_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples_tokens</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># get "tokenizer" from global variable</span>
    <span class="c1"># tokenized_inputs is a dictionary with keys input_ids and attention_mask;</span>
    <span class="c1"># each is a list (per sentence) of lists (per token).</span>

    <span class="n">remapped_heads</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># these will be lists (per sentence) of lists (per token)</span>
    <span class="n">deprel_ids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tokens_representing_words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">num_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">maxlen_t2w</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># max length of a token_to_word_here list</span>

    <span class="k">for</span> <span class="n">sentence_id</span><span class="p">,</span> <span class="n">annotated_heads</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">examples_heads</span><span class="p">):</span>
        <span class="n">deprels</span> <span class="o">=</span> <span class="n">examples_deprels</span><span class="p">[</span><span class="n">sentence_id</span><span class="p">]</span>
        <span class="n">word_ids</span> <span class="o">=</span> <span class="n">tokenized_inputs</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">batch_index</span><span class="o">=</span><span class="n">sentence_id</span><span class="p">)</span>
        <span class="n">word_pos_to_token_pos</span> <span class="o">=</span> <span class="n">map_first_occurrence</span><span class="p">(</span>
            <span class="n">word_ids</span><span class="p">)</span>  <span class="c1"># word-pos to first token-pos; both start at 0 for first word (actual) / first token (BOS)</span>

        <span class="n">previous_word_idx</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">heads_here</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">deprel_ids_here</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># list of token positions that map to words (first token of each word)</span>
        <span class="c1"># token 0 -&gt; word 0 (BOS)</span>
        <span class="n">tokens_representing_word_here</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">sentence_position</span><span class="p">,</span> <span class="n">word_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_ids</span><span class="p">):</span>
            <span class="c1"># Special tokens (BOS, EOS) have a word id that is None. We set the label to -100 so they are automatically</span>
            <span class="c1"># ignored in the loss function.</span>
            <span class="k">if</span> <span class="n">word_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">heads_here</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">skip_index</span><span class="p">)</span>
                <span class="n">deprel_ids_here</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">skip_index</span><span class="p">)</span>

            <span class="c1"># We set the label for the first token of each word;</span>
            <span class="c1"># subsequent tokens of the same word will have the same word_idx.</span>
            <span class="k">elif</span> <span class="n">word_idx</span> <span class="o">!=</span> <span class="n">previous_word_idx</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">annotated_heads</span><span class="p">[</span><span class="n">word_idx</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"None"</span><span class="p">:</span>  <span class="c1"># added by padding</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">"A 'None' head survived!"</span><span class="p">)</span>
                    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Map HEAD annotation to position of first token of head word.</span>
                    <span class="c1"># HEAD = 0 =&gt; map it to first token (BOS)</span>
                    <span class="c1"># Otherwise, look up first token for HEAD-1 (HEAD is 1-based, word positions are 0-based)</span>
                    <span class="n">head_word_pos</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">annotated_heads</span><span class="p">[</span><span class="n">word_idx</span><span class="p">])</span>
                    <span class="n">head_token_pos</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">head_word_pos</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">word_pos_to_token_pos</span><span class="p">[</span><span class="n">head_word_pos</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

                    <span class="n">heads_here</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">head_token_pos</span><span class="p">)</span>
                    <span class="n">deprel_ids_here</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">deprel_to_id</span><span class="p">[</span><span class="n">deprels</span><span class="p">[</span><span class="n">word_idx</span><span class="p">]])</span>

                    <span class="n">tokens_representing_word_here</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sentence_position</span><span class="p">)</span>  <span class="c1"># first word is index 1; index 0 is BOS</span>

            <span class="c1"># For the other tokens in a word, we set the label to either the current label or -100, depending on</span>
            <span class="c1"># the label_all_tokens flag.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">heads_here</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">skip_index</span><span class="p">)</span>
                <span class="n">deprel_ids_here</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">skip_index</span><span class="p">)</span>

            <span class="n">previous_word_idx</span> <span class="o">=</span> <span class="n">word_idx</span>

        <span class="n">remapped_heads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">heads_here</span><span class="p">)</span>
        <span class="n">deprel_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">deprel_ids_here</span><span class="p">)</span>
        <span class="n">tokens_representing_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens_representing_word_here</span><span class="p">)</span>

        <span class="n">num_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens_representing_word_here</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens_representing_word_here</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">maxlen_t2w</span><span class="p">:</span>
            <span class="n">maxlen_t2w</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens_representing_word_here</span><span class="p">)</span>

    <span class="c1"># pad t2w lists to same length</span>
    <span class="k">for</span> <span class="n">t2w</span> <span class="ow">in</span> <span class="n">tokens_representing_words</span><span class="p">:</span>
        <span class="n">t2w</span> <span class="o">+=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">maxlen_t2w</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">t2w</span><span class="p">))</span>

    <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">"head"</span><span class="p">]</span> <span class="o">=</span> <span class="n">remapped_heads</span>
    <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">"deprel_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="n">deprel_ids</span>
    <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">"tokens_representing_words"</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokens_representing_words</span>
    <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">"num_words"</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_words</span>
    <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">"tokenid_to_wordid"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenized_inputs</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">batch_index</span><span class="o">=</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>
                                             <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">examples_heads</span><span class="p">))]</span>  <span class="c1"># map token ID to word ID</span>

    <span class="k">return</span> <span class="n">tokenized_inputs</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=99990e156c16a774">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"FacebookAI/xlm-roberta-base"</span><span class="p">)</span>

<span class="c1"># test tokenization</span>
<span class="n">tokenized_inputs</span> <span class="o">=</span> <span class="n">tokenize_and_align_labels</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">deprel_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>  <span class="c1"># i </span>
    <span class="n">word_ids</span> <span class="o">=</span> <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">"tokenid_to_wordid"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Example </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="s1">'Token'</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}{</span><span class="s1">'Token ID'</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}{</span><span class="s1">'Head'</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}{</span><span class="s1">'Deprel'</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}{</span><span class="s1">'Word Mapping'</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">token_id</span> <span class="o">=</span> <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
        <span class="n">head</span> <span class="o">=</span> <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">"head"</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
        <span class="n">deprel</span> <span class="o">=</span> <span class="n">all_deprels</span><span class="p">[</span><span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">"deprel_ids"</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]]</span> <span class="k">if</span> <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">"deprel_ids"</span><span class="p">][</span><span class="n">i</span><span class="p">][</span>
                                                                          <span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span> <span class="k">else</span> <span class="s2">"None"</span>
        <span class="n">word_mapping</span> <span class="o">=</span> <span class="n">word_ids</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

        <span class="n">token_str</span> <span class="o">=</span> <span class="n">token</span> <span class="k">if</span> <span class="n">token</span> <span class="k">else</span> <span class="s2">"None"</span>
        <span class="n">token_id</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">token_id</span><span class="p">)</span>
        <span class="n">head_str</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">head</span><span class="p">)</span>
        <span class="n">deprel_str</span> <span class="o">=</span> <span class="n">deprel</span> <span class="k">if</span> <span class="n">deprel</span> <span class="k">else</span> <span class="s2">"None"</span>
        <span class="n">word_mapping_str</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">word_mapping</span><span class="p">)</span> <span class="k">if</span> <span class="n">word_mapping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">"None"</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">token_str</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}{</span><span class="n">token_id</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}{</span><span class="n">head_str</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}{</span><span class="n">deprel_str</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}{</span><span class="n">word_mapping_str</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\WangEntang\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Example 1
Token          Token ID  Head      Deprel         Word Mapping   
&lt;s&gt;            0         -100      None           None           
Al            884       0         root           0              
-             20        1         punct          1              
Zaman         53113     1         flat           2              
:             152       1         punct          3              
American      15672     6         amod           4              
forces        84616     7         nsubj          5              
killed        152388    1         parataxis      6              
Sha           7224      7         obj            7              
ikh            41336     -100      None           7              
Abdullah      34490     8         flat           8              
al            144       8         flat           9              
-             20        8         punct          10             
Ani           32340     8         flat           11             
              6         8         punct          12             
,              4         -100      None           12             
the           70        17        det            13             
prea          19542     8         appos          14             
cher           5372      -100      None           14             
at            99        21        case           15             
the           70        21        det            16             
mos           7304      7         obl            17             
que            944       -100      None           17             
in            23        25        case           18             
the           70        25        det            19             
town          59444     21        nmod           20             
of            111       27        case           21             
Qa            16785     25        nmod           22             
im             464       -100      None           22             
              6         25        punct          23             
,              4         -100      None           23             
near          43573     35        case           24             
the           70        35        det            25             
Syria         51712     35        amod           26             
n              19        -100      None           26             
border        132988    25        nmod           27             
              6         1         punct          28             
.              5         -100      None           28             
&lt;/s&gt;           2         -100      None           None           
Example 2
Token          Token ID  Head      Deprel         Word Mapping   
&lt;s&gt;            0         -100      None           None           
[             378       12        punct          0              
This          3293      3         det            1              
ki            200       12        nsubj          2              
lling          30319     -100      None           2              
of            111       9         case           3              
a             10        9         det            4              
respect       15072     9         amod           5              
ed             297       -100      None           5              
cleric        181186    3         nmod           6              
will          1221      12        aux            7              
be            186       12        aux            8              
causing       216806    0         root           9              
us            1821      12        iobj           10             
trouble       63134     12        obj            11             
for           100       16        case           12             
years         5369      12        obl            13             
to            47        18        mark           14             
come          1380      16        acl            15             
              6         12        punct          16             
.              5         -100      None           16             
]             10114     12        punct          17             
&lt;/s&gt;           2         -100      None           None           
Example 3
Token          Token ID  Head      Deprel         Word Mapping   
&lt;s&gt;            0         -100      None           None           
D             391       0         root           0              
PA             12236     -100      None           0              
:             152       1         punct          1              
Iraq          69496     6         amod           2              
i              14        -100      None           2              
authorities   207048    7         nsubj          3              
announced     171530    1         parataxis      4              
that          450       11        mark           5              
they          1836      11        nsubj          6              
had           1902      11        aux            7              
bu            373       7         ccomp          8              
sted           14437     -100      None           8              
up            1257      11        compound:prt   9              
3             138       16        nummod         10             
terrorist     42998     16        amod           11             
cell          38750     11        obj            12             
s              7         -100      None           12             
operating     172852    16        acl            13             
in            23        20        case           14             
Bagh          177161    18        obl            15             
dad            12409     -100      None           15             
              6         1         punct          16             
.              5         -100      None           16             
&lt;/s&gt;           2         -100      None           None           
Example 4
Token          Token ID  Head      Deprel         Word Mapping   
&lt;s&gt;            0         -100      None           None           
Two           32964     6         nsubj:pass     0              
of            111       3         case           1              
them          2856      1         nmod           2              
were          3542      6         aux            3              
being         8035      6         aux:pass       4              
run           11675     0         root           5              
by            390       9         case           6              
2             116       9         nummod         7              
official      51521     6         obl            8              
s              7         -100      None           8              
of            111       13        case           9              
the           70        13        det            10             
Ministry      185236    9         nmod           11             
of            111       16        case           12             
the           70        16        det            13             
Interior      123024    13        nmod           14             
!             711       6         punct          15             
&lt;/s&gt;           2         -100      None           None           
Example 5
Token          Token ID  Head      Deprel         Word Mapping   
&lt;s&gt;            0         -100      None           None           
The           581       2         det            0              
Mo            2501      7         nsubj          1              
I              568       -100      None           1              
in            23        5         case           2              
Iraq          69496     2         nmod           3              
is            83        7         cop            4              
equivalent    183234    0         root           5              
to            47        11        case           6              
the           70        11        det            7              
US            7082      11        compound       8              
FBI           83085     7         obl            9              
              6         7         punct          10             
,              4         -100      None           10             
so            221       19        advmod         11             
this          903       17        nsubj          12             
would         2806      17        aux            13             
be            186       7         parataxis      14             
like          1884      19        mark           15             
having        19441     17        advcl          16             
J             821       29        nsubj          17             
.              5         -100      None           17             
Edgar         111954    20        flat           18             
Ho            2291      20        flat           19             
over           5465      -100      None           19             
un            51        29        advmod         20             
wit            14817     -100      None           20             
ting           1916      -100      None           20             
ly             538       -100      None           20             
employ        187016    19        ccomp          21             
at            99        33        case           22             
a             10        33        det            23             
high          11192     33        amod           24             
level         17366     29        obl            25             
members       43032     29        obj            26             
of            111       39        case           27             
the           70        39        det            28             
Weather       214526    39        compound       29             
men            1055      -100      None           29             
bomb          54330     34        nmod           30             
ers            1314      -100      None           30             
back          4420      44        advmod         31             
in            23        44        case           32             
the           70        44        det            33             
1960          23936     29        obl            34             
s              7         -100      None           34             
              6         7         punct          35             
.              5         -100      None           35             
&lt;/s&gt;           2         -100      None           None           
Example 6
Token          Token ID  Head      Deprel         Word Mapping   
&lt;s&gt;            0         -100      None           None           
The           581       2         det            0              
third         50960     5         nsubj:pass     1              
was           509       5         aux            2              
being         8035      5         aux:pass       3              
run           11675     0         root           4              
by            390       8         case           5              
the           70        8         det            6              
head          10336     5         obl            7              
of            111       12        case           8              
an            142       12        det            9              
investment    77021     12        compound       10             
firm          11037     8         nmod           11             
              6         5         punct          12             
.              5         -100      None           12             
&lt;/s&gt;           2         -100      None           None           
Example 7
Token          Token ID  Head      Deprel         Word Mapping   
&lt;s&gt;            0         -100      None           None           
You           2583      2         nsubj          0              
wonder        32195     0         root           1              
if            2174      6         mark           2              
he            764       6         nsubj          3              
was           509       6         aux            4              
manipula      45258     2         ccomp          5              
ting           1916      -100      None           5              
the           70        9         det            6              
market        16839     6         obj            7              
with          678       14        case           8              
his           1919      14        nmod:poss      9              
bomb          54330     14        compound       10             
ing            214       -100      None           10             
target        30388     6         obl            11             
s              7         -100      None           11             
              6         2         punct          12             
.              5         -100      None           12             
&lt;/s&gt;           2         -100      None           None           
Example 8
Token          Token ID  Head      Deprel         Word Mapping   
&lt;s&gt;            0         -100      None           None           
The           581       2         det            0              
cell          38750     5         nsubj          1              
s              7         -100      None           1              
were          3542      5         aux            2              
operating     172852    0         root           3              
in            23        15        case           4              
the           70        15        det            5              
Ghazal        220428    15        compound       6              
iyah           62776     -100      None           6              
and           136       13        cc             7              
al            144       13        compound       8              
-             20        13        punct          9              
Ji            3291      8         conj           10             
had            8408      -100      None           10             
district      103724    5         obl            11             
s              7         -100      None           11             
of            111       19        case           12             
the           70        19        det            13             
capital       10323     15        nmod           14             
              6         5         punct          15             
.              5         -100      None           15             
&lt;/s&gt;           2         -100      None           None           
Example 9
Token          Token ID  Head      Deprel         Word Mapping   
&lt;s&gt;            0         -100      None           None           
Although      106073    8         mark           0              
the           70        3         det            1              
ann           3398      8         nsubj:pass     2              
ounce          85018     -100      None           2              
ment           674       -100      None           2              
was           509       8         aux:pass       3              
probably      31895     8         advmod         4              
made          7228      28        advcl          5              
to            47        10        mark           6              
show          7639      8         xcomp          7              
progress      42658     10        obj            8              
in            23        13        mark           9              
identify      135812    11        acl            10             
ing            214       -100      None           10             
and           136       16        cc             11             
breaking      116987    13        conj           12             
up            1257      16        compound:prt   13             
terror        20870     19        compound       14             
cell          38750     13        obj            15             
s              7         -100      None           15             
              6         28        punct          16             
,              4         -100      None           16             
I             87        28        nsubj          17             
do            54        28        aux            18             
n             653       28        advmod         19             
'              25        -100      None           19             
t              18        -100      None           19             
find          7413      0         root           20             
the           70        30        det            21             
news          7123      28        obj            22             
that          450       36        mark           23             
the           70        33        det            24             
Baat          160062    36        nsubj          25             
hist           49063     -100      None           25             
s              7         -100      None           25             
continue      21342     30        acl            26             
to            47        38        mark           27             
penetra       75076     36        xcomp          28             
te             67        -100      None           28             
the           70        43        det            29             
Iraq          69496     43        amod           30             
i              14        -100      None           30             
government    27759     38        obj            31             
very          4552      45        advmod         32             
hope          15673     28        xcomp          33             
ful            7844      -100      None           33             
              6         28        punct          34             
.              5         -100      None           34             
&lt;/s&gt;           2         -100      None           None           
Example 10
Token          Token ID  Head      Deprel         Word Mapping   
&lt;s&gt;            0         -100      None           None           
It            1650      2         nsubj          0              
remind        98911     0         root           1              
s              7         -100      None           1              
me            163       2         obj            2              
too           5792      6         advmod         3              
much          5045      2         advmod         4              
of            111       11        case           5              
the           70        11        det            6              
AR            13685     11        compound       7              
VN             40711     -100      None           7              
officer       93324     2         obl            8              
s              7         -100      None           8              
who           2750      17        nsubj          9              
were          3542      17        aux            10             
secret        23410     17        advmod         11             
ly             538       -100      None           11             
working       20697     11        acl:relcl      12             
for           100       21        case           13             
the           70        21        det            14             
other         3789      21        amod           15             
side          5609      17        obl            16             
in            23        23        case           17             
Vietnam       39272     17        obl            18             
              6         2         punct          19             
.              5         -100      None           19             
&lt;/s&gt;           2         -100      None           None           
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=9bb1acfa">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="c1"># tokenized dataset and construct dataloader</span>
<span class="n">train_tokenized_inputs</span> <span class="o">=</span> <span class="n">tokenize_and_align_labels</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">[:],</span> <span class="n">deprel_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="c1"># Convert BatchEncoding to Dataset</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">train_tokenized_inputs</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">train_dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'torch'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">,</span> <span class="s1">'attention_mask'</span><span class="p">,</span> <span class="s1">'head'</span><span class="p">,</span> <span class="s1">'deprel_ids'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>

<span class="n">valid_tokenized_inputs</span> <span class="o">=</span> <span class="n">tokenize_and_align_labels</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">[:],</span> <span class="n">deprel_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">valid_tokenized_inputs</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">valid_dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'torch'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">,</span> <span class="s1">'attention_mask'</span><span class="p">,</span> <span class="s1">'head'</span><span class="p">,</span> <span class="s1">'deprel_ids'</span><span class="p">])</span>

<span class="n">test_tokenized_inputs</span> <span class="o">=</span> <span class="n">tokenize_and_align_labels</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">[:],</span> <span class="n">deprel_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">test_tokenized_inputs</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">test_dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'torch'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">,</span> <span class="s1">'attention_mask'</span><span class="p">,</span> <span class="s1">'head'</span><span class="p">,</span> <span class="s1">'deprel_ids'</span><span class="p">])</span>

<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">valid_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Dataset({
    features: ['input_ids', 'attention_mask', 'head', 'deprel_ids', 'tokens_representing_words', 'num_words', 'tokenid_to_wordid'],
    num_rows: 12543
})
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[7]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>{'input_ids': tensor([[    0,   884,    20,  ...,     1,     1,     1],
         [    0,   378,  3293,  ...,     1,     1,     1],
         [    0,   391, 12236,  ...,     1,     1,     1],
         ...,
         [    0,   581,   262,  ...,     1,     1,     1],
         [    0, 56645, 14508,  ...,     1,     1,     1],
         [    0,  1529,    83,  ...,     1,     1,     1]]),
 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0],
         ...,
         [1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0]]),
 'head': tensor([[-100,    0,    1,  ..., -100, -100, -100],
         [-100,   12,    3,  ..., -100, -100, -100],
         [-100,    0, -100,  ..., -100, -100, -100],
         ...,
         [-100,    6,    6,  ..., -100, -100, -100],
         [-100,    7, -100,  ..., -100, -100, -100],
         [-100,   10,   10,  ..., -100, -100, -100]]),
 'deprel_ids': tensor([[-100,   63,   61,  ..., -100, -100, -100],
         [-100,   61,   27,  ..., -100, -100, -100],
         [-100,   63, -100,  ..., -100, -100, -100],
         ...,
         [-100,   27,   16,  ..., -100, -100, -100],
         [-100,   50, -100,  ..., -100, -100, -100],
         [-100,   48,   22,  ..., -100, -100, -100]])}</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d90ff586065664ae">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DependencyParserModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>  <span class="c1"># xlm-roberta-base hidden size</span>
                 <span class="n">edge_mlp_dim</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>  <span class="c1"># Dozat&amp;Manning recommend 500</span>
                 <span class="n">label_mlp_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># Dozat&amp;Manning recommend 100</span>
                 <span class="n">num_labels</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">all_deprels</span><span class="p">),</span>  <span class="c1"># number of dependency labels</span>
                 <span class="n">edge_predicting</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">label_predicting</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.33</span>
                 <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">edge_predicting</span> <span class="o">=</span> <span class="n">edge_predicting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_predicting</span> <span class="o">=</span> <span class="n">label_predicting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">num_labels</span>

        <span class="c1"># load pre-trained XLM-RoBERTa model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">XLMRobertaModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"xlm-roberta-base"</span><span class="p">)</span>

        <span class="c1"># freeze RoBERTa parameters</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># define MLP for edge head and dependency projections</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_predicting</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp_head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">edge_mlp_dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp_dep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">edge_mlp_dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># define U1 and u2 for edge scoring</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">U1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">edge_mlp_dim</span><span class="p">,</span> <span class="n">edge_mlp_dim</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">u2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">edge_mlp_dim</span><span class="p">))</span>
            <span class="c1"># apply Xavier initialization</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">U1</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">u2</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">u2</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Extra: Edge labels predicting</span>
        <span class="c1"># define MLP for label head and dependency projections</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_predicting</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">label_mlp_head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">label_mlp_dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">label_mlp_dep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">label_mlp_dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># define parameters for label scoring</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">U_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">label_mlp_dim</span><span class="p">,</span> <span class="n">label_mlp_dim</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W1_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">label_mlp_dim</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W2_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">label_mlp_dim</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_labels</span><span class="p">))</span>

            <span class="c1"># apply Xavier initialization</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">U_label</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W1_label</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W2_label</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_label</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b_label</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        returns:</span>
<span class="sd">          H_head (edge MLP): [batch_size, seq_len, edge_mlp_dim]</span>
<span class="sd">          H_dep (edge MLP):  [batch_size, seq_len, edge_mlp_dim]</span>
<span class="sd">          L_head (label MLP): [batch_size, seq_len, label_mlp_dim]</span>
<span class="sd">          L_dep (label MLP):  [batch_size, seq_len, label_mlp_dim]</span>
<span class="sd">        """</span>
        <span class="c1"># initialize as None</span>
        <span class="n">H_head</span> <span class="o">=</span> <span class="n">H_dep</span> <span class="o">=</span> <span class="n">L_head</span> <span class="o">=</span> <span class="n">L_dep</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="c1"># Shape: (batch_size, seq_length, hidden_size: 768)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_predicting</span><span class="p">:</span>
            <span class="c1"># edge MLP projections</span>
            <span class="n">H_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>
            <span class="n">H_dep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp_dep</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_predicting</span><span class="p">:</span>
            <span class="c1"># label MLP projections</span>
            <span class="n">L_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_mlp_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>
            <span class="n">L_dep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_mlp_dep</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="n">L_dep</span>

    <span class="k">def</span> <span class="nf">score_edges</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">):</span>
        <span class="c1"># score[i, j] = H_head[i] * U1 * H_dep[j].T + H_head[i] * u2</span>
        <span class="c1"># b: batch_size, s: seq_len, d: edge_mlp_dim</span>
        <span class="n">H_head_U1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bsd,dd-&gt;bsd"</span><span class="p">,</span> <span class="n">H_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U1</span><span class="p">)</span>
        <span class="n">H_head_U1_H_dep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bim,bjm-&gt;bij"</span><span class="p">,</span> <span class="n">H_head_U1</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">)</span>
        <span class="n">H_head_u2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bid,d-&gt;bi"</span><span class="p">,</span> <span class="n">H_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">u2</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len)</span>

        <span class="c1"># Shape: (batch_size, seq_len, seq_len) + (batch_size, seq_len, 1) broadcasting</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">H_head_U1_H_dep</span> <span class="o">+</span> <span class="n">H_head_u2</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span>

    <span class="k">def</span> <span class="nf">score_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="n">L_dep</span><span class="p">):</span>
        <span class="c1"># compute biaffine term: x1 U x2^T</span>
        <span class="n">L_head_U_L_dep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bid,dd,bjd-&gt;bij"</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U_label</span><span class="p">,</span>
                                      <span class="n">L_dep</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, seq_len)</span>

        <span class="c1"># compute linear terms: W1 x1 and W2 x2</span>
        <span class="n">W1_L_head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bid,dn-&gt;bin"</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1_label</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, num_labels)</span>
        <span class="n">W2_L_dep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bid,dn-&gt;bin"</span><span class="p">,</span> <span class="n">L_dep</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2_label</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, num_labels)</span>

        <span class="c1"># add bias term</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_label</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Shape: (1, 1, 1, num_labels)</span>

        <span class="c1"># combine all terms and expand dimensions</span>
        <span class="c1"># Shape: (batch_size, seq_len, seq_len, num_labels)</span>
        <span class="n">label_scores</span> <span class="o">=</span> <span class="n">L_head_U_L_dep</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">W1_L_head</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">W2_L_dep</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>

        <span class="k">return</span> <span class="n">label_scores</span>


<span class="k">class</span> <span class="nc">DependencyParserProModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>  <span class="c1"># xlm-roberta-base hidden size</span>
                 <span class="n">edge_mlp_dim</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>  <span class="c1"># Dozat&amp;Manning recommend 500</span>
                 <span class="n">label_mlp_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># Dozat&amp;Manning recommend 100</span>
                 <span class="n">num_labels</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">all_deprels</span><span class="p">),</span>  <span class="c1"># number of dependency labels</span>
                 <span class="n">edge_predicting</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">label_predicting</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.33</span>
                 <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">edge_predicting</span> <span class="o">=</span> <span class="n">edge_predicting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_predicting</span> <span class="o">=</span> <span class="n">label_predicting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">num_labels</span>

        <span class="c1"># load pre-trained XLM-RoBERTa model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">XLMRobertaModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"xlm-roberta-base"</span><span class="p">)</span>

        <span class="c1"># freeze RoBERTa parameters</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># define MLP for edge head and dependency projections</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_predicting</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp1_head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">edge_mlp_dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp1_dep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">edge_mlp_dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp2_head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">edge_mlp_dim</span><span class="p">,</span> <span class="n">edge_mlp_dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp2_dep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">edge_mlp_dim</span><span class="p">,</span> <span class="n">edge_mlp_dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># define U1 and u2 for edge scoring</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">U1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">edge_mlp_dim</span><span class="p">,</span> <span class="n">edge_mlp_dim</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">u2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">edge_mlp_dim</span><span class="p">))</span>
            <span class="c1"># apply Xavier initialization</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">U1</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">u2</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">u2</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Extra: Edge labels predicting</span>
        <span class="c1"># define MLP for label head and dependency projections</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_predicting</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">label_mlp_head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">label_mlp_dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">label_mlp_dep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">label_mlp_dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># define parameters for label scoring</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">U_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">label_mlp_dim</span><span class="p">,</span> <span class="n">label_mlp_dim</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W1_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">label_mlp_dim</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W2_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">label_mlp_dim</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_labels</span><span class="p">))</span>

            <span class="c1"># apply Xavier initialization</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">U_label</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W1_label</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W2_label</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b_label</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b_label</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        returns:</span>
<span class="sd">          H_head (edge MLP): [batch_size, seq_len, edge_mlp_dim]</span>
<span class="sd">          H_dep (edge MLP):  [batch_size, seq_len, edge_mlp_dim]</span>
<span class="sd">          L_head (label MLP): [batch_size, seq_len, label_mlp_dim]</span>
<span class="sd">          L_dep (label MLP):  [batch_size, seq_len, label_mlp_dim]</span>
<span class="sd">        """</span>
        <span class="c1"># initialize as None</span>
        <span class="n">H_head</span> <span class="o">=</span> <span class="n">H_dep</span> <span class="o">=</span> <span class="n">L_head</span> <span class="o">=</span> <span class="n">L_dep</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="c1"># Shape: (batch_size, seq_length, hidden_size: 768)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_predicting</span><span class="p">:</span>
            <span class="c1"># edge MLP projections</span>
            <span class="n">H_head_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp1_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>
            <span class="n">H_dep_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp1_dep</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>

            <span class="c1"># residual connection</span>
            <span class="n">H_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp2_head</span><span class="p">(</span><span class="n">H_head_1</span><span class="p">)</span> <span class="o">+</span> <span class="n">H_head_1</span>
            <span class="n">H_dep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_mlp2_dep</span><span class="p">(</span><span class="n">H_dep_1</span><span class="p">)</span> <span class="o">+</span> <span class="n">H_dep_1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_predicting</span><span class="p">:</span>
            <span class="c1"># label MLP projections</span>
            <span class="n">L_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_mlp_head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>
            <span class="n">L_dep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_mlp_dep</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="n">L_dep</span>

    <span class="k">def</span> <span class="nf">score_edges</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">):</span>
        <span class="c1"># score[i, j] = H_head[i] * U1 * H_dep[j].T + H_head[i] * u2</span>
        <span class="c1"># b: batch_size, s: seq_len, d: edge_mlp_dim</span>
        <span class="n">H_head_U1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bsd,dd-&gt;bsd"</span><span class="p">,</span> <span class="n">H_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U1</span><span class="p">)</span>
        <span class="n">H_head_U1_H_dep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bim,bjm-&gt;bij"</span><span class="p">,</span> <span class="n">H_head_U1</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">)</span>
        <span class="n">H_head_u2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bid,d-&gt;bi"</span><span class="p">,</span> <span class="n">H_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">u2</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len)</span>

        <span class="c1"># Shape: (batch_size, seq_len, seq_len) + (batch_size, seq_len, 1) broadcasting</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">H_head_U1_H_dep</span> <span class="o">+</span> <span class="n">H_head_u2</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span>

    <span class="k">def</span> <span class="nf">score_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="n">L_dep</span><span class="p">):</span>
        <span class="c1"># compute biaffine term: x1 U x2^T</span>
        <span class="n">L_head_U_L_dep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bid,dd,bjd-&gt;bij"</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">U_label</span><span class="p">,</span>
                                      <span class="n">L_dep</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, seq_len)</span>

        <span class="c1"># compute linear terms: W1 x1 and W2 x2</span>
        <span class="n">W1_L_head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bid,dn-&gt;bin"</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1_label</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, num_labels)</span>
        <span class="n">W2_L_dep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bid,dn-&gt;bin"</span><span class="p">,</span> <span class="n">L_dep</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2_label</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, num_labels)</span>

        <span class="c1"># add bias term</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_label</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Shape: (1, 1, 1, num_labels)</span>

        <span class="c1"># combine all terms and expand dimensions</span>
        <span class="n">label_scores</span> <span class="o">=</span> <span class="n">L_head_U_L_dep</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">W1_L_head</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">W2_L_dep</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
            <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>  <span class="c1"># Shape: (batch_size, seq_len, seq_len, num_labels)</span>

        <span class="k">return</span> <span class="n">label_scores</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c0a34ab69953ac7f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">wandb</span>
<span class="kn">from</span> <span class="nn">ufal.chu_liu_edmonds</span> <span class="kn">import</span> <span class="n">chu_liu_edmonds</span>


<span class="c1"># train the model</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">valid_dataloader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-3</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># Initialize wandb</span>
    <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s2">"dependency-parsing"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"dependency-parsing"</span><span class="p">,</span> <span class="n">resume</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"learning_rate"</span><span class="p">:</span> <span class="n">lr</span><span class="p">,</span>
        <span class="s2">"weight_decay"</span><span class="p">:</span> <span class="n">weight_decay</span><span class="p">,</span>
        <span class="s2">"num_epochs"</span><span class="p">:</span> <span class="n">num_epochs</span><span class="p">,</span>
        <span class="s2">"alpha"</span><span class="p">:</span> <span class="n">alpha</span><span class="p">,</span>
        <span class="s2">"beta"</span><span class="p">:</span> <span class="n">beta</span>
    <span class="p">})</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>

    <span class="c1"># ignore the padding tokens</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>

    <span class="n">valid_dataset</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">valid_dataloader</span><span class="p">))</span>

    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"batch num"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">head</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"head"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Extra: Train edge labels predicting</span>
            <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">label_predicting</span><span class="p">:</span>
                <span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"deprel_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="n">L_dep</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

            <span class="n">edge_scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score_edges</span><span class="p">(</span><span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, seq_len)</span>

            <span class="c1"># Extra: Train edge labels predicting</span>
            <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">label_predicting</span><span class="p">:</span>
                <span class="n">label_scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score_labels</span><span class="p">(</span><span class="n">L_head</span><span class="p">,</span> <span class="n">L_dep</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, seq_len, num_labels)</span>

            <span class="n">loss_edge</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span>
                <span class="n">edge_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">edge_scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span>  <span class="c1"># Shape: (batch_size * seq_len, seq_len)</span>
                <span class="n">head</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len)</span>
            <span class="p">)</span>

            <span class="c1"># Extra: Train edge labels predicting</span>
            <span class="n">loss_label</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">label_predicting</span><span class="p">:</span>
                <span class="c1"># Shape: (batch_size, seq_len, seq_len, num_labels)</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">num_labels</span> <span class="o">=</span> <span class="n">label_scores</span><span class="o">.</span><span class="n">shape</span>

                <span class="c1"># batch_indices: (batch_size, 1)</span>
                <span class="c1"># dep_indices:   (1,   n)</span>
                <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># batch index</span>
                <span class="n">dep_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># dep index</span>

                <span class="c1"># gather label_scores according to the gold head</span>
                <span class="c1"># Shape: (batch_size, seq_len, num_labels)</span>
                <span class="n">label_scores_for_gold_edge</span> <span class="o">=</span> <span class="n">label_scores</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">dep_indices</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="p">:]</span>

                <span class="n">loss_label</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">label_scores_for_gold_edge</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">),</span> <span class="n">label</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">loss_edge</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">loss_label</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># log the loss curve</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s2">"train_loss"</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>

            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># verify the model</span>
                <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="n">valid_dataset</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">mask</span> <span class="o">=</span> <span class="n">valid_dataset</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">head</span> <span class="o">=</span> <span class="n">valid_dataset</span><span class="p">[</span><span class="s2">"head"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                    <span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="n">L_dep</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
                    <span class="n">edge_scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score_edges</span><span class="p">(</span><span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">)</span>

                    <span class="n">valid_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">edge_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">edge_scores</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">head</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Valid Loss: </span><span class="si">{</span><span class="n">valid_loss</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                    <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s2">"valid_loss"</span><span class="p">:</span> <span class="n">valid_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>
                <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="c1"># Evaluate Head prediction</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">hta_total_count</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># head tagging accuracy</span>
        <span class="n">hta_total_correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">uas_total_count</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># unlabeled attachment score</span>
        <span class="n">uas_total_correct</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">valid_dataloader</span><span class="p">):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">head</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"head"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="n">L_dep</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
                <span class="n">edge_scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score_edges</span><span class="p">(</span><span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, seq_len)</span>

                <span class="c1"># best score heads</span>
                <span class="n">predicted_heads</span> <span class="o">=</span> <span class="n">edge_scores</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len)</span>

                <span class="c1"># construct valid mask</span>
                <span class="n">valid_positions</span> <span class="o">=</span> <span class="p">(</span><span class="n">head</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len)</span>

                <span class="c1"># count accurate predictions</span>
                <span class="n">hta_total_correct</span> <span class="o">+=</span> <span class="p">((</span><span class="n">predicted_heads</span> <span class="o">==</span> <span class="n">head</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">valid_positions</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">hta_total_count</span> <span class="o">+=</span> <span class="n">valid_positions</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

                <span class="c1"># MST parsing UAS evaluation</span>
                <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">edge_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="c1"># apply chu_liu_edmons</span>
                <span class="n">uas_predictions</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">sent_scores</span> <span class="ow">in</span> <span class="n">log_probs</span><span class="p">:</span>
                    <span class="c1"># convert to numpy</span>
                    <span class="n">scores_np</span> <span class="o">=</span> <span class="n">sent_scores</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                    <span class="n">mst_heads</span> <span class="o">=</span> <span class="n">chu_liu_edmonds</span><span class="p">(</span><span class="n">scores_np</span><span class="p">)</span>
                    <span class="n">uas_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mst_heads</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

                <span class="c1"># convert predictions and gold heads into tensors</span>
                <span class="n">uas_predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">uas_predictions</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len)</span>

                <span class="c1"># calculate correct predictions and total count</span>
                <span class="n">uas_total_correct</span> <span class="o">+=</span> <span class="p">((</span><span class="n">uas_predictions</span> <span class="o">==</span> <span class="n">head</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">valid_positions</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">uas_total_count</span> <span class="o">+=</span> <span class="n">valid_positions</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s2">"head_tagging_accuracy"</span><span class="p">:</span> <span class="n">hta_total_correct</span> <span class="o">/</span> <span class="n">hta_total_count</span><span class="p">})</span>
        <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s2">"unlabeled_attachment_score"</span><span class="p">:</span> <span class="n">uas_total_correct</span> <span class="o">/</span> <span class="n">uas_total_count</span><span class="p">})</span>

    <span class="n">wandb</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>
    <span class="c1"># torch.save(model.state_dict(), "dependency_parser.pth")</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=d296b44d6de0f75f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DependencyParserModel</span><span class="p">()</span>

<span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">valid_dataloader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">6e-4</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\WangEntang\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
C:\Users\WangEntang\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
wandb: Currently logged in as: maxewang10 (maxewang10-saarland-informatics-campus). Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Tracking run with wandb version 0.19.2
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Run data is saved locally in <code>C:\Users\WangEntang\Desktop\code\CL\a5\wandb\run-20250113_214744-iidfj4nz</code>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Syncing run <strong><a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/iidfj4nz" target="_blank">dependency-parsing</a></strong> to <a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/developer-guide" target="_blank">docs</a>)<br/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View project at <a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing" target="_blank">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View run at <a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/iidfj4nz" target="_blank">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/iidfj4nz</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>batch num 392
Epoch 0, Iteration 0, Loss: 5.293313026428223
Epoch 0, Iteration 1, Loss: 4.938961505889893
Epoch 0, Iteration 2, Loss: 4.577451705932617
Epoch 0, Iteration 3, Loss: 4.3553547859191895
Epoch 0, Iteration 4, Loss: 3.9377236366271973
Epoch 0, Iteration 5, Loss: 3.726020336151123
Epoch 0, Iteration 6, Loss: 3.594496011734009
Epoch 0, Iteration 7, Loss: 3.61702299118042
Epoch 0, Iteration 8, Loss: 3.552978515625
Epoch 0, Iteration 9, Loss: 3.5524919033050537
Epoch 0, Iteration 10, Loss: 3.2083616256713867
Epoch 0, Iteration 11, Loss: 2.9022696018218994
Epoch 0, Iteration 12, Loss: 3.012794256210327
Epoch 0, Iteration 13, Loss: 3.300163984298706
Epoch 0, Iteration 14, Loss: 3.0769410133361816
Epoch 0, Iteration 15, Loss: 3.1394999027252197
Epoch 0, Iteration 16, Loss: 2.8843491077423096
Epoch 0, Iteration 17, Loss: 3.166003465652466
Epoch 0, Iteration 18, Loss: 3.0560004711151123
Epoch 0, Iteration 19, Loss: 3.019998073577881
Epoch 0, Iteration 20, Loss: 3.076923370361328
Epoch 0, Iteration 21, Loss: 3.0805788040161133
Epoch 0, Iteration 22, Loss: 2.878321409225464
Epoch 0, Iteration 23, Loss: 2.835618019104004
Epoch 0, Iteration 24, Loss: 2.759072780609131
Epoch 0, Iteration 25, Loss: 2.7681632041931152
Epoch 0, Iteration 26, Loss: 3.0613701343536377
Epoch 0, Iteration 27, Loss: 2.777494430541992
Epoch 0, Iteration 28, Loss: 2.748018264770508
Epoch 0, Iteration 29, Loss: 2.8149659633636475
Epoch 0, Iteration 30, Loss: 2.715350389480591
Epoch 0, Iteration 31, Loss: 2.684236764907837
Epoch 0, Iteration 32, Loss: 2.560957193374634
Epoch 0, Iteration 33, Loss: 2.5528969764709473
Epoch 0, Iteration 34, Loss: 2.7786710262298584
Epoch 0, Iteration 35, Loss: 2.8638644218444824
Epoch 0, Iteration 36, Loss: 2.522407293319702
Epoch 0, Iteration 37, Loss: 2.5844948291778564
Epoch 0, Iteration 38, Loss: 2.6900365352630615
Epoch 0, Iteration 39, Loss: 2.6801321506500244
Epoch 0, Iteration 40, Loss: 2.8056252002716064
Epoch 0, Iteration 41, Loss: 2.721137762069702
Epoch 0, Iteration 42, Loss: 2.944398880004883
Epoch 0, Iteration 43, Loss: 2.771833896636963
Epoch 0, Iteration 44, Loss: 2.8429129123687744
Epoch 0, Iteration 45, Loss: 2.5934054851531982
Epoch 0, Iteration 46, Loss: 2.8014652729034424
Epoch 0, Iteration 47, Loss: 2.780602216720581
Epoch 0, Iteration 48, Loss: 2.7777340412139893
Epoch 0, Iteration 49, Loss: 2.710336923599243
Epoch 0, Iteration 50, Loss: 2.6653003692626953
Epoch 0, Iteration 50, Valid Loss: 2.6064534187316895
Epoch 0, Iteration 51, Loss: 2.56868314743042
Epoch 0, Iteration 52, Loss: 2.9699039459228516
Epoch 0, Iteration 53, Loss: 2.4082601070404053
Epoch 0, Iteration 54, Loss: 2.6699960231781006
Epoch 0, Iteration 55, Loss: 2.5597100257873535
Epoch 0, Iteration 56, Loss: 2.3730883598327637
Epoch 0, Iteration 57, Loss: 2.356968879699707
Epoch 0, Iteration 58, Loss: 2.291525363922119
Epoch 0, Iteration 59, Loss: 2.644200325012207
Epoch 0, Iteration 60, Loss: 2.433483362197876
Epoch 0, Iteration 61, Loss: 2.284102439880371
Epoch 0, Iteration 62, Loss: 2.3244693279266357
Epoch 0, Iteration 63, Loss: 2.4965860843658447
Epoch 0, Iteration 64, Loss: 2.280301809310913
Epoch 0, Iteration 65, Loss: 2.5786690711975098
Epoch 0, Iteration 66, Loss: 2.6808342933654785
Epoch 0, Iteration 67, Loss: 2.5278093814849854
Epoch 0, Iteration 68, Loss: 2.424565553665161
Epoch 0, Iteration 69, Loss: 2.439147710800171
Epoch 0, Iteration 70, Loss: 2.6384284496307373
Epoch 0, Iteration 71, Loss: 2.297332763671875
Epoch 0, Iteration 72, Loss: 2.4015448093414307
Epoch 0, Iteration 73, Loss: 2.2262609004974365
Epoch 0, Iteration 74, Loss: 2.5135462284088135
Epoch 0, Iteration 75, Loss: 2.2301666736602783
Epoch 0, Iteration 76, Loss: 2.3663644790649414
Epoch 0, Iteration 77, Loss: 2.249677896499634
Epoch 0, Iteration 78, Loss: 2.833789110183716
Epoch 0, Iteration 79, Loss: 2.916585683822632
Epoch 0, Iteration 80, Loss: 2.23276424407959
Epoch 0, Iteration 81, Loss: 2.4093916416168213
Epoch 0, Iteration 82, Loss: 2.296265125274658
Epoch 0, Iteration 83, Loss: 2.32222580909729
Epoch 0, Iteration 84, Loss: 2.4565646648406982
Epoch 0, Iteration 85, Loss: 2.5128583908081055
Epoch 0, Iteration 86, Loss: 2.2095859050750732
Epoch 0, Iteration 87, Loss: 2.32112979888916
Epoch 0, Iteration 88, Loss: 2.2472739219665527
Epoch 0, Iteration 89, Loss: 2.3849129676818848
Epoch 0, Iteration 90, Loss: 2.256828546524048
Epoch 0, Iteration 91, Loss: 2.211437702178955
Epoch 0, Iteration 92, Loss: 2.269170045852661
Epoch 0, Iteration 93, Loss: 2.2764031887054443
Epoch 0, Iteration 94, Loss: 2.703160524368286
Epoch 0, Iteration 95, Loss: 2.5071089267730713
Epoch 0, Iteration 96, Loss: 2.5417723655700684
Epoch 0, Iteration 97, Loss: 2.324132204055786
Epoch 0, Iteration 98, Loss: 2.4947903156280518
Epoch 0, Iteration 99, Loss: 2.3670740127563477
Epoch 0, Iteration 100, Loss: 2.3446226119995117
Epoch 0, Iteration 100, Valid Loss: 2.5514488220214844
Epoch 0, Iteration 101, Loss: 2.340118885040283
Epoch 0, Iteration 102, Loss: 2.481701612472534
Epoch 0, Iteration 103, Loss: 2.323291778564453
Epoch 0, Iteration 104, Loss: 2.7117552757263184
Epoch 0, Iteration 105, Loss: 2.483217716217041
Epoch 0, Iteration 106, Loss: 2.4380130767822266
Epoch 0, Iteration 107, Loss: 2.6181576251983643
Epoch 0, Iteration 108, Loss: 2.4059832096099854
Epoch 0, Iteration 109, Loss: 2.5278470516204834
Epoch 0, Iteration 110, Loss: 2.666189432144165
Epoch 0, Iteration 111, Loss: 2.413231134414673
Epoch 0, Iteration 112, Loss: 2.497483968734741
Epoch 0, Iteration 113, Loss: 2.3996026515960693
Epoch 0, Iteration 114, Loss: 2.7359955310821533
Epoch 0, Iteration 115, Loss: 2.7405648231506348
Epoch 0, Iteration 116, Loss: 2.514901876449585
Epoch 0, Iteration 117, Loss: 2.4029345512390137
Epoch 0, Iteration 118, Loss: 2.662109613418579
Epoch 0, Iteration 119, Loss: 2.6180202960968018
Epoch 0, Iteration 120, Loss: 2.2862513065338135
Epoch 0, Iteration 121, Loss: 2.3711495399475098
Epoch 0, Iteration 122, Loss: 2.290818452835083
Epoch 0, Iteration 123, Loss: 2.287122964859009
Epoch 0, Iteration 124, Loss: 2.5292773246765137
Epoch 0, Iteration 125, Loss: 2.092794418334961
Epoch 0, Iteration 126, Loss: 2.256470203399658
Epoch 0, Iteration 127, Loss: 2.3715367317199707
Epoch 0, Iteration 128, Loss: 2.160008668899536
Epoch 0, Iteration 129, Loss: 2.330143451690674
Epoch 0, Iteration 130, Loss: 2.4970309734344482
Epoch 0, Iteration 131, Loss: 2.4479825496673584
Epoch 0, Iteration 132, Loss: 2.2158830165863037
Epoch 0, Iteration 133, Loss: 2.2479283809661865
Epoch 0, Iteration 134, Loss: 2.0564258098602295
Epoch 0, Iteration 135, Loss: 2.0638434886932373
Epoch 0, Iteration 136, Loss: 2.270381212234497
Epoch 0, Iteration 137, Loss: 2.2318074703216553
Epoch 0, Iteration 138, Loss: 2.074496030807495
Epoch 0, Iteration 139, Loss: 2.0984320640563965
Epoch 0, Iteration 140, Loss: 2.2577767372131348
Epoch 0, Iteration 141, Loss: 2.3767600059509277
Epoch 0, Iteration 142, Loss: 2.1324074268341064
Epoch 0, Iteration 143, Loss: 2.453594207763672
Epoch 0, Iteration 144, Loss: 2.2677459716796875
Epoch 0, Iteration 145, Loss: 2.2690703868865967
Epoch 0, Iteration 146, Loss: 2.361482620239258
Epoch 0, Iteration 147, Loss: 2.280294418334961
Epoch 0, Iteration 148, Loss: 2.1133742332458496
Epoch 0, Iteration 149, Loss: 2.1369235515594482
Epoch 0, Iteration 150, Loss: 2.172049045562744
Epoch 0, Iteration 150, Valid Loss: 2.4276912212371826
Epoch 0, Iteration 151, Loss: 2.499953031539917
Epoch 0, Iteration 152, Loss: 2.3839824199676514
Epoch 0, Iteration 153, Loss: 1.9656903743743896
Epoch 0, Iteration 154, Loss: 2.194467306137085
Epoch 0, Iteration 155, Loss: 2.2880144119262695
Epoch 0, Iteration 156, Loss: 2.23624587059021
Epoch 0, Iteration 157, Loss: 2.591784715652466
Epoch 0, Iteration 158, Loss: 2.387669563293457
Epoch 0, Iteration 159, Loss: 2.527704954147339
Epoch 0, Iteration 160, Loss: 2.3544490337371826
Epoch 0, Iteration 161, Loss: 2.293950319290161
Epoch 0, Iteration 162, Loss: 2.486067056655884
Epoch 0, Iteration 163, Loss: 2.169045925140381
Epoch 0, Iteration 164, Loss: 1.7828142642974854
Epoch 0, Iteration 165, Loss: 2.172534704208374
Epoch 0, Iteration 166, Loss: 2.2877860069274902
Epoch 0, Iteration 167, Loss: 2.4565634727478027
Epoch 0, Iteration 168, Loss: 2.5085878372192383
Epoch 0, Iteration 169, Loss: 2.3206441402435303
Epoch 0, Iteration 170, Loss: 2.2841811180114746
Epoch 0, Iteration 171, Loss: 2.1481971740722656
Epoch 0, Iteration 172, Loss: 2.0225608348846436
Epoch 0, Iteration 173, Loss: 2.3883721828460693
Epoch 0, Iteration 174, Loss: 2.3886897563934326
Epoch 0, Iteration 175, Loss: 2.366122245788574
Epoch 0, Iteration 176, Loss: 1.949499487876892
Epoch 0, Iteration 177, Loss: 2.464360237121582
Epoch 0, Iteration 178, Loss: 2.314805269241333
Epoch 0, Iteration 179, Loss: 2.3957464694976807
Epoch 0, Iteration 180, Loss: 2.2168140411376953
Epoch 0, Iteration 181, Loss: 2.3578686714172363
Epoch 0, Iteration 182, Loss: 2.338355302810669
Epoch 0, Iteration 183, Loss: 2.137802839279175
Epoch 0, Iteration 184, Loss: 2.0827293395996094
Epoch 0, Iteration 185, Loss: 2.3388919830322266
Epoch 0, Iteration 186, Loss: 2.181838035583496
Epoch 0, Iteration 187, Loss: 2.0503289699554443
Epoch 0, Iteration 188, Loss: 2.3367087841033936
Epoch 0, Iteration 189, Loss: 2.261646032333374
Epoch 0, Iteration 190, Loss: 2.6076512336730957
Epoch 0, Iteration 191, Loss: 2.397829294204712
Epoch 0, Iteration 192, Loss: 2.3361001014709473
Epoch 0, Iteration 193, Loss: 2.1881871223449707
Epoch 0, Iteration 194, Loss: 2.1382577419281006
Epoch 0, Iteration 195, Loss: 1.918623447418213
Epoch 0, Iteration 196, Loss: 2.2002017498016357
Epoch 0, Iteration 197, Loss: 2.1321189403533936
Epoch 0, Iteration 198, Loss: 2.180223226547241
Epoch 0, Iteration 199, Loss: 2.3499999046325684
Epoch 0, Iteration 200, Loss: 2.1889548301696777
Epoch 0, Iteration 200, Valid Loss: 2.1419835090637207
Epoch 0, Iteration 201, Loss: 2.205228328704834
Epoch 0, Iteration 202, Loss: 2.232635974884033
Epoch 0, Iteration 203, Loss: 2.0964927673339844
Epoch 0, Iteration 204, Loss: 2.189911127090454
Epoch 0, Iteration 205, Loss: 1.9676791429519653
Epoch 0, Iteration 206, Loss: 1.994202971458435
Epoch 0, Iteration 207, Loss: 2.1347320079803467
Epoch 0, Iteration 208, Loss: 2.4803714752197266
Epoch 0, Iteration 209, Loss: 2.442138671875
Epoch 0, Iteration 210, Loss: 2.6462628841400146
Epoch 0, Iteration 211, Loss: 2.8036859035491943
Epoch 0, Iteration 212, Loss: 2.636012554168701
Epoch 0, Iteration 213, Loss: 2.6374146938323975
Epoch 0, Iteration 214, Loss: 2.2149617671966553
Epoch 0, Iteration 215, Loss: 2.1822621822357178
Epoch 0, Iteration 216, Loss: 2.1373696327209473
Epoch 0, Iteration 217, Loss: 2.291308641433716
Epoch 0, Iteration 218, Loss: 2.5046510696411133
Epoch 0, Iteration 219, Loss: 2.4112002849578857
Epoch 0, Iteration 220, Loss: 2.334226608276367
Epoch 0, Iteration 221, Loss: 2.24833083152771
Epoch 0, Iteration 222, Loss: 2.208850860595703
Epoch 0, Iteration 223, Loss: 2.331615447998047
Epoch 0, Iteration 224, Loss: 2.0270731449127197
Epoch 0, Iteration 225, Loss: 2.2430036067962646
Epoch 0, Iteration 226, Loss: 2.250755548477173
Epoch 0, Iteration 227, Loss: 2.2107889652252197
Epoch 0, Iteration 228, Loss: 1.9636038541793823
Epoch 0, Iteration 229, Loss: 1.926635503768921
Epoch 0, Iteration 230, Loss: 2.14142107963562
Epoch 0, Iteration 231, Loss: 2.0160539150238037
Epoch 0, Iteration 232, Loss: 2.1410484313964844
Epoch 0, Iteration 233, Loss: 2.506150007247925
Epoch 0, Iteration 234, Loss: 2.424654006958008
Epoch 0, Iteration 235, Loss: 1.8088078498840332
Epoch 0, Iteration 236, Loss: 2.0734500885009766
Epoch 0, Iteration 237, Loss: 2.1672515869140625
Epoch 0, Iteration 238, Loss: 1.8800846338272095
Epoch 0, Iteration 239, Loss: 1.935028076171875
Epoch 0, Iteration 240, Loss: 1.5924761295318604
Epoch 0, Iteration 241, Loss: 1.887436866760254
Epoch 0, Iteration 242, Loss: 1.6820483207702637
Epoch 0, Iteration 243, Loss: 1.8892682790756226
Epoch 0, Iteration 244, Loss: 1.8640919923782349
Epoch 0, Iteration 245, Loss: 1.6854296922683716
Epoch 0, Iteration 246, Loss: 2.0480403900146484
Epoch 0, Iteration 247, Loss: 1.7402540445327759
Epoch 0, Iteration 248, Loss: 2.0227530002593994
Epoch 0, Iteration 249, Loss: 1.8538298606872559
Epoch 0, Iteration 250, Loss: 2.125882148742676
Epoch 0, Iteration 250, Valid Loss: 2.023029327392578
Epoch 0, Iteration 251, Loss: 2.041682243347168
Epoch 0, Iteration 252, Loss: 1.913730502128601
Epoch 0, Iteration 253, Loss: 2.1290791034698486
Epoch 0, Iteration 254, Loss: 2.0507595539093018
Epoch 0, Iteration 255, Loss: 1.9358904361724854
Epoch 0, Iteration 256, Loss: 2.2654871940612793
Epoch 0, Iteration 257, Loss: 1.8250765800476074
Epoch 0, Iteration 258, Loss: 2.124004602432251
Epoch 0, Iteration 259, Loss: 1.7378073930740356
Epoch 0, Iteration 260, Loss: 1.9023566246032715
Epoch 0, Iteration 261, Loss: 2.116755485534668
Epoch 0, Iteration 262, Loss: 1.7549920082092285
Epoch 0, Iteration 263, Loss: 1.9981178045272827
Epoch 0, Iteration 264, Loss: 2.0859603881835938
Epoch 0, Iteration 265, Loss: 2.009338855743408
Epoch 0, Iteration 266, Loss: 2.068153142929077
Epoch 0, Iteration 267, Loss: 2.0442473888397217
Epoch 0, Iteration 268, Loss: 1.986254334449768
Epoch 0, Iteration 269, Loss: 1.9184871912002563
Epoch 0, Iteration 270, Loss: 1.8496888875961304
Epoch 0, Iteration 271, Loss: 1.9402034282684326
Epoch 0, Iteration 272, Loss: 1.8875823020935059
Epoch 0, Iteration 273, Loss: 1.7126376628875732
Epoch 0, Iteration 274, Loss: 1.8457136154174805
Epoch 0, Iteration 275, Loss: 1.5788633823394775
Epoch 0, Iteration 276, Loss: 1.8142988681793213
Epoch 0, Iteration 277, Loss: 1.7650282382965088
Epoch 0, Iteration 278, Loss: 1.7697440385818481
Epoch 0, Iteration 279, Loss: 1.740836501121521
Epoch 0, Iteration 280, Loss: 1.593930721282959
Epoch 0, Iteration 281, Loss: 1.6326850652694702
Epoch 0, Iteration 282, Loss: 1.9510093927383423
Epoch 0, Iteration 283, Loss: 1.8057465553283691
Epoch 0, Iteration 284, Loss: 1.7617456912994385
Epoch 0, Iteration 285, Loss: 1.9120334386825562
Epoch 0, Iteration 286, Loss: 1.7078251838684082
Epoch 0, Iteration 287, Loss: 1.7242710590362549
Epoch 0, Iteration 288, Loss: 1.7243287563323975
Epoch 0, Iteration 289, Loss: 1.9321365356445312
Epoch 0, Iteration 290, Loss: 1.8245257139205933
Epoch 0, Iteration 291, Loss: 1.9098973274230957
Epoch 0, Iteration 292, Loss: 2.1271586418151855
Epoch 0, Iteration 293, Loss: 1.9822949171066284
Epoch 0, Iteration 294, Loss: 1.7233569622039795
Epoch 0, Iteration 295, Loss: 1.8123167753219604
Epoch 0, Iteration 296, Loss: 2.257172107696533
Epoch 0, Iteration 297, Loss: 1.9194445610046387
Epoch 0, Iteration 298, Loss: 1.8757659196853638
Epoch 0, Iteration 299, Loss: 1.6949089765548706
Epoch 0, Iteration 300, Loss: 1.704309105873108
Epoch 0, Iteration 300, Valid Loss: 1.9237076044082642
Epoch 0, Iteration 301, Loss: 1.8295944929122925
Epoch 0, Iteration 302, Loss: 1.6169710159301758
Epoch 0, Iteration 303, Loss: 2.1130688190460205
Epoch 0, Iteration 304, Loss: 1.799018383026123
Epoch 0, Iteration 305, Loss: 1.750558614730835
Epoch 0, Iteration 306, Loss: 1.8114299774169922
Epoch 0, Iteration 307, Loss: 1.656670331954956
Epoch 0, Iteration 308, Loss: 1.7673022747039795
Epoch 0, Iteration 309, Loss: 1.814821720123291
Epoch 0, Iteration 310, Loss: 2.0571537017822266
Epoch 0, Iteration 311, Loss: 1.6968293190002441
Epoch 0, Iteration 312, Loss: 1.6636865139007568
Epoch 0, Iteration 313, Loss: 1.7652333974838257
Epoch 0, Iteration 314, Loss: 1.7469030618667603
Epoch 0, Iteration 315, Loss: 1.6532225608825684
Epoch 0, Iteration 316, Loss: 1.8241174221038818
Epoch 0, Iteration 317, Loss: 1.7290523052215576
Epoch 0, Iteration 318, Loss: 1.6914018392562866
Epoch 0, Iteration 319, Loss: 1.7496005296707153
Epoch 0, Iteration 320, Loss: 1.538475513458252
Epoch 0, Iteration 321, Loss: 1.663192629814148
Epoch 0, Iteration 322, Loss: 1.5933095216751099
Epoch 0, Iteration 323, Loss: 1.7164344787597656
Epoch 0, Iteration 324, Loss: 1.7819197177886963
Epoch 0, Iteration 325, Loss: 1.7330840826034546
Epoch 0, Iteration 326, Loss: 1.766784429550171
Epoch 0, Iteration 327, Loss: 1.6357545852661133
Epoch 0, Iteration 328, Loss: 1.6366081237792969
Epoch 0, Iteration 329, Loss: 1.9593640565872192
Epoch 0, Iteration 330, Loss: 1.4734503030776978
Epoch 0, Iteration 331, Loss: 1.5703327655792236
Epoch 0, Iteration 332, Loss: 1.86098313331604
Epoch 0, Iteration 333, Loss: 1.8669559955596924
Epoch 0, Iteration 334, Loss: 1.6712007522583008
Epoch 0, Iteration 335, Loss: 1.851320505142212
Epoch 0, Iteration 336, Loss: 1.8040916919708252
Epoch 0, Iteration 337, Loss: 1.6060833930969238
Epoch 0, Iteration 338, Loss: 1.895801067352295
Epoch 0, Iteration 339, Loss: 1.5841431617736816
Epoch 0, Iteration 340, Loss: 1.7336093187332153
Epoch 0, Iteration 341, Loss: 1.726702094078064
Epoch 0, Iteration 342, Loss: 1.5982176065444946
Epoch 0, Iteration 343, Loss: 1.732932209968567
Epoch 0, Iteration 344, Loss: 1.8160399198532104
Epoch 0, Iteration 345, Loss: 1.8418740034103394
Epoch 0, Iteration 346, Loss: 1.7283729314804077
Epoch 0, Iteration 347, Loss: 1.5126830339431763
Epoch 0, Iteration 348, Loss: 1.6131671667099
Epoch 0, Iteration 349, Loss: 1.8045183420181274
Epoch 0, Iteration 350, Loss: 1.5177820920944214
Epoch 0, Iteration 350, Valid Loss: 1.8072201013565063
Epoch 0, Iteration 351, Loss: 1.725433111190796
Epoch 0, Iteration 352, Loss: 1.6450833082199097
Epoch 0, Iteration 353, Loss: 1.8080049753189087
Epoch 0, Iteration 354, Loss: 1.6027326583862305
Epoch 0, Iteration 355, Loss: 1.6214185953140259
Epoch 0, Iteration 356, Loss: 1.7500624656677246
Epoch 0, Iteration 357, Loss: 1.6895995140075684
Epoch 0, Iteration 358, Loss: 1.588383436203003
Epoch 0, Iteration 359, Loss: 1.632638931274414
Epoch 0, Iteration 360, Loss: 1.4760310649871826
Epoch 0, Iteration 361, Loss: 1.5375547409057617
Epoch 0, Iteration 362, Loss: 1.660813570022583
Epoch 0, Iteration 363, Loss: 1.5799858570098877
Epoch 0, Iteration 364, Loss: 1.6078345775604248
Epoch 0, Iteration 365, Loss: 2.014436721801758
Epoch 0, Iteration 366, Loss: 1.8757460117340088
Epoch 0, Iteration 367, Loss: 1.8273742198944092
Epoch 0, Iteration 368, Loss: 1.7303125858306885
Epoch 0, Iteration 369, Loss: 1.7091752290725708
Epoch 0, Iteration 370, Loss: 1.7201834917068481
Epoch 0, Iteration 371, Loss: 1.765328288078308
Epoch 0, Iteration 372, Loss: 1.4966439008712769
Epoch 0, Iteration 373, Loss: 1.5061320066452026
Epoch 0, Iteration 374, Loss: 1.7286475896835327
Epoch 0, Iteration 375, Loss: 1.5949506759643555
Epoch 0, Iteration 376, Loss: 1.8836863040924072
Epoch 0, Iteration 377, Loss: 1.732139229774475
Epoch 0, Iteration 378, Loss: 1.5353249311447144
Epoch 0, Iteration 379, Loss: 1.6448931694030762
Epoch 0, Iteration 380, Loss: 1.6106901168823242
Epoch 0, Iteration 381, Loss: 1.5368834733963013
Epoch 0, Iteration 382, Loss: 1.4323704242706299
Epoch 0, Iteration 383, Loss: 1.6513986587524414
Epoch 0, Iteration 384, Loss: 1.6585413217544556
Epoch 0, Iteration 385, Loss: 1.5207016468048096
Epoch 0, Iteration 386, Loss: 1.5538904666900635
Epoch 0, Iteration 387, Loss: 1.9006428718566895
Epoch 0, Iteration 388, Loss: 1.6571624279022217
Epoch 0, Iteration 389, Loss: 1.8574897050857544
Epoch 0, Iteration 390, Loss: 1.7800096273422241
Epoch 0, Iteration 391, Loss: 1.5816171169281006
Epoch 1/15, Loss: 2.213832358316499
Epoch 1, Iteration 0, Loss: 1.766051173210144
Epoch 1, Iteration 1, Loss: 1.995905876159668
Epoch 1, Iteration 2, Loss: 2.077378511428833
Epoch 1, Iteration 3, Loss: 1.8598264455795288
Epoch 1, Iteration 4, Loss: 1.796717643737793
Epoch 1, Iteration 5, Loss: 1.871799349784851
Epoch 1, Iteration 6, Loss: 1.6857738494873047
Epoch 1, Iteration 7, Loss: 1.843631386756897
Epoch 1, Iteration 8, Loss: 1.9296656847000122
Epoch 1, Iteration 9, Loss: 2.1211795806884766
Epoch 1, Iteration 10, Loss: 1.8755935430526733
Epoch 1, Iteration 11, Loss: 1.7603155374526978
Epoch 1, Iteration 12, Loss: 1.5610754489898682
Epoch 1, Iteration 13, Loss: 1.9065208435058594
Epoch 1, Iteration 14, Loss: 1.649802327156067
Epoch 1, Iteration 15, Loss: 1.9151707887649536
Epoch 1, Iteration 16, Loss: 1.622831106185913
Epoch 1, Iteration 17, Loss: 1.926804542541504
Epoch 1, Iteration 18, Loss: 1.838024377822876
Epoch 1, Iteration 19, Loss: 1.8419631719589233
Epoch 1, Iteration 20, Loss: 1.7749192714691162
Epoch 1, Iteration 21, Loss: 1.7426409721374512
Epoch 1, Iteration 22, Loss: 1.6674745082855225
Epoch 1, Iteration 23, Loss: 1.6598283052444458
Epoch 1, Iteration 24, Loss: 1.6037359237670898
Epoch 1, Iteration 25, Loss: 1.6045829057693481
Epoch 1, Iteration 26, Loss: 2.021451950073242
Epoch 1, Iteration 27, Loss: 1.624883770942688
Epoch 1, Iteration 28, Loss: 1.6985481977462769
Epoch 1, Iteration 29, Loss: 1.840247631072998
Epoch 1, Iteration 30, Loss: 1.7929288148880005
Epoch 1, Iteration 31, Loss: 1.692771315574646
Epoch 1, Iteration 32, Loss: 1.6320322751998901
Epoch 1, Iteration 33, Loss: 1.5922681093215942
Epoch 1, Iteration 34, Loss: 1.724204421043396
Epoch 1, Iteration 35, Loss: 1.759735107421875
Epoch 1, Iteration 36, Loss: 1.5230857133865356
Epoch 1, Iteration 37, Loss: 1.5826420783996582
Epoch 1, Iteration 38, Loss: 1.5893726348876953
Epoch 1, Iteration 39, Loss: 1.6632965803146362
Epoch 1, Iteration 40, Loss: 1.6959290504455566
Epoch 1, Iteration 41, Loss: 1.7598353624343872
Epoch 1, Iteration 42, Loss: 1.882830262184143
Epoch 1, Iteration 43, Loss: 1.623390555381775
Epoch 1, Iteration 44, Loss: 1.7256306409835815
Epoch 1, Iteration 45, Loss: 1.5825321674346924
Epoch 1, Iteration 46, Loss: 1.7038308382034302
Epoch 1, Iteration 47, Loss: 1.7513830661773682
Epoch 1, Iteration 48, Loss: 1.8915873765945435
Epoch 1, Iteration 49, Loss: 1.6761095523834229
Epoch 1, Iteration 50, Loss: 1.6401169300079346
Epoch 1, Iteration 50, Valid Loss: 1.4485353231430054
Epoch 1, Iteration 51, Loss: 1.6952046155929565
Epoch 1, Iteration 52, Loss: 1.8450453281402588
Epoch 1, Iteration 53, Loss: 1.6102286577224731
Epoch 1, Iteration 54, Loss: 1.833138108253479
Epoch 1, Iteration 55, Loss: 1.7322819232940674
Epoch 1, Iteration 56, Loss: 1.4699536561965942
Epoch 1, Iteration 57, Loss: 1.527243733406067
Epoch 1, Iteration 58, Loss: 1.4674607515335083
Epoch 1, Iteration 59, Loss: 1.6903953552246094
Epoch 1, Iteration 60, Loss: 1.639920711517334
Epoch 1, Iteration 61, Loss: 1.3956018686294556
Epoch 1, Iteration 62, Loss: 1.476341724395752
Epoch 1, Iteration 63, Loss: 1.505836009979248
Epoch 1, Iteration 64, Loss: 1.4305388927459717
Epoch 1, Iteration 65, Loss: 1.6909452676773071
Epoch 1, Iteration 66, Loss: 1.6692086458206177
Epoch 1, Iteration 67, Loss: 1.5091278553009033
Epoch 1, Iteration 68, Loss: 1.5649927854537964
Epoch 1, Iteration 69, Loss: 1.8664857149124146
Epoch 1, Iteration 70, Loss: 1.7916505336761475
Epoch 1, Iteration 71, Loss: 1.5364677906036377
Epoch 1, Iteration 72, Loss: 1.6780565977096558
Epoch 1, Iteration 73, Loss: 1.382109522819519
Epoch 1, Iteration 74, Loss: 1.7476664781570435
Epoch 1, Iteration 75, Loss: 1.484573483467102
Epoch 1, Iteration 76, Loss: 1.5339640378952026
Epoch 1, Iteration 77, Loss: 1.285889983177185
Epoch 1, Iteration 78, Loss: 1.9429113864898682
Epoch 1, Iteration 79, Loss: 2.018778085708618
Epoch 1, Iteration 80, Loss: 1.3581987619400024
Epoch 1, Iteration 81, Loss: 1.7866790294647217
Epoch 1, Iteration 82, Loss: 1.5770288705825806
Epoch 1, Iteration 83, Loss: 1.3659193515777588
Epoch 1, Iteration 84, Loss: 1.6160286664962769
Epoch 1, Iteration 85, Loss: 1.7292660474777222
Epoch 1, Iteration 86, Loss: 1.4697386026382446
Epoch 1, Iteration 87, Loss: 1.361825942993164
Epoch 1, Iteration 88, Loss: 1.4253780841827393
Epoch 1, Iteration 89, Loss: 1.8314329385757446
Epoch 1, Iteration 90, Loss: 1.4844286441802979
Epoch 1, Iteration 91, Loss: 1.4202626943588257
Epoch 1, Iteration 92, Loss: 1.332923412322998
Epoch 1, Iteration 93, Loss: 1.410105586051941
Epoch 1, Iteration 94, Loss: 1.8765732049942017
Epoch 1, Iteration 95, Loss: 1.5829365253448486
Epoch 1, Iteration 96, Loss: 1.6646318435668945
Epoch 1, Iteration 97, Loss: 1.428951382637024
Epoch 1, Iteration 98, Loss: 1.6076074838638306
Epoch 1, Iteration 99, Loss: 1.5966817140579224
Epoch 1, Iteration 100, Loss: 1.3975200653076172
Epoch 1, Iteration 100, Valid Loss: 1.4718574285507202
Epoch 1, Iteration 101, Loss: 1.5891870260238647
Epoch 1, Iteration 102, Loss: 1.5884796380996704
Epoch 1, Iteration 103, Loss: 1.460829496383667
Epoch 1, Iteration 104, Loss: 1.8132630586624146
Epoch 1, Iteration 105, Loss: 1.6424587965011597
Epoch 1, Iteration 106, Loss: 1.6085997819900513
Epoch 1, Iteration 107, Loss: 2.0941970348358154
Epoch 1, Iteration 108, Loss: 1.4833672046661377
Epoch 1, Iteration 109, Loss: 1.804589867591858
Epoch 1, Iteration 110, Loss: 1.8855973482131958
Epoch 1, Iteration 111, Loss: 1.6610299348831177
Epoch 1, Iteration 112, Loss: 1.6017212867736816
Epoch 1, Iteration 113, Loss: 1.4737682342529297
Epoch 1, Iteration 114, Loss: 2.016113758087158
Epoch 1, Iteration 115, Loss: 1.7362616062164307
Epoch 1, Iteration 116, Loss: 1.494315266609192
Epoch 1, Iteration 117, Loss: 1.3312722444534302
Epoch 1, Iteration 118, Loss: 1.6696009635925293
Epoch 1, Iteration 119, Loss: 1.5295803546905518
Epoch 1, Iteration 120, Loss: 1.5091044902801514
Epoch 1, Iteration 121, Loss: 1.4985928535461426
Epoch 1, Iteration 122, Loss: 1.505478024482727
Epoch 1, Iteration 123, Loss: 1.465248942375183
Epoch 1, Iteration 124, Loss: 1.5969284772872925
Epoch 1, Iteration 125, Loss: 1.4136368036270142
Epoch 1, Iteration 126, Loss: 1.53896164894104
Epoch 1, Iteration 127, Loss: 1.7722907066345215
Epoch 1, Iteration 128, Loss: 1.5288488864898682
Epoch 1, Iteration 129, Loss: 1.63235604763031
Epoch 1, Iteration 130, Loss: 2.052145481109619
Epoch 1, Iteration 131, Loss: 1.5146156549453735
Epoch 1, Iteration 132, Loss: 1.4045555591583252
Epoch 1, Iteration 133, Loss: 1.6433234214782715
Epoch 1, Iteration 134, Loss: 1.4045995473861694
Epoch 1, Iteration 135, Loss: 1.5386195182800293
Epoch 1, Iteration 136, Loss: 1.622236967086792
Epoch 1, Iteration 137, Loss: 1.4877113103866577
Epoch 1, Iteration 138, Loss: 1.5546910762786865
Epoch 1, Iteration 139, Loss: 1.4438363313674927
Epoch 1, Iteration 140, Loss: 1.4746103286743164
Epoch 1, Iteration 141, Loss: 1.9042840003967285
Epoch 1, Iteration 142, Loss: 1.439428687095642
Epoch 1, Iteration 143, Loss: 1.5879064798355103
Epoch 1, Iteration 144, Loss: 1.4717854261398315
Epoch 1, Iteration 145, Loss: 1.4891995191574097
Epoch 1, Iteration 146, Loss: 1.6075427532196045
Epoch 1, Iteration 147, Loss: 1.4039924144744873
Epoch 1, Iteration 148, Loss: 1.3372949361801147
Epoch 1, Iteration 149, Loss: 1.4400323629379272
Epoch 1, Iteration 150, Loss: 1.5442500114440918
Epoch 1, Iteration 150, Valid Loss: 1.4371472597122192
Epoch 1, Iteration 151, Loss: 1.6685450077056885
Epoch 1, Iteration 152, Loss: 1.5474737882614136
Epoch 1, Iteration 153, Loss: 1.7160996198654175
Epoch 1, Iteration 154, Loss: 1.4971872568130493
Epoch 1, Iteration 155, Loss: 1.6425502300262451
Epoch 1, Iteration 156, Loss: 1.4629944562911987
Epoch 1, Iteration 157, Loss: 1.9071520566940308
Epoch 1, Iteration 158, Loss: 1.6500698328018188
Epoch 1, Iteration 159, Loss: 1.7259255647659302
Epoch 1, Iteration 160, Loss: 1.4649310111999512
Epoch 1, Iteration 161, Loss: 1.4678109884262085
Epoch 1, Iteration 162, Loss: 1.6791781187057495
Epoch 1, Iteration 163, Loss: 1.3076502084732056
Epoch 1, Iteration 164, Loss: 1.3697259426116943
Epoch 1, Iteration 165, Loss: 1.3495203256607056
Epoch 1, Iteration 166, Loss: 1.4124130010604858
Epoch 1, Iteration 167, Loss: 1.7203644514083862
Epoch 1, Iteration 168, Loss: 1.650087833404541
Epoch 1, Iteration 169, Loss: 1.4999709129333496
Epoch 1, Iteration 170, Loss: 1.663883924484253
Epoch 1, Iteration 171, Loss: 1.445719838142395
Epoch 1, Iteration 172, Loss: 1.4893500804901123
Epoch 1, Iteration 173, Loss: 1.4782377481460571
Epoch 1, Iteration 174, Loss: 1.5503530502319336
Epoch 1, Iteration 175, Loss: 1.4921177625656128
Epoch 1, Iteration 176, Loss: 1.2219407558441162
Epoch 1, Iteration 177, Loss: 1.8601067066192627
Epoch 1, Iteration 178, Loss: 1.7211894989013672
Epoch 1, Iteration 179, Loss: 1.5489377975463867
Epoch 1, Iteration 180, Loss: 1.3873428106307983
Epoch 1, Iteration 181, Loss: 1.6651339530944824
Epoch 1, Iteration 182, Loss: 1.579970359802246
Epoch 1, Iteration 183, Loss: 1.4689141511917114
Epoch 1, Iteration 184, Loss: 1.2910341024398804
Epoch 1, Iteration 185, Loss: 1.4617418050765991
Epoch 1, Iteration 186, Loss: 1.3598332405090332
Epoch 1, Iteration 187, Loss: 1.376189947128296
Epoch 1, Iteration 188, Loss: 1.4719971418380737
Epoch 1, Iteration 189, Loss: 1.3989486694335938
Epoch 1, Iteration 190, Loss: 1.8296527862548828
Epoch 1, Iteration 191, Loss: 1.6249594688415527
Epoch 1, Iteration 192, Loss: 1.5391182899475098
Epoch 1, Iteration 193, Loss: 1.5239903926849365
Epoch 1, Iteration 194, Loss: 1.4440900087356567
Epoch 1, Iteration 195, Loss: 1.335312843322754
Epoch 1, Iteration 196, Loss: 1.5167062282562256
Epoch 1, Iteration 197, Loss: 1.4719812870025635
Epoch 1, Iteration 198, Loss: 1.5395041704177856
Epoch 1, Iteration 199, Loss: 1.6155741214752197
Epoch 1, Iteration 200, Loss: 1.4398860931396484
Epoch 1, Iteration 200, Valid Loss: 1.3287177085876465
Epoch 1, Iteration 201, Loss: 1.4564586877822876
Epoch 1, Iteration 202, Loss: 1.554866075515747
Epoch 1, Iteration 203, Loss: 1.3892902135849
Epoch 1, Iteration 204, Loss: 1.3520312309265137
Epoch 1, Iteration 205, Loss: 1.1674052476882935
Epoch 1, Iteration 206, Loss: 1.3099141120910645
Epoch 1, Iteration 207, Loss: 1.544344425201416
Epoch 1, Iteration 208, Loss: 1.8318696022033691
Epoch 1, Iteration 209, Loss: 1.8722821474075317
Epoch 1, Iteration 210, Loss: 2.009533166885376
Epoch 1, Iteration 211, Loss: 2.0832300186157227
Epoch 1, Iteration 212, Loss: 1.8896580934524536
Epoch 1, Iteration 213, Loss: 1.9372700452804565
Epoch 1, Iteration 214, Loss: 1.4576555490493774
Epoch 1, Iteration 215, Loss: 1.436438798904419
Epoch 1, Iteration 216, Loss: 1.4982404708862305
Epoch 1, Iteration 217, Loss: 1.4801888465881348
Epoch 1, Iteration 218, Loss: 1.6848866939544678
Epoch 1, Iteration 219, Loss: 1.5623722076416016
Epoch 1, Iteration 220, Loss: 1.5125632286071777
Epoch 1, Iteration 221, Loss: 1.5053707361221313
Epoch 1, Iteration 222, Loss: 1.4483927488327026
Epoch 1, Iteration 223, Loss: 1.4626922607421875
Epoch 1, Iteration 224, Loss: 1.2656089067459106
Epoch 1, Iteration 225, Loss: 1.6071704626083374
Epoch 1, Iteration 226, Loss: 1.6016559600830078
Epoch 1, Iteration 227, Loss: 1.6246026754379272
Epoch 1, Iteration 228, Loss: 1.3415334224700928
Epoch 1, Iteration 229, Loss: 1.2451943159103394
Epoch 1, Iteration 230, Loss: 1.5020453929901123
Epoch 1, Iteration 231, Loss: 1.4825527667999268
Epoch 1, Iteration 232, Loss: 1.4887670278549194
Epoch 1, Iteration 233, Loss: 1.8426868915557861
Epoch 1, Iteration 234, Loss: 1.8202035427093506
Epoch 1, Iteration 235, Loss: 1.2147055864334106
Epoch 1, Iteration 236, Loss: 1.4637173414230347
Epoch 1, Iteration 237, Loss: 1.6213706731796265
Epoch 1, Iteration 238, Loss: 1.3079365491867065
Epoch 1, Iteration 239, Loss: 1.4553576707839966
Epoch 1, Iteration 240, Loss: 1.233763337135315
Epoch 1, Iteration 241, Loss: 1.3127710819244385
Epoch 1, Iteration 242, Loss: 1.234320044517517
Epoch 1, Iteration 243, Loss: 1.4139107465744019
Epoch 1, Iteration 244, Loss: 1.355732798576355
Epoch 1, Iteration 245, Loss: 1.1162971258163452
Epoch 1, Iteration 246, Loss: 1.3589766025543213
Epoch 1, Iteration 247, Loss: 1.2028580904006958
Epoch 1, Iteration 248, Loss: 1.4226646423339844
Epoch 1, Iteration 249, Loss: 1.3340651988983154
Epoch 1, Iteration 250, Loss: 1.516063928604126
Epoch 1, Iteration 250, Valid Loss: 1.2595428228378296
Epoch 1, Iteration 251, Loss: 1.5025378465652466
Epoch 1, Iteration 252, Loss: 1.6240767240524292
Epoch 1, Iteration 253, Loss: 1.5498967170715332
Epoch 1, Iteration 254, Loss: 1.466488003730774
Epoch 1, Iteration 255, Loss: 1.4294192790985107
Epoch 1, Iteration 256, Loss: 1.6481807231903076
Epoch 1, Iteration 257, Loss: 1.3497576713562012
Epoch 1, Iteration 258, Loss: 1.5768221616744995
Epoch 1, Iteration 259, Loss: 1.2375949621200562
Epoch 1, Iteration 260, Loss: 1.5093965530395508
Epoch 1, Iteration 261, Loss: 1.602980375289917
Epoch 1, Iteration 262, Loss: 1.139979362487793
Epoch 1, Iteration 263, Loss: 1.4755122661590576
Epoch 1, Iteration 264, Loss: 1.4224188327789307
Epoch 1, Iteration 265, Loss: 1.4788140058517456
Epoch 1, Iteration 266, Loss: 1.5071061849594116
Epoch 1, Iteration 267, Loss: 1.410326361656189
Epoch 1, Iteration 268, Loss: 1.3788782358169556
Epoch 1, Iteration 269, Loss: 1.3941408395767212
Epoch 1, Iteration 270, Loss: 1.2820186614990234
Epoch 1, Iteration 271, Loss: 1.4065803289413452
Epoch 1, Iteration 272, Loss: 1.3323187828063965
Epoch 1, Iteration 273, Loss: 1.297698974609375
Epoch 1, Iteration 274, Loss: 1.242661476135254
Epoch 1, Iteration 275, Loss: 1.1727683544158936
Epoch 1, Iteration 276, Loss: 1.1950795650482178
Epoch 1, Iteration 277, Loss: 1.2908555269241333
Epoch 1, Iteration 278, Loss: 1.4926260709762573
Epoch 1, Iteration 279, Loss: 1.2426693439483643
Epoch 1, Iteration 280, Loss: 1.1517571210861206
Epoch 1, Iteration 281, Loss: 1.1941393613815308
Epoch 1, Iteration 282, Loss: 1.3174870014190674
Epoch 1, Iteration 283, Loss: 1.3038002252578735
Epoch 1, Iteration 284, Loss: 1.311618447303772
Epoch 1, Iteration 285, Loss: 1.3152694702148438
Epoch 1, Iteration 286, Loss: 1.2771341800689697
Epoch 1, Iteration 287, Loss: 1.2807124853134155
Epoch 1, Iteration 288, Loss: 1.3183140754699707
Epoch 1, Iteration 289, Loss: 1.3771862983703613
Epoch 1, Iteration 290, Loss: 1.2496885061264038
Epoch 1, Iteration 291, Loss: 1.3170390129089355
Epoch 1, Iteration 292, Loss: 1.5337350368499756
Epoch 1, Iteration 293, Loss: 1.6001254320144653
Epoch 1, Iteration 294, Loss: 1.3099820613861084
Epoch 1, Iteration 295, Loss: 1.2929545640945435
Epoch 1, Iteration 296, Loss: 1.6021901369094849
Epoch 1, Iteration 297, Loss: 1.5571036338806152
Epoch 1, Iteration 298, Loss: 1.429477334022522
Epoch 1, Iteration 299, Loss: 1.1914278268814087
Epoch 1, Iteration 300, Loss: 1.1954827308654785
Epoch 1, Iteration 300, Valid Loss: 1.2122628688812256
Epoch 1, Iteration 301, Loss: 1.2923136949539185
Epoch 1, Iteration 302, Loss: 1.2036954164505005
Epoch 1, Iteration 303, Loss: 1.6799498796463013
Epoch 1, Iteration 304, Loss: 1.2729474306106567
Epoch 1, Iteration 305, Loss: 1.264849066734314
Epoch 1, Iteration 306, Loss: 1.1767719984054565
Epoch 1, Iteration 307, Loss: 1.380623459815979
Epoch 1, Iteration 308, Loss: 1.2801258563995361
Epoch 1, Iteration 309, Loss: 1.3980435132980347
Epoch 1, Iteration 310, Loss: 1.4534016847610474
Epoch 1, Iteration 311, Loss: 1.1469119787216187
Epoch 1, Iteration 312, Loss: 1.2580870389938354
Epoch 1, Iteration 313, Loss: 1.4177148342132568
Epoch 1, Iteration 314, Loss: 1.2190439701080322
Epoch 1, Iteration 315, Loss: 1.156395435333252
Epoch 1, Iteration 316, Loss: 1.3918241262435913
Epoch 1, Iteration 317, Loss: 1.288510799407959
Epoch 1, Iteration 318, Loss: 1.1542110443115234
Epoch 1, Iteration 319, Loss: 1.446327805519104
Epoch 1, Iteration 320, Loss: 1.096866250038147
Epoch 1, Iteration 321, Loss: 1.2655705213546753
Epoch 1, Iteration 322, Loss: 1.1605896949768066
Epoch 1, Iteration 323, Loss: 1.2532464265823364
Epoch 1, Iteration 324, Loss: 1.3412230014801025
Epoch 1, Iteration 325, Loss: 1.307773232460022
Epoch 1, Iteration 326, Loss: 1.250739574432373
Epoch 1, Iteration 327, Loss: 1.2031644582748413
Epoch 1, Iteration 328, Loss: 1.3323370218276978
Epoch 1, Iteration 329, Loss: 1.507731318473816
Epoch 1, Iteration 330, Loss: 1.061104416847229
Epoch 1, Iteration 331, Loss: 1.2235201597213745
Epoch 1, Iteration 332, Loss: 1.3565367460250854
Epoch 1, Iteration 333, Loss: 1.4489601850509644
Epoch 1, Iteration 334, Loss: 1.1563736200332642
Epoch 1, Iteration 335, Loss: 1.4401212930679321
Epoch 1, Iteration 336, Loss: 1.4102959632873535
Epoch 1, Iteration 337, Loss: 1.2070664167404175
Epoch 1, Iteration 338, Loss: 1.376094937324524
Epoch 1, Iteration 339, Loss: 1.2408021688461304
Epoch 1, Iteration 340, Loss: 1.0827453136444092
Epoch 1, Iteration 341, Loss: 1.2344764471054077
Epoch 1, Iteration 342, Loss: 1.1539145708084106
Epoch 1, Iteration 343, Loss: 1.3110121488571167
Epoch 1, Iteration 344, Loss: 1.502750039100647
Epoch 1, Iteration 345, Loss: 1.361360788345337
Epoch 1, Iteration 346, Loss: 1.3757387399673462
Epoch 1, Iteration 347, Loss: 1.135177731513977
Epoch 1, Iteration 348, Loss: 1.103747010231018
Epoch 1, Iteration 349, Loss: 1.3434001207351685
Epoch 1, Iteration 350, Loss: 1.1179286241531372
Epoch 1, Iteration 350, Valid Loss: 1.2014061212539673
Epoch 1, Iteration 351, Loss: 1.3257968425750732
Epoch 1, Iteration 352, Loss: 1.182412028312683
Epoch 1, Iteration 353, Loss: 1.3589540719985962
Epoch 1, Iteration 354, Loss: 1.2019633054733276
Epoch 1, Iteration 355, Loss: 1.283197045326233
Epoch 1, Iteration 356, Loss: 1.2631863355636597
Epoch 1, Iteration 357, Loss: 1.2213910818099976
Epoch 1, Iteration 358, Loss: 1.216498613357544
Epoch 1, Iteration 359, Loss: 1.227883219718933
Epoch 1, Iteration 360, Loss: 1.119398593902588
Epoch 1, Iteration 361, Loss: 1.1751484870910645
Epoch 1, Iteration 362, Loss: 1.3469494581222534
Epoch 1, Iteration 363, Loss: 1.132649540901184
Epoch 1, Iteration 364, Loss: 1.2452784776687622
Epoch 1, Iteration 365, Loss: 1.5839998722076416
Epoch 1, Iteration 366, Loss: 1.3641456365585327
Epoch 1, Iteration 367, Loss: 1.3855332136154175
Epoch 1, Iteration 368, Loss: 1.3177103996276855
Epoch 1, Iteration 369, Loss: 1.3508129119873047
Epoch 1, Iteration 370, Loss: 1.3511488437652588
Epoch 1, Iteration 371, Loss: 1.3325376510620117
Epoch 1, Iteration 372, Loss: 1.1225920915603638
Epoch 1, Iteration 373, Loss: 1.1216070652008057
Epoch 1, Iteration 374, Loss: 1.3553204536437988
Epoch 1, Iteration 375, Loss: 1.166293978691101
Epoch 1, Iteration 376, Loss: 1.541292667388916
Epoch 1, Iteration 377, Loss: 1.323535680770874
Epoch 1, Iteration 378, Loss: 1.1273010969161987
Epoch 1, Iteration 379, Loss: 1.1914187669754028
Epoch 1, Iteration 380, Loss: 1.122992753982544
Epoch 1, Iteration 381, Loss: 1.23050057888031
Epoch 1, Iteration 382, Loss: 0.9855629205703735
Epoch 1, Iteration 383, Loss: 1.3029712438583374
Epoch 1, Iteration 384, Loss: 1.3223323822021484
Epoch 1, Iteration 385, Loss: 1.1695373058319092
Epoch 1, Iteration 386, Loss: 1.1455535888671875
Epoch 1, Iteration 387, Loss: 1.4389551877975464
Epoch 1, Iteration 388, Loss: 1.2447060346603394
Epoch 1, Iteration 389, Loss: 1.4106117486953735
Epoch 1, Iteration 390, Loss: 1.4579824209213257
Epoch 1, Iteration 391, Loss: 1.242053747177124
Epoch 2/15, Loss: 1.4914674971784865
Epoch 2, Iteration 0, Loss: 1.3585050106048584
Epoch 2, Iteration 1, Loss: 1.7052570581436157
Epoch 2, Iteration 2, Loss: 1.6192524433135986
Epoch 2, Iteration 3, Loss: 1.4448665380477905
Epoch 2, Iteration 4, Loss: 1.3293166160583496
Epoch 2, Iteration 5, Loss: 1.5260887145996094
Epoch 2, Iteration 6, Loss: 1.275814414024353
Epoch 2, Iteration 7, Loss: 1.3211416006088257
Epoch 2, Iteration 8, Loss: 1.3960188627243042
Epoch 2, Iteration 9, Loss: 1.6212043762207031
Epoch 2, Iteration 10, Loss: 1.4986393451690674
Epoch 2, Iteration 11, Loss: 1.2949296236038208
Epoch 2, Iteration 12, Loss: 1.228013277053833
Epoch 2, Iteration 13, Loss: 1.5601650476455688
Epoch 2, Iteration 14, Loss: 1.28094482421875
Epoch 2, Iteration 15, Loss: 1.5522674322128296
Epoch 2, Iteration 16, Loss: 1.282633900642395
Epoch 2, Iteration 17, Loss: 1.5558730363845825
Epoch 2, Iteration 18, Loss: 1.4217065572738647
Epoch 2, Iteration 19, Loss: 1.4150309562683105
Epoch 2, Iteration 20, Loss: 1.3009984493255615
Epoch 2, Iteration 21, Loss: 1.3310277462005615
Epoch 2, Iteration 22, Loss: 1.3318287134170532
Epoch 2, Iteration 23, Loss: 1.2634990215301514
Epoch 2, Iteration 24, Loss: 1.2902685403823853
Epoch 2, Iteration 25, Loss: 1.199089527130127
Epoch 2, Iteration 26, Loss: 1.691246747970581
Epoch 2, Iteration 27, Loss: 1.264173150062561
Epoch 2, Iteration 28, Loss: 1.4678457975387573
Epoch 2, Iteration 29, Loss: 1.5533677339553833
Epoch 2, Iteration 30, Loss: 1.3373323678970337
Epoch 2, Iteration 31, Loss: 1.3462896347045898
Epoch 2, Iteration 32, Loss: 1.2520865201950073
Epoch 2, Iteration 33, Loss: 1.290856957435608
Epoch 2, Iteration 34, Loss: 1.305687427520752
Epoch 2, Iteration 35, Loss: 1.425973653793335
Epoch 2, Iteration 36, Loss: 1.1516977548599243
Epoch 2, Iteration 37, Loss: 1.2506530284881592
Epoch 2, Iteration 38, Loss: 1.2883967161178589
Epoch 2, Iteration 39, Loss: 1.2893283367156982
Epoch 2, Iteration 40, Loss: 1.4360722303390503
Epoch 2, Iteration 41, Loss: 1.4092897176742554
Epoch 2, Iteration 42, Loss: 1.3858414888381958
Epoch 2, Iteration 43, Loss: 1.2088695764541626
Epoch 2, Iteration 44, Loss: 1.3387315273284912
Epoch 2, Iteration 45, Loss: 1.187865138053894
Epoch 2, Iteration 46, Loss: 1.4028663635253906
Epoch 2, Iteration 47, Loss: 1.4199872016906738
Epoch 2, Iteration 48, Loss: 1.427333950996399
Epoch 2, Iteration 49, Loss: 1.2379610538482666
Epoch 2, Iteration 50, Loss: 1.2251149415969849
Epoch 2, Iteration 50, Valid Loss: 1.0527894496917725
Epoch 2, Iteration 51, Loss: 1.2291897535324097
Epoch 2, Iteration 52, Loss: 1.2887953519821167
Epoch 2, Iteration 53, Loss: 1.2500923871994019
Epoch 2, Iteration 54, Loss: 1.549117088317871
Epoch 2, Iteration 55, Loss: 1.436954140663147
Epoch 2, Iteration 56, Loss: 1.2690051794052124
Epoch 2, Iteration 57, Loss: 1.3669615983963013
Epoch 2, Iteration 58, Loss: 1.1150248050689697
Epoch 2, Iteration 59, Loss: 1.241655945777893
Epoch 2, Iteration 60, Loss: 1.2999324798583984
Epoch 2, Iteration 61, Loss: 1.1168427467346191
Epoch 2, Iteration 62, Loss: 1.157361388206482
Epoch 2, Iteration 63, Loss: 1.1970493793487549
Epoch 2, Iteration 64, Loss: 1.2324485778808594
Epoch 2, Iteration 65, Loss: 1.3852468729019165
Epoch 2, Iteration 66, Loss: 1.1769047975540161
Epoch 2, Iteration 67, Loss: 1.0401164293289185
Epoch 2, Iteration 68, Loss: 1.311206340789795
Epoch 2, Iteration 69, Loss: 1.563722848892212
Epoch 2, Iteration 70, Loss: 1.535728096961975
Epoch 2, Iteration 71, Loss: 1.317094087600708
Epoch 2, Iteration 72, Loss: 1.2594369649887085
Epoch 2, Iteration 73, Loss: 1.0540989637374878
Epoch 2, Iteration 74, Loss: 1.5050956010818481
Epoch 2, Iteration 75, Loss: 1.2701709270477295
Epoch 2, Iteration 76, Loss: 1.264011025428772
Epoch 2, Iteration 77, Loss: 1.1012470722198486
Epoch 2, Iteration 78, Loss: 1.6387596130371094
Epoch 2, Iteration 79, Loss: 1.5746967792510986
Epoch 2, Iteration 80, Loss: 1.0753511190414429
Epoch 2, Iteration 81, Loss: 1.5377706289291382
Epoch 2, Iteration 82, Loss: 1.2506130933761597
Epoch 2, Iteration 83, Loss: 1.1000291109085083
Epoch 2, Iteration 84, Loss: 1.3247207403182983
Epoch 2, Iteration 85, Loss: 1.3374651670455933
Epoch 2, Iteration 86, Loss: 1.300841212272644
Epoch 2, Iteration 87, Loss: 1.240937352180481
Epoch 2, Iteration 88, Loss: 1.0999501943588257
Epoch 2, Iteration 89, Loss: 1.3374899625778198
Epoch 2, Iteration 90, Loss: 1.0808342695236206
Epoch 2, Iteration 91, Loss: 1.0904529094696045
Epoch 2, Iteration 92, Loss: 0.9275846481323242
Epoch 2, Iteration 93, Loss: 1.1733700037002563
Epoch 2, Iteration 94, Loss: 1.545705795288086
Epoch 2, Iteration 95, Loss: 1.1878979206085205
Epoch 2, Iteration 96, Loss: 1.3061805963516235
Epoch 2, Iteration 97, Loss: 1.1048293113708496
Epoch 2, Iteration 98, Loss: 1.2530295848846436
Epoch 2, Iteration 99, Loss: 1.3652034997940063
Epoch 2, Iteration 100, Loss: 1.0712153911590576
Epoch 2, Iteration 100, Valid Loss: 1.1030675172805786
Epoch 2, Iteration 101, Loss: 1.3891394138336182
Epoch 2, Iteration 102, Loss: 1.3199727535247803
Epoch 2, Iteration 103, Loss: 1.1454373598098755
Epoch 2, Iteration 104, Loss: 1.4287923574447632
Epoch 2, Iteration 105, Loss: 1.336130976676941
Epoch 2, Iteration 106, Loss: 1.25489342212677
Epoch 2, Iteration 107, Loss: 1.6456389427185059
Epoch 2, Iteration 108, Loss: 1.2907077074050903
Epoch 2, Iteration 109, Loss: 1.5018826723098755
Epoch 2, Iteration 110, Loss: 1.5889348983764648
Epoch 2, Iteration 111, Loss: 1.3761584758758545
Epoch 2, Iteration 112, Loss: 1.2938551902770996
Epoch 2, Iteration 113, Loss: 1.1676092147827148
Epoch 2, Iteration 114, Loss: 1.665952205657959
Epoch 2, Iteration 115, Loss: 1.4273051023483276
Epoch 2, Iteration 116, Loss: 1.210695743560791
Epoch 2, Iteration 117, Loss: 1.0649958848953247
Epoch 2, Iteration 118, Loss: 1.4542828798294067
Epoch 2, Iteration 119, Loss: 1.240682601928711
Epoch 2, Iteration 120, Loss: 1.1999189853668213
Epoch 2, Iteration 121, Loss: 1.3058868646621704
Epoch 2, Iteration 122, Loss: 1.2172551155090332
Epoch 2, Iteration 123, Loss: 1.160948395729065
Epoch 2, Iteration 124, Loss: 1.2230794429779053
Epoch 2, Iteration 125, Loss: 1.1328763961791992
Epoch 2, Iteration 126, Loss: 1.1238386631011963
Epoch 2, Iteration 127, Loss: 1.4741414785385132
Epoch 2, Iteration 128, Loss: 1.184247612953186
Epoch 2, Iteration 129, Loss: 1.4000476598739624
Epoch 2, Iteration 130, Loss: 1.6121044158935547
Epoch 2, Iteration 131, Loss: 1.2381064891815186
Epoch 2, Iteration 132, Loss: 1.211764931678772
Epoch 2, Iteration 133, Loss: 1.4387640953063965
Epoch 2, Iteration 134, Loss: 1.238364815711975
Epoch 2, Iteration 135, Loss: 1.40717613697052
Epoch 2, Iteration 136, Loss: 1.2149372100830078
Epoch 2, Iteration 137, Loss: 1.1779533624649048
Epoch 2, Iteration 138, Loss: 1.1822452545166016
Epoch 2, Iteration 139, Loss: 1.0853763818740845
Epoch 2, Iteration 140, Loss: 1.1429606676101685
Epoch 2, Iteration 141, Loss: 1.6542880535125732
Epoch 2, Iteration 142, Loss: 1.1350570917129517
Epoch 2, Iteration 143, Loss: 1.1943308115005493
Epoch 2, Iteration 144, Loss: 1.1922088861465454
Epoch 2, Iteration 145, Loss: 1.2752357721328735
Epoch 2, Iteration 146, Loss: 1.2858319282531738
Epoch 2, Iteration 147, Loss: 1.2370882034301758
Epoch 2, Iteration 148, Loss: 1.0160777568817139
Epoch 2, Iteration 149, Loss: 1.128968596458435
Epoch 2, Iteration 150, Loss: 1.249684453010559
Epoch 2, Iteration 150, Valid Loss: 1.0590143203735352
Epoch 2, Iteration 151, Loss: 1.4817719459533691
Epoch 2, Iteration 152, Loss: 1.1880853176116943
Epoch 2, Iteration 153, Loss: 1.378589153289795
Epoch 2, Iteration 154, Loss: 1.130679965019226
Epoch 2, Iteration 155, Loss: 1.3513519763946533
Epoch 2, Iteration 156, Loss: 1.0652215480804443
Epoch 2, Iteration 157, Loss: 1.539331078529358
Epoch 2, Iteration 158, Loss: 1.3969093561172485
Epoch 2, Iteration 159, Loss: 1.2743374109268188
Epoch 2, Iteration 160, Loss: 1.1231715679168701
Epoch 2, Iteration 161, Loss: 1.2239477634429932
Epoch 2, Iteration 162, Loss: 1.5389460325241089
Epoch 2, Iteration 163, Loss: 0.983245849609375
Epoch 2, Iteration 164, Loss: 1.060785174369812
Epoch 2, Iteration 165, Loss: 1.1505430936813354
Epoch 2, Iteration 166, Loss: 1.094492793083191
Epoch 2, Iteration 167, Loss: 1.5016801357269287
Epoch 2, Iteration 168, Loss: 1.263330101966858
Epoch 2, Iteration 169, Loss: 1.2590782642364502
Epoch 2, Iteration 170, Loss: 1.4304208755493164
Epoch 2, Iteration 171, Loss: 1.329261064529419
Epoch 2, Iteration 172, Loss: 1.4414761066436768
Epoch 2, Iteration 173, Loss: 1.1687816381454468
Epoch 2, Iteration 174, Loss: 1.3005338907241821
Epoch 2, Iteration 175, Loss: 1.2631235122680664
Epoch 2, Iteration 176, Loss: 0.9368302822113037
Epoch 2, Iteration 177, Loss: 1.6147526502609253
Epoch 2, Iteration 178, Loss: 1.451687216758728
Epoch 2, Iteration 179, Loss: 1.3375521898269653
Epoch 2, Iteration 180, Loss: 1.2262414693832397
Epoch 2, Iteration 181, Loss: 1.3687846660614014
Epoch 2, Iteration 182, Loss: 1.3614848852157593
Epoch 2, Iteration 183, Loss: 1.0939810276031494
Epoch 2, Iteration 184, Loss: 1.0769323110580444
Epoch 2, Iteration 185, Loss: 1.1292591094970703
Epoch 2, Iteration 186, Loss: 1.1368470191955566
Epoch 2, Iteration 187, Loss: 1.2261943817138672
Epoch 2, Iteration 188, Loss: 1.1919997930526733
Epoch 2, Iteration 189, Loss: 1.1116359233856201
Epoch 2, Iteration 190, Loss: 1.4385687112808228
Epoch 2, Iteration 191, Loss: 1.3155633211135864
Epoch 2, Iteration 192, Loss: 1.3175166845321655
Epoch 2, Iteration 193, Loss: 1.2632704973220825
Epoch 2, Iteration 194, Loss: 1.1043936014175415
Epoch 2, Iteration 195, Loss: 1.1946059465408325
Epoch 2, Iteration 196, Loss: 1.320804476737976
Epoch 2, Iteration 197, Loss: 1.2107627391815186
Epoch 2, Iteration 198, Loss: 1.2173362970352173
Epoch 2, Iteration 199, Loss: 1.3772296905517578
Epoch 2, Iteration 200, Loss: 1.1935757398605347
Epoch 2, Iteration 200, Valid Loss: 1.0429996252059937
Epoch 2, Iteration 201, Loss: 1.2058026790618896
Epoch 2, Iteration 202, Loss: 1.2992796897888184
Epoch 2, Iteration 203, Loss: 1.12651789188385
Epoch 2, Iteration 204, Loss: 1.104512333869934
Epoch 2, Iteration 205, Loss: 0.9533286094665527
Epoch 2, Iteration 206, Loss: 1.0745385885238647
Epoch 2, Iteration 207, Loss: 1.3084948062896729
Epoch 2, Iteration 208, Loss: 1.6047544479370117
Epoch 2, Iteration 209, Loss: 1.568177580833435
Epoch 2, Iteration 210, Loss: 1.6548279523849487
Epoch 2, Iteration 211, Loss: 1.7727138996124268
Epoch 2, Iteration 212, Loss: 1.5409343242645264
Epoch 2, Iteration 213, Loss: 1.679283618927002
Epoch 2, Iteration 214, Loss: 1.2093660831451416
Epoch 2, Iteration 215, Loss: 1.1539944410324097
Epoch 2, Iteration 216, Loss: 1.1023638248443604
Epoch 2, Iteration 217, Loss: 1.218210220336914
Epoch 2, Iteration 218, Loss: 1.319913625717163
Epoch 2, Iteration 219, Loss: 1.2899518013000488
Epoch 2, Iteration 220, Loss: 1.2587751150131226
Epoch 2, Iteration 221, Loss: 1.2567282915115356
Epoch 2, Iteration 222, Loss: 1.1748398542404175
Epoch 2, Iteration 223, Loss: 1.0945098400115967
Epoch 2, Iteration 224, Loss: 1.0238933563232422
Epoch 2, Iteration 225, Loss: 1.3052597045898438
Epoch 2, Iteration 226, Loss: 1.4083878993988037
Epoch 2, Iteration 227, Loss: 1.3909844160079956
Epoch 2, Iteration 228, Loss: 1.0856198072433472
Epoch 2, Iteration 229, Loss: 1.117658019065857
Epoch 2, Iteration 230, Loss: 1.1797212362289429
Epoch 2, Iteration 231, Loss: 1.3231159448623657
Epoch 2, Iteration 232, Loss: 1.2190901041030884
Epoch 2, Iteration 233, Loss: 1.6343528032302856
Epoch 2, Iteration 234, Loss: 1.4636688232421875
Epoch 2, Iteration 235, Loss: 1.0065624713897705
Epoch 2, Iteration 236, Loss: 1.3197182416915894
Epoch 2, Iteration 237, Loss: 1.2396130561828613
Epoch 2, Iteration 238, Loss: 1.0741064548492432
Epoch 2, Iteration 239, Loss: 1.271323800086975
Epoch 2, Iteration 240, Loss: 1.09678053855896
Epoch 2, Iteration 241, Loss: 1.1904034614562988
Epoch 2, Iteration 242, Loss: 1.0539668798446655
Epoch 2, Iteration 243, Loss: 1.2537822723388672
Epoch 2, Iteration 244, Loss: 1.1632721424102783
Epoch 2, Iteration 245, Loss: 0.9533917903900146
Epoch 2, Iteration 246, Loss: 1.2224875688552856
Epoch 2, Iteration 247, Loss: 1.0542608499526978
Epoch 2, Iteration 248, Loss: 1.2136319875717163
Epoch 2, Iteration 249, Loss: 1.1444156169891357
Epoch 2, Iteration 250, Loss: 1.403063416481018
Epoch 2, Iteration 250, Valid Loss: 0.985991358757019
Epoch 2, Iteration 251, Loss: 1.2166633605957031
Epoch 2, Iteration 252, Loss: 1.3595855236053467
Epoch 2, Iteration 253, Loss: 1.2595386505126953
Epoch 2, Iteration 254, Loss: 1.2551109790802002
Epoch 2, Iteration 255, Loss: 1.2381298542022705
Epoch 2, Iteration 256, Loss: 1.3718409538269043
Epoch 2, Iteration 257, Loss: 1.0422435998916626
Epoch 2, Iteration 258, Loss: 1.3500118255615234
Epoch 2, Iteration 259, Loss: 0.9567878842353821
Epoch 2, Iteration 260, Loss: 1.308213233947754
Epoch 2, Iteration 261, Loss: 1.3028963804244995
Epoch 2, Iteration 262, Loss: 1.0123953819274902
Epoch 2, Iteration 263, Loss: 1.4050441980361938
Epoch 2, Iteration 264, Loss: 1.247766137123108
Epoch 2, Iteration 265, Loss: 1.3283464908599854
Epoch 2, Iteration 266, Loss: 1.2787246704101562
Epoch 2, Iteration 267, Loss: 1.1836777925491333
Epoch 2, Iteration 268, Loss: 1.1920043230056763
Epoch 2, Iteration 269, Loss: 1.1557906866073608
Epoch 2, Iteration 270, Loss: 1.1069351434707642
Epoch 2, Iteration 271, Loss: 1.2651410102844238
Epoch 2, Iteration 272, Loss: 1.0737520456314087
Epoch 2, Iteration 273, Loss: 1.1728070974349976
Epoch 2, Iteration 274, Loss: 1.1381306648254395
Epoch 2, Iteration 275, Loss: 0.9935009479522705
Epoch 2, Iteration 276, Loss: 1.0807745456695557
Epoch 2, Iteration 277, Loss: 1.0871856212615967
Epoch 2, Iteration 278, Loss: 1.1773782968521118
Epoch 2, Iteration 279, Loss: 1.049915075302124
Epoch 2, Iteration 280, Loss: 0.9355392456054688
Epoch 2, Iteration 281, Loss: 1.1026726961135864
Epoch 2, Iteration 282, Loss: 1.1246956586837769
Epoch 2, Iteration 283, Loss: 1.14919114112854
Epoch 2, Iteration 284, Loss: 1.1093398332595825
Epoch 2, Iteration 285, Loss: 1.0680955648422241
Epoch 2, Iteration 286, Loss: 1.245273232460022
Epoch 2, Iteration 287, Loss: 1.0806971788406372
Epoch 2, Iteration 288, Loss: 1.0213826894760132
Epoch 2, Iteration 289, Loss: 1.169663429260254
Epoch 2, Iteration 290, Loss: 1.0439858436584473
Epoch 2, Iteration 291, Loss: 1.0018000602722168
Epoch 2, Iteration 292, Loss: 1.3471516370773315
Epoch 2, Iteration 293, Loss: 1.359987497329712
Epoch 2, Iteration 294, Loss: 1.1301649808883667
Epoch 2, Iteration 295, Loss: 1.1145488023757935
Epoch 2, Iteration 296, Loss: 1.3848927021026611
Epoch 2, Iteration 297, Loss: 1.308872938156128
Epoch 2, Iteration 298, Loss: 1.1648415327072144
Epoch 2, Iteration 299, Loss: 1.0080671310424805
Epoch 2, Iteration 300, Loss: 0.8661330938339233
Epoch 2, Iteration 300, Valid Loss: 0.941214919090271
Epoch 2, Iteration 301, Loss: 1.0956426858901978
Epoch 2, Iteration 302, Loss: 1.1172891855239868
Epoch 2, Iteration 303, Loss: 1.3110929727554321
Epoch 2, Iteration 304, Loss: 1.062587022781372
Epoch 2, Iteration 305, Loss: 1.0172067880630493
Epoch 2, Iteration 306, Loss: 1.0366710424423218
Epoch 2, Iteration 307, Loss: 1.1005442142486572
Epoch 2, Iteration 308, Loss: 0.9919312596321106
Epoch 2, Iteration 309, Loss: 1.159654140472412
Epoch 2, Iteration 310, Loss: 1.3899539709091187
Epoch 2, Iteration 311, Loss: 0.9934293031692505
Epoch 2, Iteration 312, Loss: 1.0814578533172607
Epoch 2, Iteration 313, Loss: 1.1922624111175537
Epoch 2, Iteration 314, Loss: 1.044293761253357
Epoch 2, Iteration 315, Loss: 0.9899454712867737
Epoch 2, Iteration 316, Loss: 1.2335302829742432
Epoch 2, Iteration 317, Loss: 1.1147055625915527
Epoch 2, Iteration 318, Loss: 0.8958510160446167
Epoch 2, Iteration 319, Loss: 1.1550326347351074
Epoch 2, Iteration 320, Loss: 0.8028846979141235
Epoch 2, Iteration 321, Loss: 1.0047028064727783
Epoch 2, Iteration 322, Loss: 1.0193761587142944
Epoch 2, Iteration 323, Loss: 1.1041219234466553
Epoch 2, Iteration 324, Loss: 1.2299089431762695
Epoch 2, Iteration 325, Loss: 1.0956635475158691
Epoch 2, Iteration 326, Loss: 1.1071244478225708
Epoch 2, Iteration 327, Loss: 0.9092209339141846
Epoch 2, Iteration 328, Loss: 1.0162434577941895
Epoch 2, Iteration 329, Loss: 1.210309624671936
Epoch 2, Iteration 330, Loss: 0.8920063972473145
Epoch 2, Iteration 331, Loss: 1.0307791233062744
Epoch 2, Iteration 332, Loss: 1.2267656326293945
Epoch 2, Iteration 333, Loss: 1.3230599164962769
Epoch 2, Iteration 334, Loss: 0.9319519996643066
Epoch 2, Iteration 335, Loss: 1.2995468378067017
Epoch 2, Iteration 336, Loss: 1.1129332780838013
Epoch 2, Iteration 337, Loss: 0.8812860250473022
Epoch 2, Iteration 338, Loss: 1.1973434686660767
Epoch 2, Iteration 339, Loss: 1.0048469305038452
Epoch 2, Iteration 340, Loss: 0.9464308023452759
Epoch 2, Iteration 341, Loss: 1.07508385181427
Epoch 2, Iteration 342, Loss: 1.0455026626586914
Epoch 2, Iteration 343, Loss: 1.1671037673950195
Epoch 2, Iteration 344, Loss: 1.2346174716949463
Epoch 2, Iteration 345, Loss: 1.24411940574646
Epoch 2, Iteration 346, Loss: 1.1289629936218262
Epoch 2, Iteration 347, Loss: 0.9780958294868469
Epoch 2, Iteration 348, Loss: 0.9398738145828247
Epoch 2, Iteration 349, Loss: 1.0260486602783203
Epoch 2, Iteration 350, Loss: 0.9273669123649597
Epoch 2, Iteration 350, Valid Loss: 0.9549928307533264
Epoch 2, Iteration 351, Loss: 1.118416666984558
Epoch 2, Iteration 352, Loss: 0.9780813455581665
Epoch 2, Iteration 353, Loss: 1.0547291040420532
Epoch 2, Iteration 354, Loss: 0.9888044595718384
Epoch 2, Iteration 355, Loss: 1.0930322408676147
Epoch 2, Iteration 356, Loss: 1.110621690750122
Epoch 2, Iteration 357, Loss: 0.9997664093971252
Epoch 2, Iteration 358, Loss: 1.0730623006820679
Epoch 2, Iteration 359, Loss: 0.9565787315368652
Epoch 2, Iteration 360, Loss: 0.9841212034225464
Epoch 2, Iteration 361, Loss: 0.997808575630188
Epoch 2, Iteration 362, Loss: 1.0469703674316406
Epoch 2, Iteration 363, Loss: 0.9691178798675537
Epoch 2, Iteration 364, Loss: 1.1092610359191895
Epoch 2, Iteration 365, Loss: 1.319387674331665
Epoch 2, Iteration 366, Loss: 1.2308993339538574
Epoch 2, Iteration 367, Loss: 1.204268217086792
Epoch 2, Iteration 368, Loss: 1.1583144664764404
Epoch 2, Iteration 369, Loss: 1.1920385360717773
Epoch 2, Iteration 370, Loss: 1.136528730392456
Epoch 2, Iteration 371, Loss: 1.1459465026855469
Epoch 2, Iteration 372, Loss: 0.9772014021873474
Epoch 2, Iteration 373, Loss: 0.955428957939148
Epoch 2, Iteration 374, Loss: 1.227167010307312
Epoch 2, Iteration 375, Loss: 0.9440174102783203
Epoch 2, Iteration 376, Loss: 1.238476276397705
Epoch 2, Iteration 377, Loss: 1.096373438835144
Epoch 2, Iteration 378, Loss: 0.9520246386528015
Epoch 2, Iteration 379, Loss: 0.9661081433296204
Epoch 2, Iteration 380, Loss: 0.9770174622535706
Epoch 2, Iteration 381, Loss: 0.9658941626548767
Epoch 2, Iteration 382, Loss: 0.8490253686904907
Epoch 2, Iteration 383, Loss: 1.0020315647125244
Epoch 2, Iteration 384, Loss: 1.0898722410202026
Epoch 2, Iteration 385, Loss: 0.9513973593711853
Epoch 2, Iteration 386, Loss: 1.0404490232467651
Epoch 2, Iteration 387, Loss: 1.3068523406982422
Epoch 2, Iteration 388, Loss: 1.0356459617614746
Epoch 2, Iteration 389, Loss: 1.1310940980911255
Epoch 2, Iteration 390, Loss: 1.254923939704895
Epoch 2, Iteration 391, Loss: 1.071784496307373
Epoch 3/15, Loss: 1.2252244298555413
Epoch 3, Iteration 0, Loss: 1.1641385555267334
Epoch 3, Iteration 1, Loss: 1.3637230396270752
Epoch 3, Iteration 2, Loss: 1.3176958560943604
Epoch 3, Iteration 3, Loss: 1.2116209268569946
Epoch 3, Iteration 4, Loss: 1.148163080215454
Epoch 3, Iteration 5, Loss: 1.2777574062347412
Epoch 3, Iteration 6, Loss: 1.1102975606918335
Epoch 3, Iteration 7, Loss: 1.1176131963729858
Epoch 3, Iteration 8, Loss: 1.1874511241912842
Epoch 3, Iteration 9, Loss: 1.3173099756240845
Epoch 3, Iteration 10, Loss: 1.2071696519851685
Epoch 3, Iteration 11, Loss: 1.1192716360092163
Epoch 3, Iteration 12, Loss: 1.070862889289856
Epoch 3, Iteration 13, Loss: 1.282929539680481
Epoch 3, Iteration 14, Loss: 1.1027774810791016
Epoch 3, Iteration 15, Loss: 1.240773320198059
Epoch 3, Iteration 16, Loss: 1.0211865901947021
Epoch 3, Iteration 17, Loss: 1.368262529373169
Epoch 3, Iteration 18, Loss: 1.2020686864852905
Epoch 3, Iteration 19, Loss: 1.2434638738632202
Epoch 3, Iteration 20, Loss: 1.1259726285934448
Epoch 3, Iteration 21, Loss: 1.027970314025879
Epoch 3, Iteration 22, Loss: 1.1242753267288208
Epoch 3, Iteration 23, Loss: 1.0981554985046387
Epoch 3, Iteration 24, Loss: 1.062686562538147
Epoch 3, Iteration 25, Loss: 0.996124804019928
Epoch 3, Iteration 26, Loss: 1.4744619131088257
Epoch 3, Iteration 27, Loss: 1.1512761116027832
Epoch 3, Iteration 28, Loss: 1.2190465927124023
Epoch 3, Iteration 29, Loss: 1.4248597621917725
Epoch 3, Iteration 30, Loss: 1.2066539525985718
Epoch 3, Iteration 31, Loss: 1.22140371799469
Epoch 3, Iteration 32, Loss: 1.1933400630950928
Epoch 3, Iteration 33, Loss: 1.1959046125411987
Epoch 3, Iteration 34, Loss: 1.093108892440796
Epoch 3, Iteration 35, Loss: 1.2333601713180542
Epoch 3, Iteration 36, Loss: 1.056677222251892
Epoch 3, Iteration 37, Loss: 1.0395513772964478
Epoch 3, Iteration 38, Loss: 1.04006826877594
Epoch 3, Iteration 39, Loss: 1.070136547088623
Epoch 3, Iteration 40, Loss: 1.197893738746643
Epoch 3, Iteration 41, Loss: 1.1276686191558838
Epoch 3, Iteration 42, Loss: 1.2011077404022217
Epoch 3, Iteration 43, Loss: 0.9644978642463684
Epoch 3, Iteration 44, Loss: 1.1141523122787476
Epoch 3, Iteration 45, Loss: 1.080623984336853
Epoch 3, Iteration 46, Loss: 1.15419340133667
Epoch 3, Iteration 47, Loss: 1.1628371477127075
Epoch 3, Iteration 48, Loss: 1.3053350448608398
Epoch 3, Iteration 49, Loss: 1.103352665901184
Epoch 3, Iteration 50, Loss: 1.082261085510254
Epoch 3, Iteration 50, Valid Loss: 0.8912926316261292
Epoch 3, Iteration 51, Loss: 1.0361707210540771
Epoch 3, Iteration 52, Loss: 1.0836231708526611
Epoch 3, Iteration 53, Loss: 1.1570571660995483
Epoch 3, Iteration 54, Loss: 1.4303617477416992
Epoch 3, Iteration 55, Loss: 1.2243138551712036
Epoch 3, Iteration 56, Loss: 1.0160043239593506
Epoch 3, Iteration 57, Loss: 1.2175806760787964
Epoch 3, Iteration 58, Loss: 1.0677040815353394
Epoch 3, Iteration 59, Loss: 1.0727490186691284
Epoch 3, Iteration 60, Loss: 1.0879193544387817
Epoch 3, Iteration 61, Loss: 0.9661374688148499
Epoch 3, Iteration 62, Loss: 1.0511915683746338
Epoch 3, Iteration 63, Loss: 0.9377236366271973
Epoch 3, Iteration 64, Loss: 1.0578486919403076
Epoch 3, Iteration 65, Loss: 1.2625325918197632
Epoch 3, Iteration 66, Loss: 1.1937541961669922
Epoch 3, Iteration 67, Loss: 0.9604455828666687
Epoch 3, Iteration 68, Loss: 1.2998182773590088
Epoch 3, Iteration 69, Loss: 1.3781629800796509
Epoch 3, Iteration 70, Loss: 1.351517677307129
Epoch 3, Iteration 71, Loss: 1.17820405960083
Epoch 3, Iteration 72, Loss: 1.1281676292419434
Epoch 3, Iteration 73, Loss: 1.0633883476257324
Epoch 3, Iteration 74, Loss: 1.5201948881149292
Epoch 3, Iteration 75, Loss: 1.061237096786499
Epoch 3, Iteration 76, Loss: 1.112030029296875
Epoch 3, Iteration 77, Loss: 0.9161837697029114
Epoch 3, Iteration 78, Loss: 1.3689908981323242
Epoch 3, Iteration 79, Loss: 1.381077527999878
Epoch 3, Iteration 80, Loss: 0.9920129776000977
Epoch 3, Iteration 81, Loss: 1.290735125541687
Epoch 3, Iteration 82, Loss: 1.0997004508972168
Epoch 3, Iteration 83, Loss: 0.9230185151100159
Epoch 3, Iteration 84, Loss: 1.2292217016220093
Epoch 3, Iteration 85, Loss: 1.192324161529541
Epoch 3, Iteration 86, Loss: 1.1614898443222046
Epoch 3, Iteration 87, Loss: 1.1153331995010376
Epoch 3, Iteration 88, Loss: 1.1159100532531738
Epoch 3, Iteration 89, Loss: 1.3811321258544922
Epoch 3, Iteration 90, Loss: 0.8688266277313232
Epoch 3, Iteration 91, Loss: 1.0145989656448364
Epoch 3, Iteration 92, Loss: 0.8319767117500305
Epoch 3, Iteration 93, Loss: 0.9670761227607727
Epoch 3, Iteration 94, Loss: 1.370466947555542
Epoch 3, Iteration 95, Loss: 0.9565966129302979
Epoch 3, Iteration 96, Loss: 1.1129369735717773
Epoch 3, Iteration 97, Loss: 0.9649690985679626
Epoch 3, Iteration 98, Loss: 1.2958723306655884
Epoch 3, Iteration 99, Loss: 1.142082929611206
Epoch 3, Iteration 100, Loss: 0.9332523941993713
Epoch 3, Iteration 100, Valid Loss: 0.942785382270813
Epoch 3, Iteration 101, Loss: 1.1824315786361694
Epoch 3, Iteration 102, Loss: 1.121115803718567
Epoch 3, Iteration 103, Loss: 1.0790131092071533
Epoch 3, Iteration 104, Loss: 1.2260115146636963
Epoch 3, Iteration 105, Loss: 1.133427381515503
Epoch 3, Iteration 106, Loss: 1.1108438968658447
Epoch 3, Iteration 107, Loss: 1.571319818496704
Epoch 3, Iteration 108, Loss: 1.0042794942855835
Epoch 3, Iteration 109, Loss: 1.3666553497314453
Epoch 3, Iteration 110, Loss: 1.351923942565918
Epoch 3, Iteration 111, Loss: 1.0812662839889526
Epoch 3, Iteration 112, Loss: 1.2187719345092773
Epoch 3, Iteration 113, Loss: 0.9845358729362488
Epoch 3, Iteration 114, Loss: 1.4002174139022827
Epoch 3, Iteration 115, Loss: 1.2654874324798584
Epoch 3, Iteration 116, Loss: 1.053905963897705
Epoch 3, Iteration 117, Loss: 0.8709203600883484
Epoch 3, Iteration 118, Loss: 1.2245135307312012
Epoch 3, Iteration 119, Loss: 1.0238065719604492
Epoch 3, Iteration 120, Loss: 1.2082232236862183
Epoch 3, Iteration 121, Loss: 0.9954643249511719
Epoch 3, Iteration 122, Loss: 1.2030242681503296
Epoch 3, Iteration 123, Loss: 1.0909199714660645
Epoch 3, Iteration 124, Loss: 1.009216070175171
Epoch 3, Iteration 125, Loss: 1.0416181087493896
Epoch 3, Iteration 126, Loss: 1.044505000114441
Epoch 3, Iteration 127, Loss: 1.3698982000350952
Epoch 3, Iteration 128, Loss: 1.1695663928985596
Epoch 3, Iteration 129, Loss: 1.1511664390563965
Epoch 3, Iteration 130, Loss: 1.4750688076019287
Epoch 3, Iteration 131, Loss: 1.0000066757202148
Epoch 3, Iteration 132, Loss: 1.0541329383850098
Epoch 3, Iteration 133, Loss: 1.3387247323989868
Epoch 3, Iteration 134, Loss: 1.133695363998413
Epoch 3, Iteration 135, Loss: 1.2087537050247192
Epoch 3, Iteration 136, Loss: 1.066142201423645
Epoch 3, Iteration 137, Loss: 1.1385409832000732
Epoch 3, Iteration 138, Loss: 0.9070355892181396
Epoch 3, Iteration 139, Loss: 1.0646134614944458
Epoch 3, Iteration 140, Loss: 1.035327672958374
Epoch 3, Iteration 141, Loss: 1.503759741783142
Epoch 3, Iteration 142, Loss: 1.0821095705032349
Epoch 3, Iteration 143, Loss: 1.0831917524337769
Epoch 3, Iteration 144, Loss: 1.0703340768814087
Epoch 3, Iteration 145, Loss: 1.0316650867462158
Epoch 3, Iteration 146, Loss: 1.1452600955963135
Epoch 3, Iteration 147, Loss: 1.0145771503448486
Epoch 3, Iteration 148, Loss: 0.8990993499755859
Epoch 3, Iteration 149, Loss: 1.04999577999115
Epoch 3, Iteration 150, Loss: 1.0763837099075317
Epoch 3, Iteration 150, Valid Loss: 0.8888009190559387
Epoch 3, Iteration 151, Loss: 1.2658193111419678
Epoch 3, Iteration 152, Loss: 1.0805376768112183
Epoch 3, Iteration 153, Loss: 1.1270833015441895
Epoch 3, Iteration 154, Loss: 0.9641849398612976
Epoch 3, Iteration 155, Loss: 1.230021357536316
Epoch 3, Iteration 156, Loss: 0.9924357533454895
Epoch 3, Iteration 157, Loss: 1.3258895874023438
Epoch 3, Iteration 158, Loss: 1.143977403640747
Epoch 3, Iteration 159, Loss: 1.2297316789627075
Epoch 3, Iteration 160, Loss: 0.9877855181694031
Epoch 3, Iteration 161, Loss: 1.0751826763153076
Epoch 3, Iteration 162, Loss: 1.356728196144104
Epoch 3, Iteration 163, Loss: 0.8798678517341614
Epoch 3, Iteration 164, Loss: 1.1009190082550049
Epoch 3, Iteration 165, Loss: 0.9980824589729309
Epoch 3, Iteration 166, Loss: 0.9582971334457397
Epoch 3, Iteration 167, Loss: 1.3450957536697388
Epoch 3, Iteration 168, Loss: 1.1459277868270874
Epoch 3, Iteration 169, Loss: 1.1567248106002808
Epoch 3, Iteration 170, Loss: 1.2771905660629272
Epoch 3, Iteration 171, Loss: 1.2746353149414062
Epoch 3, Iteration 172, Loss: 1.2062175273895264
Epoch 3, Iteration 173, Loss: 1.0309292078018188
Epoch 3, Iteration 174, Loss: 1.1574032306671143
Epoch 3, Iteration 175, Loss: 1.2452576160430908
Epoch 3, Iteration 176, Loss: 0.8963403701782227
Epoch 3, Iteration 177, Loss: 1.4698046445846558
Epoch 3, Iteration 178, Loss: 1.2378549575805664
Epoch 3, Iteration 179, Loss: 1.171756625175476
Epoch 3, Iteration 180, Loss: 1.0270260572433472
Epoch 3, Iteration 181, Loss: 1.2656923532485962
Epoch 3, Iteration 182, Loss: 1.2060458660125732
Epoch 3, Iteration 183, Loss: 0.9722725749015808
Epoch 3, Iteration 184, Loss: 1.178126335144043
Epoch 3, Iteration 185, Loss: 0.9838789105415344
Epoch 3, Iteration 186, Loss: 0.9888346791267395
Epoch 3, Iteration 187, Loss: 1.0252089500427246
Epoch 3, Iteration 188, Loss: 1.0491896867752075
Epoch 3, Iteration 189, Loss: 1.0294642448425293
Epoch 3, Iteration 190, Loss: 1.3679533004760742
Epoch 3, Iteration 191, Loss: 1.1369107961654663
Epoch 3, Iteration 192, Loss: 1.1578618288040161
Epoch 3, Iteration 193, Loss: 1.1637741327285767
Epoch 3, Iteration 194, Loss: 1.0835444927215576
Epoch 3, Iteration 195, Loss: 0.9780138731002808
Epoch 3, Iteration 196, Loss: 1.1008105278015137
Epoch 3, Iteration 197, Loss: 1.1271520853042603
Epoch 3, Iteration 198, Loss: 1.097278118133545
Epoch 3, Iteration 199, Loss: 1.1458309888839722
Epoch 3, Iteration 200, Loss: 0.9791399240493774
Epoch 3, Iteration 200, Valid Loss: 0.8715465664863586
Epoch 3, Iteration 201, Loss: 1.0509777069091797
Epoch 3, Iteration 202, Loss: 1.115858554840088
Epoch 3, Iteration 203, Loss: 0.990219235420227
Epoch 3, Iteration 204, Loss: 1.0056238174438477
Epoch 3, Iteration 205, Loss: 0.8537067770957947
Epoch 3, Iteration 206, Loss: 0.9012641310691833
Epoch 3, Iteration 207, Loss: 1.1837177276611328
Epoch 3, Iteration 208, Loss: 1.5375564098358154
Epoch 3, Iteration 209, Loss: 1.4673389196395874
Epoch 3, Iteration 210, Loss: 1.4932780265808105
Epoch 3, Iteration 211, Loss: 1.5765272378921509
Epoch 3, Iteration 212, Loss: 1.3258380889892578
Epoch 3, Iteration 213, Loss: 1.5202054977416992
Epoch 3, Iteration 214, Loss: 1.1584193706512451
Epoch 3, Iteration 215, Loss: 1.0162861347198486
Epoch 3, Iteration 216, Loss: 1.0785199403762817
Epoch 3, Iteration 217, Loss: 1.0474140644073486
Epoch 3, Iteration 218, Loss: 1.1698884963989258
Epoch 3, Iteration 219, Loss: 1.2267065048217773
Epoch 3, Iteration 220, Loss: 1.0924490690231323
Epoch 3, Iteration 221, Loss: 1.067989468574524
Epoch 3, Iteration 222, Loss: 1.1484380960464478
Epoch 3, Iteration 223, Loss: 1.0206701755523682
Epoch 3, Iteration 224, Loss: 0.9237676858901978
Epoch 3, Iteration 225, Loss: 1.1694320440292358
Epoch 3, Iteration 226, Loss: 1.2708507776260376
Epoch 3, Iteration 227, Loss: 1.2210955619812012
Epoch 3, Iteration 228, Loss: 1.0144001245498657
Epoch 3, Iteration 229, Loss: 1.010628581047058
Epoch 3, Iteration 230, Loss: 1.0594536066055298
Epoch 3, Iteration 231, Loss: 1.0013259649276733
Epoch 3, Iteration 232, Loss: 1.2734309434890747
Epoch 3, Iteration 233, Loss: 1.3651546239852905
Epoch 3, Iteration 234, Loss: 1.3045594692230225
Epoch 3, Iteration 235, Loss: 0.9043905138969421
Epoch 3, Iteration 236, Loss: 1.1737165451049805
Epoch 3, Iteration 237, Loss: 1.0688029527664185
Epoch 3, Iteration 238, Loss: 0.9446855187416077
Epoch 3, Iteration 239, Loss: 1.2719115018844604
Epoch 3, Iteration 240, Loss: 0.892680287361145
Epoch 3, Iteration 241, Loss: 1.0080119371414185
Epoch 3, Iteration 242, Loss: 0.9498996734619141
Epoch 3, Iteration 243, Loss: 1.1420079469680786
Epoch 3, Iteration 244, Loss: 1.0173368453979492
Epoch 3, Iteration 245, Loss: 0.8772737979888916
Epoch 3, Iteration 246, Loss: 1.017457365989685
Epoch 3, Iteration 247, Loss: 0.9700038433074951
Epoch 3, Iteration 248, Loss: 1.070753812789917
Epoch 3, Iteration 249, Loss: 0.9691004753112793
Epoch 3, Iteration 250, Loss: 1.2310051918029785
Epoch 3, Iteration 250, Valid Loss: 0.849982500076294
Epoch 3, Iteration 251, Loss: 1.0805084705352783
Epoch 3, Iteration 252, Loss: 1.249366283416748
Epoch 3, Iteration 253, Loss: 1.2285687923431396
Epoch 3, Iteration 254, Loss: 1.1098695993423462
Epoch 3, Iteration 255, Loss: 1.1597620248794556
Epoch 3, Iteration 256, Loss: 1.1866384744644165
Epoch 3, Iteration 257, Loss: 1.0084737539291382
Epoch 3, Iteration 258, Loss: 1.1751222610473633
Epoch 3, Iteration 259, Loss: 0.8849813342094421
Epoch 3, Iteration 260, Loss: 1.1103185415267944
Epoch 3, Iteration 261, Loss: 1.0778324604034424
Epoch 3, Iteration 262, Loss: 0.8984288573265076
Epoch 3, Iteration 263, Loss: 1.2145118713378906
Epoch 3, Iteration 264, Loss: 1.104931354522705
Epoch 3, Iteration 265, Loss: 1.1422070264816284
Epoch 3, Iteration 266, Loss: 1.1350051164627075
Epoch 3, Iteration 267, Loss: 1.006165623664856
Epoch 3, Iteration 268, Loss: 0.9987993836402893
Epoch 3, Iteration 269, Loss: 0.9633957147598267
Epoch 3, Iteration 270, Loss: 0.9597208499908447
Epoch 3, Iteration 271, Loss: 1.070546269416809
Epoch 3, Iteration 272, Loss: 0.9309868216514587
Epoch 3, Iteration 273, Loss: 1.1047617197036743
Epoch 3, Iteration 274, Loss: 0.912613034248352
Epoch 3, Iteration 275, Loss: 0.9102969765663147
Epoch 3, Iteration 276, Loss: 0.9403843879699707
Epoch 3, Iteration 277, Loss: 0.9233104586601257
Epoch 3, Iteration 278, Loss: 1.1074129343032837
Epoch 3, Iteration 279, Loss: 0.9227139353752136
Epoch 3, Iteration 280, Loss: 0.8607205152511597
Epoch 3, Iteration 281, Loss: 0.9702515006065369
Epoch 3, Iteration 282, Loss: 1.0536272525787354
Epoch 3, Iteration 283, Loss: 0.9862913489341736
Epoch 3, Iteration 284, Loss: 1.0107101202011108
Epoch 3, Iteration 285, Loss: 0.9813616275787354
Epoch 3, Iteration 286, Loss: 1.0122308731079102
Epoch 3, Iteration 287, Loss: 0.9808397889137268
Epoch 3, Iteration 288, Loss: 0.8916695713996887
Epoch 3, Iteration 289, Loss: 1.0688409805297852
Epoch 3, Iteration 290, Loss: 0.8491417765617371
Epoch 3, Iteration 291, Loss: 0.9374788999557495
Epoch 3, Iteration 292, Loss: 1.1233673095703125
Epoch 3, Iteration 293, Loss: 1.2491856813430786
Epoch 3, Iteration 294, Loss: 0.989552915096283
Epoch 3, Iteration 295, Loss: 0.9867379069328308
Epoch 3, Iteration 296, Loss: 1.2609144449234009
Epoch 3, Iteration 297, Loss: 1.2031022310256958
Epoch 3, Iteration 298, Loss: 0.9968258738517761
Epoch 3, Iteration 299, Loss: 0.9741206765174866
Epoch 3, Iteration 300, Loss: 0.8281399607658386
Epoch 3, Iteration 300, Valid Loss: 0.8122333288192749
Epoch 3, Iteration 301, Loss: 0.9889159202575684
Epoch 3, Iteration 302, Loss: 0.9044119715690613
Epoch 3, Iteration 303, Loss: 1.2266942262649536
Epoch 3, Iteration 304, Loss: 0.930001437664032
Epoch 3, Iteration 305, Loss: 0.8127192854881287
Epoch 3, Iteration 306, Loss: 0.9039492011070251
Epoch 3, Iteration 307, Loss: 0.9670590162277222
Epoch 3, Iteration 308, Loss: 0.8859166502952576
Epoch 3, Iteration 309, Loss: 0.9379065036773682
Epoch 3, Iteration 310, Loss: 1.1248787641525269
Epoch 3, Iteration 311, Loss: 0.8210820555686951
Epoch 3, Iteration 312, Loss: 0.8343709111213684
Epoch 3, Iteration 313, Loss: 1.2041070461273193
Epoch 3, Iteration 314, Loss: 0.9703975915908813
Epoch 3, Iteration 315, Loss: 0.7820767760276794
Epoch 3, Iteration 316, Loss: 1.1278806924819946
Epoch 3, Iteration 317, Loss: 0.9880719780921936
Epoch 3, Iteration 318, Loss: 0.8414582014083862
Epoch 3, Iteration 319, Loss: 1.0974119901657104
Epoch 3, Iteration 320, Loss: 0.7795879244804382
Epoch 3, Iteration 321, Loss: 0.8628724217414856
Epoch 3, Iteration 322, Loss: 0.9061481952667236
Epoch 3, Iteration 323, Loss: 0.998862624168396
Epoch 3, Iteration 324, Loss: 1.0084749460220337
Epoch 3, Iteration 325, Loss: 1.0056419372558594
Epoch 3, Iteration 326, Loss: 0.9592318534851074
Epoch 3, Iteration 327, Loss: 0.8717612624168396
Epoch 3, Iteration 328, Loss: 0.9053943157196045
Epoch 3, Iteration 329, Loss: 1.1376351118087769
Epoch 3, Iteration 330, Loss: 0.7972564101219177
Epoch 3, Iteration 331, Loss: 1.015381932258606
Epoch 3, Iteration 332, Loss: 0.9913408160209656
Epoch 3, Iteration 333, Loss: 1.1639838218688965
Epoch 3, Iteration 334, Loss: 0.8892269730567932
Epoch 3, Iteration 335, Loss: 1.097224473953247
Epoch 3, Iteration 336, Loss: 0.978518545627594
Epoch 3, Iteration 337, Loss: 0.8323197364807129
Epoch 3, Iteration 338, Loss: 1.2018407583236694
Epoch 3, Iteration 339, Loss: 1.063137173652649
Epoch 3, Iteration 340, Loss: 0.8312650322914124
Epoch 3, Iteration 341, Loss: 0.8695537447929382
Epoch 3, Iteration 342, Loss: 0.9030860662460327
Epoch 3, Iteration 343, Loss: 1.0115058422088623
Epoch 3, Iteration 344, Loss: 1.1284385919570923
Epoch 3, Iteration 345, Loss: 0.9886875152587891
Epoch 3, Iteration 346, Loss: 0.9631791710853577
Epoch 3, Iteration 347, Loss: 0.7962947487831116
Epoch 3, Iteration 348, Loss: 0.8785473108291626
Epoch 3, Iteration 349, Loss: 1.01310133934021
Epoch 3, Iteration 350, Loss: 0.8364405035972595
Epoch 3, Iteration 350, Valid Loss: 0.8320814371109009
Epoch 3, Iteration 351, Loss: 1.0086307525634766
Epoch 3, Iteration 352, Loss: 0.9517488479614258
Epoch 3, Iteration 353, Loss: 0.9299371838569641
Epoch 3, Iteration 354, Loss: 0.8927383422851562
Epoch 3, Iteration 355, Loss: 0.9115539789199829
Epoch 3, Iteration 356, Loss: 1.0307031869888306
Epoch 3, Iteration 357, Loss: 0.9059122800827026
Epoch 3, Iteration 358, Loss: 0.9597399830818176
Epoch 3, Iteration 359, Loss: 0.8719119429588318
Epoch 3, Iteration 360, Loss: 0.7880678772926331
Epoch 3, Iteration 361, Loss: 0.8900988101959229
Epoch 3, Iteration 362, Loss: 1.0620691776275635
Epoch 3, Iteration 363, Loss: 0.909976601600647
Epoch 3, Iteration 364, Loss: 0.971415638923645
Epoch 3, Iteration 365, Loss: 1.2009754180908203
Epoch 3, Iteration 366, Loss: 1.0253280401229858
Epoch 3, Iteration 367, Loss: 1.069616675376892
Epoch 3, Iteration 368, Loss: 0.9698205590248108
Epoch 3, Iteration 369, Loss: 1.0620988607406616
Epoch 3, Iteration 370, Loss: 1.0688802003860474
Epoch 3, Iteration 371, Loss: 1.0139789581298828
Epoch 3, Iteration 372, Loss: 0.8720187544822693
Epoch 3, Iteration 373, Loss: 0.876653254032135
Epoch 3, Iteration 374, Loss: 1.1381500959396362
Epoch 3, Iteration 375, Loss: 0.8554465174674988
Epoch 3, Iteration 376, Loss: 1.1245790719985962
Epoch 3, Iteration 377, Loss: 0.9479237794876099
Epoch 3, Iteration 378, Loss: 0.9470189213752747
Epoch 3, Iteration 379, Loss: 0.8464686870574951
Epoch 3, Iteration 380, Loss: 0.8465123772621155
Epoch 3, Iteration 381, Loss: 0.8546815514564514
Epoch 3, Iteration 382, Loss: 0.7465135455131531
Epoch 3, Iteration 383, Loss: 0.851097047328949
Epoch 3, Iteration 384, Loss: 0.9922703504562378
Epoch 3, Iteration 385, Loss: 0.8724763989448547
Epoch 3, Iteration 386, Loss: 0.8558096885681152
Epoch 3, Iteration 387, Loss: 1.2228796482086182
Epoch 3, Iteration 388, Loss: 0.8676925301551819
Epoch 3, Iteration 389, Loss: 1.004194974899292
Epoch 3, Iteration 390, Loss: 1.1677724123001099
Epoch 3, Iteration 391, Loss: 0.9158753752708435
Epoch 4/15, Loss: 1.0835064941523027
Epoch 4, Iteration 0, Loss: 1.04493248462677
Epoch 4, Iteration 1, Loss: 1.3046401739120483
Epoch 4, Iteration 2, Loss: 1.1713558435440063
Epoch 4, Iteration 3, Loss: 1.0851577520370483
Epoch 4, Iteration 4, Loss: 1.0627371072769165
Epoch 4, Iteration 5, Loss: 1.120818018913269
Epoch 4, Iteration 6, Loss: 0.9626101851463318
Epoch 4, Iteration 7, Loss: 0.9897845387458801
Epoch 4, Iteration 8, Loss: 1.0065405368804932
Epoch 4, Iteration 9, Loss: 1.2047016620635986
Epoch 4, Iteration 10, Loss: 1.1420129537582397
Epoch 4, Iteration 11, Loss: 1.044661283493042
Epoch 4, Iteration 12, Loss: 0.8987705111503601
Epoch 4, Iteration 13, Loss: 1.0800563097000122
Epoch 4, Iteration 14, Loss: 1.0730741024017334
Epoch 4, Iteration 15, Loss: 1.1150054931640625
Epoch 4, Iteration 16, Loss: 1.0022896528244019
Epoch 4, Iteration 17, Loss: 1.1185872554779053
Epoch 4, Iteration 18, Loss: 0.9658347368240356
Epoch 4, Iteration 19, Loss: 1.0224740505218506
Epoch 4, Iteration 20, Loss: 0.9444252848625183
Epoch 4, Iteration 21, Loss: 0.9879769682884216
Epoch 4, Iteration 22, Loss: 0.9874919056892395
Epoch 4, Iteration 23, Loss: 1.0119214057922363
Epoch 4, Iteration 24, Loss: 0.9743785858154297
Epoch 4, Iteration 25, Loss: 0.9252254366874695
Epoch 4, Iteration 26, Loss: 1.3216723203659058
Epoch 4, Iteration 27, Loss: 1.02146577835083
Epoch 4, Iteration 28, Loss: 1.016772747039795
Epoch 4, Iteration 29, Loss: 1.2176268100738525
Epoch 4, Iteration 30, Loss: 1.2565345764160156
Epoch 4, Iteration 31, Loss: 0.9946423768997192
Epoch 4, Iteration 32, Loss: 1.0306272506713867
Epoch 4, Iteration 33, Loss: 1.0000278949737549
Epoch 4, Iteration 34, Loss: 0.9563770294189453
Epoch 4, Iteration 35, Loss: 1.137046456336975
Epoch 4, Iteration 36, Loss: 0.8737364411354065
Epoch 4, Iteration 37, Loss: 0.8587380051612854
Epoch 4, Iteration 38, Loss: 0.9491006731987
Epoch 4, Iteration 39, Loss: 0.9534735679626465
Epoch 4, Iteration 40, Loss: 0.9804425239562988
Epoch 4, Iteration 41, Loss: 1.0616225004196167
Epoch 4, Iteration 42, Loss: 0.9961390495300293
Epoch 4, Iteration 43, Loss: 0.9008240699768066
Epoch 4, Iteration 44, Loss: 1.0030969381332397
Epoch 4, Iteration 45, Loss: 0.9670514464378357
Epoch 4, Iteration 46, Loss: 1.0012843608856201
Epoch 4, Iteration 47, Loss: 1.0293381214141846
Epoch 4, Iteration 48, Loss: 1.1769049167633057
Epoch 4, Iteration 49, Loss: 0.9655376076698303
Epoch 4, Iteration 50, Loss: 0.9544277191162109
Epoch 4, Iteration 50, Valid Loss: 0.789033055305481
Epoch 4, Iteration 51, Loss: 0.9552796483039856
Epoch 4, Iteration 52, Loss: 0.9759229421615601
Epoch 4, Iteration 53, Loss: 1.105172038078308
Epoch 4, Iteration 54, Loss: 1.2082394361495972
Epoch 4, Iteration 55, Loss: 1.1055997610092163
Epoch 4, Iteration 56, Loss: 0.9873883724212646
Epoch 4, Iteration 57, Loss: 1.0587176084518433
Epoch 4, Iteration 58, Loss: 0.9447943568229675
Epoch 4, Iteration 59, Loss: 0.9233458638191223
Epoch 4, Iteration 60, Loss: 0.9466854929924011
Epoch 4, Iteration 61, Loss: 0.8260363936424255
Epoch 4, Iteration 62, Loss: 0.9851475358009338
Epoch 4, Iteration 63, Loss: 0.9184985756874084
Epoch 4, Iteration 64, Loss: 0.9649332761764526
Epoch 4, Iteration 65, Loss: 1.154616355895996
Epoch 4, Iteration 66, Loss: 0.9905996918678284
Epoch 4, Iteration 67, Loss: 0.7766335010528564
Epoch 4, Iteration 68, Loss: 1.0221779346466064
Epoch 4, Iteration 69, Loss: 1.1311570405960083
Epoch 4, Iteration 70, Loss: 1.2370394468307495
Epoch 4, Iteration 71, Loss: 1.0399134159088135
Epoch 4, Iteration 72, Loss: 0.975234866142273
Epoch 4, Iteration 73, Loss: 0.8397454619407654
Epoch 4, Iteration 74, Loss: 1.2318857908248901
Epoch 4, Iteration 75, Loss: 1.0675255060195923
Epoch 4, Iteration 76, Loss: 0.9524127840995789
Epoch 4, Iteration 77, Loss: 0.7767812013626099
Epoch 4, Iteration 78, Loss: 1.2348991632461548
Epoch 4, Iteration 79, Loss: 1.269155740737915
Epoch 4, Iteration 80, Loss: 0.8462423086166382
Epoch 4, Iteration 81, Loss: 1.262032151222229
Epoch 4, Iteration 82, Loss: 1.0176899433135986
Epoch 4, Iteration 83, Loss: 0.780750572681427
Epoch 4, Iteration 84, Loss: 1.1648318767547607
Epoch 4, Iteration 85, Loss: 1.0028105974197388
Epoch 4, Iteration 86, Loss: 1.2528326511383057
Epoch 4, Iteration 87, Loss: 0.9641894102096558
Epoch 4, Iteration 88, Loss: 0.9550859928131104
Epoch 4, Iteration 89, Loss: 1.1221951246261597
Epoch 4, Iteration 90, Loss: 0.8643262982368469
Epoch 4, Iteration 91, Loss: 0.9606046080589294
Epoch 4, Iteration 92, Loss: 0.7536492943763733
Epoch 4, Iteration 93, Loss: 0.9457417726516724
Epoch 4, Iteration 94, Loss: 1.2672992944717407
Epoch 4, Iteration 95, Loss: 0.8276141285896301
Epoch 4, Iteration 96, Loss: 1.0036189556121826
Epoch 4, Iteration 97, Loss: 0.8344535231590271
Epoch 4, Iteration 98, Loss: 1.0206553936004639
Epoch 4, Iteration 99, Loss: 1.0749164819717407
Epoch 4, Iteration 100, Loss: 0.8530787229537964
Epoch 4, Iteration 100, Valid Loss: 0.8358920812606812
Epoch 4, Iteration 101, Loss: 1.1367682218551636
Epoch 4, Iteration 102, Loss: 1.1306297779083252
Epoch 4, Iteration 103, Loss: 0.9723981022834778
Epoch 4, Iteration 104, Loss: 1.1775285005569458
Epoch 4, Iteration 105, Loss: 1.0836060047149658
Epoch 4, Iteration 106, Loss: 0.9983205199241638
Epoch 4, Iteration 107, Loss: 1.1856428384780884
Epoch 4, Iteration 108, Loss: 0.9502626061439514
Epoch 4, Iteration 109, Loss: 1.2779102325439453
Epoch 4, Iteration 110, Loss: 1.2964445352554321
Epoch 4, Iteration 111, Loss: 1.025543212890625
Epoch 4, Iteration 112, Loss: 1.0854824781417847
Epoch 4, Iteration 113, Loss: 0.8665279746055603
Epoch 4, Iteration 114, Loss: 1.2526308298110962
Epoch 4, Iteration 115, Loss: 1.064929723739624
Epoch 4, Iteration 116, Loss: 0.8378612399101257
Epoch 4, Iteration 117, Loss: 0.7548313736915588
Epoch 4, Iteration 118, Loss: 1.089975118637085
Epoch 4, Iteration 119, Loss: 0.8983413577079773
Epoch 4, Iteration 120, Loss: 1.0431067943572998
Epoch 4, Iteration 121, Loss: 1.026232361793518
Epoch 4, Iteration 122, Loss: 1.007375717163086
Epoch 4, Iteration 123, Loss: 0.9735695719718933
Epoch 4, Iteration 124, Loss: 0.8594191670417786
Epoch 4, Iteration 125, Loss: 0.9335448741912842
Epoch 4, Iteration 126, Loss: 0.8527103066444397
Epoch 4, Iteration 127, Loss: 1.22467839717865
Epoch 4, Iteration 128, Loss: 1.1758852005004883
Epoch 4, Iteration 129, Loss: 1.093228816986084
Epoch 4, Iteration 130, Loss: 1.3685531616210938
Epoch 4, Iteration 131, Loss: 0.8632916212081909
Epoch 4, Iteration 132, Loss: 0.8966405391693115
Epoch 4, Iteration 133, Loss: 1.2935482263565063
Epoch 4, Iteration 134, Loss: 0.9857550859451294
Epoch 4, Iteration 135, Loss: 1.164029836654663
Epoch 4, Iteration 136, Loss: 0.9759976863861084
Epoch 4, Iteration 137, Loss: 0.9328454732894897
Epoch 4, Iteration 138, Loss: 0.9089829325675964
Epoch 4, Iteration 139, Loss: 0.9844706058502197
Epoch 4, Iteration 140, Loss: 0.9412792325019836
Epoch 4, Iteration 141, Loss: 1.5075807571411133
Epoch 4, Iteration 142, Loss: 0.9746980667114258
Epoch 4, Iteration 143, Loss: 0.9841091632843018
Epoch 4, Iteration 144, Loss: 0.9087378978729248
Epoch 4, Iteration 145, Loss: 1.0709941387176514
Epoch 4, Iteration 146, Loss: 1.0315333604812622
Epoch 4, Iteration 147, Loss: 0.9750149250030518
Epoch 4, Iteration 148, Loss: 0.8013947606086731
Epoch 4, Iteration 149, Loss: 0.9442793130874634
Epoch 4, Iteration 150, Loss: 0.9885415434837341
Epoch 4, Iteration 150, Valid Loss: 0.8005504012107849
Epoch 4, Iteration 151, Loss: 1.048242449760437
Epoch 4, Iteration 152, Loss: 0.9130152463912964
Epoch 4, Iteration 153, Loss: 1.0007877349853516
Epoch 4, Iteration 154, Loss: 0.785528302192688
Epoch 4, Iteration 155, Loss: 1.0587940216064453
Epoch 4, Iteration 156, Loss: 0.9069067239761353
Epoch 4, Iteration 157, Loss: 1.395507574081421
Epoch 4, Iteration 158, Loss: 1.0592247247695923
Epoch 4, Iteration 159, Loss: 1.123335599899292
Epoch 4, Iteration 160, Loss: 0.973874032497406
Epoch 4, Iteration 161, Loss: 1.0100972652435303
Epoch 4, Iteration 162, Loss: 1.163033127784729
Epoch 4, Iteration 163, Loss: 0.7532022595405579
Epoch 4, Iteration 164, Loss: 0.9793041348457336
Epoch 4, Iteration 165, Loss: 0.944180965423584
Epoch 4, Iteration 166, Loss: 0.9875420928001404
Epoch 4, Iteration 167, Loss: 1.0977280139923096
Epoch 4, Iteration 168, Loss: 1.0338459014892578
Epoch 4, Iteration 169, Loss: 0.9565362930297852
Epoch 4, Iteration 170, Loss: 1.2069339752197266
Epoch 4, Iteration 171, Loss: 1.3006178140640259
Epoch 4, Iteration 172, Loss: 0.9766036868095398
Epoch 4, Iteration 173, Loss: 0.9348631501197815
Epoch 4, Iteration 174, Loss: 1.1666356325149536
Epoch 4, Iteration 175, Loss: 1.0312952995300293
Epoch 4, Iteration 176, Loss: 0.7275395393371582
Epoch 4, Iteration 177, Loss: 1.3922151327133179
Epoch 4, Iteration 178, Loss: 1.255466341972351
Epoch 4, Iteration 179, Loss: 1.0007988214492798
Epoch 4, Iteration 180, Loss: 0.9192686080932617
Epoch 4, Iteration 181, Loss: 1.0477421283721924
Epoch 4, Iteration 182, Loss: 1.1131047010421753
Epoch 4, Iteration 183, Loss: 0.9019898176193237
Epoch 4, Iteration 184, Loss: 0.8490493893623352
Epoch 4, Iteration 185, Loss: 0.8884671330451965
Epoch 4, Iteration 186, Loss: 1.005288004875183
Epoch 4, Iteration 187, Loss: 0.9145441651344299
Epoch 4, Iteration 188, Loss: 0.9966986775398254
Epoch 4, Iteration 189, Loss: 0.923087477684021
Epoch 4, Iteration 190, Loss: 1.0873467922210693
Epoch 4, Iteration 191, Loss: 1.0415529012680054
Epoch 4, Iteration 192, Loss: 1.051780343055725
Epoch 4, Iteration 193, Loss: 1.0371942520141602
Epoch 4, Iteration 194, Loss: 0.9284853935241699
Epoch 4, Iteration 195, Loss: 0.8590038418769836
Epoch 4, Iteration 196, Loss: 1.0250333547592163
Epoch 4, Iteration 197, Loss: 0.9701385498046875
Epoch 4, Iteration 198, Loss: 1.002532958984375
Epoch 4, Iteration 199, Loss: 1.0520479679107666
Epoch 4, Iteration 200, Loss: 0.906775176525116
Epoch 4, Iteration 200, Valid Loss: 0.7884825468063354
Epoch 4, Iteration 201, Loss: 0.9057056307792664
Epoch 4, Iteration 202, Loss: 1.0271027088165283
Epoch 4, Iteration 203, Loss: 0.8727607131004333
Epoch 4, Iteration 204, Loss: 0.9244159460067749
Epoch 4, Iteration 205, Loss: 0.7402735948562622
Epoch 4, Iteration 206, Loss: 0.8623390197753906
Epoch 4, Iteration 207, Loss: 1.0889272689819336
Epoch 4, Iteration 208, Loss: 1.375203251838684
Epoch 4, Iteration 209, Loss: 1.3705648183822632
Epoch 4, Iteration 210, Loss: 1.2871265411376953
Epoch 4, Iteration 211, Loss: 1.4108376502990723
Epoch 4, Iteration 212, Loss: 1.2058161497116089
Epoch 4, Iteration 213, Loss: 1.4323828220367432
Epoch 4, Iteration 214, Loss: 1.0427522659301758
Epoch 4, Iteration 215, Loss: 0.842722475528717
Epoch 4, Iteration 216, Loss: 0.8903362154960632
Epoch 4, Iteration 217, Loss: 0.938605010509491
Epoch 4, Iteration 218, Loss: 1.0334800481796265
Epoch 4, Iteration 219, Loss: 1.1145973205566406
Epoch 4, Iteration 220, Loss: 0.9439339637756348
Epoch 4, Iteration 221, Loss: 1.028719425201416
Epoch 4, Iteration 222, Loss: 1.0397146940231323
Epoch 4, Iteration 223, Loss: 0.9531627297401428
Epoch 4, Iteration 224, Loss: 0.8542208671569824
Epoch 4, Iteration 225, Loss: 1.1103458404541016
Epoch 4, Iteration 226, Loss: 1.1312689781188965
Epoch 4, Iteration 227, Loss: 1.1284061670303345
Epoch 4, Iteration 228, Loss: 1.06633722782135
Epoch 4, Iteration 229, Loss: 0.9239729642868042
Epoch 4, Iteration 230, Loss: 0.9833782315254211
Epoch 4, Iteration 231, Loss: 1.0202014446258545
Epoch 4, Iteration 232, Loss: 1.0726557970046997
Epoch 4, Iteration 233, Loss: 1.2659790515899658
Epoch 4, Iteration 234, Loss: 1.1072067022323608
Epoch 4, Iteration 235, Loss: 0.7578668594360352
Epoch 4, Iteration 236, Loss: 1.037058711051941
Epoch 4, Iteration 237, Loss: 0.9714236855506897
Epoch 4, Iteration 238, Loss: 0.8227131962776184
Epoch 4, Iteration 239, Loss: 1.1707442998886108
Epoch 4, Iteration 240, Loss: 0.90836501121521
Epoch 4, Iteration 241, Loss: 0.8773682117462158
Epoch 4, Iteration 242, Loss: 0.9065148830413818
Epoch 4, Iteration 243, Loss: 0.9834981560707092
Epoch 4, Iteration 244, Loss: 0.9067969918251038
Epoch 4, Iteration 245, Loss: 0.7182429432868958
Epoch 4, Iteration 246, Loss: 0.9426921010017395
Epoch 4, Iteration 247, Loss: 0.8898072242736816
Epoch 4, Iteration 248, Loss: 0.9476214647293091
Epoch 4, Iteration 249, Loss: 0.8944735527038574
Epoch 4, Iteration 250, Loss: 1.1961883306503296
Epoch 4, Iteration 250, Valid Loss: 0.7561030387878418
Epoch 4, Iteration 251, Loss: 0.9501894116401672
Epoch 4, Iteration 252, Loss: 1.0761252641677856
Epoch 4, Iteration 253, Loss: 1.1006118059158325
Epoch 4, Iteration 254, Loss: 1.0629301071166992
Epoch 4, Iteration 255, Loss: 0.9724357724189758
Epoch 4, Iteration 256, Loss: 1.0841792821884155
Epoch 4, Iteration 257, Loss: 0.7964072227478027
Epoch 4, Iteration 258, Loss: 1.1883913278579712
Epoch 4, Iteration 259, Loss: 0.8510756492614746
Epoch 4, Iteration 260, Loss: 1.0325790643692017
Epoch 4, Iteration 261, Loss: 0.9506208896636963
Epoch 4, Iteration 262, Loss: 0.8163789510726929
Epoch 4, Iteration 263, Loss: 1.102190375328064
Epoch 4, Iteration 264, Loss: 0.9832547307014465
Epoch 4, Iteration 265, Loss: 1.0239163637161255
Epoch 4, Iteration 266, Loss: 1.09663724899292
Epoch 4, Iteration 267, Loss: 0.9625368714332581
Epoch 4, Iteration 268, Loss: 0.9333730340003967
Epoch 4, Iteration 269, Loss: 0.8945204615592957
Epoch 4, Iteration 270, Loss: 0.9753462076187134
Epoch 4, Iteration 271, Loss: 1.0707552433013916
Epoch 4, Iteration 272, Loss: 0.8913940191268921
Epoch 4, Iteration 273, Loss: 0.9140327572822571
Epoch 4, Iteration 274, Loss: 0.8474997282028198
Epoch 4, Iteration 275, Loss: 0.810172975063324
Epoch 4, Iteration 276, Loss: 0.8478854894638062
Epoch 4, Iteration 277, Loss: 0.8520662188529968
Epoch 4, Iteration 278, Loss: 1.1034595966339111
Epoch 4, Iteration 279, Loss: 0.8682688474655151
Epoch 4, Iteration 280, Loss: 0.7712135910987854
Epoch 4, Iteration 281, Loss: 0.8801217675209045
Epoch 4, Iteration 282, Loss: 0.8711457848548889
Epoch 4, Iteration 283, Loss: 0.8997748494148254
Epoch 4, Iteration 284, Loss: 0.9355165958404541
Epoch 4, Iteration 285, Loss: 0.7878260612487793
Epoch 4, Iteration 286, Loss: 0.8657180666923523
Epoch 4, Iteration 287, Loss: 0.8548921942710876
Epoch 4, Iteration 288, Loss: 0.8136802911758423
Epoch 4, Iteration 289, Loss: 0.8319209218025208
Epoch 4, Iteration 290, Loss: 0.8363760113716125
Epoch 4, Iteration 291, Loss: 0.7681821584701538
Epoch 4, Iteration 292, Loss: 1.032207727432251
Epoch 4, Iteration 293, Loss: 1.0473662614822388
Epoch 4, Iteration 294, Loss: 0.899074375629425
Epoch 4, Iteration 295, Loss: 0.8355186581611633
Epoch 4, Iteration 296, Loss: 1.136673927307129
Epoch 4, Iteration 297, Loss: 1.0927625894546509
Epoch 4, Iteration 298, Loss: 0.9679494500160217
Epoch 4, Iteration 299, Loss: 0.885459303855896
Epoch 4, Iteration 300, Loss: 0.7614717483520508
Epoch 4, Iteration 300, Valid Loss: 0.751833438873291
Epoch 4, Iteration 301, Loss: 0.8769534230232239
Epoch 4, Iteration 302, Loss: 0.8666693568229675
Epoch 4, Iteration 303, Loss: 1.0209133625030518
Epoch 4, Iteration 304, Loss: 0.9227496981620789
Epoch 4, Iteration 305, Loss: 0.785307765007019
Epoch 4, Iteration 306, Loss: 0.8360674381256104
Epoch 4, Iteration 307, Loss: 0.9192147850990295
Epoch 4, Iteration 308, Loss: 0.8306171894073486
Epoch 4, Iteration 309, Loss: 0.9216207265853882
Epoch 4, Iteration 310, Loss: 1.0503677129745483
Epoch 4, Iteration 311, Loss: 0.8514872789382935
Epoch 4, Iteration 312, Loss: 0.8770484924316406
Epoch 4, Iteration 313, Loss: 1.1406798362731934
Epoch 4, Iteration 314, Loss: 0.84687739610672
Epoch 4, Iteration 315, Loss: 0.836872398853302
Epoch 4, Iteration 316, Loss: 0.9976776838302612
Epoch 4, Iteration 317, Loss: 0.923591673374176
Epoch 4, Iteration 318, Loss: 0.8875922560691833
Epoch 4, Iteration 319, Loss: 1.1049572229385376
Epoch 4, Iteration 320, Loss: 0.6908894181251526
Epoch 4, Iteration 321, Loss: 0.9398485422134399
Epoch 4, Iteration 322, Loss: 0.826592743396759
Epoch 4, Iteration 323, Loss: 0.8781220316886902
Epoch 4, Iteration 324, Loss: 0.976137638092041
Epoch 4, Iteration 325, Loss: 0.8855038285255432
Epoch 4, Iteration 326, Loss: 0.8487350940704346
Epoch 4, Iteration 327, Loss: 0.8048567771911621
Epoch 4, Iteration 328, Loss: 0.7943364977836609
Epoch 4, Iteration 329, Loss: 1.0495710372924805
Epoch 4, Iteration 330, Loss: 0.7530981302261353
Epoch 4, Iteration 331, Loss: 0.8933329582214355
Epoch 4, Iteration 332, Loss: 1.1955134868621826
Epoch 4, Iteration 333, Loss: 1.1015403270721436
Epoch 4, Iteration 334, Loss: 0.8689898252487183
Epoch 4, Iteration 335, Loss: 0.9878103733062744
Epoch 4, Iteration 336, Loss: 0.9170424938201904
Epoch 4, Iteration 337, Loss: 0.7995640635490417
Epoch 4, Iteration 338, Loss: 1.0757330656051636
Epoch 4, Iteration 339, Loss: 0.9188842177391052
Epoch 4, Iteration 340, Loss: 0.785496711730957
Epoch 4, Iteration 341, Loss: 0.8162081241607666
Epoch 4, Iteration 342, Loss: 0.8331969380378723
Epoch 4, Iteration 343, Loss: 0.844719409942627
Epoch 4, Iteration 344, Loss: 0.9911347031593323
Epoch 4, Iteration 345, Loss: 0.9650741815567017
Epoch 4, Iteration 346, Loss: 0.8199370503425598
Epoch 4, Iteration 347, Loss: 0.8269384503364563
Epoch 4, Iteration 348, Loss: 0.7300985455513
Epoch 4, Iteration 349, Loss: 0.9278534650802612
Epoch 4, Iteration 350, Loss: 0.7726345062255859
Epoch 4, Iteration 350, Valid Loss: 0.7559427618980408
Epoch 4, Iteration 351, Loss: 0.9087878465652466
Epoch 4, Iteration 352, Loss: 0.8309539556503296
Epoch 4, Iteration 353, Loss: 0.9211757183074951
Epoch 4, Iteration 354, Loss: 0.7994195222854614
Epoch 4, Iteration 355, Loss: 0.8558520078659058
Epoch 4, Iteration 356, Loss: 0.9186088442802429
Epoch 4, Iteration 357, Loss: 0.804610013961792
Epoch 4, Iteration 358, Loss: 0.8464780449867249
Epoch 4, Iteration 359, Loss: 0.8857812285423279
Epoch 4, Iteration 360, Loss: 0.7915959358215332
Epoch 4, Iteration 361, Loss: 0.8301997780799866
Epoch 4, Iteration 362, Loss: 0.8871037364006042
Epoch 4, Iteration 363, Loss: 0.8019837141036987
Epoch 4, Iteration 364, Loss: 0.9528142809867859
Epoch 4, Iteration 365, Loss: 1.0461305379867554
Epoch 4, Iteration 366, Loss: 0.9767888188362122
Epoch 4, Iteration 367, Loss: 0.9091461896896362
Epoch 4, Iteration 368, Loss: 0.8691506385803223
Epoch 4, Iteration 369, Loss: 0.9510273337364197
Epoch 4, Iteration 370, Loss: 0.9364050030708313
Epoch 4, Iteration 371, Loss: 0.9894388914108276
Epoch 4, Iteration 372, Loss: 0.7686964869499207
Epoch 4, Iteration 373, Loss: 0.8886090517044067
Epoch 4, Iteration 374, Loss: 0.9911856651306152
Epoch 4, Iteration 375, Loss: 0.9209069609642029
Epoch 4, Iteration 376, Loss: 0.9870278239250183
Epoch 4, Iteration 377, Loss: 0.8758935332298279
Epoch 4, Iteration 378, Loss: 0.7851151823997498
Epoch 4, Iteration 379, Loss: 0.8106439709663391
Epoch 4, Iteration 380, Loss: 0.7495490908622742
Epoch 4, Iteration 381, Loss: 0.8121059536933899
Epoch 4, Iteration 382, Loss: 0.65568608045578
Epoch 4, Iteration 383, Loss: 0.8477744460105896
Epoch 4, Iteration 384, Loss: 0.8916410207748413
Epoch 4, Iteration 385, Loss: 0.8133599162101746
Epoch 4, Iteration 386, Loss: 0.8121547698974609
Epoch 4, Iteration 387, Loss: 1.2138957977294922
Epoch 4, Iteration 388, Loss: 0.911602795124054
Epoch 4, Iteration 389, Loss: 0.8992431163787842
Epoch 4, Iteration 390, Loss: 1.0743464231491089
Epoch 4, Iteration 391, Loss: 0.7966566681861877
Epoch 5/15, Loss: 0.9814584411832751
Epoch 5, Iteration 0, Loss: 0.884394645690918
Epoch 5, Iteration 1, Loss: 1.2579989433288574
Epoch 5, Iteration 2, Loss: 1.0682510137557983
Epoch 5, Iteration 3, Loss: 1.0359280109405518
Epoch 5, Iteration 4, Loss: 0.9036135673522949
Epoch 5, Iteration 5, Loss: 1.0303943157196045
Epoch 5, Iteration 6, Loss: 0.7774478793144226
Epoch 5, Iteration 7, Loss: 0.9535058736801147
Epoch 5, Iteration 8, Loss: 0.9151707887649536
Epoch 5, Iteration 9, Loss: 1.106384515762329
Epoch 5, Iteration 10, Loss: 1.083556056022644
Epoch 5, Iteration 11, Loss: 0.8934468030929565
Epoch 5, Iteration 12, Loss: 0.868467390537262
Epoch 5, Iteration 13, Loss: 1.0571789741516113
Epoch 5, Iteration 14, Loss: 0.9470281004905701
Epoch 5, Iteration 15, Loss: 1.0468180179595947
Epoch 5, Iteration 16, Loss: 0.8617860674858093
Epoch 5, Iteration 17, Loss: 1.0487252473831177
Epoch 5, Iteration 18, Loss: 0.8896021246910095
Epoch 5, Iteration 19, Loss: 0.9605768322944641
Epoch 5, Iteration 20, Loss: 0.8591107726097107
Epoch 5, Iteration 21, Loss: 0.8626886606216431
Epoch 5, Iteration 22, Loss: 0.9261230230331421
Epoch 5, Iteration 23, Loss: 0.9751741886138916
Epoch 5, Iteration 24, Loss: 0.8572389483451843
Epoch 5, Iteration 25, Loss: 0.796962559223175
Epoch 5, Iteration 26, Loss: 1.2469053268432617
Epoch 5, Iteration 27, Loss: 1.0358610153198242
Epoch 5, Iteration 28, Loss: 0.9995282888412476
Epoch 5, Iteration 29, Loss: 1.0603505373001099
Epoch 5, Iteration 30, Loss: 1.001497507095337
Epoch 5, Iteration 31, Loss: 0.944724440574646
Epoch 5, Iteration 32, Loss: 0.9418246746063232
Epoch 5, Iteration 33, Loss: 0.9851481318473816
Epoch 5, Iteration 34, Loss: 0.9350507259368896
Epoch 5, Iteration 35, Loss: 1.0324138402938843
Epoch 5, Iteration 36, Loss: 0.7898975014686584
Epoch 5, Iteration 37, Loss: 0.8038830757141113
Epoch 5, Iteration 38, Loss: 0.9231508374214172
Epoch 5, Iteration 39, Loss: 0.97707599401474
Epoch 5, Iteration 40, Loss: 0.915244460105896
Epoch 5, Iteration 41, Loss: 0.9892341494560242
Epoch 5, Iteration 42, Loss: 0.9090835452079773
Epoch 5, Iteration 43, Loss: 0.7637709379196167
Epoch 5, Iteration 44, Loss: 0.8577999472618103
Epoch 5, Iteration 45, Loss: 0.8639558553695679
Epoch 5, Iteration 46, Loss: 0.984246551990509
Epoch 5, Iteration 47, Loss: 0.9978799819946289
Epoch 5, Iteration 48, Loss: 1.0946075916290283
Epoch 5, Iteration 49, Loss: 0.899505615234375
Epoch 5, Iteration 50, Loss: 0.8936815857887268
Epoch 5, Iteration 50, Valid Loss: 0.7263271808624268
Epoch 5, Iteration 51, Loss: 0.9744623899459839
Epoch 5, Iteration 52, Loss: 0.9084485173225403
Epoch 5, Iteration 53, Loss: 0.999485194683075
Epoch 5, Iteration 54, Loss: 1.0679084062576294
Epoch 5, Iteration 55, Loss: 0.9844765663146973
Epoch 5, Iteration 56, Loss: 0.9324766993522644
Epoch 5, Iteration 57, Loss: 1.0865638256072998
Epoch 5, Iteration 58, Loss: 0.907564640045166
Epoch 5, Iteration 59, Loss: 0.9828992486000061
Epoch 5, Iteration 60, Loss: 0.9950827360153198
Epoch 5, Iteration 61, Loss: 0.7588973641395569
Epoch 5, Iteration 62, Loss: 0.836691677570343
Epoch 5, Iteration 63, Loss: 0.7743106484413147
Epoch 5, Iteration 64, Loss: 0.9402368068695068
Epoch 5, Iteration 65, Loss: 1.085877537727356
Epoch 5, Iteration 66, Loss: 0.9116991758346558
Epoch 5, Iteration 67, Loss: 0.7354905605316162
Epoch 5, Iteration 68, Loss: 1.0090091228485107
Epoch 5, Iteration 69, Loss: 1.0770787000656128
Epoch 5, Iteration 70, Loss: 0.982099711894989
Epoch 5, Iteration 71, Loss: 0.9817728400230408
Epoch 5, Iteration 72, Loss: 0.9383513331413269
Epoch 5, Iteration 73, Loss: 0.8100853562355042
Epoch 5, Iteration 74, Loss: 1.1846343278884888
Epoch 5, Iteration 75, Loss: 1.007015347480774
Epoch 5, Iteration 76, Loss: 0.8788227438926697
Epoch 5, Iteration 77, Loss: 0.7252619862556458
Epoch 5, Iteration 78, Loss: 1.1216944456100464
Epoch 5, Iteration 79, Loss: 1.1840555667877197
Epoch 5, Iteration 80, Loss: 0.795661211013794
Epoch 5, Iteration 81, Loss: 1.2214897871017456
Epoch 5, Iteration 82, Loss: 1.029752254486084
Epoch 5, Iteration 83, Loss: 0.8207478523254395
Epoch 5, Iteration 84, Loss: 1.041855812072754
Epoch 5, Iteration 85, Loss: 0.968654990196228
Epoch 5, Iteration 86, Loss: 1.0116924047470093
Epoch 5, Iteration 87, Loss: 0.9409275650978088
Epoch 5, Iteration 88, Loss: 0.9252507090568542
Epoch 5, Iteration 89, Loss: 1.0267913341522217
Epoch 5, Iteration 90, Loss: 0.7833248972892761
Epoch 5, Iteration 91, Loss: 0.8055408596992493
Epoch 5, Iteration 92, Loss: 0.7650659084320068
Epoch 5, Iteration 93, Loss: 0.9255911111831665
Epoch 5, Iteration 94, Loss: 1.0926527976989746
Epoch 5, Iteration 95, Loss: 1.1771776676177979
Epoch 5, Iteration 96, Loss: 0.9264001846313477
Epoch 5, Iteration 97, Loss: 0.7479454874992371
Epoch 5, Iteration 98, Loss: 0.9520297646522522
Epoch 5, Iteration 99, Loss: 0.9193183779716492
Epoch 5, Iteration 100, Loss: 0.8965380191802979
Epoch 5, Iteration 100, Valid Loss: 0.7713515162467957
Epoch 5, Iteration 101, Loss: 1.0484068393707275
Epoch 5, Iteration 102, Loss: 0.9897743463516235
Epoch 5, Iteration 103, Loss: 1.0063323974609375
Epoch 5, Iteration 104, Loss: 1.2180206775665283
Epoch 5, Iteration 105, Loss: 0.9058063626289368
Epoch 5, Iteration 106, Loss: 0.9317479729652405
Epoch 5, Iteration 107, Loss: 1.1895027160644531
Epoch 5, Iteration 108, Loss: 0.889420747756958
Epoch 5, Iteration 109, Loss: 1.2259386777877808
Epoch 5, Iteration 110, Loss: 1.2136846780776978
Epoch 5, Iteration 111, Loss: 1.086555004119873
Epoch 5, Iteration 112, Loss: 0.9792348146438599
Epoch 5, Iteration 113, Loss: 0.905613899230957
Epoch 5, Iteration 114, Loss: 1.2203993797302246
Epoch 5, Iteration 115, Loss: 1.074102520942688
Epoch 5, Iteration 116, Loss: 0.8712133765220642
Epoch 5, Iteration 117, Loss: 0.686720073223114
Epoch 5, Iteration 118, Loss: 1.027168869972229
Epoch 5, Iteration 119, Loss: 0.896329402923584
Epoch 5, Iteration 120, Loss: 1.0841903686523438
Epoch 5, Iteration 121, Loss: 0.8501887321472168
Epoch 5, Iteration 122, Loss: 0.9680610299110413
Epoch 5, Iteration 123, Loss: 0.8577520847320557
Epoch 5, Iteration 124, Loss: 0.7532271146774292
Epoch 5, Iteration 125, Loss: 0.7792224287986755
Epoch 5, Iteration 126, Loss: 0.820408821105957
Epoch 5, Iteration 127, Loss: 1.1120648384094238
Epoch 5, Iteration 128, Loss: 1.0324070453643799
Epoch 5, Iteration 129, Loss: 1.0030714273452759
Epoch 5, Iteration 130, Loss: 1.1942297220230103
Epoch 5, Iteration 131, Loss: 0.7158713936805725
Epoch 5, Iteration 132, Loss: 0.8963804841041565
Epoch 5, Iteration 133, Loss: 1.2581897974014282
Epoch 5, Iteration 134, Loss: 0.967275083065033
Epoch 5, Iteration 135, Loss: 0.9848108291625977
Epoch 5, Iteration 136, Loss: 0.9150444865226746
Epoch 5, Iteration 137, Loss: 0.9483888149261475
Epoch 5, Iteration 138, Loss: 0.8299671411514282
Epoch 5, Iteration 139, Loss: 0.8539013862609863
Epoch 5, Iteration 140, Loss: 0.8154962062835693
Epoch 5, Iteration 141, Loss: 1.2493064403533936
Epoch 5, Iteration 142, Loss: 0.9076390862464905
Epoch 5, Iteration 143, Loss: 0.8412817716598511
Epoch 5, Iteration 144, Loss: 0.8421246409416199
Epoch 5, Iteration 145, Loss: 1.0139886140823364
Epoch 5, Iteration 146, Loss: 0.9888368844985962
Epoch 5, Iteration 147, Loss: 0.8701480627059937
Epoch 5, Iteration 148, Loss: 0.7839016318321228
Epoch 5, Iteration 149, Loss: 0.7649833559989929
Epoch 5, Iteration 150, Loss: 0.9790865778923035
Epoch 5, Iteration 150, Valid Loss: 0.7327585220336914
Epoch 5, Iteration 151, Loss: 1.013650894165039
Epoch 5, Iteration 152, Loss: 0.824421226978302
Epoch 5, Iteration 153, Loss: 0.8685926198959351
Epoch 5, Iteration 154, Loss: 0.7251030206680298
Epoch 5, Iteration 155, Loss: 0.8768936395645142
Epoch 5, Iteration 156, Loss: 0.77884441614151
Epoch 5, Iteration 157, Loss: 1.3412221670150757
Epoch 5, Iteration 158, Loss: 1.007033109664917
Epoch 5, Iteration 159, Loss: 1.0104783773422241
Epoch 5, Iteration 160, Loss: 0.840215265750885
Epoch 5, Iteration 161, Loss: 0.8758112192153931
Epoch 5, Iteration 162, Loss: 1.200938105583191
Epoch 5, Iteration 163, Loss: 0.7719393968582153
Epoch 5, Iteration 164, Loss: 0.9270544648170471
Epoch 5, Iteration 165, Loss: 0.8956316709518433
Epoch 5, Iteration 166, Loss: 0.8040003776550293
Epoch 5, Iteration 167, Loss: 1.1292665004730225
Epoch 5, Iteration 168, Loss: 0.9512305855751038
Epoch 5, Iteration 169, Loss: 0.9633181095123291
Epoch 5, Iteration 170, Loss: 1.0990756750106812
Epoch 5, Iteration 171, Loss: 1.1014375686645508
Epoch 5, Iteration 172, Loss: 0.9069215655326843
Epoch 5, Iteration 173, Loss: 0.7975600957870483
Epoch 5, Iteration 174, Loss: 1.0024791955947876
Epoch 5, Iteration 175, Loss: 1.1846356391906738
Epoch 5, Iteration 176, Loss: 0.7934995293617249
Epoch 5, Iteration 177, Loss: 1.351770043373108
Epoch 5, Iteration 178, Loss: 1.0763659477233887
Epoch 5, Iteration 179, Loss: 0.879447340965271
Epoch 5, Iteration 180, Loss: 0.9023174047470093
Epoch 5, Iteration 181, Loss: 1.1140152215957642
Epoch 5, Iteration 182, Loss: 0.9993410706520081
Epoch 5, Iteration 183, Loss: 0.8394052982330322
Epoch 5, Iteration 184, Loss: 0.8442624807357788
Epoch 5, Iteration 185, Loss: 0.8365813493728638
Epoch 5, Iteration 186, Loss: 0.8980485200881958
Epoch 5, Iteration 187, Loss: 0.8350398540496826
Epoch 5, Iteration 188, Loss: 0.9306190609931946
Epoch 5, Iteration 189, Loss: 0.839415967464447
Epoch 5, Iteration 190, Loss: 1.0248287916183472
Epoch 5, Iteration 191, Loss: 1.0215461254119873
Epoch 5, Iteration 192, Loss: 0.9501733183860779
Epoch 5, Iteration 193, Loss: 0.91331946849823
Epoch 5, Iteration 194, Loss: 0.9884145855903625
Epoch 5, Iteration 195, Loss: 0.8908544778823853
Epoch 5, Iteration 196, Loss: 0.927757203578949
Epoch 5, Iteration 197, Loss: 0.8201237320899963
Epoch 5, Iteration 198, Loss: 0.8810646533966064
Epoch 5, Iteration 199, Loss: 0.8962581753730774
Epoch 5, Iteration 200, Loss: 0.8041595816612244
Epoch 5, Iteration 200, Valid Loss: 0.715997040271759
Epoch 5, Iteration 201, Loss: 0.8966109156608582
Epoch 5, Iteration 202, Loss: 0.9744777679443359
Epoch 5, Iteration 203, Loss: 0.855931282043457
Epoch 5, Iteration 204, Loss: 0.8002690672874451
Epoch 5, Iteration 205, Loss: 0.69626784324646
Epoch 5, Iteration 206, Loss: 0.809296190738678
Epoch 5, Iteration 207, Loss: 1.161781668663025
Epoch 5, Iteration 208, Loss: 1.317414402961731
Epoch 5, Iteration 209, Loss: 1.2656656503677368
Epoch 5, Iteration 210, Loss: 1.1894060373306274
Epoch 5, Iteration 211, Loss: 1.3405773639678955
Epoch 5, Iteration 212, Loss: 1.154932975769043
Epoch 5, Iteration 213, Loss: 1.259844422340393
Epoch 5, Iteration 214, Loss: 0.9338314533233643
Epoch 5, Iteration 215, Loss: 0.808469295501709
Epoch 5, Iteration 216, Loss: 0.8174273371696472
Epoch 5, Iteration 217, Loss: 0.9014086127281189
Epoch 5, Iteration 218, Loss: 0.8996066451072693
Epoch 5, Iteration 219, Loss: 0.9397246837615967
Epoch 5, Iteration 220, Loss: 0.8986825942993164
Epoch 5, Iteration 221, Loss: 0.9103706479072571
Epoch 5, Iteration 222, Loss: 0.8917944431304932
Epoch 5, Iteration 223, Loss: 0.8377556800842285
Epoch 5, Iteration 224, Loss: 0.7738320827484131
Epoch 5, Iteration 225, Loss: 1.014127492904663
Epoch 5, Iteration 226, Loss: 0.9477823972702026
Epoch 5, Iteration 227, Loss: 0.9253453612327576
Epoch 5, Iteration 228, Loss: 1.0045671463012695
Epoch 5, Iteration 229, Loss: 0.8474324941635132
Epoch 5, Iteration 230, Loss: 0.9586063027381897
Epoch 5, Iteration 231, Loss: 0.9113729596138
Epoch 5, Iteration 232, Loss: 1.0241596698760986
Epoch 5, Iteration 233, Loss: 1.2133082151412964
Epoch 5, Iteration 234, Loss: 1.0862135887145996
Epoch 5, Iteration 235, Loss: 0.7320005893707275
Epoch 5, Iteration 236, Loss: 0.9795392751693726
Epoch 5, Iteration 237, Loss: 1.0128926038742065
Epoch 5, Iteration 238, Loss: 0.7699392437934875
Epoch 5, Iteration 239, Loss: 1.0507103204727173
Epoch 5, Iteration 240, Loss: 0.8903288245201111
Epoch 5, Iteration 241, Loss: 0.8081431984901428
Epoch 5, Iteration 242, Loss: 0.8634990453720093
Epoch 5, Iteration 243, Loss: 1.0006242990493774
Epoch 5, Iteration 244, Loss: 0.9304997324943542
Epoch 5, Iteration 245, Loss: 0.7217282056808472
Epoch 5, Iteration 246, Loss: 0.9478335976600647
Epoch 5, Iteration 247, Loss: 0.842568039894104
Epoch 5, Iteration 248, Loss: 0.9337475895881653
Epoch 5, Iteration 249, Loss: 0.8714432716369629
Epoch 5, Iteration 250, Loss: 1.1429134607315063
Epoch 5, Iteration 250, Valid Loss: 0.6889339685440063
Epoch 5, Iteration 251, Loss: 0.887907087802887
Epoch 5, Iteration 252, Loss: 1.1008117198944092
Epoch 5, Iteration 253, Loss: 0.9467989206314087
Epoch 5, Iteration 254, Loss: 0.9279571175575256
Epoch 5, Iteration 255, Loss: 0.9493245482444763
Epoch 5, Iteration 256, Loss: 0.9809020161628723
Epoch 5, Iteration 257, Loss: 0.9299946427345276
Epoch 5, Iteration 258, Loss: 1.0286271572113037
Epoch 5, Iteration 259, Loss: 0.7050159573554993
Epoch 5, Iteration 260, Loss: 1.0526057481765747
Epoch 5, Iteration 261, Loss: 0.9715385437011719
Epoch 5, Iteration 262, Loss: 0.8461102843284607
Epoch 5, Iteration 263, Loss: 0.9969690442085266
Epoch 5, Iteration 264, Loss: 0.9042397141456604
Epoch 5, Iteration 265, Loss: 0.8671963810920715
Epoch 5, Iteration 266, Loss: 0.9439837336540222
Epoch 5, Iteration 267, Loss: 0.8273338675498962
Epoch 5, Iteration 268, Loss: 0.8154692053794861
Epoch 5, Iteration 269, Loss: 0.7519480586051941
Epoch 5, Iteration 270, Loss: 0.8198016881942749
Epoch 5, Iteration 271, Loss: 0.9873077273368835
Epoch 5, Iteration 272, Loss: 0.810758113861084
Epoch 5, Iteration 273, Loss: 0.9276093244552612
Epoch 5, Iteration 274, Loss: 0.7980026602745056
Epoch 5, Iteration 275, Loss: 0.6889552474021912
Epoch 5, Iteration 276, Loss: 0.8470510840415955
Epoch 5, Iteration 277, Loss: 0.8548813462257385
Epoch 5, Iteration 278, Loss: 1.0595389604568481
Epoch 5, Iteration 279, Loss: 0.7674639821052551
Epoch 5, Iteration 280, Loss: 0.755033016204834
Epoch 5, Iteration 281, Loss: 0.7684227228164673
Epoch 5, Iteration 282, Loss: 0.8803818821907043
Epoch 5, Iteration 283, Loss: 0.8441841006278992
Epoch 5, Iteration 284, Loss: 0.8623666167259216
Epoch 5, Iteration 285, Loss: 0.9237198829650879
Epoch 5, Iteration 286, Loss: 0.7794068455696106
Epoch 5, Iteration 287, Loss: 0.8073089718818665
Epoch 5, Iteration 288, Loss: 0.7571302056312561
Epoch 5, Iteration 289, Loss: 0.9105303883552551
Epoch 5, Iteration 290, Loss: 0.76063472032547
Epoch 5, Iteration 291, Loss: 0.7636615633964539
Epoch 5, Iteration 292, Loss: 1.0067775249481201
Epoch 5, Iteration 293, Loss: 1.0767441987991333
Epoch 5, Iteration 294, Loss: 0.8436024785041809
Epoch 5, Iteration 295, Loss: 0.7997060418128967
Epoch 5, Iteration 296, Loss: 1.0766769647598267
Epoch 5, Iteration 297, Loss: 0.9747413992881775
Epoch 5, Iteration 298, Loss: 0.862576961517334
Epoch 5, Iteration 299, Loss: 0.8780789971351624
Epoch 5, Iteration 300, Loss: 0.6310812830924988
Epoch 5, Iteration 300, Valid Loss: 0.6788691878318787
Epoch 5, Iteration 301, Loss: 0.7935691475868225
Epoch 5, Iteration 302, Loss: 0.7139683365821838
Epoch 5, Iteration 303, Loss: 0.9619147777557373
Epoch 5, Iteration 304, Loss: 0.7929319739341736
Epoch 5, Iteration 305, Loss: 0.6757140159606934
Epoch 5, Iteration 306, Loss: 0.7397986054420471
Epoch 5, Iteration 307, Loss: 0.8250908851623535
Epoch 5, Iteration 308, Loss: 0.7750455141067505
Epoch 5, Iteration 309, Loss: 0.9023435115814209
Epoch 5, Iteration 310, Loss: 0.8966914415359497
Epoch 5, Iteration 311, Loss: 0.8531238436698914
Epoch 5, Iteration 312, Loss: 0.7686631083488464
Epoch 5, Iteration 313, Loss: 0.9623885750770569
Epoch 5, Iteration 314, Loss: 0.7524663209915161
Epoch 5, Iteration 315, Loss: 0.7766845226287842
Epoch 5, Iteration 316, Loss: 0.9719342589378357
Epoch 5, Iteration 317, Loss: 0.9286282658576965
Epoch 5, Iteration 318, Loss: 0.7626041173934937
Epoch 5, Iteration 319, Loss: 0.9856462478637695
Epoch 5, Iteration 320, Loss: 0.7051263451576233
Epoch 5, Iteration 321, Loss: 0.8057668209075928
Epoch 5, Iteration 322, Loss: 0.7782263159751892
Epoch 5, Iteration 323, Loss: 0.9969339966773987
Epoch 5, Iteration 324, Loss: 0.8862449526786804
Epoch 5, Iteration 325, Loss: 0.9713782072067261
Epoch 5, Iteration 326, Loss: 0.7887703776359558
Epoch 5, Iteration 327, Loss: 0.7665274739265442
Epoch 5, Iteration 328, Loss: 0.7282541394233704
Epoch 5, Iteration 329, Loss: 0.8933536410331726
Epoch 5, Iteration 330, Loss: 0.6923465132713318
Epoch 5, Iteration 331, Loss: 0.8224169015884399
Epoch 5, Iteration 332, Loss: 0.8870472311973572
Epoch 5, Iteration 333, Loss: 1.0216788053512573
Epoch 5, Iteration 334, Loss: 0.81321781873703
Epoch 5, Iteration 335, Loss: 0.9781239032745361
Epoch 5, Iteration 336, Loss: 0.91551274061203
Epoch 5, Iteration 337, Loss: 0.6454699039459229
Epoch 5, Iteration 338, Loss: 0.910150408744812
Epoch 5, Iteration 339, Loss: 0.8162691593170166
Epoch 5, Iteration 340, Loss: 0.6940956711769104
Epoch 5, Iteration 341, Loss: 0.8579455614089966
Epoch 5, Iteration 342, Loss: 0.7647557854652405
Epoch 5, Iteration 343, Loss: 0.8266955614089966
Epoch 5, Iteration 344, Loss: 0.8656100630760193
Epoch 5, Iteration 345, Loss: 0.8155207633972168
Epoch 5, Iteration 346, Loss: 0.8622501492500305
Epoch 5, Iteration 347, Loss: 0.7275625467300415
Epoch 5, Iteration 348, Loss: 0.6842432022094727
Epoch 5, Iteration 349, Loss: 0.8444309234619141
Epoch 5, Iteration 350, Loss: 0.7168900966644287
Epoch 5, Iteration 350, Valid Loss: 0.7086721062660217
Epoch 5, Iteration 351, Loss: 0.8990697860717773
Epoch 5, Iteration 352, Loss: 0.7834992408752441
Epoch 5, Iteration 353, Loss: 0.8197696208953857
Epoch 5, Iteration 354, Loss: 0.8659552931785583
Epoch 5, Iteration 355, Loss: 0.7832712531089783
Epoch 5, Iteration 356, Loss: 0.9046263694763184
Epoch 5, Iteration 357, Loss: 0.7476892471313477
Epoch 5, Iteration 358, Loss: 0.7681100964546204
Epoch 5, Iteration 359, Loss: 0.8769936561584473
Epoch 5, Iteration 360, Loss: 0.7673957347869873
Epoch 5, Iteration 361, Loss: 0.7609142661094666
Epoch 5, Iteration 362, Loss: 0.8012883067131042
Epoch 5, Iteration 363, Loss: 0.7415135502815247
Epoch 5, Iteration 364, Loss: 0.8306207656860352
Epoch 5, Iteration 365, Loss: 0.9381104707717896
Epoch 5, Iteration 366, Loss: 0.8485137820243835
Epoch 5, Iteration 367, Loss: 0.8111186623573303
Epoch 5, Iteration 368, Loss: 0.7907158732414246
Epoch 5, Iteration 369, Loss: 0.8870471715927124
Epoch 5, Iteration 370, Loss: 0.8408163189888
Epoch 5, Iteration 371, Loss: 0.8109263777732849
Epoch 5, Iteration 372, Loss: 0.7910946011543274
Epoch 5, Iteration 373, Loss: 0.8093287944793701
Epoch 5, Iteration 374, Loss: 0.9760022759437561
Epoch 5, Iteration 375, Loss: 0.7164379358291626
Epoch 5, Iteration 376, Loss: 0.9291775226593018
Epoch 5, Iteration 377, Loss: 0.8528191447257996
Epoch 5, Iteration 378, Loss: 0.8250283598899841
Epoch 5, Iteration 379, Loss: 0.7422199845314026
Epoch 5, Iteration 380, Loss: 0.7351760268211365
Epoch 5, Iteration 381, Loss: 0.7573671936988831
Epoch 5, Iteration 382, Loss: 0.6338618993759155
Epoch 5, Iteration 383, Loss: 0.7606631517410278
Epoch 5, Iteration 384, Loss: 0.8873195052146912
Epoch 5, Iteration 385, Loss: 0.792352020740509
Epoch 5, Iteration 386, Loss: 0.8870450854301453
Epoch 5, Iteration 387, Loss: 1.0583899021148682
Epoch 5, Iteration 388, Loss: 0.7912602424621582
Epoch 5, Iteration 389, Loss: 0.9016507267951965
Epoch 5, Iteration 390, Loss: 1.0358854532241821
Epoch 5, Iteration 391, Loss: 0.7164614796638489
Epoch 6/15, Loss: 0.9147950722550859
Epoch 6, Iteration 0, Loss: 0.8594175577163696
Epoch 6, Iteration 1, Loss: 1.0759161710739136
Epoch 6, Iteration 2, Loss: 0.9430351853370667
Epoch 6, Iteration 3, Loss: 1.0035046339035034
Epoch 6, Iteration 4, Loss: 0.8436338901519775
Epoch 6, Iteration 5, Loss: 0.9538593888282776
Epoch 6, Iteration 6, Loss: 0.7391550540924072
Epoch 6, Iteration 7, Loss: 0.8385874629020691
Epoch 6, Iteration 8, Loss: 0.8361474871635437
Epoch 6, Iteration 9, Loss: 1.0280513763427734
Epoch 6, Iteration 10, Loss: 0.9331762194633484
Epoch 6, Iteration 11, Loss: 0.8299064636230469
Epoch 6, Iteration 12, Loss: 0.822239339351654
Epoch 6, Iteration 13, Loss: 0.9905847311019897
Epoch 6, Iteration 14, Loss: 0.8632118105888367
Epoch 6, Iteration 15, Loss: 0.967007040977478
Epoch 6, Iteration 16, Loss: 0.8198705315589905
Epoch 6, Iteration 17, Loss: 0.9969351291656494
Epoch 6, Iteration 18, Loss: 0.857495903968811
Epoch 6, Iteration 19, Loss: 0.9545444250106812
Epoch 6, Iteration 20, Loss: 0.7880669832229614
Epoch 6, Iteration 21, Loss: 0.7877264618873596
Epoch 6, Iteration 22, Loss: 0.9097462296485901
Epoch 6, Iteration 23, Loss: 0.8413934111595154
Epoch 6, Iteration 24, Loss: 0.7834715843200684
Epoch 6, Iteration 25, Loss: 0.7236260771751404
Epoch 6, Iteration 26, Loss: 1.1891891956329346
Epoch 6, Iteration 27, Loss: 0.9398706555366516
Epoch 6, Iteration 28, Loss: 0.8925186991691589
Epoch 6, Iteration 29, Loss: 1.1595838069915771
Epoch 6, Iteration 30, Loss: 1.0795261859893799
Epoch 6, Iteration 31, Loss: 0.9852692484855652
Epoch 6, Iteration 32, Loss: 0.8619179725646973
Epoch 6, Iteration 33, Loss: 0.8509320616722107
Epoch 6, Iteration 34, Loss: 0.8306223154067993
Epoch 6, Iteration 35, Loss: 0.9848834872245789
Epoch 6, Iteration 36, Loss: 0.7861210107803345
Epoch 6, Iteration 37, Loss: 0.7184361219406128
Epoch 6, Iteration 38, Loss: 0.7965115904808044
Epoch 6, Iteration 39, Loss: 0.884726881980896
Epoch 6, Iteration 40, Loss: 0.8014770746231079
Epoch 6, Iteration 41, Loss: 0.9835236668586731
Epoch 6, Iteration 42, Loss: 0.8483246564865112
Epoch 6, Iteration 43, Loss: 0.7528148889541626
Epoch 6, Iteration 44, Loss: 0.889013409614563
Epoch 6, Iteration 45, Loss: 0.933479368686676
Epoch 6, Iteration 46, Loss: 0.8894347548484802
Epoch 6, Iteration 47, Loss: 0.8640682697296143
Epoch 6, Iteration 48, Loss: 0.9934970140457153
Epoch 6, Iteration 49, Loss: 0.8580187559127808
Epoch 6, Iteration 50, Loss: 0.7986711859703064
Epoch 6, Iteration 50, Valid Loss: 0.6698076725006104
Epoch 6, Iteration 51, Loss: 0.8216466307640076
Epoch 6, Iteration 52, Loss: 0.7602869272232056
Epoch 6, Iteration 53, Loss: 0.9279433488845825
Epoch 6, Iteration 54, Loss: 1.1672775745391846
Epoch 6, Iteration 55, Loss: 1.0171258449554443
Epoch 6, Iteration 56, Loss: 0.9538783431053162
Epoch 6, Iteration 57, Loss: 1.068547248840332
Epoch 6, Iteration 58, Loss: 0.7690836191177368
Epoch 6, Iteration 59, Loss: 0.769170880317688
Epoch 6, Iteration 60, Loss: 0.8431824445724487
Epoch 6, Iteration 61, Loss: 0.652539074420929
Epoch 6, Iteration 62, Loss: 0.8628672361373901
Epoch 6, Iteration 63, Loss: 0.8644426465034485
Epoch 6, Iteration 64, Loss: 0.9093078374862671
Epoch 6, Iteration 65, Loss: 0.9307922124862671
Epoch 6, Iteration 66, Loss: 0.8702802658081055
Epoch 6, Iteration 67, Loss: 0.7116170525550842
Epoch 6, Iteration 68, Loss: 0.9372941851615906
Epoch 6, Iteration 69, Loss: 1.0184491872787476
Epoch 6, Iteration 70, Loss: 0.9362576007843018
Epoch 6, Iteration 71, Loss: 0.971579372882843
Epoch 6, Iteration 72, Loss: 0.8725391626358032
Epoch 6, Iteration 73, Loss: 0.7473114132881165
Epoch 6, Iteration 74, Loss: 1.119905948638916
Epoch 6, Iteration 75, Loss: 0.8806660175323486
Epoch 6, Iteration 76, Loss: 0.8578635454177856
Epoch 6, Iteration 77, Loss: 0.6962897777557373
Epoch 6, Iteration 78, Loss: 1.1401362419128418
Epoch 6, Iteration 79, Loss: 1.1277107000350952
Epoch 6, Iteration 80, Loss: 0.7647656202316284
Epoch 6, Iteration 81, Loss: 1.0688159465789795
Epoch 6, Iteration 82, Loss: 0.9314989447593689
Epoch 6, Iteration 83, Loss: 0.7180019617080688
Epoch 6, Iteration 84, Loss: 1.082261562347412
Epoch 6, Iteration 85, Loss: 0.9355524778366089
Epoch 6, Iteration 86, Loss: 0.988848090171814
Epoch 6, Iteration 87, Loss: 0.8042548298835754
Epoch 6, Iteration 88, Loss: 0.9078264832496643
Epoch 6, Iteration 89, Loss: 1.0582998991012573
Epoch 6, Iteration 90, Loss: 0.7811892628669739
Epoch 6, Iteration 91, Loss: 0.8530024290084839
Epoch 6, Iteration 92, Loss: 0.6403101086616516
Epoch 6, Iteration 93, Loss: 0.853617250919342
Epoch 6, Iteration 94, Loss: 1.0597187280654907
Epoch 6, Iteration 95, Loss: 0.6890443563461304
Epoch 6, Iteration 96, Loss: 0.8532246351242065
Epoch 6, Iteration 97, Loss: 0.6852580308914185
Epoch 6, Iteration 98, Loss: 0.8767299652099609
Epoch 6, Iteration 99, Loss: 0.8512222170829773
Epoch 6, Iteration 100, Loss: 0.7403267025947571
Epoch 6, Iteration 100, Valid Loss: 0.7105984687805176
Epoch 6, Iteration 101, Loss: 1.0109994411468506
Epoch 6, Iteration 102, Loss: 0.875578761100769
Epoch 6, Iteration 103, Loss: 0.9779369831085205
Epoch 6, Iteration 104, Loss: 1.1340125799179077
Epoch 6, Iteration 105, Loss: 0.9576484560966492
Epoch 6, Iteration 106, Loss: 0.8547083735466003
Epoch 6, Iteration 107, Loss: 1.2294483184814453
Epoch 6, Iteration 108, Loss: 0.8852860927581787
Epoch 6, Iteration 109, Loss: 1.1563729047775269
Epoch 6, Iteration 110, Loss: 1.037299633026123
Epoch 6, Iteration 111, Loss: 0.9818747043609619
Epoch 6, Iteration 112, Loss: 0.9753317832946777
Epoch 6, Iteration 113, Loss: 0.851366400718689
Epoch 6, Iteration 114, Loss: 1.1085752248764038
Epoch 6, Iteration 115, Loss: 0.972334623336792
Epoch 6, Iteration 116, Loss: 0.7220052480697632
Epoch 6, Iteration 117, Loss: 0.7063284516334534
Epoch 6, Iteration 118, Loss: 0.9192146062850952
Epoch 6, Iteration 119, Loss: 0.7702147960662842
Epoch 6, Iteration 120, Loss: 1.0825676918029785
Epoch 6, Iteration 121, Loss: 0.9004216194152832
Epoch 6, Iteration 122, Loss: 0.8970498442649841
Epoch 6, Iteration 123, Loss: 0.8422186374664307
Epoch 6, Iteration 124, Loss: 0.7777725458145142
Epoch 6, Iteration 125, Loss: 0.8352141976356506
Epoch 6, Iteration 126, Loss: 0.7271013855934143
Epoch 6, Iteration 127, Loss: 0.9582388997077942
Epoch 6, Iteration 128, Loss: 0.9088283777236938
Epoch 6, Iteration 129, Loss: 1.0948553085327148
Epoch 6, Iteration 130, Loss: 1.1835355758666992
Epoch 6, Iteration 131, Loss: 0.7275248765945435
Epoch 6, Iteration 132, Loss: 0.7363311052322388
Epoch 6, Iteration 133, Loss: 1.2110555171966553
Epoch 6, Iteration 134, Loss: 0.9864180088043213
Epoch 6, Iteration 135, Loss: 0.9770371317863464
Epoch 6, Iteration 136, Loss: 0.8202813863754272
Epoch 6, Iteration 137, Loss: 0.8311468958854675
Epoch 6, Iteration 138, Loss: 0.7413490414619446
Epoch 6, Iteration 139, Loss: 0.7856631278991699
Epoch 6, Iteration 140, Loss: 0.7632600665092468
Epoch 6, Iteration 141, Loss: 1.1401373147964478
Epoch 6, Iteration 142, Loss: 0.950867235660553
Epoch 6, Iteration 143, Loss: 0.8413683772087097
Epoch 6, Iteration 144, Loss: 0.9080895781517029
Epoch 6, Iteration 145, Loss: 0.9222009778022766
Epoch 6, Iteration 146, Loss: 0.9166352152824402
Epoch 6, Iteration 147, Loss: 0.8098812699317932
Epoch 6, Iteration 148, Loss: 0.6633644104003906
Epoch 6, Iteration 149, Loss: 0.8278647661209106
Epoch 6, Iteration 150, Loss: 0.8787568211555481
Epoch 6, Iteration 150, Valid Loss: 0.6959850192070007
Epoch 6, Iteration 151, Loss: 0.9233571887016296
Epoch 6, Iteration 152, Loss: 0.7299131751060486
Epoch 6, Iteration 153, Loss: 0.9517775774002075
Epoch 6, Iteration 154, Loss: 0.7029328346252441
Epoch 6, Iteration 155, Loss: 0.8918576240539551
Epoch 6, Iteration 156, Loss: 0.7118650078773499
Epoch 6, Iteration 157, Loss: 1.1638963222503662
Epoch 6, Iteration 158, Loss: 0.8969911336898804
Epoch 6, Iteration 159, Loss: 0.9525386095046997
Epoch 6, Iteration 160, Loss: 0.8004649877548218
Epoch 6, Iteration 161, Loss: 0.7844141721725464
Epoch 6, Iteration 162, Loss: 1.1929374933242798
Epoch 6, Iteration 163, Loss: 0.6502330899238586
Epoch 6, Iteration 164, Loss: 0.8407535552978516
Epoch 6, Iteration 165, Loss: 0.8082656860351562
Epoch 6, Iteration 166, Loss: 0.7633293867111206
Epoch 6, Iteration 167, Loss: 1.024147391319275
Epoch 6, Iteration 168, Loss: 0.9337095022201538
Epoch 6, Iteration 169, Loss: 0.833037257194519
Epoch 6, Iteration 170, Loss: 1.075677514076233
Epoch 6, Iteration 171, Loss: 0.8775444626808167
Epoch 6, Iteration 172, Loss: 0.9009642601013184
Epoch 6, Iteration 173, Loss: 0.7857665419578552
Epoch 6, Iteration 174, Loss: 0.9361878633499146
Epoch 6, Iteration 175, Loss: 0.8959227800369263
Epoch 6, Iteration 176, Loss: 0.6326186060905457
Epoch 6, Iteration 177, Loss: 1.2717725038528442
Epoch 6, Iteration 178, Loss: 1.0488193035125732
Epoch 6, Iteration 179, Loss: 0.8356373906135559
Epoch 6, Iteration 180, Loss: 0.8522340655326843
Epoch 6, Iteration 181, Loss: 0.9670564532279968
Epoch 6, Iteration 182, Loss: 0.9664798378944397
Epoch 6, Iteration 183, Loss: 0.8280174732208252
Epoch 6, Iteration 184, Loss: 0.9147965312004089
Epoch 6, Iteration 185, Loss: 0.7495693564414978
Epoch 6, Iteration 186, Loss: 0.8091386556625366
Epoch 6, Iteration 187, Loss: 0.8829290866851807
Epoch 6, Iteration 188, Loss: 0.75847989320755
Epoch 6, Iteration 189, Loss: 0.7839475870132446
Epoch 6, Iteration 190, Loss: 1.0049636363983154
Epoch 6, Iteration 191, Loss: 0.8256292343139648
Epoch 6, Iteration 192, Loss: 0.9336563944816589
Epoch 6, Iteration 193, Loss: 0.8860770463943481
Epoch 6, Iteration 194, Loss: 0.9615448713302612
Epoch 6, Iteration 195, Loss: 0.7781893610954285
Epoch 6, Iteration 196, Loss: 0.8948297500610352
Epoch 6, Iteration 197, Loss: 0.80575031042099
Epoch 6, Iteration 198, Loss: 0.8384503126144409
Epoch 6, Iteration 199, Loss: 0.9516425728797913
Epoch 6, Iteration 200, Loss: 0.7909442782402039
Epoch 6, Iteration 200, Valid Loss: 0.6812244057655334
Epoch 6, Iteration 201, Loss: 0.8874480128288269
Epoch 6, Iteration 202, Loss: 0.8987159132957458
Epoch 6, Iteration 203, Loss: 0.8180950880050659
Epoch 6, Iteration 204, Loss: 0.8456130027770996
Epoch 6, Iteration 205, Loss: 0.6218857169151306
Epoch 6, Iteration 206, Loss: 0.7247331142425537
Epoch 6, Iteration 207, Loss: 0.9171234965324402
Epoch 6, Iteration 208, Loss: 1.2368780374526978
Epoch 6, Iteration 209, Loss: 1.249558925628662
Epoch 6, Iteration 210, Loss: 1.1673370599746704
Epoch 6, Iteration 211, Loss: 1.2390395402908325
Epoch 6, Iteration 212, Loss: 1.0033774375915527
Epoch 6, Iteration 213, Loss: 1.103043794631958
Epoch 6, Iteration 214, Loss: 0.8786277174949646
Epoch 6, Iteration 215, Loss: 0.7632778882980347
Epoch 6, Iteration 216, Loss: 0.7618739008903503
Epoch 6, Iteration 217, Loss: 0.9002114534378052
Epoch 6, Iteration 218, Loss: 0.8652570843696594
Epoch 6, Iteration 219, Loss: 0.9015162587165833
Epoch 6, Iteration 220, Loss: 0.8281254768371582
Epoch 6, Iteration 221, Loss: 0.8962692618370056
Epoch 6, Iteration 222, Loss: 0.9535874128341675
Epoch 6, Iteration 223, Loss: 0.7960355877876282
Epoch 6, Iteration 224, Loss: 0.7278186082839966
Epoch 6, Iteration 225, Loss: 0.9836578965187073
Epoch 6, Iteration 226, Loss: 1.0390868186950684
Epoch 6, Iteration 227, Loss: 0.9871849417686462
Epoch 6, Iteration 228, Loss: 0.8470430970191956
Epoch 6, Iteration 229, Loss: 0.7417988181114197
Epoch 6, Iteration 230, Loss: 0.861361026763916
Epoch 6, Iteration 231, Loss: 0.8611049652099609
Epoch 6, Iteration 232, Loss: 0.9214960336685181
Epoch 6, Iteration 233, Loss: 1.0909143686294556
Epoch 6, Iteration 234, Loss: 1.0404163599014282
Epoch 6, Iteration 235, Loss: 0.6633018255233765
Epoch 6, Iteration 236, Loss: 0.9294000267982483
Epoch 6, Iteration 237, Loss: 0.8215016722679138
Epoch 6, Iteration 238, Loss: 0.7634950876235962
Epoch 6, Iteration 239, Loss: 1.0576125383377075
Epoch 6, Iteration 240, Loss: 0.8406976461410522
Epoch 6, Iteration 241, Loss: 0.7521454095840454
Epoch 6, Iteration 242, Loss: 0.7204045653343201
Epoch 6, Iteration 243, Loss: 0.7986714839935303
Epoch 6, Iteration 244, Loss: 0.8359876871109009
Epoch 6, Iteration 245, Loss: 0.6613052487373352
Epoch 6, Iteration 246, Loss: 0.8303834199905396
Epoch 6, Iteration 247, Loss: 0.8313811421394348
Epoch 6, Iteration 248, Loss: 0.9128468036651611
Epoch 6, Iteration 249, Loss: 0.8257373571395874
Epoch 6, Iteration 250, Loss: 0.9813454151153564
Epoch 6, Iteration 250, Valid Loss: 0.6654561758041382
Epoch 6, Iteration 251, Loss: 0.7748180031776428
Epoch 6, Iteration 252, Loss: 0.965237557888031
Epoch 6, Iteration 253, Loss: 0.9755709767341614
Epoch 6, Iteration 254, Loss: 0.9613968133926392
Epoch 6, Iteration 255, Loss: 0.9102425575256348
Epoch 6, Iteration 256, Loss: 0.9660977125167847
Epoch 6, Iteration 257, Loss: 0.7240080833435059
Epoch 6, Iteration 258, Loss: 1.108786940574646
Epoch 6, Iteration 259, Loss: 0.7054703235626221
Epoch 6, Iteration 260, Loss: 0.9374891519546509
Epoch 6, Iteration 261, Loss: 0.8519404530525208
Epoch 6, Iteration 262, Loss: 0.7075837850570679
Epoch 6, Iteration 263, Loss: 0.9603397250175476
Epoch 6, Iteration 264, Loss: 0.8382358551025391
Epoch 6, Iteration 265, Loss: 0.8742111325263977
Epoch 6, Iteration 266, Loss: 0.9610850811004639
Epoch 6, Iteration 267, Loss: 0.9026502966880798
Epoch 6, Iteration 268, Loss: 0.7826032638549805
Epoch 6, Iteration 269, Loss: 0.7698972821235657
Epoch 6, Iteration 270, Loss: 0.7667831778526306
Epoch 6, Iteration 271, Loss: 0.9711408615112305
Epoch 6, Iteration 272, Loss: 0.784895122051239
Epoch 6, Iteration 273, Loss: 0.9007733464241028
Epoch 6, Iteration 274, Loss: 0.7812345027923584
Epoch 6, Iteration 275, Loss: 0.8091396689414978
Epoch 6, Iteration 276, Loss: 0.753328800201416
Epoch 6, Iteration 277, Loss: 0.760895311832428
Epoch 6, Iteration 278, Loss: 1.0093129873275757
Epoch 6, Iteration 279, Loss: 0.7493207454681396
Epoch 6, Iteration 280, Loss: 0.7046375870704651
Epoch 6, Iteration 281, Loss: 0.8463343977928162
Epoch 6, Iteration 282, Loss: 0.814285159111023
Epoch 6, Iteration 283, Loss: 0.8032712936401367
Epoch 6, Iteration 284, Loss: 0.8146592974662781
Epoch 6, Iteration 285, Loss: 0.77628093957901
Epoch 6, Iteration 286, Loss: 0.7743847370147705
Epoch 6, Iteration 287, Loss: 0.8297123908996582
Epoch 6, Iteration 288, Loss: 0.7838547825813293
Epoch 6, Iteration 289, Loss: 0.7465675473213196
Epoch 6, Iteration 290, Loss: 0.7675719261169434
Epoch 6, Iteration 291, Loss: 0.7113670110702515
Epoch 6, Iteration 292, Loss: 0.8983471989631653
Epoch 6, Iteration 293, Loss: 1.0260648727416992
Epoch 6, Iteration 294, Loss: 0.8463627696037292
Epoch 6, Iteration 295, Loss: 0.6781067252159119
Epoch 6, Iteration 296, Loss: 0.9508926868438721
Epoch 6, Iteration 297, Loss: 0.9280799031257629
Epoch 6, Iteration 298, Loss: 0.8318788409233093
Epoch 6, Iteration 299, Loss: 0.8049178123474121
Epoch 6, Iteration 300, Loss: 0.656991183757782
Epoch 6, Iteration 300, Valid Loss: 0.6723811626434326
Epoch 6, Iteration 301, Loss: 0.7700635194778442
Epoch 6, Iteration 302, Loss: 0.7209358811378479
Epoch 6, Iteration 303, Loss: 0.8965691328048706
Epoch 6, Iteration 304, Loss: 0.8522589802742004
Epoch 6, Iteration 305, Loss: 0.872219979763031
Epoch 6, Iteration 306, Loss: 0.7363453507423401
Epoch 6, Iteration 307, Loss: 0.8276856541633606
Epoch 6, Iteration 308, Loss: 0.7736508250236511
Epoch 6, Iteration 309, Loss: 0.8277667760848999
Epoch 6, Iteration 310, Loss: 0.8952963948249817
Epoch 6, Iteration 311, Loss: 0.7993751168251038
Epoch 6, Iteration 312, Loss: 0.6925045847892761
Epoch 6, Iteration 313, Loss: 0.9354389309883118
Epoch 6, Iteration 314, Loss: 0.7068763971328735
Epoch 6, Iteration 315, Loss: 0.7324127554893494
Epoch 6, Iteration 316, Loss: 0.975164532661438
Epoch 6, Iteration 317, Loss: 0.8804457783699036
Epoch 6, Iteration 318, Loss: 0.7506800889968872
Epoch 6, Iteration 319, Loss: 0.8671658039093018
Epoch 6, Iteration 320, Loss: 0.6032296419143677
Epoch 6, Iteration 321, Loss: 0.8408318161964417
Epoch 6, Iteration 322, Loss: 0.7719646096229553
Epoch 6, Iteration 323, Loss: 0.8662304282188416
Epoch 6, Iteration 324, Loss: 0.8822501301765442
Epoch 6, Iteration 325, Loss: 0.8453167080879211
Epoch 6, Iteration 326, Loss: 0.7752760648727417
Epoch 6, Iteration 327, Loss: 0.6854181289672852
Epoch 6, Iteration 328, Loss: 0.7351261973381042
Epoch 6, Iteration 329, Loss: 0.9411176443099976
Epoch 6, Iteration 330, Loss: 0.7343527674674988
Epoch 6, Iteration 331, Loss: 0.7623389959335327
Epoch 6, Iteration 332, Loss: 0.845463752746582
Epoch 6, Iteration 333, Loss: 1.007458209991455
Epoch 6, Iteration 334, Loss: 0.7348706126213074
Epoch 6, Iteration 335, Loss: 0.928810179233551
Epoch 6, Iteration 336, Loss: 0.750552237033844
Epoch 6, Iteration 337, Loss: 0.6319979429244995
Epoch 6, Iteration 338, Loss: 0.8097029328346252
Epoch 6, Iteration 339, Loss: 0.8039312362670898
Epoch 6, Iteration 340, Loss: 0.6279537081718445
Epoch 6, Iteration 341, Loss: 0.7890238761901855
Epoch 6, Iteration 342, Loss: 0.7407404184341431
Epoch 6, Iteration 343, Loss: 0.7449665069580078
Epoch 6, Iteration 344, Loss: 0.8400739431381226
Epoch 6, Iteration 345, Loss: 0.8390224575996399
Epoch 6, Iteration 346, Loss: 0.7813303470611572
Epoch 6, Iteration 347, Loss: 0.7464632391929626
Epoch 6, Iteration 348, Loss: 0.6240138411521912
Epoch 6, Iteration 349, Loss: 0.8102728724479675
Epoch 6, Iteration 350, Loss: 0.6840846538543701
Epoch 6, Iteration 350, Valid Loss: 0.6733394265174866
Epoch 6, Iteration 351, Loss: 0.8618155717849731
Epoch 6, Iteration 352, Loss: 0.6849775314331055
Epoch 6, Iteration 353, Loss: 0.7676188945770264
Epoch 6, Iteration 354, Loss: 0.7115797400474548
Epoch 6, Iteration 355, Loss: 0.7843700051307678
Epoch 6, Iteration 356, Loss: 0.8108558058738708
Epoch 6, Iteration 357, Loss: 0.7276828289031982
Epoch 6, Iteration 358, Loss: 0.7395079135894775
Epoch 6, Iteration 359, Loss: 0.6970458626747131
Epoch 6, Iteration 360, Loss: 0.6652472019195557
Epoch 6, Iteration 361, Loss: 0.7837364673614502
Epoch 6, Iteration 362, Loss: 0.7856165170669556
Epoch 6, Iteration 363, Loss: 0.8579668402671814
Epoch 6, Iteration 364, Loss: 0.7963702082633972
Epoch 6, Iteration 365, Loss: 0.9225480556488037
Epoch 6, Iteration 366, Loss: 0.8435182571411133
Epoch 6, Iteration 367, Loss: 0.8026506900787354
Epoch 6, Iteration 368, Loss: 0.7779878377914429
Epoch 6, Iteration 369, Loss: 0.774703323841095
Epoch 6, Iteration 370, Loss: 0.8537799119949341
Epoch 6, Iteration 371, Loss: 0.8410583138465881
Epoch 6, Iteration 372, Loss: 0.699502170085907
Epoch 6, Iteration 373, Loss: 0.7817139029502869
Epoch 6, Iteration 374, Loss: 0.947114884853363
Epoch 6, Iteration 375, Loss: 0.7886350750923157
Epoch 6, Iteration 376, Loss: 0.8739297986030579
Epoch 6, Iteration 377, Loss: 0.7294161319732666
Epoch 6, Iteration 378, Loss: 0.7348195314407349
Epoch 6, Iteration 379, Loss: 0.6775740385055542
Epoch 6, Iteration 380, Loss: 0.7235389947891235
Epoch 6, Iteration 381, Loss: 0.7107208371162415
Epoch 6, Iteration 382, Loss: 0.5475091934204102
Epoch 6, Iteration 383, Loss: 0.7564748525619507
Epoch 6, Iteration 384, Loss: 0.7827076315879822
Epoch 6, Iteration 385, Loss: 0.6911670565605164
Epoch 6, Iteration 386, Loss: 0.6493139266967773
Epoch 6, Iteration 387, Loss: 1.0093966722488403
Epoch 6, Iteration 388, Loss: 0.725048303604126
Epoch 6, Iteration 389, Loss: 0.7678507566452026
Epoch 6, Iteration 390, Loss: 0.9057670831680298
Epoch 6, Iteration 391, Loss: 0.7412339448928833
Epoch 7/15, Loss: 0.8613868478913697
Epoch 7, Iteration 0, Loss: 0.7807587385177612
Epoch 7, Iteration 1, Loss: 1.0001089572906494
Epoch 7, Iteration 2, Loss: 0.9142112731933594
Epoch 7, Iteration 3, Loss: 0.8562283515930176
Epoch 7, Iteration 4, Loss: 0.7417850494384766
Epoch 7, Iteration 5, Loss: 0.9724060893058777
Epoch 7, Iteration 6, Loss: 0.6957538723945618
Epoch 7, Iteration 7, Loss: 0.832771897315979
Epoch 7, Iteration 8, Loss: 0.7268743515014648
Epoch 7, Iteration 9, Loss: 0.9779843688011169
Epoch 7, Iteration 10, Loss: 0.8833597302436829
Epoch 7, Iteration 11, Loss: 0.7476693987846375
Epoch 7, Iteration 12, Loss: 0.6793031096458435
Epoch 7, Iteration 13, Loss: 0.9141262173652649
Epoch 7, Iteration 14, Loss: 0.7714180946350098
Epoch 7, Iteration 15, Loss: 0.8723652362823486
Epoch 7, Iteration 16, Loss: 0.821052074432373
Epoch 7, Iteration 17, Loss: 0.8993593454360962
Epoch 7, Iteration 18, Loss: 0.8166019916534424
Epoch 7, Iteration 19, Loss: 0.899639904499054
Epoch 7, Iteration 20, Loss: 0.7197369337081909
Epoch 7, Iteration 21, Loss: 0.7466498613357544
Epoch 7, Iteration 22, Loss: 0.751678466796875
Epoch 7, Iteration 23, Loss: 0.8705089092254639
Epoch 7, Iteration 24, Loss: 0.7678444385528564
Epoch 7, Iteration 25, Loss: 0.7687444686889648
Epoch 7, Iteration 26, Loss: 1.06522798538208
Epoch 7, Iteration 27, Loss: 0.9706442356109619
Epoch 7, Iteration 28, Loss: 0.7914252877235413
Epoch 7, Iteration 29, Loss: 1.109683871269226
Epoch 7, Iteration 30, Loss: 0.8063588738441467
Epoch 7, Iteration 31, Loss: 0.8573222756385803
Epoch 7, Iteration 32, Loss: 0.8311380743980408
Epoch 7, Iteration 33, Loss: 0.815100908279419
Epoch 7, Iteration 34, Loss: 0.7969427704811096
Epoch 7, Iteration 35, Loss: 0.9582988619804382
Epoch 7, Iteration 36, Loss: 0.6743867993354797
Epoch 7, Iteration 37, Loss: 0.7531744837760925
Epoch 7, Iteration 38, Loss: 0.7738423347473145
Epoch 7, Iteration 39, Loss: 0.8731199502944946
Epoch 7, Iteration 40, Loss: 0.8307611346244812
Epoch 7, Iteration 41, Loss: 0.8794283270835876
Epoch 7, Iteration 42, Loss: 0.771126389503479
Epoch 7, Iteration 43, Loss: 0.6778325438499451
Epoch 7, Iteration 44, Loss: 0.8004294633865356
Epoch 7, Iteration 45, Loss: 0.7408488392829895
Epoch 7, Iteration 46, Loss: 0.8696110844612122
Epoch 7, Iteration 47, Loss: 0.8234677314758301
Epoch 7, Iteration 48, Loss: 0.9729644656181335
Epoch 7, Iteration 49, Loss: 0.7887186408042908
Epoch 7, Iteration 50, Loss: 0.7869209051132202
Epoch 7, Iteration 50, Valid Loss: 0.6288120746612549
Epoch 7, Iteration 51, Loss: 0.792698323726654
Epoch 7, Iteration 52, Loss: 0.9187628626823425
Epoch 7, Iteration 53, Loss: 0.9170027375221252
Epoch 7, Iteration 54, Loss: 1.1417737007141113
Epoch 7, Iteration 55, Loss: 0.9357349276542664
Epoch 7, Iteration 56, Loss: 0.8049165606498718
Epoch 7, Iteration 57, Loss: 0.9790053963661194
Epoch 7, Iteration 58, Loss: 0.8553678393363953
Epoch 7, Iteration 59, Loss: 0.7022162675857544
Epoch 7, Iteration 60, Loss: 0.8281328678131104
Epoch 7, Iteration 61, Loss: 0.6451684832572937
Epoch 7, Iteration 62, Loss: 0.7270545363426208
Epoch 7, Iteration 63, Loss: 0.7217098474502563
Epoch 7, Iteration 64, Loss: 0.9404202699661255
Epoch 7, Iteration 65, Loss: 1.0401118993759155
Epoch 7, Iteration 66, Loss: 0.8350660800933838
Epoch 7, Iteration 67, Loss: 0.6777006387710571
Epoch 7, Iteration 68, Loss: 0.9667060375213623
Epoch 7, Iteration 69, Loss: 0.9481001496315002
Epoch 7, Iteration 70, Loss: 0.872506320476532
Epoch 7, Iteration 71, Loss: 0.8490266799926758
Epoch 7, Iteration 72, Loss: 0.7356780171394348
Epoch 7, Iteration 73, Loss: 0.8106260299682617
Epoch 7, Iteration 74, Loss: 1.068251609802246
Epoch 7, Iteration 75, Loss: 0.9205963015556335
Epoch 7, Iteration 76, Loss: 0.8189281821250916
Epoch 7, Iteration 77, Loss: 0.7224507927894592
Epoch 7, Iteration 78, Loss: 1.0833154916763306
Epoch 7, Iteration 79, Loss: 1.0487403869628906
Epoch 7, Iteration 80, Loss: 0.7666139602661133
Epoch 7, Iteration 81, Loss: 1.1327390670776367
Epoch 7, Iteration 82, Loss: 0.9706342220306396
Epoch 7, Iteration 83, Loss: 0.7399483323097229
Epoch 7, Iteration 84, Loss: 1.0021376609802246
Epoch 7, Iteration 85, Loss: 0.865048348903656
Epoch 7, Iteration 86, Loss: 0.9029746055603027
Epoch 7, Iteration 87, Loss: 0.8550764918327332
Epoch 7, Iteration 88, Loss: 0.8765568733215332
Epoch 7, Iteration 89, Loss: 0.8712485432624817
Epoch 7, Iteration 90, Loss: 0.7038930654525757
Epoch 7, Iteration 91, Loss: 0.6517089605331421
Epoch 7, Iteration 92, Loss: 0.7897401452064514
Epoch 7, Iteration 93, Loss: 0.8190420866012573
Epoch 7, Iteration 94, Loss: 1.0281083583831787
Epoch 7, Iteration 95, Loss: 0.5801501870155334
Epoch 7, Iteration 96, Loss: 0.8187569379806519
Epoch 7, Iteration 97, Loss: 0.637419581413269
Epoch 7, Iteration 98, Loss: 0.8148011565208435
Epoch 7, Iteration 99, Loss: 0.8036811351776123
Epoch 7, Iteration 100, Loss: 0.7397749423980713
Epoch 7, Iteration 100, Valid Loss: 0.7026565670967102
Epoch 7, Iteration 101, Loss: 1.0668973922729492
Epoch 7, Iteration 102, Loss: 0.9446930289268494
Epoch 7, Iteration 103, Loss: 0.7974791526794434
Epoch 7, Iteration 104, Loss: 1.0220744609832764
Epoch 7, Iteration 105, Loss: 0.8688572645187378
Epoch 7, Iteration 106, Loss: 0.7336182594299316
Epoch 7, Iteration 107, Loss: 1.13853919506073
Epoch 7, Iteration 108, Loss: 0.7906147241592407
Epoch 7, Iteration 109, Loss: 1.0961719751358032
Epoch 7, Iteration 110, Loss: 0.9893525838851929
Epoch 7, Iteration 111, Loss: 0.7828582525253296
Epoch 7, Iteration 112, Loss: 0.9076240062713623
Epoch 7, Iteration 113, Loss: 0.6746313571929932
Epoch 7, Iteration 114, Loss: 1.0412676334381104
Epoch 7, Iteration 115, Loss: 0.9247589111328125
Epoch 7, Iteration 116, Loss: 0.7271558046340942
Epoch 7, Iteration 117, Loss: 0.5627004504203796
Epoch 7, Iteration 118, Loss: 0.9107057452201843
Epoch 7, Iteration 119, Loss: 0.7778766751289368
Epoch 7, Iteration 120, Loss: 0.9817062020301819
Epoch 7, Iteration 121, Loss: 0.7532413601875305
Epoch 7, Iteration 122, Loss: 0.8131604194641113
Epoch 7, Iteration 123, Loss: 0.8326100707054138
Epoch 7, Iteration 124, Loss: 0.7116755247116089
Epoch 7, Iteration 125, Loss: 0.8256444931030273
Epoch 7, Iteration 126, Loss: 0.7144587635993958
Epoch 7, Iteration 127, Loss: 0.8898270726203918
Epoch 7, Iteration 128, Loss: 0.8527031540870667
Epoch 7, Iteration 129, Loss: 0.9023072123527527
Epoch 7, Iteration 130, Loss: 1.0497626066207886
Epoch 7, Iteration 131, Loss: 0.7554911375045776
Epoch 7, Iteration 132, Loss: 0.7451658248901367
Epoch 7, Iteration 133, Loss: 1.0863544940948486
Epoch 7, Iteration 134, Loss: 0.886617124080658
Epoch 7, Iteration 135, Loss: 0.8720197677612305
Epoch 7, Iteration 136, Loss: 0.8927602171897888
Epoch 7, Iteration 137, Loss: 0.8644930720329285
Epoch 7, Iteration 138, Loss: 0.6755325794219971
Epoch 7, Iteration 139, Loss: 0.857010006904602
Epoch 7, Iteration 140, Loss: 0.7116042971611023
Epoch 7, Iteration 141, Loss: 1.158316969871521
Epoch 7, Iteration 142, Loss: 0.8740724921226501
Epoch 7, Iteration 143, Loss: 0.771639883518219
Epoch 7, Iteration 144, Loss: 0.7964283227920532
Epoch 7, Iteration 145, Loss: 0.795951247215271
Epoch 7, Iteration 146, Loss: 0.8856279253959656
Epoch 7, Iteration 147, Loss: 0.8656407594680786
Epoch 7, Iteration 148, Loss: 0.7230308055877686
Epoch 7, Iteration 149, Loss: 0.7785241007804871
Epoch 7, Iteration 150, Loss: 0.9398717880249023
Epoch 7, Iteration 150, Valid Loss: 0.6531211137771606
Epoch 7, Iteration 151, Loss: 0.886091411113739
Epoch 7, Iteration 152, Loss: 0.7791476845741272
Epoch 7, Iteration 153, Loss: 1.007896900177002
Epoch 7, Iteration 154, Loss: 0.7259949445724487
Epoch 7, Iteration 155, Loss: 0.8568269610404968
Epoch 7, Iteration 156, Loss: 0.7256686687469482
Epoch 7, Iteration 157, Loss: 1.1538000106811523
Epoch 7, Iteration 158, Loss: 0.8428224921226501
Epoch 7, Iteration 159, Loss: 0.8633370995521545
Epoch 7, Iteration 160, Loss: 0.742670476436615
Epoch 7, Iteration 161, Loss: 0.7796298861503601
Epoch 7, Iteration 162, Loss: 1.1156240701675415
Epoch 7, Iteration 163, Loss: 0.6733419895172119
Epoch 7, Iteration 164, Loss: 0.8307566046714783
Epoch 7, Iteration 165, Loss: 0.7248823642730713
Epoch 7, Iteration 166, Loss: 0.7284976840019226
Epoch 7, Iteration 167, Loss: 0.974826991558075
Epoch 7, Iteration 168, Loss: 0.8500487804412842
Epoch 7, Iteration 169, Loss: 0.8158398866653442
Epoch 7, Iteration 170, Loss: 0.9765042066574097
Epoch 7, Iteration 171, Loss: 0.9286813735961914
Epoch 7, Iteration 172, Loss: 0.8045305013656616
Epoch 7, Iteration 173, Loss: 0.708111584186554
Epoch 7, Iteration 174, Loss: 0.8857104778289795
Epoch 7, Iteration 175, Loss: 0.8836666941642761
Epoch 7, Iteration 176, Loss: 0.6345216035842896
Epoch 7, Iteration 177, Loss: 1.1519745588302612
Epoch 7, Iteration 178, Loss: 1.034260630607605
Epoch 7, Iteration 179, Loss: 0.7985706329345703
Epoch 7, Iteration 180, Loss: 0.7984645962715149
Epoch 7, Iteration 181, Loss: 0.9611436724662781
Epoch 7, Iteration 182, Loss: 0.934952974319458
Epoch 7, Iteration 183, Loss: 0.7081317901611328
Epoch 7, Iteration 184, Loss: 0.7929804921150208
Epoch 7, Iteration 185, Loss: 0.7154640555381775
Epoch 7, Iteration 186, Loss: 0.7990390658378601
Epoch 7, Iteration 187, Loss: 0.7476531863212585
Epoch 7, Iteration 188, Loss: 0.7328268885612488
Epoch 7, Iteration 189, Loss: 0.7467116117477417
Epoch 7, Iteration 190, Loss: 0.9646768569946289
Epoch 7, Iteration 191, Loss: 0.8866062164306641
Epoch 7, Iteration 192, Loss: 0.9186716675758362
Epoch 7, Iteration 193, Loss: 0.7944021821022034
Epoch 7, Iteration 194, Loss: 0.8060475587844849
Epoch 7, Iteration 195, Loss: 0.8513054847717285
Epoch 7, Iteration 196, Loss: 0.8343818783760071
Epoch 7, Iteration 197, Loss: 0.823411762714386
Epoch 7, Iteration 198, Loss: 0.8194068670272827
Epoch 7, Iteration 199, Loss: 0.9225341081619263
Epoch 7, Iteration 200, Loss: 0.7667677402496338
Epoch 7, Iteration 200, Valid Loss: 0.6609580516815186
Epoch 7, Iteration 201, Loss: 0.7626671195030212
Epoch 7, Iteration 202, Loss: 0.8589255213737488
Epoch 7, Iteration 203, Loss: 0.7599508762359619
Epoch 7, Iteration 204, Loss: 0.7599952220916748
Epoch 7, Iteration 205, Loss: 0.6746100187301636
Epoch 7, Iteration 206, Loss: 0.7011725902557373
Epoch 7, Iteration 207, Loss: 0.9271546006202698
Epoch 7, Iteration 208, Loss: 1.2229461669921875
Epoch 7, Iteration 209, Loss: 1.041476845741272
Epoch 7, Iteration 210, Loss: 1.065731406211853
Epoch 7, Iteration 211, Loss: 1.1518243551254272
Epoch 7, Iteration 212, Loss: 0.9851424694061279
Epoch 7, Iteration 213, Loss: 1.1388111114501953
Epoch 7, Iteration 214, Loss: 0.7997285723686218
Epoch 7, Iteration 215, Loss: 0.7460931539535522
Epoch 7, Iteration 216, Loss: 0.7811265587806702
Epoch 7, Iteration 217, Loss: 0.7082324624061584
Epoch 7, Iteration 218, Loss: 0.8397226333618164
Epoch 7, Iteration 219, Loss: 0.9248088002204895
Epoch 7, Iteration 220, Loss: 0.7643944621086121
Epoch 7, Iteration 221, Loss: 0.8548832535743713
Epoch 7, Iteration 222, Loss: 0.8093224763870239
Epoch 7, Iteration 223, Loss: 0.7124581933021545
Epoch 7, Iteration 224, Loss: 0.7258070111274719
Epoch 7, Iteration 225, Loss: 0.8755045533180237
Epoch 7, Iteration 226, Loss: 0.893294632434845
Epoch 7, Iteration 227, Loss: 0.8423646688461304
Epoch 7, Iteration 228, Loss: 0.9031882882118225
Epoch 7, Iteration 229, Loss: 0.7102035880088806
Epoch 7, Iteration 230, Loss: 0.8160422444343567
Epoch 7, Iteration 231, Loss: 0.7218504548072815
Epoch 7, Iteration 232, Loss: 0.9350214004516602
Epoch 7, Iteration 233, Loss: 1.0997238159179688
Epoch 7, Iteration 234, Loss: 0.9350956082344055
Epoch 7, Iteration 235, Loss: 0.631614625453949
Epoch 7, Iteration 236, Loss: 0.8057868480682373
Epoch 7, Iteration 237, Loss: 0.8099339604377747
Epoch 7, Iteration 238, Loss: 0.6778084635734558
Epoch 7, Iteration 239, Loss: 0.844700038433075
Epoch 7, Iteration 240, Loss: 0.7368040084838867
Epoch 7, Iteration 241, Loss: 0.7869619727134705
Epoch 7, Iteration 242, Loss: 0.7878249287605286
Epoch 7, Iteration 243, Loss: 0.7885993123054504
Epoch 7, Iteration 244, Loss: 0.8073578476905823
Epoch 7, Iteration 245, Loss: 0.5742329955101013
Epoch 7, Iteration 246, Loss: 0.8221732974052429
Epoch 7, Iteration 247, Loss: 0.7262783050537109
Epoch 7, Iteration 248, Loss: 0.9164894223213196
Epoch 7, Iteration 249, Loss: 0.8047215342521667
Epoch 7, Iteration 250, Loss: 0.9085180163383484
Epoch 7, Iteration 250, Valid Loss: 0.627094030380249
Epoch 7, Iteration 251, Loss: 0.7317272424697876
Epoch 7, Iteration 252, Loss: 0.9444628953933716
Epoch 7, Iteration 253, Loss: 0.8962719440460205
Epoch 7, Iteration 254, Loss: 0.9491063952445984
Epoch 7, Iteration 255, Loss: 0.9014657139778137
Epoch 7, Iteration 256, Loss: 0.8931984305381775
Epoch 7, Iteration 257, Loss: 0.7183470726013184
Epoch 7, Iteration 258, Loss: 1.0030369758605957
Epoch 7, Iteration 259, Loss: 0.7068907022476196
Epoch 7, Iteration 260, Loss: 0.9327701926231384
Epoch 7, Iteration 261, Loss: 0.8705060482025146
Epoch 7, Iteration 262, Loss: 0.7120449542999268
Epoch 7, Iteration 263, Loss: 0.8602250814437866
Epoch 7, Iteration 264, Loss: 0.7601279616355896
Epoch 7, Iteration 265, Loss: 0.8962103724479675
Epoch 7, Iteration 266, Loss: 0.8069462180137634
Epoch 7, Iteration 267, Loss: 0.7904554605484009
Epoch 7, Iteration 268, Loss: 0.7126280069351196
Epoch 7, Iteration 269, Loss: 0.7954975962638855
Epoch 7, Iteration 270, Loss: 0.80109703540802
Epoch 7, Iteration 271, Loss: 0.9165465831756592
Epoch 7, Iteration 272, Loss: 0.7227517366409302
Epoch 7, Iteration 273, Loss: 0.9179272651672363
Epoch 7, Iteration 274, Loss: 0.7401010394096375
Epoch 7, Iteration 275, Loss: 0.6845203638076782
Epoch 7, Iteration 276, Loss: 0.6621360778808594
Epoch 7, Iteration 277, Loss: 0.7227115631103516
Epoch 7, Iteration 278, Loss: 0.8167589902877808
Epoch 7, Iteration 279, Loss: 0.6525457501411438
Epoch 7, Iteration 280, Loss: 0.6281647682189941
Epoch 7, Iteration 281, Loss: 0.7477750182151794
Epoch 7, Iteration 282, Loss: 0.7669596076011658
Epoch 7, Iteration 283, Loss: 0.7667103409767151
Epoch 7, Iteration 284, Loss: 0.8369082808494568
Epoch 7, Iteration 285, Loss: 0.6811191439628601
Epoch 7, Iteration 286, Loss: 0.8009266257286072
Epoch 7, Iteration 287, Loss: 0.7053644061088562
Epoch 7, Iteration 288, Loss: 0.7682638764381409
Epoch 7, Iteration 289, Loss: 0.6455366611480713
Epoch 7, Iteration 290, Loss: 0.6104114651679993
Epoch 7, Iteration 291, Loss: 0.5994965434074402
Epoch 7, Iteration 292, Loss: 0.8337138295173645
Epoch 7, Iteration 293, Loss: 0.948569655418396
Epoch 7, Iteration 294, Loss: 0.7552927732467651
Epoch 7, Iteration 295, Loss: 0.5968097448348999
Epoch 7, Iteration 296, Loss: 0.9142616987228394
Epoch 7, Iteration 297, Loss: 0.9165624976158142
Epoch 7, Iteration 298, Loss: 0.7819798588752747
Epoch 7, Iteration 299, Loss: 0.7430936694145203
Epoch 7, Iteration 300, Loss: 0.6683322787284851
Epoch 7, Iteration 300, Valid Loss: 0.6409880518913269
Epoch 7, Iteration 301, Loss: 0.6985966563224792
Epoch 7, Iteration 302, Loss: 0.6870680451393127
Epoch 7, Iteration 303, Loss: 0.9397466778755188
Epoch 7, Iteration 304, Loss: 0.7313122749328613
Epoch 7, Iteration 305, Loss: 0.6557945013046265
Epoch 7, Iteration 306, Loss: 0.6574673652648926
Epoch 7, Iteration 307, Loss: 0.6491492390632629
Epoch 7, Iteration 308, Loss: 0.7109147906303406
Epoch 7, Iteration 309, Loss: 0.776026725769043
Epoch 7, Iteration 310, Loss: 0.8346061706542969
Epoch 7, Iteration 311, Loss: 0.8232980370521545
Epoch 7, Iteration 312, Loss: 0.7135326266288757
Epoch 7, Iteration 313, Loss: 0.9989252686500549
Epoch 7, Iteration 314, Loss: 0.6837284564971924
Epoch 7, Iteration 315, Loss: 0.6759939193725586
Epoch 7, Iteration 316, Loss: 0.8613730072975159
Epoch 7, Iteration 317, Loss: 0.79620361328125
Epoch 7, Iteration 318, Loss: 0.6262668371200562
Epoch 7, Iteration 319, Loss: 0.9005968570709229
Epoch 7, Iteration 320, Loss: 0.5295506119728088
Epoch 7, Iteration 321, Loss: 0.8226194977760315
Epoch 7, Iteration 322, Loss: 0.7078061103820801
Epoch 7, Iteration 323, Loss: 0.7586493492126465
Epoch 7, Iteration 324, Loss: 0.8237130045890808
Epoch 7, Iteration 325, Loss: 0.8329858183860779
Epoch 7, Iteration 326, Loss: 0.7182661294937134
Epoch 7, Iteration 327, Loss: 0.713573694229126
Epoch 7, Iteration 328, Loss: 0.6911936402320862
Epoch 7, Iteration 329, Loss: 0.8356702923774719
Epoch 7, Iteration 330, Loss: 0.6107012629508972
Epoch 7, Iteration 331, Loss: 0.7576987147331238
Epoch 7, Iteration 332, Loss: 0.7525964975357056
Epoch 7, Iteration 333, Loss: 0.8894855976104736
Epoch 7, Iteration 334, Loss: 0.7548941373825073
Epoch 7, Iteration 335, Loss: 0.8681884407997131
Epoch 7, Iteration 336, Loss: 0.8685750961303711
Epoch 7, Iteration 337, Loss: 0.6563715934753418
Epoch 7, Iteration 338, Loss: 0.9659472107887268
Epoch 7, Iteration 339, Loss: 0.7712991237640381
Epoch 7, Iteration 340, Loss: 0.603645384311676
Epoch 7, Iteration 341, Loss: 0.747826099395752
Epoch 7, Iteration 342, Loss: 0.7541590929031372
Epoch 7, Iteration 343, Loss: 0.7641313076019287
Epoch 7, Iteration 344, Loss: 0.8549860119819641
Epoch 7, Iteration 345, Loss: 0.7608649730682373
Epoch 7, Iteration 346, Loss: 0.7051149606704712
Epoch 7, Iteration 347, Loss: 0.6613991856575012
Epoch 7, Iteration 348, Loss: 0.5783682465553284
Epoch 7, Iteration 349, Loss: 0.7442683577537537
Epoch 7, Iteration 350, Loss: 0.7483622431755066
Epoch 7, Iteration 350, Valid Loss: 0.663077175617218
Epoch 7, Iteration 351, Loss: 0.7348140478134155
Epoch 7, Iteration 352, Loss: 0.6448012590408325
Epoch 7, Iteration 353, Loss: 0.7284764647483826
Epoch 7, Iteration 354, Loss: 0.6336501240730286
Epoch 7, Iteration 355, Loss: 0.7166846394538879
Epoch 7, Iteration 356, Loss: 0.7972567677497864
Epoch 7, Iteration 357, Loss: 0.6800937652587891
Epoch 7, Iteration 358, Loss: 0.6545470356941223
Epoch 7, Iteration 359, Loss: 0.7312576174736023
Epoch 7, Iteration 360, Loss: 0.5788196325302124
Epoch 7, Iteration 361, Loss: 0.7070230841636658
Epoch 7, Iteration 362, Loss: 0.724740207195282
Epoch 7, Iteration 363, Loss: 0.7240682244300842
Epoch 7, Iteration 364, Loss: 0.7444657683372498
Epoch 7, Iteration 365, Loss: 0.8479467034339905
Epoch 7, Iteration 366, Loss: 0.7864348888397217
Epoch 7, Iteration 367, Loss: 0.743439257144928
Epoch 7, Iteration 368, Loss: 0.7411530613899231
Epoch 7, Iteration 369, Loss: 0.7810795307159424
Epoch 7, Iteration 370, Loss: 0.7564297914505005
Epoch 7, Iteration 371, Loss: 0.7863228917121887
Epoch 7, Iteration 372, Loss: 0.7309903502464294
Epoch 7, Iteration 373, Loss: 0.683765172958374
Epoch 7, Iteration 374, Loss: 0.9511817693710327
Epoch 7, Iteration 375, Loss: 0.7082496285438538
Epoch 7, Iteration 376, Loss: 0.8383395671844482
Epoch 7, Iteration 377, Loss: 0.8339261412620544
Epoch 7, Iteration 378, Loss: 0.6917149424552917
Epoch 7, Iteration 379, Loss: 0.6655032634735107
Epoch 7, Iteration 380, Loss: 0.6975058317184448
Epoch 7, Iteration 381, Loss: 0.7019885182380676
Epoch 7, Iteration 382, Loss: 0.5766870379447937
Epoch 7, Iteration 383, Loss: 0.8348678350448608
Epoch 7, Iteration 384, Loss: 0.7688816785812378
Epoch 7, Iteration 385, Loss: 0.6765870451927185
Epoch 7, Iteration 386, Loss: 0.6258069276809692
Epoch 7, Iteration 387, Loss: 0.9047914743423462
Epoch 7, Iteration 388, Loss: 0.6695424914360046
Epoch 7, Iteration 389, Loss: 0.7368347644805908
Epoch 7, Iteration 390, Loss: 0.9141240119934082
Epoch 7, Iteration 391, Loss: 0.6896296143531799
Epoch 8/15, Loss: 0.8145121693307039
Epoch 8, Iteration 0, Loss: 0.7723314166069031
Epoch 8, Iteration 1, Loss: 0.9333529472351074
Epoch 8, Iteration 2, Loss: 0.9342523217201233
Epoch 8, Iteration 3, Loss: 0.8840513229370117
Epoch 8, Iteration 4, Loss: 0.7673226594924927
Epoch 8, Iteration 5, Loss: 0.869283139705658
Epoch 8, Iteration 6, Loss: 0.6541190147399902
Epoch 8, Iteration 7, Loss: 0.7809027433395386
Epoch 8, Iteration 8, Loss: 0.6874029636383057
Epoch 8, Iteration 9, Loss: 0.8833932876586914
Epoch 8, Iteration 10, Loss: 0.8319272994995117
Epoch 8, Iteration 11, Loss: 0.7463294267654419
Epoch 8, Iteration 12, Loss: 0.7184756994247437
Epoch 8, Iteration 13, Loss: 0.916750431060791
Epoch 8, Iteration 14, Loss: 0.8316477537155151
Epoch 8, Iteration 15, Loss: 0.9646238088607788
Epoch 8, Iteration 16, Loss: 0.7435988783836365
Epoch 8, Iteration 17, Loss: 0.8644112348556519
Epoch 8, Iteration 18, Loss: 0.7481285333633423
Epoch 8, Iteration 19, Loss: 0.8123845458030701
Epoch 8, Iteration 20, Loss: 0.6940682530403137
Epoch 8, Iteration 21, Loss: 0.7382153868675232
Epoch 8, Iteration 22, Loss: 0.6932944655418396
Epoch 8, Iteration 23, Loss: 0.7573882937431335
Epoch 8, Iteration 24, Loss: 0.648131787776947
Epoch 8, Iteration 25, Loss: 0.6381077170372009
Epoch 8, Iteration 26, Loss: 1.0425432920455933
Epoch 8, Iteration 27, Loss: 0.7811651825904846
Epoch 8, Iteration 28, Loss: 0.7566598653793335
Epoch 8, Iteration 29, Loss: 1.0129474401474
Epoch 8, Iteration 30, Loss: 0.8601498603820801
Epoch 8, Iteration 31, Loss: 0.7387085556983948
Epoch 8, Iteration 32, Loss: 0.7336837649345398
Epoch 8, Iteration 33, Loss: 0.8684208393096924
Epoch 8, Iteration 34, Loss: 0.717205822467804
Epoch 8, Iteration 35, Loss: 0.8121412992477417
Epoch 8, Iteration 36, Loss: 0.637554943561554
Epoch 8, Iteration 37, Loss: 0.6456922888755798
Epoch 8, Iteration 38, Loss: 0.7362203001976013
Epoch 8, Iteration 39, Loss: 0.7768717408180237
Epoch 8, Iteration 40, Loss: 0.7174690961837769
Epoch 8, Iteration 41, Loss: 0.8613415360450745
Epoch 8, Iteration 42, Loss: 0.7906469702720642
Epoch 8, Iteration 43, Loss: 0.7483316659927368
Epoch 8, Iteration 44, Loss: 0.7168462872505188
Epoch 8, Iteration 45, Loss: 0.6911691427230835
Epoch 8, Iteration 46, Loss: 0.8288882374763489
Epoch 8, Iteration 47, Loss: 0.74136883020401
Epoch 8, Iteration 48, Loss: 0.8879780769348145
Epoch 8, Iteration 49, Loss: 0.7466059923171997
Epoch 8, Iteration 50, Loss: 0.7328747510910034
Epoch 8, Iteration 50, Valid Loss: 0.6179069876670837
Epoch 8, Iteration 51, Loss: 0.7690640091896057
Epoch 8, Iteration 52, Loss: 0.6960077881813049
Epoch 8, Iteration 53, Loss: 0.733831524848938
Epoch 8, Iteration 54, Loss: 1.0023905038833618
Epoch 8, Iteration 55, Loss: 0.8943944573402405
Epoch 8, Iteration 56, Loss: 0.8267918825149536
Epoch 8, Iteration 57, Loss: 1.002090334892273
Epoch 8, Iteration 58, Loss: 0.7740632891654968
Epoch 8, Iteration 59, Loss: 0.7052252292633057
Epoch 8, Iteration 60, Loss: 0.7883196473121643
Epoch 8, Iteration 61, Loss: 0.6196491122245789
Epoch 8, Iteration 62, Loss: 0.7479863166809082
Epoch 8, Iteration 63, Loss: 0.7060490846633911
Epoch 8, Iteration 64, Loss: 0.7857531309127808
Epoch 8, Iteration 65, Loss: 0.9729222059249878
Epoch 8, Iteration 66, Loss: 0.7388310432434082
Epoch 8, Iteration 67, Loss: 0.5539466142654419
Epoch 8, Iteration 68, Loss: 0.9452335834503174
Epoch 8, Iteration 69, Loss: 0.8556772470474243
Epoch 8, Iteration 70, Loss: 0.7945333123207092
Epoch 8, Iteration 71, Loss: 0.9160225987434387
Epoch 8, Iteration 72, Loss: 0.7246264219284058
Epoch 8, Iteration 73, Loss: 0.6762353777885437
Epoch 8, Iteration 74, Loss: 0.9243794679641724
Epoch 8, Iteration 75, Loss: 0.8630112409591675
Epoch 8, Iteration 76, Loss: 0.7713794708251953
Epoch 8, Iteration 77, Loss: 0.6407169103622437
Epoch 8, Iteration 78, Loss: 0.9536401033401489
Epoch 8, Iteration 79, Loss: 0.9975892901420593
Epoch 8, Iteration 80, Loss: 0.7111271619796753
Epoch 8, Iteration 81, Loss: 0.961274266242981
Epoch 8, Iteration 82, Loss: 0.9065226912498474
Epoch 8, Iteration 83, Loss: 0.6358131170272827
Epoch 8, Iteration 84, Loss: 0.8619581460952759
Epoch 8, Iteration 85, Loss: 0.8530079126358032
Epoch 8, Iteration 86, Loss: 0.8726113438606262
Epoch 8, Iteration 87, Loss: 0.9020432233810425
Epoch 8, Iteration 88, Loss: 0.8701348304748535
Epoch 8, Iteration 89, Loss: 1.1263315677642822
Epoch 8, Iteration 90, Loss: 0.6347926259040833
Epoch 8, Iteration 91, Loss: 0.7265244126319885
Epoch 8, Iteration 92, Loss: 0.5613253116607666
Epoch 8, Iteration 93, Loss: 0.7694932818412781
Epoch 8, Iteration 94, Loss: 0.9448686838150024
Epoch 8, Iteration 95, Loss: 0.688216507434845
Epoch 8, Iteration 96, Loss: 0.8024515509605408
Epoch 8, Iteration 97, Loss: 0.6573487520217896
Epoch 8, Iteration 98, Loss: 0.761480450630188
Epoch 8, Iteration 99, Loss: 0.8248940110206604
Epoch 8, Iteration 100, Loss: 0.5826283693313599
Epoch 8, Iteration 100, Valid Loss: 0.6737052798271179
Epoch 8, Iteration 101, Loss: 0.904826283454895
Epoch 8, Iteration 102, Loss: 0.9116339683532715
Epoch 8, Iteration 103, Loss: 0.7551209926605225
Epoch 8, Iteration 104, Loss: 0.943060576915741
Epoch 8, Iteration 105, Loss: 0.7408137917518616
Epoch 8, Iteration 106, Loss: 0.7344620227813721
Epoch 8, Iteration 107, Loss: 0.9208418726921082
Epoch 8, Iteration 108, Loss: 0.7266799211502075
Epoch 8, Iteration 109, Loss: 0.9877540469169617
Epoch 8, Iteration 110, Loss: 0.9691860675811768
Epoch 8, Iteration 111, Loss: 0.9345133900642395
Epoch 8, Iteration 112, Loss: 0.870850682258606
Epoch 8, Iteration 113, Loss: 0.6397858262062073
Epoch 8, Iteration 114, Loss: 0.8992208242416382
Epoch 8, Iteration 115, Loss: 0.9015724062919617
Epoch 8, Iteration 116, Loss: 0.6488872766494751
Epoch 8, Iteration 117, Loss: 0.5476059913635254
Epoch 8, Iteration 118, Loss: 0.7780063152313232
Epoch 8, Iteration 119, Loss: 0.7030429244041443
Epoch 8, Iteration 120, Loss: 0.8754627108573914
Epoch 8, Iteration 121, Loss: 0.7071446180343628
Epoch 8, Iteration 122, Loss: 0.8263083100318909
Epoch 8, Iteration 123, Loss: 0.7530467510223389
Epoch 8, Iteration 124, Loss: 0.6616172790527344
Epoch 8, Iteration 125, Loss: 0.7474989295005798
Epoch 8, Iteration 126, Loss: 0.7333531975746155
Epoch 8, Iteration 127, Loss: 0.804903507232666
Epoch 8, Iteration 128, Loss: 0.8953490257263184
Epoch 8, Iteration 129, Loss: 0.9186651110649109
Epoch 8, Iteration 130, Loss: 1.0772676467895508
Epoch 8, Iteration 131, Loss: 0.689012348651886
Epoch 8, Iteration 132, Loss: 0.7412035465240479
Epoch 8, Iteration 133, Loss: 1.1038097143173218
Epoch 8, Iteration 134, Loss: 0.932055652141571
Epoch 8, Iteration 135, Loss: 1.0814642906188965
Epoch 8, Iteration 136, Loss: 0.7482474446296692
Epoch 8, Iteration 137, Loss: 0.7650377750396729
Epoch 8, Iteration 138, Loss: 0.7316889762878418
Epoch 8, Iteration 139, Loss: 0.8316344022750854
Epoch 8, Iteration 140, Loss: 0.6388121247291565
Epoch 8, Iteration 141, Loss: 1.235364317893982
Epoch 8, Iteration 142, Loss: 0.8545442819595337
Epoch 8, Iteration 143, Loss: 0.7343149185180664
Epoch 8, Iteration 144, Loss: 0.7430980801582336
Epoch 8, Iteration 145, Loss: 0.7858081459999084
Epoch 8, Iteration 146, Loss: 0.8181036114692688
Epoch 8, Iteration 147, Loss: 0.7796630263328552
Epoch 8, Iteration 148, Loss: 0.6740622520446777
Epoch 8, Iteration 149, Loss: 0.7987098693847656
Epoch 8, Iteration 150, Loss: 0.8844696283340454
Epoch 8, Iteration 150, Valid Loss: 0.6491300463676453
Epoch 8, Iteration 151, Loss: 0.8823569416999817
Epoch 8, Iteration 152, Loss: 0.6598419547080994
Epoch 8, Iteration 153, Loss: 0.921294629573822
Epoch 8, Iteration 154, Loss: 0.626501739025116
Epoch 8, Iteration 155, Loss: 0.7858733534812927
Epoch 8, Iteration 156, Loss: 0.6248051524162292
Epoch 8, Iteration 157, Loss: 1.1559652090072632
Epoch 8, Iteration 158, Loss: 0.8027684092521667
Epoch 8, Iteration 159, Loss: 0.8869659900665283
Epoch 8, Iteration 160, Loss: 0.7894882559776306
Epoch 8, Iteration 161, Loss: 0.7609946131706238
Epoch 8, Iteration 162, Loss: 1.0725276470184326
Epoch 8, Iteration 163, Loss: 0.6728034019470215
Epoch 8, Iteration 164, Loss: 0.721744179725647
Epoch 8, Iteration 165, Loss: 0.7102029323577881
Epoch 8, Iteration 166, Loss: 0.7352766990661621
Epoch 8, Iteration 167, Loss: 0.9089962840080261
Epoch 8, Iteration 168, Loss: 0.750244140625
Epoch 8, Iteration 169, Loss: 0.760367214679718
Epoch 8, Iteration 170, Loss: 0.9600703716278076
Epoch 8, Iteration 171, Loss: 1.0451133251190186
Epoch 8, Iteration 172, Loss: 0.7953265309333801
Epoch 8, Iteration 173, Loss: 0.7146149277687073
Epoch 8, Iteration 174, Loss: 0.9229426980018616
Epoch 8, Iteration 175, Loss: 0.8634028434753418
Epoch 8, Iteration 176, Loss: 0.5767017602920532
Epoch 8, Iteration 177, Loss: 1.1635457277297974
Epoch 8, Iteration 178, Loss: 0.9362952709197998
Epoch 8, Iteration 179, Loss: 0.7171728610992432
Epoch 8, Iteration 180, Loss: 0.7928593158721924
Epoch 8, Iteration 181, Loss: 0.9365570545196533
Epoch 8, Iteration 182, Loss: 0.7867081761360168
Epoch 8, Iteration 183, Loss: 0.672962486743927
Epoch 8, Iteration 184, Loss: 0.791535496711731
Epoch 8, Iteration 185, Loss: 0.692977786064148
Epoch 8, Iteration 186, Loss: 0.7312636375427246
Epoch 8, Iteration 187, Loss: 0.7403221726417542
Epoch 8, Iteration 188, Loss: 0.7097447514533997
Epoch 8, Iteration 189, Loss: 0.7065077424049377
Epoch 8, Iteration 190, Loss: 0.8884391188621521
Epoch 8, Iteration 191, Loss: 0.7985609173774719
Epoch 8, Iteration 192, Loss: 0.8718447089195251
Epoch 8, Iteration 193, Loss: 0.803961992263794
Epoch 8, Iteration 194, Loss: 0.8012021780014038
Epoch 8, Iteration 195, Loss: 0.6494418978691101
Epoch 8, Iteration 196, Loss: 0.8047611117362976
Epoch 8, Iteration 197, Loss: 0.7607775330543518
Epoch 8, Iteration 198, Loss: 0.769475519657135
Epoch 8, Iteration 199, Loss: 0.9120612144470215
Epoch 8, Iteration 200, Loss: 0.6760099530220032
Epoch 8, Iteration 200, Valid Loss: 0.6328034400939941
Epoch 8, Iteration 201, Loss: 0.7792357206344604
Epoch 8, Iteration 202, Loss: 0.8484064936637878
Epoch 8, Iteration 203, Loss: 0.7280508875846863
Epoch 8, Iteration 204, Loss: 0.6681937575340271
Epoch 8, Iteration 205, Loss: 0.579771101474762
Epoch 8, Iteration 206, Loss: 0.785650908946991
Epoch 8, Iteration 207, Loss: 0.9252059459686279
Epoch 8, Iteration 208, Loss: 1.1565996408462524
Epoch 8, Iteration 209, Loss: 1.0047434568405151
Epoch 8, Iteration 210, Loss: 1.0386778116226196
Epoch 8, Iteration 211, Loss: 1.0903306007385254
Epoch 8, Iteration 212, Loss: 0.9303727746009827
Epoch 8, Iteration 213, Loss: 1.0466989278793335
Epoch 8, Iteration 214, Loss: 0.8871487379074097
Epoch 8, Iteration 215, Loss: 0.7567092180252075
Epoch 8, Iteration 216, Loss: 0.6968377828598022
Epoch 8, Iteration 217, Loss: 0.6954073309898376
Epoch 8, Iteration 218, Loss: 0.7415695786476135
Epoch 8, Iteration 219, Loss: 0.8556378483772278
Epoch 8, Iteration 220, Loss: 0.7421422600746155
Epoch 8, Iteration 221, Loss: 0.7626097798347473
Epoch 8, Iteration 222, Loss: 0.7737033367156982
Epoch 8, Iteration 223, Loss: 0.6479175686836243
Epoch 8, Iteration 224, Loss: 0.7104569673538208
Epoch 8, Iteration 225, Loss: 0.9054924249649048
Epoch 8, Iteration 226, Loss: 0.8818700313568115
Epoch 8, Iteration 227, Loss: 0.8464369773864746
Epoch 8, Iteration 228, Loss: 1.3620857000350952
Epoch 8, Iteration 229, Loss: 0.7524613738059998
Epoch 8, Iteration 230, Loss: 0.7878682017326355
Epoch 8, Iteration 231, Loss: 0.8529690504074097
Epoch 8, Iteration 232, Loss: 0.8215423226356506
Epoch 8, Iteration 233, Loss: 1.0554969310760498
Epoch 8, Iteration 234, Loss: 0.9237075448036194
Epoch 8, Iteration 235, Loss: 0.6037575602531433
Epoch 8, Iteration 236, Loss: 0.7795801162719727
Epoch 8, Iteration 237, Loss: 0.7507767081260681
Epoch 8, Iteration 238, Loss: 0.6513497829437256
Epoch 8, Iteration 239, Loss: 0.9009432792663574
Epoch 8, Iteration 240, Loss: 0.7980290651321411
Epoch 8, Iteration 241, Loss: 0.6793574690818787
Epoch 8, Iteration 242, Loss: 0.7437796592712402
Epoch 8, Iteration 243, Loss: 0.7811084389686584
Epoch 8, Iteration 244, Loss: 0.7688733339309692
Epoch 8, Iteration 245, Loss: 0.653424084186554
Epoch 8, Iteration 246, Loss: 0.7568688988685608
Epoch 8, Iteration 247, Loss: 0.6615779399871826
Epoch 8, Iteration 248, Loss: 0.7636298537254333
Epoch 8, Iteration 249, Loss: 0.8164746761322021
Epoch 8, Iteration 250, Loss: 0.9223189353942871
Epoch 8, Iteration 250, Valid Loss: 0.5939038395881653
Epoch 8, Iteration 251, Loss: 0.6304099559783936
Epoch 8, Iteration 252, Loss: 0.9045558571815491
Epoch 8, Iteration 253, Loss: 0.8142085075378418
Epoch 8, Iteration 254, Loss: 0.8590786457061768
Epoch 8, Iteration 255, Loss: 0.7709758877754211
Epoch 8, Iteration 256, Loss: 0.8363463878631592
Epoch 8, Iteration 257, Loss: 0.714305579662323
Epoch 8, Iteration 258, Loss: 0.9932155013084412
Epoch 8, Iteration 259, Loss: 0.7078449726104736
Epoch 8, Iteration 260, Loss: 0.9480971693992615
Epoch 8, Iteration 261, Loss: 0.7829503417015076
Epoch 8, Iteration 262, Loss: 0.7747987508773804
Epoch 8, Iteration 263, Loss: 0.8574857711791992
Epoch 8, Iteration 264, Loss: 0.6991180777549744
Epoch 8, Iteration 265, Loss: 0.8100812435150146
Epoch 8, Iteration 266, Loss: 0.8409783840179443
Epoch 8, Iteration 267, Loss: 0.7668648958206177
Epoch 8, Iteration 268, Loss: 0.6646692156791687
Epoch 8, Iteration 269, Loss: 0.683812141418457
Epoch 8, Iteration 270, Loss: 0.7261198163032532
Epoch 8, Iteration 271, Loss: 0.8448901176452637
Epoch 8, Iteration 272, Loss: 0.7102393507957458
Epoch 8, Iteration 273, Loss: 0.7826150059700012
Epoch 8, Iteration 274, Loss: 0.6290282607078552
Epoch 8, Iteration 275, Loss: 0.7079584002494812
Epoch 8, Iteration 276, Loss: 0.72487473487854
Epoch 8, Iteration 277, Loss: 0.6930526494979858
Epoch 8, Iteration 278, Loss: 0.9064182639122009
Epoch 8, Iteration 279, Loss: 0.6494485139846802
Epoch 8, Iteration 280, Loss: 0.6803093552589417
Epoch 8, Iteration 281, Loss: 0.6965422034263611
Epoch 8, Iteration 282, Loss: 0.7065133452415466
Epoch 8, Iteration 283, Loss: 0.7496386170387268
Epoch 8, Iteration 284, Loss: 0.7932451367378235
Epoch 8, Iteration 285, Loss: 0.7486773133277893
Epoch 8, Iteration 286, Loss: 0.7502000331878662
Epoch 8, Iteration 287, Loss: 0.842180073261261
Epoch 8, Iteration 288, Loss: 0.6883050203323364
Epoch 8, Iteration 289, Loss: 0.7455174326896667
Epoch 8, Iteration 290, Loss: 0.6704219579696655
Epoch 8, Iteration 291, Loss: 0.5835399031639099
Epoch 8, Iteration 292, Loss: 0.8237259387969971
Epoch 8, Iteration 293, Loss: 0.8928553462028503
Epoch 8, Iteration 294, Loss: 0.7327695488929749
Epoch 8, Iteration 295, Loss: 0.6476801037788391
Epoch 8, Iteration 296, Loss: 0.8370634317398071
Epoch 8, Iteration 297, Loss: 0.850090503692627
Epoch 8, Iteration 298, Loss: 0.7561913132667542
Epoch 8, Iteration 299, Loss: 0.730344831943512
Epoch 8, Iteration 300, Loss: 0.6372542977333069
Epoch 8, Iteration 300, Valid Loss: 0.6033097505569458
Epoch 8, Iteration 301, Loss: 0.6617618799209595
Epoch 8, Iteration 302, Loss: 0.6903174519538879
Epoch 8, Iteration 303, Loss: 0.8731063604354858
Epoch 8, Iteration 304, Loss: 0.6515899896621704
Epoch 8, Iteration 305, Loss: 0.6236833930015564
Epoch 8, Iteration 306, Loss: 0.6717817783355713
Epoch 8, Iteration 307, Loss: 0.7009583711624146
Epoch 8, Iteration 308, Loss: 0.735345184803009
Epoch 8, Iteration 309, Loss: 0.7400347590446472
Epoch 8, Iteration 310, Loss: 0.766219973564148
Epoch 8, Iteration 311, Loss: 0.6998669505119324
Epoch 8, Iteration 312, Loss: 0.5787780284881592
Epoch 8, Iteration 313, Loss: 0.9783509373664856
Epoch 8, Iteration 314, Loss: 0.7547295689582825
Epoch 8, Iteration 315, Loss: 0.6450908184051514
Epoch 8, Iteration 316, Loss: 1.0493972301483154
Epoch 8, Iteration 317, Loss: 0.8147729635238647
Epoch 8, Iteration 318, Loss: 0.6884039640426636
Epoch 8, Iteration 319, Loss: 0.8708907961845398
Epoch 8, Iteration 320, Loss: 0.5926024913787842
Epoch 8, Iteration 321, Loss: 0.8083166480064392
Epoch 8, Iteration 322, Loss: 0.6785273551940918
Epoch 8, Iteration 323, Loss: 0.7413535118103027
Epoch 8, Iteration 324, Loss: 0.8233560919761658
Epoch 8, Iteration 325, Loss: 0.815676748752594
Epoch 8, Iteration 326, Loss: 0.6826938390731812
Epoch 8, Iteration 327, Loss: 0.6460734605789185
Epoch 8, Iteration 328, Loss: 0.6977885961532593
Epoch 8, Iteration 329, Loss: 0.8502275347709656
Epoch 8, Iteration 330, Loss: 0.6362216472625732
Epoch 8, Iteration 331, Loss: 0.7267106771469116
Epoch 8, Iteration 332, Loss: 0.7606513500213623
Epoch 8, Iteration 333, Loss: 0.8923608660697937
Epoch 8, Iteration 334, Loss: 0.6787383556365967
Epoch 8, Iteration 335, Loss: 0.9468817114830017
Epoch 8, Iteration 336, Loss: 0.7534368634223938
Epoch 8, Iteration 337, Loss: 0.5926101207733154
Epoch 8, Iteration 338, Loss: 0.8432535529136658
Epoch 8, Iteration 339, Loss: 0.7032656669616699
Epoch 8, Iteration 340, Loss: 0.5530920624732971
Epoch 8, Iteration 341, Loss: 0.7305872440338135
Epoch 8, Iteration 342, Loss: 0.6303037405014038
Epoch 8, Iteration 343, Loss: 0.6865366101264954
Epoch 8, Iteration 344, Loss: 0.7347243428230286
Epoch 8, Iteration 345, Loss: 0.7859869599342346
Epoch 8, Iteration 346, Loss: 0.6723437309265137
Epoch 8, Iteration 347, Loss: 0.6415800452232361
Epoch 8, Iteration 348, Loss: 0.5984153747558594
Epoch 8, Iteration 349, Loss: 0.7107439637184143
Epoch 8, Iteration 350, Loss: 0.6521517634391785
Epoch 8, Iteration 350, Valid Loss: 0.6143916845321655
Epoch 8, Iteration 351, Loss: 0.7411661744117737
Epoch 8, Iteration 352, Loss: 0.6050631999969482
Epoch 8, Iteration 353, Loss: 0.7594234943389893
Epoch 8, Iteration 354, Loss: 0.6397687196731567
Epoch 8, Iteration 355, Loss: 0.7249277234077454
Epoch 8, Iteration 356, Loss: 0.7719408273696899
Epoch 8, Iteration 357, Loss: 0.5798927545547485
Epoch 8, Iteration 358, Loss: 0.6239722371101379
Epoch 8, Iteration 359, Loss: 0.6329523324966431
Epoch 8, Iteration 360, Loss: 0.6414828896522522
Epoch 8, Iteration 361, Loss: 0.630365788936615
Epoch 8, Iteration 362, Loss: 0.7138701677322388
Epoch 8, Iteration 363, Loss: 0.6001951694488525
Epoch 8, Iteration 364, Loss: 0.6211658716201782
Epoch 8, Iteration 365, Loss: 0.7817566394805908
Epoch 8, Iteration 366, Loss: 0.7351313829421997
Epoch 8, Iteration 367, Loss: 0.8156201839447021
Epoch 8, Iteration 368, Loss: 0.7189382910728455
Epoch 8, Iteration 369, Loss: 0.7429583072662354
Epoch 8, Iteration 370, Loss: 0.6975385546684265
Epoch 8, Iteration 371, Loss: 0.6611320972442627
Epoch 8, Iteration 372, Loss: 0.6636021733283997
Epoch 8, Iteration 373, Loss: 0.6655458807945251
Epoch 8, Iteration 374, Loss: 0.8084112405776978
Epoch 8, Iteration 375, Loss: 0.7208682298660278
Epoch 8, Iteration 376, Loss: 0.7731591463088989
Epoch 8, Iteration 377, Loss: 0.7027878761291504
Epoch 8, Iteration 378, Loss: 0.6478819251060486
Epoch 8, Iteration 379, Loss: 0.6351879239082336
Epoch 8, Iteration 380, Loss: 0.6914784908294678
Epoch 8, Iteration 381, Loss: 0.6582031846046448
Epoch 8, Iteration 382, Loss: 0.5600651502609253
Epoch 8, Iteration 383, Loss: 0.7266520857810974
Epoch 8, Iteration 384, Loss: 0.6990786790847778
Epoch 8, Iteration 385, Loss: 0.6761002540588379
Epoch 8, Iteration 386, Loss: 0.5896896719932556
Epoch 8, Iteration 387, Loss: 0.9530796408653259
Epoch 8, Iteration 388, Loss: 0.6738588809967041
Epoch 8, Iteration 389, Loss: 0.7628483772277832
Epoch 8, Iteration 390, Loss: 0.9321476817131042
Epoch 8, Iteration 391, Loss: 0.6669455766677856
Epoch 9/15, Loss: 0.779655286091931
Epoch 9, Iteration 0, Loss: 0.7308241724967957
Epoch 9, Iteration 1, Loss: 0.9465048313140869
Epoch 9, Iteration 2, Loss: 0.8350211381912231
Epoch 9, Iteration 3, Loss: 0.8826941251754761
Epoch 9, Iteration 4, Loss: 0.6940438747406006
Epoch 9, Iteration 5, Loss: 0.7862438559532166
Epoch 9, Iteration 6, Loss: 0.6901811957359314
Epoch 9, Iteration 7, Loss: 0.7413474917411804
Epoch 9, Iteration 8, Loss: 0.6922501921653748
Epoch 9, Iteration 9, Loss: 0.8858712911605835
Epoch 9, Iteration 10, Loss: 0.7279002070426941
Epoch 9, Iteration 11, Loss: 0.7368876338005066
Epoch 9, Iteration 12, Loss: 0.6380183696746826
Epoch 9, Iteration 13, Loss: 0.8103405237197876
Epoch 9, Iteration 14, Loss: 0.7480902671813965
Epoch 9, Iteration 15, Loss: 0.8882638216018677
Epoch 9, Iteration 16, Loss: 0.7155240774154663
Epoch 9, Iteration 17, Loss: 0.7721894383430481
Epoch 9, Iteration 18, Loss: 0.7131714224815369
Epoch 9, Iteration 19, Loss: 0.8018889427185059
Epoch 9, Iteration 20, Loss: 0.630958080291748
Epoch 9, Iteration 21, Loss: 0.7048038244247437
Epoch 9, Iteration 22, Loss: 0.6879834532737732
Epoch 9, Iteration 23, Loss: 0.7734124660491943
Epoch 9, Iteration 24, Loss: 0.6751303672790527
Epoch 9, Iteration 25, Loss: 0.6605287790298462
Epoch 9, Iteration 26, Loss: 1.0718750953674316
Epoch 9, Iteration 27, Loss: 0.8207003474235535
Epoch 9, Iteration 28, Loss: 0.8042131066322327
Epoch 9, Iteration 29, Loss: 0.928263247013092
Epoch 9, Iteration 30, Loss: 0.7363741397857666
Epoch 9, Iteration 31, Loss: 0.7310901284217834
Epoch 9, Iteration 32, Loss: 0.7527525424957275
Epoch 9, Iteration 33, Loss: 0.7155691385269165
Epoch 9, Iteration 34, Loss: 0.6964547634124756
Epoch 9, Iteration 35, Loss: 0.7616868019104004
Epoch 9, Iteration 36, Loss: 0.6030333638191223
Epoch 9, Iteration 37, Loss: 0.6083875894546509
Epoch 9, Iteration 38, Loss: 0.6841759085655212
Epoch 9, Iteration 39, Loss: 0.7393600344657898
Epoch 9, Iteration 40, Loss: 0.7011383771896362
Epoch 9, Iteration 41, Loss: 0.8121935725212097
Epoch 9, Iteration 42, Loss: 0.7478061318397522
Epoch 9, Iteration 43, Loss: 0.6560901999473572
Epoch 9, Iteration 44, Loss: 0.6721383333206177
Epoch 9, Iteration 45, Loss: 0.7498594522476196
Epoch 9, Iteration 46, Loss: 0.7984776496887207
Epoch 9, Iteration 47, Loss: 0.7617945075035095
Epoch 9, Iteration 48, Loss: 0.9457508325576782
Epoch 9, Iteration 49, Loss: 0.7176660895347595
Epoch 9, Iteration 50, Loss: 0.739052414894104
Epoch 9, Iteration 50, Valid Loss: 0.5829022526741028
Epoch 9, Iteration 51, Loss: 0.7087310552597046
Epoch 9, Iteration 52, Loss: 0.6508805155754089
Epoch 9, Iteration 53, Loss: 0.7918763160705566
Epoch 9, Iteration 54, Loss: 0.9624983072280884
Epoch 9, Iteration 55, Loss: 0.7900527715682983
Epoch 9, Iteration 56, Loss: 0.7544360160827637
Epoch 9, Iteration 57, Loss: 0.9780333042144775
Epoch 9, Iteration 58, Loss: 0.7307156920433044
Epoch 9, Iteration 59, Loss: 0.6337934732437134
Epoch 9, Iteration 60, Loss: 0.7773463129997253
Epoch 9, Iteration 61, Loss: 0.6249812245368958
Epoch 9, Iteration 62, Loss: 0.7629194855690002
Epoch 9, Iteration 63, Loss: 0.6283774971961975
Epoch 9, Iteration 64, Loss: 0.712641179561615
Epoch 9, Iteration 65, Loss: 0.8886286020278931
Epoch 9, Iteration 66, Loss: 0.7949448227882385
Epoch 9, Iteration 67, Loss: 0.5640498995780945
Epoch 9, Iteration 68, Loss: 0.9276461005210876
Epoch 9, Iteration 69, Loss: 0.9058628082275391
Epoch 9, Iteration 70, Loss: 0.7991460561752319
Epoch 9, Iteration 71, Loss: 0.7800227403640747
Epoch 9, Iteration 72, Loss: 0.7259728908538818
Epoch 9, Iteration 73, Loss: 0.667104959487915
Epoch 9, Iteration 74, Loss: 0.9484119415283203
Epoch 9, Iteration 75, Loss: 0.9026415348052979
Epoch 9, Iteration 76, Loss: 0.7131703495979309
Epoch 9, Iteration 77, Loss: 0.6673716902732849
Epoch 9, Iteration 78, Loss: 0.8565850257873535
Epoch 9, Iteration 79, Loss: 0.9942042231559753
Epoch 9, Iteration 80, Loss: 0.6868330240249634
Epoch 9, Iteration 81, Loss: 0.9082403182983398
Epoch 9, Iteration 82, Loss: 0.9861974716186523
Epoch 9, Iteration 83, Loss: 0.6732343435287476
Epoch 9, Iteration 84, Loss: 0.9294126629829407
Epoch 9, Iteration 85, Loss: 0.7786957621574402
Epoch 9, Iteration 86, Loss: 0.8111003041267395
Epoch 9, Iteration 87, Loss: 0.901748776435852
Epoch 9, Iteration 88, Loss: 0.9353042840957642
Epoch 9, Iteration 89, Loss: 0.7882871031761169
Epoch 9, Iteration 90, Loss: 0.6331316232681274
Epoch 9, Iteration 91, Loss: 0.7667720913887024
Epoch 9, Iteration 92, Loss: 0.5190502405166626
Epoch 9, Iteration 93, Loss: 0.8116421699523926
Epoch 9, Iteration 94, Loss: 0.8437437415122986
Epoch 9, Iteration 95, Loss: 0.5981365442276001
Epoch 9, Iteration 96, Loss: 0.7559809684753418
Epoch 9, Iteration 97, Loss: 0.6261113286018372
Epoch 9, Iteration 98, Loss: 0.7746842503547668
Epoch 9, Iteration 99, Loss: 0.7415294051170349
Epoch 9, Iteration 100, Loss: 0.7066133618354797
Epoch 9, Iteration 100, Valid Loss: 0.6670176982879639
Epoch 9, Iteration 101, Loss: 0.9464544057846069
Epoch 9, Iteration 102, Loss: 0.9147846698760986
Epoch 9, Iteration 103, Loss: 0.7796867489814758
Epoch 9, Iteration 104, Loss: 0.8210607171058655
Epoch 9, Iteration 105, Loss: 0.8024045825004578
Epoch 9, Iteration 106, Loss: 0.5901901125907898
Epoch 9, Iteration 107, Loss: 1.0022063255310059
Epoch 9, Iteration 108, Loss: 0.7651452422142029
Epoch 9, Iteration 109, Loss: 0.9115220904350281
Epoch 9, Iteration 110, Loss: 1.0262733697891235
Epoch 9, Iteration 111, Loss: 0.8397724032402039
Epoch 9, Iteration 112, Loss: 0.7979373335838318
Epoch 9, Iteration 113, Loss: 0.7071547508239746
Epoch 9, Iteration 114, Loss: 0.888207733631134
Epoch 9, Iteration 115, Loss: 0.9418433904647827
Epoch 9, Iteration 116, Loss: 0.699211597442627
Epoch 9, Iteration 117, Loss: 0.5538270473480225
Epoch 9, Iteration 118, Loss: 0.8070988655090332
Epoch 9, Iteration 119, Loss: 0.6610010862350464
Epoch 9, Iteration 120, Loss: 0.8447210788726807
Epoch 9, Iteration 121, Loss: 0.7047539353370667
Epoch 9, Iteration 122, Loss: 0.7228104472160339
Epoch 9, Iteration 123, Loss: 0.7019258141517639
Epoch 9, Iteration 124, Loss: 0.6570252180099487
Epoch 9, Iteration 125, Loss: 0.7151142954826355
Epoch 9, Iteration 126, Loss: 0.5851654410362244
Epoch 9, Iteration 127, Loss: 0.8285530805587769
Epoch 9, Iteration 128, Loss: 0.7521159648895264
Epoch 9, Iteration 129, Loss: 0.8374610543251038
Epoch 9, Iteration 130, Loss: 1.025610089302063
Epoch 9, Iteration 131, Loss: 0.6196690201759338
Epoch 9, Iteration 132, Loss: 0.7580296993255615
Epoch 9, Iteration 133, Loss: 1.1228702068328857
Epoch 9, Iteration 134, Loss: 0.8963144421577454
Epoch 9, Iteration 135, Loss: 0.9527294635772705
Epoch 9, Iteration 136, Loss: 0.8718239068984985
Epoch 9, Iteration 137, Loss: 0.7491002082824707
Epoch 9, Iteration 138, Loss: 0.6815611720085144
Epoch 9, Iteration 139, Loss: 0.7123928070068359
Epoch 9, Iteration 140, Loss: 0.7916346788406372
Epoch 9, Iteration 141, Loss: 0.9130162596702576
Epoch 9, Iteration 142, Loss: 0.8228277564048767
Epoch 9, Iteration 143, Loss: 0.658242404460907
Epoch 9, Iteration 144, Loss: 0.7395537495613098
Epoch 9, Iteration 145, Loss: 0.8068856000900269
Epoch 9, Iteration 146, Loss: 1.0959910154342651
Epoch 9, Iteration 147, Loss: 0.8362852334976196
Epoch 9, Iteration 148, Loss: 0.63627028465271
Epoch 9, Iteration 149, Loss: 0.7026801705360413
Epoch 9, Iteration 150, Loss: 0.8587268590927124
Epoch 9, Iteration 150, Valid Loss: 0.6298393607139587
Epoch 9, Iteration 151, Loss: 0.7882240414619446
Epoch 9, Iteration 152, Loss: 0.6431488990783691
Epoch 9, Iteration 153, Loss: 0.7746491432189941
Epoch 9, Iteration 154, Loss: 0.5729898810386658
Epoch 9, Iteration 155, Loss: 0.9210677742958069
Epoch 9, Iteration 156, Loss: 0.6389308571815491
Epoch 9, Iteration 157, Loss: 1.0677552223205566
Epoch 9, Iteration 158, Loss: 0.7799802422523499
Epoch 9, Iteration 159, Loss: 0.8569083213806152
Epoch 9, Iteration 160, Loss: 0.7181677222251892
Epoch 9, Iteration 161, Loss: 0.7701174020767212
Epoch 9, Iteration 162, Loss: 1.0319719314575195
Epoch 9, Iteration 163, Loss: 0.588152289390564
Epoch 9, Iteration 164, Loss: 0.7557175755500793
Epoch 9, Iteration 165, Loss: 0.6992985010147095
Epoch 9, Iteration 166, Loss: 0.7738352417945862
Epoch 9, Iteration 167, Loss: 0.8576986789703369
Epoch 9, Iteration 168, Loss: 0.7841176986694336
Epoch 9, Iteration 169, Loss: 0.7145552635192871
Epoch 9, Iteration 170, Loss: 0.8393662571907043
Epoch 9, Iteration 171, Loss: 0.8896420001983643
Epoch 9, Iteration 172, Loss: 0.7893933057785034
Epoch 9, Iteration 173, Loss: 0.6683463454246521
Epoch 9, Iteration 174, Loss: 0.8080376982688904
Epoch 9, Iteration 175, Loss: 0.9036136865615845
Epoch 9, Iteration 176, Loss: 0.5323201417922974
Epoch 9, Iteration 177, Loss: 1.2218198776245117
Epoch 9, Iteration 178, Loss: 0.8702268004417419
Epoch 9, Iteration 179, Loss: 0.7944566011428833
Epoch 9, Iteration 180, Loss: 0.7439631223678589
Epoch 9, Iteration 181, Loss: 0.9097834229469299
Epoch 9, Iteration 182, Loss: 0.8733106851577759
Epoch 9, Iteration 183, Loss: 0.6372396349906921
Epoch 9, Iteration 184, Loss: 0.7922108173370361
Epoch 9, Iteration 185, Loss: 0.6060683727264404
Epoch 9, Iteration 186, Loss: 0.734189510345459
Epoch 9, Iteration 187, Loss: 0.7575653791427612
Epoch 9, Iteration 188, Loss: 0.6478419899940491
Epoch 9, Iteration 189, Loss: 0.7230785489082336
Epoch 9, Iteration 190, Loss: 0.8440952301025391
Epoch 9, Iteration 191, Loss: 0.7497754693031311
Epoch 9, Iteration 192, Loss: 0.8223125338554382
Epoch 9, Iteration 193, Loss: 0.7719319462776184
Epoch 9, Iteration 194, Loss: 0.7919462323188782
Epoch 9, Iteration 195, Loss: 0.689611554145813
Epoch 9, Iteration 196, Loss: 0.7912310361862183
Epoch 9, Iteration 197, Loss: 0.7527295351028442
Epoch 9, Iteration 198, Loss: 0.7306608557701111
Epoch 9, Iteration 199, Loss: 0.7667921185493469
Epoch 9, Iteration 200, Loss: 0.6846836805343628
Epoch 9, Iteration 200, Valid Loss: 0.5953073501586914
Epoch 9, Iteration 201, Loss: 0.7450470328330994
Epoch 9, Iteration 202, Loss: 0.7820550799369812
Epoch 9, Iteration 203, Loss: 0.764255940914154
Epoch 9, Iteration 204, Loss: 0.7193447947502136
Epoch 9, Iteration 205, Loss: 0.5334543585777283
Epoch 9, Iteration 206, Loss: 0.6467786431312561
Epoch 9, Iteration 207, Loss: 0.8735195398330688
Epoch 9, Iteration 208, Loss: 1.1428240537643433
Epoch 9, Iteration 209, Loss: 1.049645185470581
Epoch 9, Iteration 210, Loss: 0.999504566192627
Epoch 9, Iteration 211, Loss: 1.0677597522735596
Epoch 9, Iteration 212, Loss: 0.8813190460205078
Epoch 9, Iteration 213, Loss: 0.9970723390579224
Epoch 9, Iteration 214, Loss: 0.7564807534217834
Epoch 9, Iteration 215, Loss: 0.6702298521995544
Epoch 9, Iteration 216, Loss: 0.7057702541351318
Epoch 9, Iteration 217, Loss: 0.6815439462661743
Epoch 9, Iteration 218, Loss: 0.7581273913383484
Epoch 9, Iteration 219, Loss: 0.7732579112052917
Epoch 9, Iteration 220, Loss: 0.6858800053596497
Epoch 9, Iteration 221, Loss: 0.7110912799835205
Epoch 9, Iteration 222, Loss: 0.8320658206939697
Epoch 9, Iteration 223, Loss: 0.6944090127944946
Epoch 9, Iteration 224, Loss: 0.6148818731307983
Epoch 9, Iteration 225, Loss: 0.8730373382568359
Epoch 9, Iteration 226, Loss: 0.8541958928108215
Epoch 9, Iteration 227, Loss: 0.7688654065132141
Epoch 9, Iteration 228, Loss: 0.7504370212554932
Epoch 9, Iteration 229, Loss: 0.7154635787010193
Epoch 9, Iteration 230, Loss: 0.8089712262153625
Epoch 9, Iteration 231, Loss: 0.6641757488250732
Epoch 9, Iteration 232, Loss: 0.8076749444007874
Epoch 9, Iteration 233, Loss: 0.9533230066299438
Epoch 9, Iteration 234, Loss: 0.9400076270103455
Epoch 9, Iteration 235, Loss: 0.6595286726951599
Epoch 9, Iteration 236, Loss: 0.7435531616210938
Epoch 9, Iteration 237, Loss: 0.6790565252304077
Epoch 9, Iteration 238, Loss: 0.5297216773033142
Epoch 9, Iteration 239, Loss: 0.9798758625984192
Epoch 9, Iteration 240, Loss: 0.6965345144271851
Epoch 9, Iteration 241, Loss: 0.6893171668052673
Epoch 9, Iteration 242, Loss: 0.7391933798789978
Epoch 9, Iteration 243, Loss: 0.7433725595474243
Epoch 9, Iteration 244, Loss: 0.6921083331108093
Epoch 9, Iteration 245, Loss: 0.5473451614379883
Epoch 9, Iteration 246, Loss: 0.8021020889282227
Epoch 9, Iteration 247, Loss: 0.708761990070343
Epoch 9, Iteration 248, Loss: 0.7335415482521057
Epoch 9, Iteration 249, Loss: 0.7621409893035889
Epoch 9, Iteration 250, Loss: 0.8938029408454895
Epoch 9, Iteration 250, Valid Loss: 0.5728676915168762
Epoch 9, Iteration 251, Loss: 0.7511805295944214
Epoch 9, Iteration 252, Loss: 0.9164286851882935
Epoch 9, Iteration 253, Loss: 0.7527827024459839
Epoch 9, Iteration 254, Loss: 0.8836206197738647
Epoch 9, Iteration 255, Loss: 0.8197833299636841
Epoch 9, Iteration 256, Loss: 0.9068291783332825
Epoch 9, Iteration 257, Loss: 0.8051047325134277
Epoch 9, Iteration 258, Loss: 0.9544641971588135
Epoch 9, Iteration 259, Loss: 0.6037098169326782
Epoch 9, Iteration 260, Loss: 0.8705965280532837
Epoch 9, Iteration 261, Loss: 0.7494175434112549
Epoch 9, Iteration 262, Loss: 0.6916728019714355
Epoch 9, Iteration 263, Loss: 0.9497328400611877
Epoch 9, Iteration 264, Loss: 0.7760531902313232
Epoch 9, Iteration 265, Loss: 0.8184323906898499
Epoch 9, Iteration 266, Loss: 0.8153864741325378
Epoch 9, Iteration 267, Loss: 0.7028369903564453
Epoch 9, Iteration 268, Loss: 0.6515352129936218
Epoch 9, Iteration 269, Loss: 0.6851238012313843
Epoch 9, Iteration 270, Loss: 0.6824880242347717
Epoch 9, Iteration 271, Loss: 0.7732277512550354
Epoch 9, Iteration 272, Loss: 0.789249837398529
Epoch 9, Iteration 273, Loss: 0.7601547837257385
Epoch 9, Iteration 274, Loss: 0.7538531422615051
Epoch 9, Iteration 275, Loss: 0.6882658004760742
Epoch 9, Iteration 276, Loss: 0.6120421886444092
Epoch 9, Iteration 277, Loss: 0.6577579975128174
Epoch 9, Iteration 278, Loss: 0.9504434466362
Epoch 9, Iteration 279, Loss: 0.6039125323295593
Epoch 9, Iteration 280, Loss: 0.5849049091339111
Epoch 9, Iteration 281, Loss: 0.7317599654197693
Epoch 9, Iteration 282, Loss: 0.6749445199966431
Epoch 9, Iteration 283, Loss: 0.7124390006065369
Epoch 9, Iteration 284, Loss: 0.7541038990020752
Epoch 9, Iteration 285, Loss: 0.636192798614502
Epoch 9, Iteration 286, Loss: 0.6868395209312439
Epoch 9, Iteration 287, Loss: 0.6887539625167847
Epoch 9, Iteration 288, Loss: 0.6970751881599426
Epoch 9, Iteration 289, Loss: 0.6849132776260376
Epoch 9, Iteration 290, Loss: 0.5872591733932495
Epoch 9, Iteration 291, Loss: 0.5689773559570312
Epoch 9, Iteration 292, Loss: 0.8312098383903503
Epoch 9, Iteration 293, Loss: 0.9277780055999756
Epoch 9, Iteration 294, Loss: 0.6969774961471558
Epoch 9, Iteration 295, Loss: 0.6408982276916504
Epoch 9, Iteration 296, Loss: 0.9372750520706177
Epoch 9, Iteration 297, Loss: 0.8353238105773926
Epoch 9, Iteration 298, Loss: 0.7484338879585266
Epoch 9, Iteration 299, Loss: 0.7396026849746704
Epoch 9, Iteration 300, Loss: 0.5660464763641357
Epoch 9, Iteration 300, Valid Loss: 0.5725482106208801
Epoch 9, Iteration 301, Loss: 0.6376891136169434
Epoch 9, Iteration 302, Loss: 0.6009703874588013
Epoch 9, Iteration 303, Loss: 0.83037269115448
Epoch 9, Iteration 304, Loss: 0.6454769968986511
Epoch 9, Iteration 305, Loss: 0.6138301491737366
Epoch 9, Iteration 306, Loss: 0.5859816074371338
Epoch 9, Iteration 307, Loss: 0.654991865158081
Epoch 9, Iteration 308, Loss: 0.6098645329475403
Epoch 9, Iteration 309, Loss: 0.7446835041046143
Epoch 9, Iteration 310, Loss: 0.8695237040519714
Epoch 9, Iteration 311, Loss: 0.7682286500930786
Epoch 9, Iteration 312, Loss: 0.6362056732177734
Epoch 9, Iteration 313, Loss: 0.9041121602058411
Epoch 9, Iteration 314, Loss: 0.6587628722190857
Epoch 9, Iteration 315, Loss: 0.6505283713340759
Epoch 9, Iteration 316, Loss: 0.974670946598053
Epoch 9, Iteration 317, Loss: 0.6791342496871948
Epoch 9, Iteration 318, Loss: 0.6791183948516846
Epoch 9, Iteration 319, Loss: 0.8520302176475525
Epoch 9, Iteration 320, Loss: 0.5372095108032227
Epoch 9, Iteration 321, Loss: 0.7100684642791748
Epoch 9, Iteration 322, Loss: 0.6891412138938904
Epoch 9, Iteration 323, Loss: 0.7517508268356323
Epoch 9, Iteration 324, Loss: 0.6871730089187622
Epoch 9, Iteration 325, Loss: 0.7374390959739685
Epoch 9, Iteration 326, Loss: 0.7335934042930603
Epoch 9, Iteration 327, Loss: 0.5759890079498291
Epoch 9, Iteration 328, Loss: 0.5664149522781372
Epoch 9, Iteration 329, Loss: 0.8054542541503906
Epoch 9, Iteration 330, Loss: 0.5474905967712402
Epoch 9, Iteration 331, Loss: 0.6857388019561768
Epoch 9, Iteration 332, Loss: 0.7410913109779358
Epoch 9, Iteration 333, Loss: 0.8653730154037476
Epoch 9, Iteration 334, Loss: 0.691712498664856
Epoch 9, Iteration 335, Loss: 0.8222153186798096
Epoch 9, Iteration 336, Loss: 0.6993371844291687
Epoch 9, Iteration 337, Loss: 0.5066707730293274
Epoch 9, Iteration 338, Loss: 0.7924480438232422
Epoch 9, Iteration 339, Loss: 0.6859145760536194
Epoch 9, Iteration 340, Loss: 0.6151975393295288
Epoch 9, Iteration 341, Loss: 0.6986725926399231
Epoch 9, Iteration 342, Loss: 0.6431083083152771
Epoch 9, Iteration 343, Loss: 0.6375458240509033
Epoch 9, Iteration 344, Loss: 0.782133162021637
Epoch 9, Iteration 345, Loss: 0.7242532968521118
Epoch 9, Iteration 346, Loss: 0.6952593922615051
Epoch 9, Iteration 347, Loss: 0.6812937259674072
Epoch 9, Iteration 348, Loss: 0.6292823553085327
Epoch 9, Iteration 349, Loss: 0.812605082988739
Epoch 9, Iteration 350, Loss: 0.6835975646972656
Epoch 9, Iteration 350, Valid Loss: 0.6027361750602722
Epoch 9, Iteration 351, Loss: 0.727184534072876
Epoch 9, Iteration 352, Loss: 0.6032825708389282
Epoch 9, Iteration 353, Loss: 0.7752993702888489
Epoch 9, Iteration 354, Loss: 0.6014406681060791
Epoch 9, Iteration 355, Loss: 0.5959606766700745
Epoch 9, Iteration 356, Loss: 0.704997181892395
Epoch 9, Iteration 357, Loss: 0.6224934458732605
Epoch 9, Iteration 358, Loss: 0.6084299087524414
Epoch 9, Iteration 359, Loss: 0.6161983013153076
Epoch 9, Iteration 360, Loss: 0.6085823774337769
Epoch 9, Iteration 361, Loss: 0.7766433954238892
Epoch 9, Iteration 362, Loss: 0.7282424569129944
Epoch 9, Iteration 363, Loss: 0.6602478623390198
Epoch 9, Iteration 364, Loss: 0.7501680850982666
Epoch 9, Iteration 365, Loss: 0.818701982498169
Epoch 9, Iteration 366, Loss: 0.7815721035003662
Epoch 9, Iteration 367, Loss: 0.6480079889297485
Epoch 9, Iteration 368, Loss: 0.6935354471206665
Epoch 9, Iteration 369, Loss: 0.7383248209953308
Epoch 9, Iteration 370, Loss: 0.6945074796676636
Epoch 9, Iteration 371, Loss: 0.7064132690429688
Epoch 9, Iteration 372, Loss: 0.6064289808273315
Epoch 9, Iteration 373, Loss: 0.7421363592147827
Epoch 9, Iteration 374, Loss: 0.8901442885398865
Epoch 9, Iteration 375, Loss: 0.614199161529541
Epoch 9, Iteration 376, Loss: 0.7550197243690491
Epoch 9, Iteration 377, Loss: 0.6971667408943176
Epoch 9, Iteration 378, Loss: 0.7137177586555481
Epoch 9, Iteration 379, Loss: 0.6186983585357666
Epoch 9, Iteration 380, Loss: 0.617209255695343
Epoch 9, Iteration 381, Loss: 0.622280478477478
Epoch 9, Iteration 382, Loss: 0.5838761329650879
Epoch 9, Iteration 383, Loss: 0.689920961856842
Epoch 9, Iteration 384, Loss: 0.7811902761459351
Epoch 9, Iteration 385, Loss: 0.6319943070411682
Epoch 9, Iteration 386, Loss: 0.554760754108429
Epoch 9, Iteration 387, Loss: 0.8771708011627197
Epoch 9, Iteration 388, Loss: 0.7034414410591125
Epoch 9, Iteration 389, Loss: 0.6532014012336731
Epoch 9, Iteration 390, Loss: 0.8668028712272644
Epoch 9, Iteration 391, Loss: 0.6327071189880371
Epoch 10/15, Loss: 0.7544707242323427
Epoch 10, Iteration 0, Loss: 0.6941603422164917
Epoch 10, Iteration 1, Loss: 0.9121127724647522
Epoch 10, Iteration 2, Loss: 0.801249623298645
Epoch 10, Iteration 3, Loss: 0.8549344539642334
Epoch 10, Iteration 4, Loss: 0.7360625267028809
Epoch 10, Iteration 5, Loss: 0.8426286578178406
Epoch 10, Iteration 6, Loss: 0.6116130948066711
Epoch 10, Iteration 7, Loss: 0.7815054059028625
Epoch 10, Iteration 8, Loss: 0.6729539632797241
Epoch 10, Iteration 9, Loss: 0.795387327671051
Epoch 10, Iteration 10, Loss: 0.773945152759552
Epoch 10, Iteration 11, Loss: 0.7456849813461304
Epoch 10, Iteration 12, Loss: 0.6912989616394043
Epoch 10, Iteration 13, Loss: 0.8152952194213867
Epoch 10, Iteration 14, Loss: 0.746321976184845
Epoch 10, Iteration 15, Loss: 0.8382605314254761
Epoch 10, Iteration 16, Loss: 0.7041322588920593
Epoch 10, Iteration 17, Loss: 0.7595746517181396
Epoch 10, Iteration 18, Loss: 0.6280320882797241
Epoch 10, Iteration 19, Loss: 0.7818087935447693
Epoch 10, Iteration 20, Loss: 0.6089149713516235
Epoch 10, Iteration 21, Loss: 0.6628143787384033
Epoch 10, Iteration 22, Loss: 0.6843055486679077
Epoch 10, Iteration 23, Loss: 0.7910325527191162
Epoch 10, Iteration 24, Loss: 0.6703596711158752
Epoch 10, Iteration 25, Loss: 0.6111669540405273
Epoch 10, Iteration 26, Loss: 1.0091544389724731
Epoch 10, Iteration 27, Loss: 0.8511229157447815
Epoch 10, Iteration 28, Loss: 0.8185375332832336
Epoch 10, Iteration 29, Loss: 0.9332282543182373
Epoch 10, Iteration 30, Loss: 0.7859931588172913
Epoch 10, Iteration 31, Loss: 0.7047330737113953
Epoch 10, Iteration 32, Loss: 0.7782139778137207
Epoch 10, Iteration 33, Loss: 0.7272104620933533
Epoch 10, Iteration 34, Loss: 0.7002266049385071
Epoch 10, Iteration 35, Loss: 0.7363100647926331
Epoch 10, Iteration 36, Loss: 0.5131064653396606
Epoch 10, Iteration 37, Loss: 0.5824116468429565
Epoch 10, Iteration 38, Loss: 0.6742957830429077
Epoch 10, Iteration 39, Loss: 0.6837071180343628
Epoch 10, Iteration 40, Loss: 0.6474604606628418
Epoch 10, Iteration 41, Loss: 0.8162046670913696
Epoch 10, Iteration 42, Loss: 0.7184785604476929
Epoch 10, Iteration 43, Loss: 0.6038572192192078
Epoch 10, Iteration 44, Loss: 0.6682573556900024
Epoch 10, Iteration 45, Loss: 0.738457977771759
Epoch 10, Iteration 46, Loss: 0.7508317828178406
Epoch 10, Iteration 47, Loss: 0.7143579721450806
Epoch 10, Iteration 48, Loss: 0.9684680104255676
Epoch 10, Iteration 49, Loss: 0.7504740357398987
Epoch 10, Iteration 50, Loss: 0.6774473786354065
Epoch 10, Iteration 50, Valid Loss: 0.5646939277648926
Epoch 10, Iteration 51, Loss: 0.7655192017555237
Epoch 10, Iteration 52, Loss: 0.6305049061775208
Epoch 10, Iteration 53, Loss: 0.7779906988143921
Epoch 10, Iteration 54, Loss: 0.9764517545700073
Epoch 10, Iteration 55, Loss: 0.841484785079956
Epoch 10, Iteration 56, Loss: 0.7210801839828491
Epoch 10, Iteration 57, Loss: 0.8888958692550659
Epoch 10, Iteration 58, Loss: 0.6804090142250061
Epoch 10, Iteration 59, Loss: 0.919581413269043
Epoch 10, Iteration 60, Loss: 0.6897583603858948
Epoch 10, Iteration 61, Loss: 0.6125369071960449
Epoch 10, Iteration 62, Loss: 0.7075123190879822
Epoch 10, Iteration 63, Loss: 0.7648829221725464
Epoch 10, Iteration 64, Loss: 0.7166243195533752
Epoch 10, Iteration 65, Loss: 1.0398520231246948
Epoch 10, Iteration 66, Loss: 0.7432412505149841
Epoch 10, Iteration 67, Loss: 0.5931038856506348
Epoch 10, Iteration 68, Loss: 0.8035410046577454
Epoch 10, Iteration 69, Loss: 0.8078197240829468
Epoch 10, Iteration 70, Loss: 0.7530755996704102
Epoch 10, Iteration 71, Loss: 0.8132965564727783
Epoch 10, Iteration 72, Loss: 0.8127638697624207
Epoch 10, Iteration 73, Loss: 0.7058660984039307
Epoch 10, Iteration 74, Loss: 0.9644125699996948
Epoch 10, Iteration 75, Loss: 1.046295166015625
Epoch 10, Iteration 76, Loss: 0.6951137185096741
Epoch 10, Iteration 77, Loss: 0.613939642906189
Epoch 10, Iteration 78, Loss: 0.9001856446266174
Epoch 10, Iteration 79, Loss: 1.0647848844528198
Epoch 10, Iteration 80, Loss: 0.5911253690719604
Epoch 10, Iteration 81, Loss: 0.965757429599762
Epoch 10, Iteration 82, Loss: 0.7568934559822083
Epoch 10, Iteration 83, Loss: 0.5859569311141968
Epoch 10, Iteration 84, Loss: 0.9343733191490173
Epoch 10, Iteration 85, Loss: 0.7163803577423096
Epoch 10, Iteration 86, Loss: 0.8680612444877625
Epoch 10, Iteration 87, Loss: 0.6720395684242249
Epoch 10, Iteration 88, Loss: 0.7886990904808044
Epoch 10, Iteration 89, Loss: 0.8365699052810669
Epoch 10, Iteration 90, Loss: 0.5694050192832947
Epoch 10, Iteration 91, Loss: 0.7421297430992126
Epoch 10, Iteration 92, Loss: 0.6328316330909729
Epoch 10, Iteration 93, Loss: 0.76069176197052
Epoch 10, Iteration 94, Loss: 0.8262234926223755
Epoch 10, Iteration 95, Loss: 0.51578688621521
Epoch 10, Iteration 96, Loss: 0.7220892906188965
Epoch 10, Iteration 97, Loss: 0.6005618572235107
Epoch 10, Iteration 98, Loss: 0.7297185659408569
Epoch 10, Iteration 99, Loss: 0.8142919540405273
Epoch 10, Iteration 100, Loss: 0.6186021566390991
Epoch 10, Iteration 100, Valid Loss: 0.5935762524604797
Epoch 10, Iteration 101, Loss: 0.9378654956817627
Epoch 10, Iteration 102, Loss: 0.7896130681037903
Epoch 10, Iteration 103, Loss: 0.8839844465255737
Epoch 10, Iteration 104, Loss: 0.9371281862258911
Epoch 10, Iteration 105, Loss: 0.7133784890174866
Epoch 10, Iteration 106, Loss: 0.5743808746337891
Epoch 10, Iteration 107, Loss: 0.8873748779296875
Epoch 10, Iteration 108, Loss: 0.7285658121109009
Epoch 10, Iteration 109, Loss: 0.9432621598243713
Epoch 10, Iteration 110, Loss: 0.8967631459236145
Epoch 10, Iteration 111, Loss: 0.7806292772293091
Epoch 10, Iteration 112, Loss: 0.7485266327857971
Epoch 10, Iteration 113, Loss: 0.6144186854362488
Epoch 10, Iteration 114, Loss: 1.01872718334198
Epoch 10, Iteration 115, Loss: 0.9313822388648987
Epoch 10, Iteration 116, Loss: 0.685436487197876
Epoch 10, Iteration 117, Loss: 0.505425214767456
Epoch 10, Iteration 118, Loss: 0.7342525720596313
Epoch 10, Iteration 119, Loss: 0.6460834741592407
Epoch 10, Iteration 120, Loss: 0.9409077167510986
Epoch 10, Iteration 121, Loss: 0.7103379964828491
Epoch 10, Iteration 122, Loss: 0.7750341296195984
Epoch 10, Iteration 123, Loss: 0.7703580856323242
Epoch 10, Iteration 124, Loss: 0.5846555829048157
Epoch 10, Iteration 125, Loss: 0.7210730314254761
Epoch 10, Iteration 126, Loss: 0.5922013521194458
Epoch 10, Iteration 127, Loss: 0.8443806767463684
Epoch 10, Iteration 128, Loss: 0.9767475724220276
Epoch 10, Iteration 129, Loss: 0.8052440285682678
Epoch 10, Iteration 130, Loss: 0.9829390048980713
Epoch 10, Iteration 131, Loss: 0.7366750836372375
Epoch 10, Iteration 132, Loss: 0.6292023062705994
Epoch 10, Iteration 133, Loss: 1.1387101411819458
Epoch 10, Iteration 134, Loss: 0.8499776124954224
Epoch 10, Iteration 135, Loss: 0.8184048533439636
Epoch 10, Iteration 136, Loss: 0.7745359539985657
Epoch 10, Iteration 137, Loss: 0.7071245312690735
Epoch 10, Iteration 138, Loss: 0.6762761473655701
Epoch 10, Iteration 139, Loss: 0.6998870968818665
Epoch 10, Iteration 140, Loss: 0.6813833117485046
Epoch 10, Iteration 141, Loss: 0.9120232462882996
Epoch 10, Iteration 142, Loss: 0.7615529894828796
Epoch 10, Iteration 143, Loss: 0.6769667267799377
Epoch 10, Iteration 144, Loss: 0.6910528540611267
Epoch 10, Iteration 145, Loss: 0.7742070555686951
Epoch 10, Iteration 146, Loss: 0.8384502530097961
Epoch 10, Iteration 147, Loss: 0.7873073816299438
Epoch 10, Iteration 148, Loss: 0.5566807985305786
Epoch 10, Iteration 149, Loss: 0.7326437830924988
Epoch 10, Iteration 150, Loss: 0.7690501809120178
Epoch 10, Iteration 150, Valid Loss: 0.6186087131500244
Epoch 10, Iteration 151, Loss: 0.766526460647583
Epoch 10, Iteration 152, Loss: 0.6648402810096741
Epoch 10, Iteration 153, Loss: 0.6817734241485596
Epoch 10, Iteration 154, Loss: 0.5006027817726135
Epoch 10, Iteration 155, Loss: 0.8818252086639404
Epoch 10, Iteration 156, Loss: 0.7000746726989746
Epoch 10, Iteration 157, Loss: 1.0607458353042603
Epoch 10, Iteration 158, Loss: 0.7856533527374268
Epoch 10, Iteration 159, Loss: 0.7563639283180237
Epoch 10, Iteration 160, Loss: 0.6540985107421875
Epoch 10, Iteration 161, Loss: 0.6927825808525085
Epoch 10, Iteration 162, Loss: 0.9577944874763489
Epoch 10, Iteration 163, Loss: 0.5840796828269958
Epoch 10, Iteration 164, Loss: 0.6740800738334656
Epoch 10, Iteration 165, Loss: 0.767454981803894
Epoch 10, Iteration 166, Loss: 0.6642184853553772
Epoch 10, Iteration 167, Loss: 0.9544862508773804
Epoch 10, Iteration 168, Loss: 0.7741713523864746
Epoch 10, Iteration 169, Loss: 0.7487072348594666
Epoch 10, Iteration 170, Loss: 0.9114552140235901
Epoch 10, Iteration 171, Loss: 0.8929017782211304
Epoch 10, Iteration 172, Loss: 0.6960786581039429
Epoch 10, Iteration 173, Loss: 0.7404016256332397
Epoch 10, Iteration 174, Loss: 0.8023961782455444
Epoch 10, Iteration 175, Loss: 0.7905619740486145
Epoch 10, Iteration 176, Loss: 0.581392228603363
Epoch 10, Iteration 177, Loss: 1.029721736907959
Epoch 10, Iteration 178, Loss: 0.82248455286026
Epoch 10, Iteration 179, Loss: 0.6775387525558472
Epoch 10, Iteration 180, Loss: 0.702146589756012
Epoch 10, Iteration 181, Loss: 0.75860995054245
Epoch 10, Iteration 182, Loss: 0.8195267915725708
Epoch 10, Iteration 183, Loss: 0.6623228192329407
Epoch 10, Iteration 184, Loss: 0.7173915505409241
Epoch 10, Iteration 185, Loss: 0.6233001947402954
Epoch 10, Iteration 186, Loss: 0.6487166881561279
Epoch 10, Iteration 187, Loss: 0.7055772542953491
Epoch 10, Iteration 188, Loss: 0.6488667130470276
Epoch 10, Iteration 189, Loss: 0.6604911684989929
Epoch 10, Iteration 190, Loss: 0.8460264801979065
Epoch 10, Iteration 191, Loss: 0.7295841574668884
Epoch 10, Iteration 192, Loss: 0.7800165414810181
Epoch 10, Iteration 193, Loss: 0.7407735586166382
Epoch 10, Iteration 194, Loss: 0.7662461400032043
Epoch 10, Iteration 195, Loss: 0.6357730031013489
Epoch 10, Iteration 196, Loss: 0.742868959903717
Epoch 10, Iteration 197, Loss: 0.684909999370575
Epoch 10, Iteration 198, Loss: 0.6679373979568481
Epoch 10, Iteration 199, Loss: 0.8563295602798462
Epoch 10, Iteration 200, Loss: 0.6409726738929749
Epoch 10, Iteration 200, Valid Loss: 0.5743182301521301
Epoch 10, Iteration 201, Loss: 0.6706652045249939
Epoch 10, Iteration 202, Loss: 0.7648370862007141
Epoch 10, Iteration 203, Loss: 0.6519437432289124
Epoch 10, Iteration 204, Loss: 0.5481966733932495
Epoch 10, Iteration 205, Loss: 0.5176790952682495
Epoch 10, Iteration 206, Loss: 0.6611182689666748
Epoch 10, Iteration 207, Loss: 0.9427447319030762
Epoch 10, Iteration 208, Loss: 1.1028672456741333
Epoch 10, Iteration 209, Loss: 1.07598078250885
Epoch 10, Iteration 210, Loss: 1.02100670337677
Epoch 10, Iteration 211, Loss: 1.098413109779358
Epoch 10, Iteration 212, Loss: 0.8806502223014832
Epoch 10, Iteration 213, Loss: 0.9492775797843933
Epoch 10, Iteration 214, Loss: 0.7214271426200867
Epoch 10, Iteration 215, Loss: 0.6506844758987427
Epoch 10, Iteration 216, Loss: 0.6766752004623413
Epoch 10, Iteration 217, Loss: 0.6437801122665405
Epoch 10, Iteration 218, Loss: 0.7815642952919006
Epoch 10, Iteration 219, Loss: 0.8523040413856506
Epoch 10, Iteration 220, Loss: 0.672357976436615
Epoch 10, Iteration 221, Loss: 0.7504586577415466
Epoch 10, Iteration 222, Loss: 0.6701706051826477
Epoch 10, Iteration 223, Loss: 0.6027670502662659
Epoch 10, Iteration 224, Loss: 0.6325260996818542
Epoch 10, Iteration 225, Loss: 0.8266700506210327
Epoch 10, Iteration 226, Loss: 0.770339846611023
Epoch 10, Iteration 227, Loss: 0.8777580857276917
Epoch 10, Iteration 228, Loss: 0.7112290263175964
Epoch 10, Iteration 229, Loss: 0.6070895791053772
Epoch 10, Iteration 230, Loss: 0.7547205090522766
Epoch 10, Iteration 231, Loss: 0.7307193279266357
Epoch 10, Iteration 232, Loss: 0.7967984080314636
Epoch 10, Iteration 233, Loss: 1.0054471492767334
Epoch 10, Iteration 234, Loss: 0.8256056904792786
Epoch 10, Iteration 235, Loss: 1.0016772747039795
Epoch 10, Iteration 236, Loss: 0.7089691758155823
Epoch 10, Iteration 237, Loss: 0.6708499193191528
Epoch 10, Iteration 238, Loss: 0.6065192818641663
Epoch 10, Iteration 239, Loss: 0.7347651720046997
Epoch 10, Iteration 240, Loss: 0.6613755822181702
Epoch 10, Iteration 241, Loss: 0.7381796836853027
Epoch 10, Iteration 242, Loss: 0.6906258463859558
Epoch 10, Iteration 243, Loss: 0.7288327217102051
Epoch 10, Iteration 244, Loss: 0.7345824837684631
Epoch 10, Iteration 245, Loss: 0.4920479953289032
Epoch 10, Iteration 246, Loss: 0.715110719203949
Epoch 10, Iteration 247, Loss: 0.6111136078834534
Epoch 10, Iteration 248, Loss: 0.7207657098770142
Epoch 10, Iteration 249, Loss: 0.7677707672119141
Epoch 10, Iteration 250, Loss: 0.8631688952445984
Epoch 10, Iteration 250, Valid Loss: 0.5504011511802673
Epoch 10, Iteration 251, Loss: 0.6561771035194397
Epoch 10, Iteration 252, Loss: 0.9588308930397034
Epoch 10, Iteration 253, Loss: 0.7828091382980347
Epoch 10, Iteration 254, Loss: 0.8474927544593811
Epoch 10, Iteration 255, Loss: 0.8627131581306458
Epoch 10, Iteration 256, Loss: 0.761641263961792
Epoch 10, Iteration 257, Loss: 0.5783657431602478
Epoch 10, Iteration 258, Loss: 0.9014609456062317
Epoch 10, Iteration 259, Loss: 0.6264707446098328
Epoch 10, Iteration 260, Loss: 0.7745126485824585
Epoch 10, Iteration 261, Loss: 0.7050879001617432
Epoch 10, Iteration 262, Loss: 0.5822306871414185
Epoch 10, Iteration 263, Loss: 0.8365021347999573
Epoch 10, Iteration 264, Loss: 0.7574198842048645
Epoch 10, Iteration 265, Loss: 0.7455630302429199
Epoch 10, Iteration 266, Loss: 0.700753390789032
Epoch 10, Iteration 267, Loss: 0.718884289264679
Epoch 10, Iteration 268, Loss: 0.6650766134262085
Epoch 10, Iteration 269, Loss: 0.7242072820663452
Epoch 10, Iteration 270, Loss: 0.6870084404945374
Epoch 10, Iteration 271, Loss: 0.7116851806640625
Epoch 10, Iteration 272, Loss: 0.657995343208313
Epoch 10, Iteration 273, Loss: 0.6438336372375488
Epoch 10, Iteration 274, Loss: 0.7360741496086121
Epoch 10, Iteration 275, Loss: 0.701709508895874
Epoch 10, Iteration 276, Loss: 0.6678832173347473
Epoch 10, Iteration 277, Loss: 0.7032811641693115
Epoch 10, Iteration 278, Loss: 0.8900871872901917
Epoch 10, Iteration 279, Loss: 0.6256862282752991
Epoch 10, Iteration 280, Loss: 0.51121586561203
Epoch 10, Iteration 281, Loss: 0.6841310858726501
Epoch 10, Iteration 282, Loss: 0.688979983329773
Epoch 10, Iteration 283, Loss: 0.674055278301239
Epoch 10, Iteration 284, Loss: 0.6844362616539001
Epoch 10, Iteration 285, Loss: 0.6384543776512146
Epoch 10, Iteration 286, Loss: 0.7142648100852966
Epoch 10, Iteration 287, Loss: 0.667657196521759
Epoch 10, Iteration 288, Loss: 0.5384620428085327
Epoch 10, Iteration 289, Loss: 0.6726642847061157
Epoch 10, Iteration 290, Loss: 0.6010797619819641
Epoch 10, Iteration 291, Loss: 0.6296411752700806
Epoch 10, Iteration 292, Loss: 0.8138992190361023
Epoch 10, Iteration 293, Loss: 0.8846218585968018
Epoch 10, Iteration 294, Loss: 0.6795259714126587
Epoch 10, Iteration 295, Loss: 0.5945744514465332
Epoch 10, Iteration 296, Loss: 0.8290864825248718
Epoch 10, Iteration 297, Loss: 0.7845622301101685
Epoch 10, Iteration 298, Loss: 0.7777056097984314
Epoch 10, Iteration 299, Loss: 0.7214019298553467
Epoch 10, Iteration 300, Loss: 0.6035046577453613
Epoch 10, Iteration 300, Valid Loss: 0.5557551980018616
Epoch 10, Iteration 301, Loss: 0.6539008021354675
Epoch 10, Iteration 302, Loss: 0.6069904565811157
Epoch 10, Iteration 303, Loss: 0.7707574367523193
Epoch 10, Iteration 304, Loss: 0.7117477655410767
Epoch 10, Iteration 305, Loss: 0.5745503902435303
Epoch 10, Iteration 306, Loss: 0.6007079482078552
Epoch 10, Iteration 307, Loss: 0.6313223838806152
Epoch 10, Iteration 308, Loss: 0.6527943015098572
Epoch 10, Iteration 309, Loss: 0.6324784755706787
Epoch 10, Iteration 310, Loss: 0.8787057995796204
Epoch 10, Iteration 311, Loss: 0.6591940522193909
Epoch 10, Iteration 312, Loss: 0.589541494846344
Epoch 10, Iteration 313, Loss: 0.7831821441650391
Epoch 10, Iteration 314, Loss: 0.6550608277320862
Epoch 10, Iteration 315, Loss: 0.6151639223098755
Epoch 10, Iteration 316, Loss: 0.8148202896118164
Epoch 10, Iteration 317, Loss: 0.7439497709274292
Epoch 10, Iteration 318, Loss: 0.7473531365394592
Epoch 10, Iteration 319, Loss: 0.8006598353385925
Epoch 10, Iteration 320, Loss: 0.5651202201843262
Epoch 10, Iteration 321, Loss: 0.6667767763137817
Epoch 10, Iteration 322, Loss: 0.6580484509468079
Epoch 10, Iteration 323, Loss: 0.666596531867981
Epoch 10, Iteration 324, Loss: 0.7767106294631958
Epoch 10, Iteration 325, Loss: 0.7353595495223999
Epoch 10, Iteration 326, Loss: 0.715372622013092
Epoch 10, Iteration 327, Loss: 0.6271296143531799
Epoch 10, Iteration 328, Loss: 0.6826103329658508
Epoch 10, Iteration 329, Loss: 0.8405982255935669
Epoch 10, Iteration 330, Loss: 0.5478883385658264
Epoch 10, Iteration 331, Loss: 0.6306619048118591
Epoch 10, Iteration 332, Loss: 0.7813678979873657
Epoch 10, Iteration 333, Loss: 0.814903199672699
Epoch 10, Iteration 334, Loss: 0.7711772918701172
Epoch 10, Iteration 335, Loss: 0.9561102986335754
Epoch 10, Iteration 336, Loss: 0.7349669337272644
Epoch 10, Iteration 337, Loss: 0.6829584240913391
Epoch 10, Iteration 338, Loss: 0.7673381567001343
Epoch 10, Iteration 339, Loss: 0.7515866756439209
Epoch 10, Iteration 340, Loss: 0.568173885345459
Epoch 10, Iteration 341, Loss: 0.6508288979530334
Epoch 10, Iteration 342, Loss: 0.7587412595748901
Epoch 10, Iteration 343, Loss: 0.6001796126365662
Epoch 10, Iteration 344, Loss: 0.7477498054504395
Epoch 10, Iteration 345, Loss: 0.7083740234375
Epoch 10, Iteration 346, Loss: 0.6925488114356995
Epoch 10, Iteration 347, Loss: 0.5945508480072021
Epoch 10, Iteration 348, Loss: 0.6250824928283691
Epoch 10, Iteration 349, Loss: 0.6370269060134888
Epoch 10, Iteration 350, Loss: 0.553481936454773
Epoch 10, Iteration 350, Valid Loss: 0.5864779949188232
Epoch 10, Iteration 351, Loss: 0.7451006770133972
Epoch 10, Iteration 352, Loss: 0.5752909183502197
Epoch 10, Iteration 353, Loss: 0.7061761617660522
Epoch 10, Iteration 354, Loss: 0.5584087371826172
Epoch 10, Iteration 355, Loss: 0.6344838738441467
Epoch 10, Iteration 356, Loss: 0.705750584602356
Epoch 10, Iteration 357, Loss: 0.6736392974853516
Epoch 10, Iteration 358, Loss: 0.6159852147102356
Epoch 10, Iteration 359, Loss: 0.6367098093032837
Epoch 10, Iteration 360, Loss: 0.5615674257278442
Epoch 10, Iteration 361, Loss: 0.7077704668045044
Epoch 10, Iteration 362, Loss: 0.7054967880249023
Epoch 10, Iteration 363, Loss: 0.6565071940422058
Epoch 10, Iteration 364, Loss: 0.6527601480484009
Epoch 10, Iteration 365, Loss: 0.7506481409072876
Epoch 10, Iteration 366, Loss: 0.7201902270317078
Epoch 10, Iteration 367, Loss: 0.6431786417961121
Epoch 10, Iteration 368, Loss: 0.6408833861351013
Epoch 10, Iteration 369, Loss: 0.6227806210517883
Epoch 10, Iteration 370, Loss: 0.6809090375900269
Epoch 10, Iteration 371, Loss: 0.6802048087120056
Epoch 10, Iteration 372, Loss: 0.5819459557533264
Epoch 10, Iteration 373, Loss: 0.6230805516242981
Epoch 10, Iteration 374, Loss: 0.9183531999588013
Epoch 10, Iteration 375, Loss: 0.6080155968666077
Epoch 10, Iteration 376, Loss: 0.7125503420829773
Epoch 10, Iteration 377, Loss: 0.6342138051986694
Epoch 10, Iteration 378, Loss: 0.5610761642456055
Epoch 10, Iteration 379, Loss: 0.5570324063301086
Epoch 10, Iteration 380, Loss: 0.6039465665817261
Epoch 10, Iteration 381, Loss: 0.5890289545059204
Epoch 10, Iteration 382, Loss: 0.4715224504470825
Epoch 10, Iteration 383, Loss: 0.6662879586219788
Epoch 10, Iteration 384, Loss: 0.6853749752044678
Epoch 10, Iteration 385, Loss: 0.6175462007522583
Epoch 10, Iteration 386, Loss: 0.5697802901268005
Epoch 10, Iteration 387, Loss: 0.9229133129119873
Epoch 10, Iteration 388, Loss: 0.5914174914360046
Epoch 10, Iteration 389, Loss: 0.7061011791229248
Epoch 10, Iteration 390, Loss: 0.8191821575164795
Epoch 10, Iteration 391, Loss: 0.6114296317100525
Epoch 11/15, Loss: 0.7327804779063682
Epoch 11, Iteration 0, Loss: 0.7016355395317078
Epoch 11, Iteration 1, Loss: 0.7958127856254578
Epoch 11, Iteration 2, Loss: 0.7641939520835876
Epoch 11, Iteration 3, Loss: 0.8292186856269836
Epoch 11, Iteration 4, Loss: 0.6860394477844238
Epoch 11, Iteration 5, Loss: 0.7741234302520752
Epoch 11, Iteration 6, Loss: 0.5703004598617554
Epoch 11, Iteration 7, Loss: 0.6935155987739563
Epoch 11, Iteration 8, Loss: 0.5974258184432983
Epoch 11, Iteration 9, Loss: 0.7895264625549316
Epoch 11, Iteration 10, Loss: 0.7498070597648621
Epoch 11, Iteration 11, Loss: 0.6577965617179871
Epoch 11, Iteration 12, Loss: 0.6262067556381226
Epoch 11, Iteration 13, Loss: 0.7220155000686646
Epoch 11, Iteration 14, Loss: 0.6378489136695862
Epoch 11, Iteration 15, Loss: 0.8371140360832214
Epoch 11, Iteration 16, Loss: 0.6864043474197388
Epoch 11, Iteration 17, Loss: 0.7997525930404663
Epoch 11, Iteration 18, Loss: 0.6409203410148621
Epoch 11, Iteration 19, Loss: 0.7335864901542664
Epoch 11, Iteration 20, Loss: 0.5980731248855591
Epoch 11, Iteration 21, Loss: 0.6108127236366272
Epoch 11, Iteration 22, Loss: 0.6074884533882141
Epoch 11, Iteration 23, Loss: 0.7290917634963989
Epoch 11, Iteration 24, Loss: 0.680160403251648
Epoch 11, Iteration 25, Loss: 0.5797147154808044
Epoch 11, Iteration 26, Loss: 0.9656983017921448
Epoch 11, Iteration 27, Loss: 0.6812348365783691
Epoch 11, Iteration 28, Loss: 0.7334570288658142
Epoch 11, Iteration 29, Loss: 0.8356876373291016
Epoch 11, Iteration 30, Loss: 0.675916314125061
Epoch 11, Iteration 31, Loss: 0.7303800582885742
Epoch 11, Iteration 32, Loss: 0.7287982106208801
Epoch 11, Iteration 33, Loss: 0.7361166477203369
Epoch 11, Iteration 34, Loss: 0.6768385767936707
Epoch 11, Iteration 35, Loss: 0.7603305578231812
Epoch 11, Iteration 36, Loss: 0.6040888428688049
Epoch 11, Iteration 37, Loss: 0.5855453014373779
Epoch 11, Iteration 38, Loss: 0.631179928779602
Epoch 11, Iteration 39, Loss: 0.7580052614212036
Epoch 11, Iteration 40, Loss: 0.6634421944618225
Epoch 11, Iteration 41, Loss: 0.7394941449165344
Epoch 11, Iteration 42, Loss: 0.6453126668930054
Epoch 11, Iteration 43, Loss: 0.5635202527046204
Epoch 11, Iteration 44, Loss: 0.6407620906829834
Epoch 11, Iteration 45, Loss: 0.7344962954521179
Epoch 11, Iteration 46, Loss: 0.7255427241325378
Epoch 11, Iteration 47, Loss: 0.6627433896064758
Epoch 11, Iteration 48, Loss: 0.8032912015914917
Epoch 11, Iteration 49, Loss: 0.660552442073822
Epoch 11, Iteration 50, Loss: 0.6443440318107605
Epoch 11, Iteration 50, Valid Loss: 0.5848653316497803
Epoch 11, Iteration 51, Loss: 0.8275666236877441
Epoch 11, Iteration 52, Loss: 0.5462636947631836
Epoch 11, Iteration 53, Loss: 0.6839600205421448
Epoch 11, Iteration 54, Loss: 0.980158269405365
Epoch 11, Iteration 55, Loss: 0.7693561911582947
Epoch 11, Iteration 56, Loss: 0.6987866759300232
Epoch 11, Iteration 57, Loss: 0.7813132405281067
Epoch 11, Iteration 58, Loss: 0.6944959163665771
Epoch 11, Iteration 59, Loss: 0.6436046957969666
Epoch 11, Iteration 60, Loss: 0.6738095283508301
Epoch 11, Iteration 61, Loss: 0.5575127005577087
Epoch 11, Iteration 62, Loss: 0.6775184273719788
Epoch 11, Iteration 63, Loss: 0.5660097599029541
Epoch 11, Iteration 64, Loss: 0.8429458737373352
Epoch 11, Iteration 65, Loss: 0.9692674279212952
Epoch 11, Iteration 66, Loss: 0.6977846026420593
Epoch 11, Iteration 67, Loss: 0.5850030779838562
Epoch 11, Iteration 68, Loss: 0.9167719483375549
Epoch 11, Iteration 69, Loss: 0.7904755473136902
Epoch 11, Iteration 70, Loss: 0.5794886350631714
Epoch 11, Iteration 71, Loss: 0.8602553606033325
Epoch 11, Iteration 72, Loss: 0.7160502076148987
Epoch 11, Iteration 73, Loss: 0.7043562531471252
Epoch 11, Iteration 74, Loss: 0.9686292409896851
Epoch 11, Iteration 75, Loss: 0.7644162178039551
Epoch 11, Iteration 76, Loss: 0.7246971726417542
Epoch 11, Iteration 77, Loss: 0.6934483647346497
Epoch 11, Iteration 78, Loss: 0.8779032826423645
Epoch 11, Iteration 79, Loss: 0.899221658706665
Epoch 11, Iteration 80, Loss: 0.5616312623023987
Epoch 11, Iteration 81, Loss: 0.8780055046081543
Epoch 11, Iteration 82, Loss: 0.829157829284668
Epoch 11, Iteration 83, Loss: 0.6211012601852417
Epoch 11, Iteration 84, Loss: 0.8381749987602234
Epoch 11, Iteration 85, Loss: 0.6625975370407104
Epoch 11, Iteration 86, Loss: 0.8480207324028015
Epoch 11, Iteration 87, Loss: 0.7154961824417114
Epoch 11, Iteration 88, Loss: 0.8197358846664429
Epoch 11, Iteration 89, Loss: 0.7898690700531006
Epoch 11, Iteration 90, Loss: 0.5682766437530518
Epoch 11, Iteration 91, Loss: 0.6831996440887451
Epoch 11, Iteration 92, Loss: 0.4894576668739319
Epoch 11, Iteration 93, Loss: 0.7669128775596619
Epoch 11, Iteration 94, Loss: 0.8113728165626526
Epoch 11, Iteration 95, Loss: 0.5814016461372375
Epoch 11, Iteration 96, Loss: 0.7451983690261841
Epoch 11, Iteration 97, Loss: 0.5599753856658936
Epoch 11, Iteration 98, Loss: 0.6928027272224426
Epoch 11, Iteration 99, Loss: 0.6592565178871155
Epoch 11, Iteration 100, Loss: 0.6010146737098694
Epoch 11, Iteration 100, Valid Loss: 0.6339892745018005
Epoch 11, Iteration 101, Loss: 0.9322881102561951
Epoch 11, Iteration 102, Loss: 0.7911035418510437
Epoch 11, Iteration 103, Loss: 0.7354932427406311
Epoch 11, Iteration 104, Loss: 0.8568803668022156
Epoch 11, Iteration 105, Loss: 0.750784695148468
Epoch 11, Iteration 106, Loss: 0.6459167003631592
Epoch 11, Iteration 107, Loss: 0.8916794657707214
Epoch 11, Iteration 108, Loss: 0.6391187310218811
Epoch 11, Iteration 109, Loss: 0.852087140083313
Epoch 11, Iteration 110, Loss: 0.8874152898788452
Epoch 11, Iteration 111, Loss: 0.6859068274497986
Epoch 11, Iteration 112, Loss: 0.7303762435913086
Epoch 11, Iteration 113, Loss: 0.6080019474029541
Epoch 11, Iteration 114, Loss: 0.8980609774589539
Epoch 11, Iteration 115, Loss: 0.8728168606758118
Epoch 11, Iteration 116, Loss: 0.6574658751487732
Epoch 11, Iteration 117, Loss: 0.5024585723876953
Epoch 11, Iteration 118, Loss: 0.6969947814941406
Epoch 11, Iteration 119, Loss: 0.5606737732887268
Epoch 11, Iteration 120, Loss: 1.1385856866836548
Epoch 11, Iteration 121, Loss: 0.6212441921234131
Epoch 11, Iteration 122, Loss: 0.6351438164710999
Epoch 11, Iteration 123, Loss: 0.6661232709884644
Epoch 11, Iteration 124, Loss: 0.5899012088775635
Epoch 11, Iteration 125, Loss: 0.683624804019928
Epoch 11, Iteration 126, Loss: 0.5934281349182129
Epoch 11, Iteration 127, Loss: 0.7890117168426514
Epoch 11, Iteration 128, Loss: 0.7473239898681641
Epoch 11, Iteration 129, Loss: 0.8331882953643799
Epoch 11, Iteration 130, Loss: 0.9669003486633301
Epoch 11, Iteration 131, Loss: 0.665132999420166
Epoch 11, Iteration 132, Loss: 0.6973826885223389
Epoch 11, Iteration 133, Loss: 0.9957250356674194
Epoch 11, Iteration 134, Loss: 0.7772979140281677
Epoch 11, Iteration 135, Loss: 0.7940321564674377
Epoch 11, Iteration 136, Loss: 0.5943321585655212
Epoch 11, Iteration 137, Loss: 0.6846792101860046
Epoch 11, Iteration 138, Loss: 0.6196141839027405
Epoch 11, Iteration 139, Loss: 0.6141042709350586
Epoch 11, Iteration 140, Loss: 0.5955337882041931
Epoch 11, Iteration 141, Loss: 1.021721601486206
Epoch 11, Iteration 142, Loss: 0.8055428862571716
Epoch 11, Iteration 143, Loss: 0.6650749444961548
Epoch 11, Iteration 144, Loss: 0.6552848219871521
Epoch 11, Iteration 145, Loss: 0.7500748038291931
Epoch 11, Iteration 146, Loss: 0.755699872970581
Epoch 11, Iteration 147, Loss: 0.7468070983886719
Epoch 11, Iteration 148, Loss: 0.5697548985481262
Epoch 11, Iteration 149, Loss: 0.6593852043151855
Epoch 11, Iteration 150, Loss: 0.7460546493530273
Epoch 11, Iteration 150, Valid Loss: 0.589823305606842
Epoch 11, Iteration 151, Loss: 0.7059467434883118
Epoch 11, Iteration 152, Loss: 0.6114403605461121
Epoch 11, Iteration 153, Loss: 0.7800742387771606
Epoch 11, Iteration 154, Loss: 0.5697222948074341
Epoch 11, Iteration 155, Loss: 0.7189901471138
Epoch 11, Iteration 156, Loss: 0.5989800691604614
Epoch 11, Iteration 157, Loss: 1.1997401714324951
Epoch 11, Iteration 158, Loss: 0.7328051924705505
Epoch 11, Iteration 159, Loss: 0.7885064482688904
Epoch 11, Iteration 160, Loss: 0.6559047102928162
Epoch 11, Iteration 161, Loss: 0.6434035301208496
Epoch 11, Iteration 162, Loss: 0.884608268737793
Epoch 11, Iteration 163, Loss: 0.6101012825965881
Epoch 11, Iteration 164, Loss: 0.7243504524230957
Epoch 11, Iteration 165, Loss: 0.7038038372993469
Epoch 11, Iteration 166, Loss: 0.6093520522117615
Epoch 11, Iteration 167, Loss: 0.8356261849403381
Epoch 11, Iteration 168, Loss: 0.7441189885139465
Epoch 11, Iteration 169, Loss: 0.6703001260757446
Epoch 11, Iteration 170, Loss: 0.8500682711601257
Epoch 11, Iteration 171, Loss: 0.8364060521125793
Epoch 11, Iteration 172, Loss: 0.6594241261482239
Epoch 11, Iteration 173, Loss: 0.5889284610748291
Epoch 11, Iteration 174, Loss: 0.806122899055481
Epoch 11, Iteration 175, Loss: 0.7159833312034607
Epoch 11, Iteration 176, Loss: 0.49993664026260376
Epoch 11, Iteration 177, Loss: 0.9869811534881592
Epoch 11, Iteration 178, Loss: 0.8179988265037537
Epoch 11, Iteration 179, Loss: 0.6375972628593445
Epoch 11, Iteration 180, Loss: 0.6958033442497253
Epoch 11, Iteration 181, Loss: 0.8222627639770508
Epoch 11, Iteration 182, Loss: 0.765865683555603
Epoch 11, Iteration 183, Loss: 0.6059404015541077
Epoch 11, Iteration 184, Loss: 0.6953216791152954
Epoch 11, Iteration 185, Loss: 0.5777532458305359
Epoch 11, Iteration 186, Loss: 0.5829187035560608
Epoch 11, Iteration 187, Loss: 0.7430180907249451
Epoch 11, Iteration 188, Loss: 0.5966593623161316
Epoch 11, Iteration 189, Loss: 0.6424821615219116
Epoch 11, Iteration 190, Loss: 0.818540096282959
Epoch 11, Iteration 191, Loss: 0.7179632186889648
Epoch 11, Iteration 192, Loss: 0.8015732765197754
Epoch 11, Iteration 193, Loss: 0.6872007250785828
Epoch 11, Iteration 194, Loss: 0.929430365562439
Epoch 11, Iteration 195, Loss: 0.6512874364852905
Epoch 11, Iteration 196, Loss: 0.7549152374267578
Epoch 11, Iteration 197, Loss: 0.7201743125915527
Epoch 11, Iteration 198, Loss: 0.7050696611404419
Epoch 11, Iteration 199, Loss: 0.7407805323600769
Epoch 11, Iteration 200, Loss: 0.6100530028343201
Epoch 11, Iteration 200, Valid Loss: 0.5724600553512573
Epoch 11, Iteration 201, Loss: 0.7007068395614624
Epoch 11, Iteration 202, Loss: 0.8313390016555786
Epoch 11, Iteration 203, Loss: 0.6462913751602173
Epoch 11, Iteration 204, Loss: 0.6313725113868713
Epoch 11, Iteration 205, Loss: 0.5766469836235046
Epoch 11, Iteration 206, Loss: 0.5910604596138
Epoch 11, Iteration 207, Loss: 0.8092253804206848
Epoch 11, Iteration 208, Loss: 1.0709021091461182
Epoch 11, Iteration 209, Loss: 1.0633827447891235
Epoch 11, Iteration 210, Loss: 0.9673187136650085
Epoch 11, Iteration 211, Loss: 0.9665571451187134
Epoch 11, Iteration 212, Loss: 0.8132576942443848
Epoch 11, Iteration 213, Loss: 0.8810999989509583
Epoch 11, Iteration 214, Loss: 0.6756473779678345
Epoch 11, Iteration 215, Loss: 0.6649514436721802
Epoch 11, Iteration 216, Loss: 0.577170193195343
Epoch 11, Iteration 217, Loss: 0.6059964299201965
Epoch 11, Iteration 218, Loss: 0.70539790391922
Epoch 11, Iteration 219, Loss: 0.7151912450790405
Epoch 11, Iteration 220, Loss: 0.5927819609642029
Epoch 11, Iteration 221, Loss: 0.7252864837646484
Epoch 11, Iteration 222, Loss: 0.7283315062522888
Epoch 11, Iteration 223, Loss: 0.6460860371589661
Epoch 11, Iteration 224, Loss: 0.59513920545578
Epoch 11, Iteration 225, Loss: 0.8099744915962219
Epoch 11, Iteration 226, Loss: 0.7911078929901123
Epoch 11, Iteration 227, Loss: 0.7437506914138794
Epoch 11, Iteration 228, Loss: 0.7431491017341614
Epoch 11, Iteration 229, Loss: 0.6552041172981262
Epoch 11, Iteration 230, Loss: 0.7400884628295898
Epoch 11, Iteration 231, Loss: 0.701139509677887
Epoch 11, Iteration 232, Loss: 0.8653304576873779
Epoch 11, Iteration 233, Loss: 0.8134966492652893
Epoch 11, Iteration 234, Loss: 0.8714884519577026
Epoch 11, Iteration 235, Loss: 0.5782630443572998
Epoch 11, Iteration 236, Loss: 0.7403836846351624
Epoch 11, Iteration 237, Loss: 0.5734928250312805
Epoch 11, Iteration 238, Loss: 0.5748691558837891
Epoch 11, Iteration 239, Loss: 0.8499137759208679
Epoch 11, Iteration 240, Loss: 0.7124727368354797
Epoch 11, Iteration 241, Loss: 0.6292638182640076
Epoch 11, Iteration 242, Loss: 0.6428384780883789
Epoch 11, Iteration 243, Loss: 0.6717172861099243
Epoch 11, Iteration 244, Loss: 0.6868097186088562
Epoch 11, Iteration 245, Loss: 0.5418257117271423
Epoch 11, Iteration 246, Loss: 0.6941010355949402
Epoch 11, Iteration 247, Loss: 0.6320768594741821
Epoch 11, Iteration 248, Loss: 0.7983066439628601
Epoch 11, Iteration 249, Loss: 0.691463828086853
Epoch 11, Iteration 250, Loss: 0.8682649731636047
Epoch 11, Iteration 250, Valid Loss: 0.5585439205169678
Epoch 11, Iteration 251, Loss: 0.6655974388122559
Epoch 11, Iteration 252, Loss: 0.787370502948761
Epoch 11, Iteration 253, Loss: 0.7895509004592896
Epoch 11, Iteration 254, Loss: 0.7818082571029663
Epoch 11, Iteration 255, Loss: 0.7468234300613403
Epoch 11, Iteration 256, Loss: 0.7632429003715515
Epoch 11, Iteration 257, Loss: 0.7654913067817688
Epoch 11, Iteration 258, Loss: 0.8914918899536133
Epoch 11, Iteration 259, Loss: 0.6005858778953552
Epoch 11, Iteration 260, Loss: 0.6866844892501831
Epoch 11, Iteration 261, Loss: 0.670460045337677
Epoch 11, Iteration 262, Loss: 0.6071642637252808
Epoch 11, Iteration 263, Loss: 0.8285476565361023
Epoch 11, Iteration 264, Loss: 0.6786581873893738
Epoch 11, Iteration 265, Loss: 0.7620296478271484
Epoch 11, Iteration 266, Loss: 0.8228822350502014
Epoch 11, Iteration 267, Loss: 0.5967289805412292
Epoch 11, Iteration 268, Loss: 0.6166386008262634
Epoch 11, Iteration 269, Loss: 0.6990569829940796
Epoch 11, Iteration 270, Loss: 0.7128511667251587
Epoch 11, Iteration 271, Loss: 0.8369568586349487
Epoch 11, Iteration 272, Loss: 0.5953711271286011
Epoch 11, Iteration 273, Loss: 0.7517962455749512
Epoch 11, Iteration 274, Loss: 0.6434080600738525
Epoch 11, Iteration 275, Loss: 0.6169404983520508
Epoch 11, Iteration 276, Loss: 0.6136918663978577
Epoch 11, Iteration 277, Loss: 0.6511863470077515
Epoch 11, Iteration 278, Loss: 0.847588062286377
Epoch 11, Iteration 279, Loss: 0.6365419030189514
Epoch 11, Iteration 280, Loss: 0.49029719829559326
Epoch 11, Iteration 281, Loss: 0.6643944382667542
Epoch 11, Iteration 282, Loss: 0.6970327496528625
Epoch 11, Iteration 283, Loss: 0.6669929623603821
Epoch 11, Iteration 284, Loss: 0.62897127866745
Epoch 11, Iteration 285, Loss: 0.5734574794769287
Epoch 11, Iteration 286, Loss: 0.7050370573997498
Epoch 11, Iteration 287, Loss: 0.6435580253601074
Epoch 11, Iteration 288, Loss: 0.6610857248306274
Epoch 11, Iteration 289, Loss: 0.6897035241127014
Epoch 11, Iteration 290, Loss: 0.5676261782646179
Epoch 11, Iteration 291, Loss: 0.5837550163269043
Epoch 11, Iteration 292, Loss: 0.7596551179885864
Epoch 11, Iteration 293, Loss: 0.8433253169059753
Epoch 11, Iteration 294, Loss: 0.7853558659553528
Epoch 11, Iteration 295, Loss: 0.6030640006065369
Epoch 11, Iteration 296, Loss: 0.8234965801239014
Epoch 11, Iteration 297, Loss: 0.8190540671348572
Epoch 11, Iteration 298, Loss: 0.7436918020248413
Epoch 11, Iteration 299, Loss: 0.6077051162719727
Epoch 11, Iteration 300, Loss: 0.5548031330108643
Epoch 11, Iteration 300, Valid Loss: 0.5653539896011353
Epoch 11, Iteration 301, Loss: 0.6484084725379944
Epoch 11, Iteration 302, Loss: 0.7153722643852234
Epoch 11, Iteration 303, Loss: 0.8034527897834778
Epoch 11, Iteration 304, Loss: 0.5746414661407471
Epoch 11, Iteration 305, Loss: 0.6737744212150574
Epoch 11, Iteration 306, Loss: 0.5071656107902527
Epoch 11, Iteration 307, Loss: 0.6133145689964294
Epoch 11, Iteration 308, Loss: 0.6595855355262756
Epoch 11, Iteration 309, Loss: 0.6158607006072998
Epoch 11, Iteration 310, Loss: 0.7340025305747986
Epoch 11, Iteration 311, Loss: 0.8770658373832703
Epoch 11, Iteration 312, Loss: 0.5982293486595154
Epoch 11, Iteration 313, Loss: 0.7901511192321777
Epoch 11, Iteration 314, Loss: 0.6006894707679749
Epoch 11, Iteration 315, Loss: 0.5855558514595032
Epoch 11, Iteration 316, Loss: 0.9190396070480347
Epoch 11, Iteration 317, Loss: 0.7441155314445496
Epoch 11, Iteration 318, Loss: 0.7304530143737793
Epoch 11, Iteration 319, Loss: 0.7392343878746033
Epoch 11, Iteration 320, Loss: 0.5354325175285339
Epoch 11, Iteration 321, Loss: 0.6731188893318176
Epoch 11, Iteration 322, Loss: 0.6749829649925232
Epoch 11, Iteration 323, Loss: 0.6267775893211365
Epoch 11, Iteration 324, Loss: 0.6869545578956604
Epoch 11, Iteration 325, Loss: 0.6481311917304993
Epoch 11, Iteration 326, Loss: 0.5557000637054443
Epoch 11, Iteration 327, Loss: 0.6065683364868164
Epoch 11, Iteration 328, Loss: 0.6545423865318298
Epoch 11, Iteration 329, Loss: 0.7377175092697144
Epoch 11, Iteration 330, Loss: 0.5383874177932739
Epoch 11, Iteration 331, Loss: 0.6167901158332825
Epoch 11, Iteration 332, Loss: 0.6958348155021667
Epoch 11, Iteration 333, Loss: 0.796514093875885
Epoch 11, Iteration 334, Loss: 0.6370205879211426
Epoch 11, Iteration 335, Loss: 0.8901472091674805
Epoch 11, Iteration 336, Loss: 0.6548120975494385
Epoch 11, Iteration 337, Loss: 0.701209545135498
Epoch 11, Iteration 338, Loss: 0.7411563992500305
Epoch 11, Iteration 339, Loss: 0.571604311466217
Epoch 11, Iteration 340, Loss: 0.5827687382698059
Epoch 11, Iteration 341, Loss: 0.6270954608917236
Epoch 11, Iteration 342, Loss: 0.6360418796539307
Epoch 11, Iteration 343, Loss: 0.5646310448646545
Epoch 11, Iteration 344, Loss: 0.6880703568458557
Epoch 11, Iteration 345, Loss: 0.7217388153076172
Epoch 11, Iteration 346, Loss: 0.5983189940452576
Epoch 11, Iteration 347, Loss: 0.5801386833190918
Epoch 11, Iteration 348, Loss: 0.5331215262413025
Epoch 11, Iteration 349, Loss: 0.6848185658454895
Epoch 11, Iteration 350, Loss: 0.5643404722213745
Epoch 11, Iteration 350, Valid Loss: 0.5931947231292725
Epoch 11, Iteration 351, Loss: 0.7489498257637024
Epoch 11, Iteration 352, Loss: 0.6710003018379211
Epoch 11, Iteration 353, Loss: 0.7125159502029419
Epoch 11, Iteration 354, Loss: 0.6138120293617249
Epoch 11, Iteration 355, Loss: 0.6622584462165833
Epoch 11, Iteration 356, Loss: 0.6305034756660461
Epoch 11, Iteration 357, Loss: 0.6317611336708069
Epoch 11, Iteration 358, Loss: 0.6196842789649963
Epoch 11, Iteration 359, Loss: 0.5871303677558899
Epoch 11, Iteration 360, Loss: 0.5740067958831787
Epoch 11, Iteration 361, Loss: 0.7250677347183228
Epoch 11, Iteration 362, Loss: 0.6824941039085388
Epoch 11, Iteration 363, Loss: 0.54549241065979
Epoch 11, Iteration 364, Loss: 0.695803165435791
Epoch 11, Iteration 365, Loss: 0.7150589227676392
Epoch 11, Iteration 366, Loss: 0.7291958928108215
Epoch 11, Iteration 367, Loss: 0.6595176458358765
Epoch 11, Iteration 368, Loss: 0.653155505657196
Epoch 11, Iteration 369, Loss: 0.6384285092353821
Epoch 11, Iteration 370, Loss: 0.6439762711524963
Epoch 11, Iteration 371, Loss: 0.6569918990135193
Epoch 11, Iteration 372, Loss: 0.6582575440406799
Epoch 11, Iteration 373, Loss: 0.6460606455802917
Epoch 11, Iteration 374, Loss: 0.7778922915458679
Epoch 11, Iteration 375, Loss: 0.6334316730499268
Epoch 11, Iteration 376, Loss: 0.7363283634185791
Epoch 11, Iteration 377, Loss: 0.6645421981811523
Epoch 11, Iteration 378, Loss: 0.583436131477356
Epoch 11, Iteration 379, Loss: 0.6230367422103882
Epoch 11, Iteration 380, Loss: 0.5789002180099487
Epoch 11, Iteration 381, Loss: 0.5322118401527405
Epoch 11, Iteration 382, Loss: 0.42642709612846375
Epoch 11, Iteration 383, Loss: 0.5685510039329529
Epoch 11, Iteration 384, Loss: 0.6603624820709229
Epoch 11, Iteration 385, Loss: 0.6078435778617859
Epoch 11, Iteration 386, Loss: 0.5949740409851074
Epoch 11, Iteration 387, Loss: 0.8615894913673401
Epoch 11, Iteration 388, Loss: 0.5877623558044434
Epoch 11, Iteration 389, Loss: 0.6272212862968445
Epoch 11, Iteration 390, Loss: 0.787345826625824
Epoch 11, Iteration 391, Loss: 0.556693971157074
Epoch 12/15, Loss: 0.7037499479341264
Epoch 12, Iteration 0, Loss: 0.6568449139595032
Epoch 12, Iteration 1, Loss: 0.8363197445869446
Epoch 12, Iteration 2, Loss: 0.6939600706100464
Epoch 12, Iteration 3, Loss: 0.7406249046325684
Epoch 12, Iteration 4, Loss: 0.6112293601036072
Epoch 12, Iteration 5, Loss: 0.7106138467788696
Epoch 12, Iteration 6, Loss: 0.5636447668075562
Epoch 12, Iteration 7, Loss: 0.6249756217002869
Epoch 12, Iteration 8, Loss: 0.6454930305480957
Epoch 12, Iteration 9, Loss: 0.7738726139068604
Epoch 12, Iteration 10, Loss: 0.8000297546386719
Epoch 12, Iteration 11, Loss: 0.6427926421165466
Epoch 12, Iteration 12, Loss: 0.6537761092185974
Epoch 12, Iteration 13, Loss: 0.7890626192092896
Epoch 12, Iteration 14, Loss: 0.629528820514679
Epoch 12, Iteration 15, Loss: 0.8550499081611633
Epoch 12, Iteration 16, Loss: 0.7084514498710632
Epoch 12, Iteration 17, Loss: 0.7026957869529724
Epoch 12, Iteration 18, Loss: 0.6070947051048279
Epoch 12, Iteration 19, Loss: 0.6933379173278809
Epoch 12, Iteration 20, Loss: 0.5957990884780884
Epoch 12, Iteration 21, Loss: 0.5882430672645569
Epoch 12, Iteration 22, Loss: 0.5802764296531677
Epoch 12, Iteration 23, Loss: 0.7259061932563782
Epoch 12, Iteration 24, Loss: 0.6018282771110535
Epoch 12, Iteration 25, Loss: 0.6287326216697693
Epoch 12, Iteration 26, Loss: 0.8585256934165955
Epoch 12, Iteration 27, Loss: 0.6512879729270935
Epoch 12, Iteration 28, Loss: 0.7434072494506836
Epoch 12, Iteration 29, Loss: 0.9457018971443176
Epoch 12, Iteration 30, Loss: 0.6782987117767334
Epoch 12, Iteration 31, Loss: 0.6574193239212036
Epoch 12, Iteration 32, Loss: 0.6569512486457825
Epoch 12, Iteration 33, Loss: 0.7287991642951965
Epoch 12, Iteration 34, Loss: 0.6434160470962524
Epoch 12, Iteration 35, Loss: 0.7559475302696228
Epoch 12, Iteration 36, Loss: 0.520266592502594
Epoch 12, Iteration 37, Loss: 0.5748946666717529
Epoch 12, Iteration 38, Loss: 0.5849885940551758
Epoch 12, Iteration 39, Loss: 0.68671053647995
Epoch 12, Iteration 40, Loss: 0.6592679023742676
Epoch 12, Iteration 41, Loss: 0.8109171986579895
Epoch 12, Iteration 42, Loss: 0.6601985692977905
Epoch 12, Iteration 43, Loss: 0.6227901577949524
Epoch 12, Iteration 44, Loss: 0.6272566914558411
Epoch 12, Iteration 45, Loss: 0.6960394978523254
Epoch 12, Iteration 46, Loss: 0.6591695547103882
Epoch 12, Iteration 47, Loss: 0.6766395568847656
Epoch 12, Iteration 48, Loss: 0.816263735294342
Epoch 12, Iteration 49, Loss: 0.6474166512489319
Epoch 12, Iteration 50, Loss: 0.6094987392425537
Epoch 12, Iteration 50, Valid Loss: 0.563954770565033
Epoch 12, Iteration 51, Loss: 0.6565244197845459
Epoch 12, Iteration 52, Loss: 0.5281943082809448
Epoch 12, Iteration 53, Loss: 0.7587250471115112
Epoch 12, Iteration 54, Loss: 0.780896782875061
Epoch 12, Iteration 55, Loss: 0.7491088509559631
Epoch 12, Iteration 56, Loss: 0.7986566424369812
Epoch 12, Iteration 57, Loss: 0.9435210824012756
Epoch 12, Iteration 58, Loss: 0.6520014405250549
Epoch 12, Iteration 59, Loss: 0.7157620787620544
Epoch 12, Iteration 60, Loss: 0.6735484004020691
Epoch 12, Iteration 61, Loss: 0.5055965185165405
Epoch 12, Iteration 62, Loss: 0.7518361806869507
Epoch 12, Iteration 63, Loss: 0.5689620971679688
Epoch 12, Iteration 64, Loss: 0.8421869874000549
Epoch 12, Iteration 65, Loss: 0.8406164646148682
Epoch 12, Iteration 66, Loss: 0.6962373852729797
Epoch 12, Iteration 67, Loss: 0.571485698223114
Epoch 12, Iteration 68, Loss: 0.899570643901825
Epoch 12, Iteration 69, Loss: 0.6802021265029907
Epoch 12, Iteration 70, Loss: 0.6841991543769836
Epoch 12, Iteration 71, Loss: 0.6993293762207031
Epoch 12, Iteration 72, Loss: 0.6266782283782959
Epoch 12, Iteration 73, Loss: 0.6834997534751892
Epoch 12, Iteration 74, Loss: 0.8830878734588623
Epoch 12, Iteration 75, Loss: 0.6842490434646606
Epoch 12, Iteration 76, Loss: 0.6975783705711365
Epoch 12, Iteration 77, Loss: 0.6040670871734619
Epoch 12, Iteration 78, Loss: 0.8781250715255737
Epoch 12, Iteration 79, Loss: 0.9807980060577393
Epoch 12, Iteration 80, Loss: 0.5059412717819214
Epoch 12, Iteration 81, Loss: 0.8432604670524597
Epoch 12, Iteration 82, Loss: 0.7260284423828125
Epoch 12, Iteration 83, Loss: 0.6457304954528809
Epoch 12, Iteration 84, Loss: 0.7079273462295532
Epoch 12, Iteration 85, Loss: 0.7358682751655579
Epoch 12, Iteration 86, Loss: 0.728297770023346
Epoch 12, Iteration 87, Loss: 0.6911118030548096
Epoch 12, Iteration 88, Loss: 0.8331302404403687
Epoch 12, Iteration 89, Loss: 0.7691755294799805
Epoch 12, Iteration 90, Loss: 0.571700930595398
Epoch 12, Iteration 91, Loss: 0.6095790863037109
Epoch 12, Iteration 92, Loss: 0.5775955319404602
Epoch 12, Iteration 93, Loss: 0.6707546710968018
Epoch 12, Iteration 94, Loss: 0.796422004699707
Epoch 12, Iteration 95, Loss: 0.5224534869194031
Epoch 12, Iteration 96, Loss: 0.708753228187561
Epoch 12, Iteration 97, Loss: 0.5089064240455627
Epoch 12, Iteration 98, Loss: 0.6705639958381653
Epoch 12, Iteration 99, Loss: 0.6046554446220398
Epoch 12, Iteration 100, Loss: 0.6060697436332703
Epoch 12, Iteration 100, Valid Loss: 0.6063714027404785
Epoch 12, Iteration 101, Loss: 0.8856608867645264
Epoch 12, Iteration 102, Loss: 0.748146116733551
Epoch 12, Iteration 103, Loss: 0.6142606735229492
Epoch 12, Iteration 104, Loss: 0.8580472469329834
Epoch 12, Iteration 105, Loss: 0.678429365158081
Epoch 12, Iteration 106, Loss: 0.6253702044487
Epoch 12, Iteration 107, Loss: 0.7625164985656738
Epoch 12, Iteration 108, Loss: 0.6941782236099243
Epoch 12, Iteration 109, Loss: 0.8000922799110413
Epoch 12, Iteration 110, Loss: 0.997535228729248
Epoch 12, Iteration 111, Loss: 0.7261545062065125
Epoch 12, Iteration 112, Loss: 0.6955594420433044
Epoch 12, Iteration 113, Loss: 0.6432717442512512
Epoch 12, Iteration 114, Loss: 0.8935322165489197
Epoch 12, Iteration 115, Loss: 0.8152012825012207
Epoch 12, Iteration 116, Loss: 0.565895676612854
Epoch 12, Iteration 117, Loss: 0.4386434555053711
Epoch 12, Iteration 118, Loss: 0.7434420585632324
Epoch 12, Iteration 119, Loss: 0.6413832902908325
Epoch 12, Iteration 120, Loss: 0.969093382358551
Epoch 12, Iteration 121, Loss: 0.6549428105354309
Epoch 12, Iteration 122, Loss: 0.7277224659919739
Epoch 12, Iteration 123, Loss: 0.6684926152229309
Epoch 12, Iteration 124, Loss: 0.5910991430282593
Epoch 12, Iteration 125, Loss: 0.705163300037384
Epoch 12, Iteration 126, Loss: 0.5926144123077393
Epoch 12, Iteration 127, Loss: 0.7386837005615234
Epoch 12, Iteration 128, Loss: 0.7567336559295654
Epoch 12, Iteration 129, Loss: 0.7389841675758362
Epoch 12, Iteration 130, Loss: 0.8871852159500122
Epoch 12, Iteration 131, Loss: 0.6655809879302979
Epoch 12, Iteration 132, Loss: 0.5964688658714294
Epoch 12, Iteration 133, Loss: 0.9687548279762268
Epoch 12, Iteration 134, Loss: 0.9597373604774475
Epoch 12, Iteration 135, Loss: 0.8521321415901184
Epoch 12, Iteration 136, Loss: 0.6660590767860413
Epoch 12, Iteration 137, Loss: 0.6042613983154297
Epoch 12, Iteration 138, Loss: 0.5834081172943115
Epoch 12, Iteration 139, Loss: 0.6738053560256958
Epoch 12, Iteration 140, Loss: 0.5924054384231567
Epoch 12, Iteration 141, Loss: 1.0954848527908325
Epoch 12, Iteration 142, Loss: 0.6713139414787292
Epoch 12, Iteration 143, Loss: 0.7075262665748596
Epoch 12, Iteration 144, Loss: 0.6683007478713989
Epoch 12, Iteration 145, Loss: 0.7556949853897095
Epoch 12, Iteration 146, Loss: 0.7553350329399109
Epoch 12, Iteration 147, Loss: 0.6976659893989563
Epoch 12, Iteration 148, Loss: 0.5887616872787476
Epoch 12, Iteration 149, Loss: 0.7087492942810059
Epoch 12, Iteration 150, Loss: 0.7918977737426758
Epoch 12, Iteration 150, Valid Loss: 0.5703864097595215
Epoch 12, Iteration 151, Loss: 0.7726421356201172
Epoch 12, Iteration 152, Loss: 0.5964515805244446
Epoch 12, Iteration 153, Loss: 0.7313245534896851
Epoch 12, Iteration 154, Loss: 0.48841890692710876
Epoch 12, Iteration 155, Loss: 0.7229663133621216
Epoch 12, Iteration 156, Loss: 0.6014457941055298
Epoch 12, Iteration 157, Loss: 1.0660055875778198
Epoch 12, Iteration 158, Loss: 0.6798323392868042
Epoch 12, Iteration 159, Loss: 0.7526645064353943
Epoch 12, Iteration 160, Loss: 0.6885420083999634
Epoch 12, Iteration 161, Loss: 0.686445951461792
Epoch 12, Iteration 162, Loss: 0.8842900395393372
Epoch 12, Iteration 163, Loss: 0.58762526512146
Epoch 12, Iteration 164, Loss: 0.6145221590995789
Epoch 12, Iteration 165, Loss: 0.7226593494415283
Epoch 12, Iteration 166, Loss: 0.8252316117286682
Epoch 12, Iteration 167, Loss: 0.8111972808837891
Epoch 12, Iteration 168, Loss: 0.706730306148529
Epoch 12, Iteration 169, Loss: 0.650056004524231
Epoch 12, Iteration 170, Loss: 0.7392907738685608
Epoch 12, Iteration 171, Loss: 0.796772301197052
Epoch 12, Iteration 172, Loss: 0.7585059404373169
Epoch 12, Iteration 173, Loss: 0.6386265158653259
Epoch 12, Iteration 174, Loss: 0.7307277321815491
Epoch 12, Iteration 175, Loss: 0.7716832160949707
Epoch 12, Iteration 176, Loss: 0.4602678418159485
Epoch 12, Iteration 177, Loss: 0.9706681370735168
Epoch 12, Iteration 178, Loss: 0.8278900384902954
Epoch 12, Iteration 179, Loss: 0.676292896270752
Epoch 12, Iteration 180, Loss: 0.7914326786994934
Epoch 12, Iteration 181, Loss: 0.7538164854049683
Epoch 12, Iteration 182, Loss: 0.7207825183868408
Epoch 12, Iteration 183, Loss: 0.6469002962112427
Epoch 12, Iteration 184, Loss: 0.6337469220161438
Epoch 12, Iteration 185, Loss: 0.5959925651550293
Epoch 12, Iteration 186, Loss: 0.7088250517845154
Epoch 12, Iteration 187, Loss: 0.6467636227607727
Epoch 12, Iteration 188, Loss: 0.5925607681274414
Epoch 12, Iteration 189, Loss: 0.5739235877990723
Epoch 12, Iteration 190, Loss: 0.7374098300933838
Epoch 12, Iteration 191, Loss: 0.6971942782402039
Epoch 12, Iteration 192, Loss: 0.7413206696510315
Epoch 12, Iteration 193, Loss: 0.6942657232284546
Epoch 12, Iteration 194, Loss: 0.6878374814987183
Epoch 12, Iteration 195, Loss: 0.7424432039260864
Epoch 12, Iteration 196, Loss: 0.6616921424865723
Epoch 12, Iteration 197, Loss: 0.6250096559524536
Epoch 12, Iteration 198, Loss: 0.6376633644104004
Epoch 12, Iteration 199, Loss: 0.6936150193214417
Epoch 12, Iteration 200, Loss: 0.6370465755462646
Epoch 12, Iteration 200, Valid Loss: 0.5690305829048157
Epoch 12, Iteration 201, Loss: 0.6837143301963806
Epoch 12, Iteration 202, Loss: 0.7606225609779358
Epoch 12, Iteration 203, Loss: 0.5787481069564819
Epoch 12, Iteration 204, Loss: 0.6492983102798462
Epoch 12, Iteration 205, Loss: 0.4708752930164337
Epoch 12, Iteration 206, Loss: 0.5557031035423279
Epoch 12, Iteration 207, Loss: 0.8978127241134644
Epoch 12, Iteration 208, Loss: 0.999801754951477
Epoch 12, Iteration 209, Loss: 0.8413430452346802
Epoch 12, Iteration 210, Loss: 0.9724841117858887
Epoch 12, Iteration 211, Loss: 1.0207953453063965
Epoch 12, Iteration 212, Loss: 0.789883017539978
Epoch 12, Iteration 213, Loss: 0.9093247652053833
Epoch 12, Iteration 214, Loss: 0.7432177662849426
Epoch 12, Iteration 215, Loss: 0.5946078300476074
Epoch 12, Iteration 216, Loss: 0.5530593991279602
Epoch 12, Iteration 217, Loss: 0.577680230140686
Epoch 12, Iteration 218, Loss: 0.6331996321678162
Epoch 12, Iteration 219, Loss: 0.7227334976196289
Epoch 12, Iteration 220, Loss: 0.606448233127594
Epoch 12, Iteration 221, Loss: 0.628558337688446
Epoch 12, Iteration 222, Loss: 0.6845555901527405
Epoch 12, Iteration 223, Loss: 0.6538602709770203
Epoch 12, Iteration 224, Loss: 0.5873462557792664
Epoch 12, Iteration 225, Loss: 0.7254934310913086
Epoch 12, Iteration 226, Loss: 0.7594764828681946
Epoch 12, Iteration 227, Loss: 0.7750918865203857
Epoch 12, Iteration 228, Loss: 0.715153694152832
Epoch 12, Iteration 229, Loss: 0.667525589466095
Epoch 12, Iteration 230, Loss: 0.7703633308410645
Epoch 12, Iteration 231, Loss: 0.6601042747497559
Epoch 12, Iteration 232, Loss: 0.7759727239608765
Epoch 12, Iteration 233, Loss: 0.9126551151275635
Epoch 12, Iteration 234, Loss: 0.7199811339378357
Epoch 12, Iteration 235, Loss: 0.5557383298873901
Epoch 12, Iteration 236, Loss: 0.734442949295044
Epoch 12, Iteration 237, Loss: 0.6448183059692383
Epoch 12, Iteration 238, Loss: 0.5656598806381226
Epoch 12, Iteration 239, Loss: 0.8552877902984619
Epoch 12, Iteration 240, Loss: 0.6980603337287903
Epoch 12, Iteration 241, Loss: 0.6050447225570679
Epoch 12, Iteration 242, Loss: 0.5921297073364258
Epoch 12, Iteration 243, Loss: 0.6841475963592529
Epoch 12, Iteration 244, Loss: 0.6367369890213013
Epoch 12, Iteration 245, Loss: 0.5076414346694946
Epoch 12, Iteration 246, Loss: 0.6375519037246704
Epoch 12, Iteration 247, Loss: 0.6253756880760193
Epoch 12, Iteration 248, Loss: 0.7796540856361389
Epoch 12, Iteration 249, Loss: 0.5495964884757996
Epoch 12, Iteration 250, Loss: 0.8081930875778198
Epoch 12, Iteration 250, Valid Loss: 0.5570864677429199
Epoch 12, Iteration 251, Loss: 0.6231285929679871
Epoch 12, Iteration 252, Loss: 0.8807200193405151
Epoch 12, Iteration 253, Loss: 0.7529283165931702
Epoch 12, Iteration 254, Loss: 0.7224769592285156
Epoch 12, Iteration 255, Loss: 0.7146294713020325
Epoch 12, Iteration 256, Loss: 0.8091220259666443
Epoch 12, Iteration 257, Loss: 0.5838637351989746
Epoch 12, Iteration 258, Loss: 0.8697358965873718
Epoch 12, Iteration 259, Loss: 0.5803751945495605
Epoch 12, Iteration 260, Loss: 0.8070374727249146
Epoch 12, Iteration 261, Loss: 0.6652547121047974
Epoch 12, Iteration 262, Loss: 0.6954692602157593
Epoch 12, Iteration 263, Loss: 0.8529732823371887
Epoch 12, Iteration 264, Loss: 0.6528290510177612
Epoch 12, Iteration 265, Loss: 0.7455966472625732
Epoch 12, Iteration 266, Loss: 0.7429371476173401
Epoch 12, Iteration 267, Loss: 0.6317426562309265
Epoch 12, Iteration 268, Loss: 0.612075686454773
Epoch 12, Iteration 269, Loss: 0.6068467497825623
Epoch 12, Iteration 270, Loss: 0.6601804494857788
Epoch 12, Iteration 271, Loss: 0.6480994820594788
Epoch 12, Iteration 272, Loss: 0.6568427681922913
Epoch 12, Iteration 273, Loss: 0.7169361710548401
Epoch 12, Iteration 274, Loss: 0.6717016100883484
Epoch 12, Iteration 275, Loss: 0.685418426990509
Epoch 12, Iteration 276, Loss: 0.6204932332038879
Epoch 12, Iteration 277, Loss: 0.6174273490905762
Epoch 12, Iteration 278, Loss: 0.8457092046737671
Epoch 12, Iteration 279, Loss: 0.6185529232025146
Epoch 12, Iteration 280, Loss: 0.5341119170188904
Epoch 12, Iteration 281, Loss: 0.6238279342651367
Epoch 12, Iteration 282, Loss: 0.5895655155181885
Epoch 12, Iteration 283, Loss: 0.6020956635475159
Epoch 12, Iteration 284, Loss: 0.6760523319244385
Epoch 12, Iteration 285, Loss: 0.5873041152954102
Epoch 12, Iteration 286, Loss: 0.650956928730011
Epoch 12, Iteration 287, Loss: 0.6200615763664246
Epoch 12, Iteration 288, Loss: 0.5651206970214844
Epoch 12, Iteration 289, Loss: 0.5838401913642883
Epoch 12, Iteration 290, Loss: 0.6087412238121033
Epoch 12, Iteration 291, Loss: 0.4839743673801422
Epoch 12, Iteration 292, Loss: 0.7084386348724365
Epoch 12, Iteration 293, Loss: 0.8436455726623535
Epoch 12, Iteration 294, Loss: 0.5915015339851379
Epoch 12, Iteration 295, Loss: 0.5725932717323303
Epoch 12, Iteration 296, Loss: 0.7548174262046814
Epoch 12, Iteration 297, Loss: 0.7328206896781921
Epoch 12, Iteration 298, Loss: 0.7239324450492859
Epoch 12, Iteration 299, Loss: 0.6120402216911316
Epoch 12, Iteration 300, Loss: 0.5409374833106995
Epoch 12, Iteration 300, Valid Loss: 0.5632458329200745
Epoch 12, Iteration 301, Loss: 0.621661901473999
Epoch 12, Iteration 302, Loss: 0.71905118227005
Epoch 12, Iteration 303, Loss: 0.7211012840270996
Epoch 12, Iteration 304, Loss: 0.688876748085022
Epoch 12, Iteration 305, Loss: 0.6217105388641357
Epoch 12, Iteration 306, Loss: 0.6494845151901245
Epoch 12, Iteration 307, Loss: 0.5677961707115173
Epoch 12, Iteration 308, Loss: 0.6185854077339172
Epoch 12, Iteration 309, Loss: 0.6474722623825073
Epoch 12, Iteration 310, Loss: 0.7559255957603455
Epoch 12, Iteration 311, Loss: 0.6249098777770996
Epoch 12, Iteration 312, Loss: 0.5213160514831543
Epoch 12, Iteration 313, Loss: 0.7171804308891296
Epoch 12, Iteration 314, Loss: 0.5642819404602051
Epoch 12, Iteration 315, Loss: 0.5938356518745422
Epoch 12, Iteration 316, Loss: 0.8674930334091187
Epoch 12, Iteration 317, Loss: 0.6228688359260559
Epoch 12, Iteration 318, Loss: 0.6613593101501465
Epoch 12, Iteration 319, Loss: 0.7291938662528992
Epoch 12, Iteration 320, Loss: 0.48885679244995117
Epoch 12, Iteration 321, Loss: 0.6275345683097839
Epoch 12, Iteration 322, Loss: 0.6997523903846741
Epoch 12, Iteration 323, Loss: 0.7474322319030762
Epoch 12, Iteration 324, Loss: 0.7203731536865234
Epoch 12, Iteration 325, Loss: 0.6890019774436951
Epoch 12, Iteration 326, Loss: 0.5753498077392578
Epoch 12, Iteration 327, Loss: 0.5635858178138733
Epoch 12, Iteration 328, Loss: 0.6164000630378723
Epoch 12, Iteration 329, Loss: 0.7439813613891602
Epoch 12, Iteration 330, Loss: 0.660763144493103
Epoch 12, Iteration 331, Loss: 0.6519826054573059
Epoch 12, Iteration 332, Loss: 0.6265028715133667
Epoch 12, Iteration 333, Loss: 0.8475507497787476
Epoch 12, Iteration 334, Loss: 0.6307401657104492
Epoch 12, Iteration 335, Loss: 0.6913691163063049
Epoch 12, Iteration 336, Loss: 0.696003258228302
Epoch 12, Iteration 337, Loss: 0.5770631432533264
Epoch 12, Iteration 338, Loss: 0.7392356991767883
Epoch 12, Iteration 339, Loss: 0.649958610534668
Epoch 12, Iteration 340, Loss: 0.5495017766952515
Epoch 12, Iteration 341, Loss: 0.6343424916267395
Epoch 12, Iteration 342, Loss: 0.5970922708511353
Epoch 12, Iteration 343, Loss: 0.5644674897193909
Epoch 12, Iteration 344, Loss: 0.6338986754417419
Epoch 12, Iteration 345, Loss: 0.6702957153320312
Epoch 12, Iteration 346, Loss: 0.6458607912063599
Epoch 12, Iteration 347, Loss: 0.47897201776504517
Epoch 12, Iteration 348, Loss: 0.606807291507721
Epoch 12, Iteration 349, Loss: 0.6542986035346985
Epoch 12, Iteration 350, Loss: 0.592146635055542
Epoch 12, Iteration 350, Valid Loss: 0.5827816724777222
Epoch 12, Iteration 351, Loss: 0.6612714529037476
Epoch 12, Iteration 352, Loss: 0.5448667407035828
Epoch 12, Iteration 353, Loss: 0.670574426651001
Epoch 12, Iteration 354, Loss: 0.5367546677589417
Epoch 12, Iteration 355, Loss: 0.5937746167182922
Epoch 12, Iteration 356, Loss: 0.7581662535667419
Epoch 12, Iteration 357, Loss: 0.5133867859840393
Epoch 12, Iteration 358, Loss: 0.5833415389060974
Epoch 12, Iteration 359, Loss: 0.6494256258010864
Epoch 12, Iteration 360, Loss: 0.5523288249969482
Epoch 12, Iteration 361, Loss: 0.5583425164222717
Epoch 12, Iteration 362, Loss: 0.6068685054779053
Epoch 12, Iteration 363, Loss: 0.5048115849494934
Epoch 12, Iteration 364, Loss: 0.6159924864768982
Epoch 12, Iteration 365, Loss: 0.7303284406661987
Epoch 12, Iteration 366, Loss: 0.6977646350860596
Epoch 12, Iteration 367, Loss: 0.6605126857757568
Epoch 12, Iteration 368, Loss: 0.6227686405181885
Epoch 12, Iteration 369, Loss: 0.673995852470398
Epoch 12, Iteration 370, Loss: 0.6440722942352295
Epoch 12, Iteration 371, Loss: 0.6991247534751892
Epoch 12, Iteration 372, Loss: 0.5909242630004883
Epoch 12, Iteration 373, Loss: 0.6207037568092346
Epoch 12, Iteration 374, Loss: 0.8465309143066406
Epoch 12, Iteration 375, Loss: 0.5937764048576355
Epoch 12, Iteration 376, Loss: 0.7147645950317383
Epoch 12, Iteration 377, Loss: 0.5935096740722656
Epoch 12, Iteration 378, Loss: 0.598800003528595
Epoch 12, Iteration 379, Loss: 0.529411256313324
Epoch 12, Iteration 380, Loss: 0.5322306156158447
Epoch 12, Iteration 381, Loss: 0.5625734329223633
Epoch 12, Iteration 382, Loss: 0.4410038888454437
Epoch 12, Iteration 383, Loss: 0.592435359954834
Epoch 12, Iteration 384, Loss: 0.6555529236793518
Epoch 12, Iteration 385, Loss: 0.5720872282981873
Epoch 12, Iteration 386, Loss: 0.5232383608818054
Epoch 12, Iteration 387, Loss: 0.9207006692886353
Epoch 12, Iteration 388, Loss: 0.6442392468452454
Epoch 12, Iteration 389, Loss: 0.6525067090988159
Epoch 12, Iteration 390, Loss: 0.8506009578704834
Epoch 12, Iteration 391, Loss: 0.5534098148345947
Epoch 13/15, Loss: 0.684108627541941
Epoch 13, Iteration 0, Loss: 0.7541003823280334
Epoch 13, Iteration 1, Loss: 0.783478856086731
Epoch 13, Iteration 2, Loss: 0.7238490581512451
Epoch 13, Iteration 3, Loss: 0.7502067685127258
Epoch 13, Iteration 4, Loss: 0.6422494649887085
Epoch 13, Iteration 5, Loss: 0.67867511510849
Epoch 13, Iteration 6, Loss: 0.5702010989189148
Epoch 13, Iteration 7, Loss: 0.6090347766876221
Epoch 13, Iteration 8, Loss: 0.6087172031402588
Epoch 13, Iteration 9, Loss: 0.7656405568122864
Epoch 13, Iteration 10, Loss: 0.6494601964950562
Epoch 13, Iteration 11, Loss: 0.6146577000617981
Epoch 13, Iteration 12, Loss: 0.5765826106071472
Epoch 13, Iteration 13, Loss: 0.6959236860275269
Epoch 13, Iteration 14, Loss: 0.6492409706115723
Epoch 13, Iteration 15, Loss: 0.8547345995903015
Epoch 13, Iteration 16, Loss: 0.6393290162086487
Epoch 13, Iteration 17, Loss: 0.6324170231819153
Epoch 13, Iteration 18, Loss: 0.5769569277763367
Epoch 13, Iteration 19, Loss: 0.7111883759498596
Epoch 13, Iteration 20, Loss: 0.5435076355934143
Epoch 13, Iteration 21, Loss: 0.5327888131141663
Epoch 13, Iteration 22, Loss: 0.6851834654808044
Epoch 13, Iteration 23, Loss: 0.7486628890037537
Epoch 13, Iteration 24, Loss: 0.5313237309455872
Epoch 13, Iteration 25, Loss: 0.5879163146018982
Epoch 13, Iteration 26, Loss: 0.8286535739898682
Epoch 13, Iteration 27, Loss: 0.721372663974762
Epoch 13, Iteration 28, Loss: 0.6661770939826965
Epoch 13, Iteration 29, Loss: 0.9194177985191345
Epoch 13, Iteration 30, Loss: 0.6865109801292419
Epoch 13, Iteration 31, Loss: 0.6843438744544983
Epoch 13, Iteration 32, Loss: 0.7301740646362305
Epoch 13, Iteration 33, Loss: 0.7438278794288635
Epoch 13, Iteration 34, Loss: 0.6451728940010071
Epoch 13, Iteration 35, Loss: 0.7180076837539673
Epoch 13, Iteration 36, Loss: 0.6137770414352417
Epoch 13, Iteration 37, Loss: 0.5490659475326538
Epoch 13, Iteration 38, Loss: 0.599900484085083
Epoch 13, Iteration 39, Loss: 0.6683241724967957
Epoch 13, Iteration 40, Loss: 0.6499337553977966
Epoch 13, Iteration 41, Loss: 0.6423831582069397
Epoch 13, Iteration 42, Loss: 0.6735366582870483
Epoch 13, Iteration 43, Loss: 0.6329287886619568
Epoch 13, Iteration 44, Loss: 0.6158536672592163
Epoch 13, Iteration 45, Loss: 0.6671175956726074
Epoch 13, Iteration 46, Loss: 0.7408674955368042
Epoch 13, Iteration 47, Loss: 0.6664164066314697
Epoch 13, Iteration 48, Loss: 0.7449133992195129
Epoch 13, Iteration 49, Loss: 0.6372542381286621
Epoch 13, Iteration 50, Loss: 0.5818886756896973
Epoch 13, Iteration 50, Valid Loss: 0.5560964345932007
Epoch 13, Iteration 51, Loss: 0.6562507152557373
Epoch 13, Iteration 52, Loss: 0.526974618434906
Epoch 13, Iteration 53, Loss: 0.7882638573646545
Epoch 13, Iteration 54, Loss: 0.7874255180358887
Epoch 13, Iteration 55, Loss: 0.7323215007781982
Epoch 13, Iteration 56, Loss: 0.6586768627166748
Epoch 13, Iteration 57, Loss: 0.868309736251831
Epoch 13, Iteration 58, Loss: 0.6994621157646179
Epoch 13, Iteration 59, Loss: 0.5581678152084351
Epoch 13, Iteration 60, Loss: 0.7153843641281128
Epoch 13, Iteration 61, Loss: 0.4522961974143982
Epoch 13, Iteration 62, Loss: 0.5858079195022583
Epoch 13, Iteration 63, Loss: 0.5750867128372192
Epoch 13, Iteration 64, Loss: 0.7118102312088013
Epoch 13, Iteration 65, Loss: 0.9629397988319397
Epoch 13, Iteration 66, Loss: 0.6497518420219421
Epoch 13, Iteration 67, Loss: 0.5441535115242004
Epoch 13, Iteration 68, Loss: 0.7706834673881531
Epoch 13, Iteration 69, Loss: 0.7151171565055847
Epoch 13, Iteration 70, Loss: 0.59974205493927
Epoch 13, Iteration 71, Loss: 0.7497006058692932
Epoch 13, Iteration 72, Loss: 0.6522829532623291
Epoch 13, Iteration 73, Loss: 0.6292251348495483
Epoch 13, Iteration 74, Loss: 0.9844247102737427
Epoch 13, Iteration 75, Loss: 0.7954590916633606
Epoch 13, Iteration 76, Loss: 0.6947998404502869
Epoch 13, Iteration 77, Loss: 0.5397679805755615
Epoch 13, Iteration 78, Loss: 0.7122768759727478
Epoch 13, Iteration 79, Loss: 0.8440039753913879
Epoch 13, Iteration 80, Loss: 0.6008031964302063
Epoch 13, Iteration 81, Loss: 0.787492573261261
Epoch 13, Iteration 82, Loss: 0.7382606267929077
Epoch 13, Iteration 83, Loss: 0.565271258354187
Epoch 13, Iteration 84, Loss: 0.8115630745887756
Epoch 13, Iteration 85, Loss: 0.6607409715652466
Epoch 13, Iteration 86, Loss: 0.7663052678108215
Epoch 13, Iteration 87, Loss: 0.7205507755279541
Epoch 13, Iteration 88, Loss: 0.6648988723754883
Epoch 13, Iteration 89, Loss: 0.7992281317710876
Epoch 13, Iteration 90, Loss: 0.7071175575256348
Epoch 13, Iteration 91, Loss: 0.6645991206169128
Epoch 13, Iteration 92, Loss: 0.5037364363670349
Epoch 13, Iteration 93, Loss: 0.6623037457466125
Epoch 13, Iteration 94, Loss: 0.7610778212547302
Epoch 13, Iteration 95, Loss: 0.5692229270935059
Epoch 13, Iteration 96, Loss: 0.6044310927391052
Epoch 13, Iteration 97, Loss: 0.5181778073310852
Epoch 13, Iteration 98, Loss: 0.6062214374542236
Epoch 13, Iteration 99, Loss: 0.6324540376663208
Epoch 13, Iteration 100, Loss: 0.6238957047462463
Epoch 13, Iteration 100, Valid Loss: 0.6154080033302307
Epoch 13, Iteration 101, Loss: 0.8368492722511292
Epoch 13, Iteration 102, Loss: 0.829004168510437
Epoch 13, Iteration 103, Loss: 0.6824560165405273
Epoch 13, Iteration 104, Loss: 0.8424863219261169
Epoch 13, Iteration 105, Loss: 0.5624220371246338
Epoch 13, Iteration 106, Loss: 0.5815985202789307
Epoch 13, Iteration 107, Loss: 0.8960512280464172
Epoch 13, Iteration 108, Loss: 0.6542640328407288
Epoch 13, Iteration 109, Loss: 0.7891736030578613
Epoch 13, Iteration 110, Loss: 0.8442267775535583
Epoch 13, Iteration 111, Loss: 0.8150606751441956
Epoch 13, Iteration 112, Loss: 0.7490018606185913
Epoch 13, Iteration 113, Loss: 0.6131221652030945
Epoch 13, Iteration 114, Loss: 0.7513152956962585
Epoch 13, Iteration 115, Loss: 0.8177233934402466
Epoch 13, Iteration 116, Loss: 0.5306409597396851
Epoch 13, Iteration 117, Loss: 0.44816723465919495
Epoch 13, Iteration 118, Loss: 0.7066409587860107
Epoch 13, Iteration 119, Loss: 0.6137055158615112
Epoch 13, Iteration 120, Loss: 0.9079365730285645
Epoch 13, Iteration 121, Loss: 0.5389054417610168
Epoch 13, Iteration 122, Loss: 0.723271369934082
Epoch 13, Iteration 123, Loss: 0.7076818346977234
Epoch 13, Iteration 124, Loss: 0.5592305660247803
Epoch 13, Iteration 125, Loss: 0.7922192215919495
Epoch 13, Iteration 126, Loss: 0.5883603096008301
Epoch 13, Iteration 127, Loss: 0.7508406043052673
Epoch 13, Iteration 128, Loss: 0.6401393413543701
Epoch 13, Iteration 129, Loss: 0.7533931732177734
Epoch 13, Iteration 130, Loss: 0.8719950914382935
Epoch 13, Iteration 131, Loss: 0.570587694644928
Epoch 13, Iteration 132, Loss: 0.6623926758766174
Epoch 13, Iteration 133, Loss: 1.0167325735092163
Epoch 13, Iteration 134, Loss: 0.7704488039016724
Epoch 13, Iteration 135, Loss: 0.7834851741790771
Epoch 13, Iteration 136, Loss: 0.6425802111625671
Epoch 13, Iteration 137, Loss: 0.6069326996803284
Epoch 13, Iteration 138, Loss: 0.5687665343284607
Epoch 13, Iteration 139, Loss: 0.5773062109947205
Epoch 13, Iteration 140, Loss: 0.5783829092979431
Epoch 13, Iteration 141, Loss: 0.8599048256874084
Epoch 13, Iteration 142, Loss: 0.7549682855606079
Epoch 13, Iteration 143, Loss: 0.6474561095237732
Epoch 13, Iteration 144, Loss: 0.5916203260421753
Epoch 13, Iteration 145, Loss: 0.7156282663345337
Epoch 13, Iteration 146, Loss: 0.7230775952339172
Epoch 13, Iteration 147, Loss: 0.7079474925994873
Epoch 13, Iteration 148, Loss: 0.5656014680862427
Epoch 13, Iteration 149, Loss: 0.6895380020141602
Epoch 13, Iteration 150, Loss: 0.6728443503379822
Epoch 13, Iteration 150, Valid Loss: 0.5873237252235413
Epoch 13, Iteration 151, Loss: 0.6351728439331055
Epoch 13, Iteration 152, Loss: 0.5728060603141785
Epoch 13, Iteration 153, Loss: 0.775867223739624
Epoch 13, Iteration 154, Loss: 0.4732632339000702
Epoch 13, Iteration 155, Loss: 0.6163564324378967
Epoch 13, Iteration 156, Loss: 0.4872061014175415
Epoch 13, Iteration 157, Loss: 0.9976734519004822
Epoch 13, Iteration 158, Loss: 0.6617039442062378
Epoch 13, Iteration 159, Loss: 0.787868320941925
Epoch 13, Iteration 160, Loss: 0.6608572006225586
Epoch 13, Iteration 161, Loss: 0.6384433507919312
Epoch 13, Iteration 162, Loss: 0.8547366261482239
Epoch 13, Iteration 163, Loss: 0.5074565410614014
Epoch 13, Iteration 164, Loss: 0.5984549522399902
Epoch 13, Iteration 165, Loss: 0.6070508360862732
Epoch 13, Iteration 166, Loss: 0.6343818306922913
Epoch 13, Iteration 167, Loss: 0.732688307762146
Epoch 13, Iteration 168, Loss: 0.6967794299125671
Epoch 13, Iteration 169, Loss: 0.61832195520401
Epoch 13, Iteration 170, Loss: 0.8238261342048645
Epoch 13, Iteration 171, Loss: 0.8007268309593201
Epoch 13, Iteration 172, Loss: 0.6817476153373718
Epoch 13, Iteration 173, Loss: 0.6136518716812134
Epoch 13, Iteration 174, Loss: 0.7078288197517395
Epoch 13, Iteration 175, Loss: 0.7275047898292542
Epoch 13, Iteration 176, Loss: 0.4744356572628021
Epoch 13, Iteration 177, Loss: 1.0671250820159912
Epoch 13, Iteration 178, Loss: 0.8809389472007751
Epoch 13, Iteration 179, Loss: 0.6243717074394226
Epoch 13, Iteration 180, Loss: 0.6775081157684326
Epoch 13, Iteration 181, Loss: 0.7407517433166504
Epoch 13, Iteration 182, Loss: 0.7046541571617126
Epoch 13, Iteration 183, Loss: 0.5937149524688721
Epoch 13, Iteration 184, Loss: 0.6121909618377686
Epoch 13, Iteration 185, Loss: 0.5971145629882812
Epoch 13, Iteration 186, Loss: 0.6460646390914917
Epoch 13, Iteration 187, Loss: 0.5642869472503662
Epoch 13, Iteration 188, Loss: 0.619149386882782
Epoch 13, Iteration 189, Loss: 0.5810993313789368
Epoch 13, Iteration 190, Loss: 0.73895263671875
Epoch 13, Iteration 191, Loss: 0.6988013386726379
Epoch 13, Iteration 192, Loss: 0.6811591386795044
Epoch 13, Iteration 193, Loss: 0.7397804856300354
Epoch 13, Iteration 194, Loss: 0.6332772970199585
Epoch 13, Iteration 195, Loss: 0.5570693612098694
Epoch 13, Iteration 196, Loss: 0.7056151628494263
Epoch 13, Iteration 197, Loss: 0.6118237376213074
Epoch 13, Iteration 198, Loss: 0.6452517509460449
Epoch 13, Iteration 199, Loss: 0.7044764161109924
Epoch 13, Iteration 200, Loss: 0.5488788485527039
Epoch 13, Iteration 200, Valid Loss: 0.5695847868919373
Epoch 13, Iteration 201, Loss: 0.6758789420127869
Epoch 13, Iteration 202, Loss: 0.6968638300895691
Epoch 13, Iteration 203, Loss: 0.5795868039131165
Epoch 13, Iteration 204, Loss: 0.6678858399391174
Epoch 13, Iteration 205, Loss: 0.4858320653438568
Epoch 13, Iteration 206, Loss: 0.6744530200958252
Epoch 13, Iteration 207, Loss: 0.7605844736099243
Epoch 13, Iteration 208, Loss: 0.9807529449462891
Epoch 13, Iteration 209, Loss: 0.9328815340995789
Epoch 13, Iteration 210, Loss: 0.8611658215522766
Epoch 13, Iteration 211, Loss: 0.9245225787162781
Epoch 13, Iteration 212, Loss: 0.7676400542259216
Epoch 13, Iteration 213, Loss: 0.9141910076141357
Epoch 13, Iteration 214, Loss: 0.6467384099960327
Epoch 13, Iteration 215, Loss: 0.6123660206794739
Epoch 13, Iteration 216, Loss: 0.5324030518531799
Epoch 13, Iteration 217, Loss: 0.5807316899299622
Epoch 13, Iteration 218, Loss: 0.6334500312805176
Epoch 13, Iteration 219, Loss: 0.780245304107666
Epoch 13, Iteration 220, Loss: 0.6437515020370483
Epoch 13, Iteration 221, Loss: 0.6986150145530701
Epoch 13, Iteration 222, Loss: 0.7466448545455933
Epoch 13, Iteration 223, Loss: 0.5040305852890015
Epoch 13, Iteration 224, Loss: 0.5401215553283691
Epoch 13, Iteration 225, Loss: 0.7418670058250427
Epoch 13, Iteration 226, Loss: 0.760698676109314
Epoch 13, Iteration 227, Loss: 0.6345388293266296
Epoch 13, Iteration 228, Loss: 0.7022950053215027
Epoch 13, Iteration 229, Loss: 0.6047330498695374
Epoch 13, Iteration 230, Loss: 0.8091482520103455
Epoch 13, Iteration 231, Loss: 0.6328886151313782
Epoch 13, Iteration 232, Loss: 0.8165364861488342
Epoch 13, Iteration 233, Loss: 1.0576192140579224
Epoch 13, Iteration 234, Loss: 0.6914346814155579
Epoch 13, Iteration 235, Loss: 0.5203074812889099
Epoch 13, Iteration 236, Loss: 0.6695610880851746
Epoch 13, Iteration 237, Loss: 0.5994977355003357
Epoch 13, Iteration 238, Loss: 0.5840198397636414
Epoch 13, Iteration 239, Loss: 0.6806063652038574
Epoch 13, Iteration 240, Loss: 0.7116633653640747
Epoch 13, Iteration 241, Loss: 0.6646847128868103
Epoch 13, Iteration 242, Loss: 0.6214671730995178
Epoch 13, Iteration 243, Loss: 0.6909418702125549
Epoch 13, Iteration 244, Loss: 0.6153965592384338
Epoch 13, Iteration 245, Loss: 0.5088751912117004
Epoch 13, Iteration 246, Loss: 0.7483754754066467
Epoch 13, Iteration 247, Loss: 0.6015863418579102
Epoch 13, Iteration 248, Loss: 0.6290282607078552
Epoch 13, Iteration 249, Loss: 0.6048339605331421
Epoch 13, Iteration 250, Loss: 0.7910892367362976
Epoch 13, Iteration 250, Valid Loss: 0.5347652435302734
Epoch 13, Iteration 251, Loss: 0.5908398628234863
Epoch 13, Iteration 252, Loss: 0.7410852909088135
Epoch 13, Iteration 253, Loss: 0.7645257711410522
Epoch 13, Iteration 254, Loss: 0.7330166697502136
Epoch 13, Iteration 255, Loss: 0.7289804816246033
Epoch 13, Iteration 256, Loss: 0.7072290182113647
Epoch 13, Iteration 257, Loss: 0.5610490441322327
Epoch 13, Iteration 258, Loss: 0.8671119809150696
Epoch 13, Iteration 259, Loss: 0.595016598701477
Epoch 13, Iteration 260, Loss: 0.749261200428009
Epoch 13, Iteration 261, Loss: 0.707294762134552
Epoch 13, Iteration 262, Loss: 0.5703744888305664
Epoch 13, Iteration 263, Loss: 0.770214319229126
Epoch 13, Iteration 264, Loss: 0.6916406750679016
Epoch 13, Iteration 265, Loss: 0.6724513173103333
Epoch 13, Iteration 266, Loss: 0.7457991242408752
Epoch 13, Iteration 267, Loss: 0.6293697357177734
Epoch 13, Iteration 268, Loss: 0.6934030652046204
Epoch 13, Iteration 269, Loss: 0.6129976511001587
Epoch 13, Iteration 270, Loss: 0.6333630681037903
Epoch 13, Iteration 271, Loss: 0.6826475858688354
Epoch 13, Iteration 272, Loss: 0.6189637184143066
Epoch 13, Iteration 273, Loss: 0.7521273493766785
Epoch 13, Iteration 274, Loss: 0.653328537940979
Epoch 13, Iteration 275, Loss: 0.5675582885742188
Epoch 13, Iteration 276, Loss: 0.687293529510498
Epoch 13, Iteration 277, Loss: 0.6252930760383606
Epoch 13, Iteration 278, Loss: 0.833075225353241
Epoch 13, Iteration 279, Loss: 0.5777323842048645
Epoch 13, Iteration 280, Loss: 0.5078616738319397
Epoch 13, Iteration 281, Loss: 0.6338142156600952
Epoch 13, Iteration 282, Loss: 0.6405162811279297
Epoch 13, Iteration 283, Loss: 0.6930682063102722
Epoch 13, Iteration 284, Loss: 0.612786591053009
Epoch 13, Iteration 285, Loss: 0.6405318975448608
Epoch 13, Iteration 286, Loss: 0.765401303768158
Epoch 13, Iteration 287, Loss: 0.6150934100151062
Epoch 13, Iteration 288, Loss: 0.6038191318511963
Epoch 13, Iteration 289, Loss: 0.5971079468727112
Epoch 13, Iteration 290, Loss: 0.520405650138855
Epoch 13, Iteration 291, Loss: 0.5022106766700745
Epoch 13, Iteration 292, Loss: 0.6974437832832336
Epoch 13, Iteration 293, Loss: 0.7724741101264954
Epoch 13, Iteration 294, Loss: 0.5957705974578857
Epoch 13, Iteration 295, Loss: 0.5561946630477905
Epoch 13, Iteration 296, Loss: 0.7553843259811401
Epoch 13, Iteration 297, Loss: 0.7711932063102722
Epoch 13, Iteration 298, Loss: 0.645752489566803
Epoch 13, Iteration 299, Loss: 0.5873874425888062
Epoch 13, Iteration 300, Loss: 0.4908904731273651
Epoch 13, Iteration 300, Valid Loss: 0.5704235434532166
Epoch 13, Iteration 301, Loss: 0.5666582584381104
Epoch 13, Iteration 302, Loss: 0.5472562313079834
Epoch 13, Iteration 303, Loss: 0.754734218120575
Epoch 13, Iteration 304, Loss: 0.58228999376297
Epoch 13, Iteration 305, Loss: 0.6257962584495544
Epoch 13, Iteration 306, Loss: 0.5940038561820984
Epoch 13, Iteration 307, Loss: 0.5864546298980713
Epoch 13, Iteration 308, Loss: 0.5858905911445618
Epoch 13, Iteration 309, Loss: 0.653976321220398
Epoch 13, Iteration 310, Loss: 0.7096396088600159
Epoch 13, Iteration 311, Loss: 0.6059466600418091
Epoch 13, Iteration 312, Loss: 0.49178212881088257
Epoch 13, Iteration 313, Loss: 0.7358594536781311
Epoch 13, Iteration 314, Loss: 0.5361140370368958
Epoch 13, Iteration 315, Loss: 0.581423819065094
Epoch 13, Iteration 316, Loss: 0.8073376417160034
Epoch 13, Iteration 317, Loss: 0.6147488951683044
Epoch 13, Iteration 318, Loss: 0.6333742141723633
Epoch 13, Iteration 319, Loss: 0.8631830215454102
Epoch 13, Iteration 320, Loss: 0.4510369300842285
Epoch 13, Iteration 321, Loss: 0.6638768911361694
Epoch 13, Iteration 322, Loss: 0.6662980914115906
Epoch 13, Iteration 323, Loss: 0.6045491099357605
Epoch 13, Iteration 324, Loss: 0.6574144959449768
Epoch 13, Iteration 325, Loss: 0.649509072303772
Epoch 13, Iteration 326, Loss: 0.6110826134681702
Epoch 13, Iteration 327, Loss: 0.5298998355865479
Epoch 13, Iteration 328, Loss: 0.6194984316825867
Epoch 13, Iteration 329, Loss: 0.6904320120811462
Epoch 13, Iteration 330, Loss: 0.5360917448997498
Epoch 13, Iteration 331, Loss: 0.7175260186195374
Epoch 13, Iteration 332, Loss: 0.5732147693634033
Epoch 13, Iteration 333, Loss: 0.8104386925697327
Epoch 13, Iteration 334, Loss: 0.6015546917915344
Epoch 13, Iteration 335, Loss: 0.8036767840385437
Epoch 13, Iteration 336, Loss: 0.60140061378479
Epoch 13, Iteration 337, Loss: 0.5180644989013672
Epoch 13, Iteration 338, Loss: 0.6857075095176697
Epoch 13, Iteration 339, Loss: 0.6543766856193542
Epoch 13, Iteration 340, Loss: 0.5158476829528809
Epoch 13, Iteration 341, Loss: 0.6035600304603577
Epoch 13, Iteration 342, Loss: 0.5383918881416321
Epoch 13, Iteration 343, Loss: 0.6697701215744019
Epoch 13, Iteration 344, Loss: 0.7228982448577881
Epoch 13, Iteration 345, Loss: 0.7348682284355164
Epoch 13, Iteration 346, Loss: 0.6109570264816284
Epoch 13, Iteration 347, Loss: 0.541968822479248
Epoch 13, Iteration 348, Loss: 0.49446702003479004
Epoch 13, Iteration 349, Loss: 0.6562536358833313
Epoch 13, Iteration 350, Loss: 0.5655328631401062
Epoch 13, Iteration 350, Valid Loss: 0.5587517023086548
Epoch 13, Iteration 351, Loss: 0.6605768799781799
Epoch 13, Iteration 352, Loss: 0.5775132775306702
Epoch 13, Iteration 353, Loss: 0.6496226787567139
Epoch 13, Iteration 354, Loss: 0.6064326167106628
Epoch 13, Iteration 355, Loss: 0.6445798277854919
Epoch 13, Iteration 356, Loss: 0.6118136048316956
Epoch 13, Iteration 357, Loss: 0.49839720129966736
Epoch 13, Iteration 358, Loss: 0.5637742280960083
Epoch 13, Iteration 359, Loss: 0.6159358024597168
Epoch 13, Iteration 360, Loss: 0.5916247367858887
Epoch 13, Iteration 361, Loss: 0.5800893902778625
Epoch 13, Iteration 362, Loss: 0.6295937895774841
Epoch 13, Iteration 363, Loss: 0.5432410836219788
Epoch 13, Iteration 364, Loss: 0.5756300091743469
Epoch 13, Iteration 365, Loss: 0.6275191903114319
Epoch 13, Iteration 366, Loss: 0.6630136370658875
Epoch 13, Iteration 367, Loss: 0.6169953942298889
Epoch 13, Iteration 368, Loss: 0.5924999713897705
Epoch 13, Iteration 369, Loss: 0.6686941385269165
Epoch 13, Iteration 370, Loss: 0.6524847745895386
Epoch 13, Iteration 371, Loss: 0.5996202826499939
Epoch 13, Iteration 372, Loss: 0.4916153848171234
Epoch 13, Iteration 373, Loss: 0.6191608905792236
Epoch 13, Iteration 374, Loss: 0.695557713508606
Epoch 13, Iteration 375, Loss: 0.5635856986045837
Epoch 13, Iteration 376, Loss: 0.664953887462616
Epoch 13, Iteration 377, Loss: 0.581725001335144
Epoch 13, Iteration 378, Loss: 0.5087494254112244
Epoch 13, Iteration 379, Loss: 0.49017998576164246
Epoch 13, Iteration 380, Loss: 0.5243696570396423
Epoch 13, Iteration 381, Loss: 0.5907266736030579
Epoch 13, Iteration 382, Loss: 0.48291856050491333
Epoch 13, Iteration 383, Loss: 0.6286081075668335
Epoch 13, Iteration 384, Loss: 0.665001630783081
Epoch 13, Iteration 385, Loss: 0.5468718409538269
Epoch 13, Iteration 386, Loss: 0.525941789150238
Epoch 13, Iteration 387, Loss: 0.8434012532234192
Epoch 13, Iteration 388, Loss: 0.6217186450958252
Epoch 13, Iteration 389, Loss: 0.6176244616508484
Epoch 13, Iteration 390, Loss: 0.7099931240081787
Epoch 13, Iteration 391, Loss: 0.485775887966156
Epoch 14/15, Loss: 0.6644022194098453
Epoch 14, Iteration 0, Loss: 0.6107072234153748
Epoch 14, Iteration 1, Loss: 0.7651224136352539
Epoch 14, Iteration 2, Loss: 0.7542082667350769
Epoch 14, Iteration 3, Loss: 0.7045360207557678
Epoch 14, Iteration 4, Loss: 0.6236481070518494
Epoch 14, Iteration 5, Loss: 0.7284846305847168
Epoch 14, Iteration 6, Loss: 0.5345916152000427
Epoch 14, Iteration 7, Loss: 0.6490391492843628
Epoch 14, Iteration 8, Loss: 0.5848985314369202
Epoch 14, Iteration 9, Loss: 0.6946085691452026
Epoch 14, Iteration 10, Loss: 0.6922003030776978
Epoch 14, Iteration 11, Loss: 0.6338554620742798
Epoch 14, Iteration 12, Loss: 0.5765876173973083
Epoch 14, Iteration 13, Loss: 0.7579389810562134
Epoch 14, Iteration 14, Loss: 0.6010501980781555
Epoch 14, Iteration 15, Loss: 0.7896642684936523
Epoch 14, Iteration 16, Loss: 0.6226028800010681
Epoch 14, Iteration 17, Loss: 0.6683012247085571
Epoch 14, Iteration 18, Loss: 0.611186683177948
Epoch 14, Iteration 19, Loss: 0.6884034872055054
Epoch 14, Iteration 20, Loss: 0.5485475659370422
Epoch 14, Iteration 21, Loss: 0.5578159689903259
Epoch 14, Iteration 22, Loss: 0.6009384393692017
Epoch 14, Iteration 23, Loss: 0.6593762636184692
Epoch 14, Iteration 24, Loss: 0.533119797706604
Epoch 14, Iteration 25, Loss: 0.5607812404632568
Epoch 14, Iteration 26, Loss: 0.8831156492233276
Epoch 14, Iteration 27, Loss: 0.6404747366905212
Epoch 14, Iteration 28, Loss: 0.6023459434509277
Epoch 14, Iteration 29, Loss: 0.7307798862457275
Epoch 14, Iteration 30, Loss: 0.6242746114730835
Epoch 14, Iteration 31, Loss: 0.6713030338287354
Epoch 14, Iteration 32, Loss: 0.7076610326766968
Epoch 14, Iteration 33, Loss: 0.6733909845352173
Epoch 14, Iteration 34, Loss: 0.5910251140594482
Epoch 14, Iteration 35, Loss: 0.7176856398582458
Epoch 14, Iteration 36, Loss: 0.5264396667480469
Epoch 14, Iteration 37, Loss: 0.5404697060585022
Epoch 14, Iteration 38, Loss: 0.5552389025688171
Epoch 14, Iteration 39, Loss: 0.6175008416175842
Epoch 14, Iteration 40, Loss: 0.6083337068557739
Epoch 14, Iteration 41, Loss: 0.7262285947799683
Epoch 14, Iteration 42, Loss: 0.6179590821266174
Epoch 14, Iteration 43, Loss: 0.5438846945762634
Epoch 14, Iteration 44, Loss: 0.6162163615226746
Epoch 14, Iteration 45, Loss: 0.6688645482063293
Epoch 14, Iteration 46, Loss: 0.6636140942573547
Epoch 14, Iteration 47, Loss: 0.5997145771980286
Epoch 14, Iteration 48, Loss: 0.731526792049408
Epoch 14, Iteration 49, Loss: 0.5901693105697632
Epoch 14, Iteration 50, Loss: 0.6146365404129028
Epoch 14, Iteration 50, Valid Loss: 0.5303436517715454
Epoch 14, Iteration 51, Loss: 0.6098006367683411
Epoch 14, Iteration 52, Loss: 0.5015780329704285
Epoch 14, Iteration 53, Loss: 0.6410586833953857
Epoch 14, Iteration 54, Loss: 0.7204937934875488
Epoch 14, Iteration 55, Loss: 0.7217893600463867
Epoch 14, Iteration 56, Loss: 0.6654707789421082
Epoch 14, Iteration 57, Loss: 0.7897247672080994
Epoch 14, Iteration 58, Loss: 0.6385961174964905
Epoch 14, Iteration 59, Loss: 0.5876182317733765
Epoch 14, Iteration 60, Loss: 0.576600193977356
Epoch 14, Iteration 61, Loss: 0.5191096067428589
Epoch 14, Iteration 62, Loss: 0.5531730651855469
Epoch 14, Iteration 63, Loss: 0.571182131767273
Epoch 14, Iteration 64, Loss: 0.7183638215065002
Epoch 14, Iteration 65, Loss: 0.8945601582527161
Epoch 14, Iteration 66, Loss: 0.6167870163917542
Epoch 14, Iteration 67, Loss: 0.5402092933654785
Epoch 14, Iteration 68, Loss: 0.7772874236106873
Epoch 14, Iteration 69, Loss: 0.6737895607948303
Epoch 14, Iteration 70, Loss: 0.5463982820510864
Epoch 14, Iteration 71, Loss: 0.8060462474822998
Epoch 14, Iteration 72, Loss: 0.6323464512825012
Epoch 14, Iteration 73, Loss: 0.5288059711456299
Epoch 14, Iteration 74, Loss: 0.7712869048118591
Epoch 14, Iteration 75, Loss: 0.6805619597434998
Epoch 14, Iteration 76, Loss: 0.5741592049598694
Epoch 14, Iteration 77, Loss: 0.5228039622306824
Epoch 14, Iteration 78, Loss: 0.7863436341285706
Epoch 14, Iteration 79, Loss: 0.8232721090316772
Epoch 14, Iteration 80, Loss: 0.6366491913795471
Epoch 14, Iteration 81, Loss: 0.9476251602172852
Epoch 14, Iteration 82, Loss: 0.7868000268936157
Epoch 14, Iteration 83, Loss: 0.5359039902687073
Epoch 14, Iteration 84, Loss: 0.6653822064399719
Epoch 14, Iteration 85, Loss: 0.6701843738555908
Epoch 14, Iteration 86, Loss: 0.7432978749275208
Epoch 14, Iteration 87, Loss: 0.6623572111129761
Epoch 14, Iteration 88, Loss: 0.807832658290863
Epoch 14, Iteration 89, Loss: 0.8560396432876587
Epoch 14, Iteration 90, Loss: 0.5428947806358337
Epoch 14, Iteration 91, Loss: 0.5731862783432007
Epoch 14, Iteration 92, Loss: 0.5128257274627686
Epoch 14, Iteration 93, Loss: 0.6731340885162354
Epoch 14, Iteration 94, Loss: 0.7486782073974609
Epoch 14, Iteration 95, Loss: 0.5300270318984985
Epoch 14, Iteration 96, Loss: 0.6259327530860901
Epoch 14, Iteration 97, Loss: 0.5357972979545593
Epoch 14, Iteration 98, Loss: 0.5948169231414795
Epoch 14, Iteration 99, Loss: 0.6051891446113586
Epoch 14, Iteration 100, Loss: 0.5017278790473938
Epoch 14, Iteration 100, Valid Loss: 0.5842671394348145
Epoch 14, Iteration 101, Loss: 0.8009840846061707
Epoch 14, Iteration 102, Loss: 0.695949375629425
Epoch 14, Iteration 103, Loss: 0.6009212136268616
Epoch 14, Iteration 104, Loss: 0.7615051865577698
Epoch 14, Iteration 105, Loss: 0.6280305981636047
Epoch 14, Iteration 106, Loss: 0.5717982053756714
Epoch 14, Iteration 107, Loss: 0.714202880859375
Epoch 14, Iteration 108, Loss: 0.6158840656280518
Epoch 14, Iteration 109, Loss: 0.8252966403961182
Epoch 14, Iteration 110, Loss: 0.7647902369499207
Epoch 14, Iteration 111, Loss: 0.6622416973114014
Epoch 14, Iteration 112, Loss: 0.6175422072410583
Epoch 14, Iteration 113, Loss: 0.5685501098632812
Epoch 14, Iteration 114, Loss: 0.7397603392601013
Epoch 14, Iteration 115, Loss: 0.7571986317634583
Epoch 14, Iteration 116, Loss: 0.5805146098136902
Epoch 14, Iteration 117, Loss: 0.4187462627887726
Epoch 14, Iteration 118, Loss: 0.6895751953125
Epoch 14, Iteration 119, Loss: 0.627479076385498
Epoch 14, Iteration 120, Loss: 0.7280772924423218
Epoch 14, Iteration 121, Loss: 0.5843213200569153
Epoch 14, Iteration 122, Loss: 0.8341134786605835
Epoch 14, Iteration 123, Loss: 0.6324335336685181
Epoch 14, Iteration 124, Loss: 0.5473833084106445
Epoch 14, Iteration 125, Loss: 0.5971825122833252
Epoch 14, Iteration 126, Loss: 0.5486607551574707
Epoch 14, Iteration 127, Loss: 0.6734400391578674
Epoch 14, Iteration 128, Loss: 0.8162709474563599
Epoch 14, Iteration 129, Loss: 0.8056705594062805
Epoch 14, Iteration 130, Loss: 0.8165773153305054
Epoch 14, Iteration 131, Loss: 0.5977594256401062
Epoch 14, Iteration 132, Loss: 0.5945350527763367
Epoch 14, Iteration 133, Loss: 0.9014208912849426
Epoch 14, Iteration 134, Loss: 0.7289546132087708
Epoch 14, Iteration 135, Loss: 0.6860867738723755
Epoch 14, Iteration 136, Loss: 0.6348538398742676
Epoch 14, Iteration 137, Loss: 0.6420860886573792
Epoch 14, Iteration 138, Loss: 0.5706411600112915
Epoch 14, Iteration 139, Loss: 0.5723981857299805
Epoch 14, Iteration 140, Loss: 0.5500221252441406
Epoch 14, Iteration 141, Loss: 0.9982789754867554
Epoch 14, Iteration 142, Loss: 0.7401317954063416
Epoch 14, Iteration 143, Loss: 0.6136390566825867
Epoch 14, Iteration 144, Loss: 0.5692453384399414
Epoch 14, Iteration 145, Loss: 0.6214507818222046
Epoch 14, Iteration 146, Loss: 0.6983564496040344
Epoch 14, Iteration 147, Loss: 0.6879479885101318
Epoch 14, Iteration 148, Loss: 0.5575490593910217
Epoch 14, Iteration 149, Loss: 0.6807994842529297
Epoch 14, Iteration 150, Loss: 0.6685407161712646
Epoch 14, Iteration 150, Valid Loss: 0.5642458200454712
Epoch 14, Iteration 151, Loss: 0.579749345779419
Epoch 14, Iteration 152, Loss: 0.5240301489830017
Epoch 14, Iteration 153, Loss: 1.1266608238220215
Epoch 14, Iteration 154, Loss: 0.49107685685157776
Epoch 14, Iteration 155, Loss: 0.6667110919952393
Epoch 14, Iteration 156, Loss: 0.6271978616714478
Epoch 14, Iteration 157, Loss: 0.8935822248458862
Epoch 14, Iteration 158, Loss: 0.6608150005340576
Epoch 14, Iteration 159, Loss: 0.7523893117904663
Epoch 14, Iteration 160, Loss: 0.6851851344108582
Epoch 14, Iteration 161, Loss: 0.6141988039016724
Epoch 14, Iteration 162, Loss: 0.8240697979927063
Epoch 14, Iteration 163, Loss: 0.5382514595985413
Epoch 14, Iteration 164, Loss: 0.6509600877761841
Epoch 14, Iteration 165, Loss: 0.6785654425621033
Epoch 14, Iteration 166, Loss: 0.5972216725349426
Epoch 14, Iteration 167, Loss: 0.8067429661750793
Epoch 14, Iteration 168, Loss: 0.6710888147354126
Epoch 14, Iteration 169, Loss: 0.6239954829216003
Epoch 14, Iteration 170, Loss: 0.780057430267334
Epoch 14, Iteration 171, Loss: 0.780393660068512
Epoch 14, Iteration 172, Loss: 0.6408101916313171
Epoch 14, Iteration 173, Loss: 0.5783522129058838
Epoch 14, Iteration 174, Loss: 0.75457364320755
Epoch 14, Iteration 175, Loss: 0.7148157358169556
Epoch 14, Iteration 176, Loss: 0.48266106843948364
Epoch 14, Iteration 177, Loss: 1.0304592847824097
Epoch 14, Iteration 178, Loss: 0.8249485492706299
Epoch 14, Iteration 179, Loss: 0.6282718181610107
Epoch 14, Iteration 180, Loss: 0.5859596729278564
Epoch 14, Iteration 181, Loss: 0.7526180148124695
Epoch 14, Iteration 182, Loss: 0.627107560634613
Epoch 14, Iteration 183, Loss: 0.6058009266853333
Epoch 14, Iteration 184, Loss: 0.6545619964599609
Epoch 14, Iteration 185, Loss: 0.5878801941871643
Epoch 14, Iteration 186, Loss: 0.6426315307617188
Epoch 14, Iteration 187, Loss: 0.6183637380599976
Epoch 14, Iteration 188, Loss: 0.5870542526245117
Epoch 14, Iteration 189, Loss: 0.5854374766349792
Epoch 14, Iteration 190, Loss: 0.7203412055969238
Epoch 14, Iteration 191, Loss: 0.6264134049415588
Epoch 14, Iteration 192, Loss: 0.6295006275177002
Epoch 14, Iteration 193, Loss: 0.6148236393928528
Epoch 14, Iteration 194, Loss: 0.685322642326355
Epoch 14, Iteration 195, Loss: 0.6773267388343811
Epoch 14, Iteration 196, Loss: 0.7126807570457458
Epoch 14, Iteration 197, Loss: 0.6542168259620667
Epoch 14, Iteration 198, Loss: 0.6497117280960083
Epoch 14, Iteration 199, Loss: 0.7460067272186279
Epoch 14, Iteration 200, Loss: 0.5301912426948547
Epoch 14, Iteration 200, Valid Loss: 0.5511583089828491
Epoch 14, Iteration 201, Loss: 0.6087249517440796
Epoch 14, Iteration 202, Loss: 0.6487732529640198
Epoch 14, Iteration 203, Loss: 0.6043097376823425
Epoch 14, Iteration 204, Loss: 0.624016284942627
Epoch 14, Iteration 205, Loss: 0.45410171151161194
Epoch 14, Iteration 206, Loss: 0.567425012588501
Epoch 14, Iteration 207, Loss: 0.7623317241668701
Epoch 14, Iteration 208, Loss: 1.0109169483184814
Epoch 14, Iteration 209, Loss: 0.9114813804626465
Epoch 14, Iteration 210, Loss: 0.8696741461753845
Epoch 14, Iteration 211, Loss: 0.9141328930854797
Epoch 14, Iteration 212, Loss: 0.7774336934089661
Epoch 14, Iteration 213, Loss: 0.8093644380569458
Epoch 14, Iteration 214, Loss: 0.617143452167511
Epoch 14, Iteration 215, Loss: 0.5544579029083252
Epoch 14, Iteration 216, Loss: 0.4904244840145111
Epoch 14, Iteration 217, Loss: 0.5986142158508301
Epoch 14, Iteration 218, Loss: 0.5790656208992004
Epoch 14, Iteration 219, Loss: 0.6976546049118042
Epoch 14, Iteration 220, Loss: 0.5925308465957642
Epoch 14, Iteration 221, Loss: 0.6425665616989136
Epoch 14, Iteration 222, Loss: 0.7172425985336304
Epoch 14, Iteration 223, Loss: 0.5189924240112305
Epoch 14, Iteration 224, Loss: 0.5511044859886169
Epoch 14, Iteration 225, Loss: 0.6958479285240173
Epoch 14, Iteration 226, Loss: 0.7633757591247559
Epoch 14, Iteration 227, Loss: 0.623642086982727
Epoch 14, Iteration 228, Loss: 0.6867386698722839
Epoch 14, Iteration 229, Loss: 0.8627510070800781
Epoch 14, Iteration 230, Loss: 0.6126768589019775
Epoch 14, Iteration 231, Loss: 0.6582214832305908
Epoch 14, Iteration 232, Loss: 0.689460039138794
Epoch 14, Iteration 233, Loss: 0.8365787267684937
Epoch 14, Iteration 234, Loss: 0.8240333795547485
Epoch 14, Iteration 235, Loss: 0.5648580193519592
Epoch 14, Iteration 236, Loss: 0.5750499367713928
Epoch 14, Iteration 237, Loss: 0.6929522752761841
Epoch 14, Iteration 238, Loss: 0.533454954624176
Epoch 14, Iteration 239, Loss: 0.7671695947647095
Epoch 14, Iteration 240, Loss: 0.7719932198524475
Epoch 14, Iteration 241, Loss: 0.5722347497940063
Epoch 14, Iteration 242, Loss: 0.6432735919952393
Epoch 14, Iteration 243, Loss: 0.615272045135498
Epoch 14, Iteration 244, Loss: 0.6483468413352966
Epoch 14, Iteration 245, Loss: 0.5017063617706299
Epoch 14, Iteration 246, Loss: 0.5431618094444275
Epoch 14, Iteration 247, Loss: 0.5860621929168701
Epoch 14, Iteration 248, Loss: 0.6446748971939087
Epoch 14, Iteration 249, Loss: 0.5970962643623352
Epoch 14, Iteration 250, Loss: 0.746163547039032
Epoch 14, Iteration 250, Valid Loss: 0.5581231117248535
Epoch 14, Iteration 251, Loss: 0.5273298025131226
Epoch 14, Iteration 252, Loss: 0.8119019269943237
Epoch 14, Iteration 253, Loss: 0.7783250212669373
Epoch 14, Iteration 254, Loss: 0.6832113265991211
Epoch 14, Iteration 255, Loss: 0.6472228765487671
Epoch 14, Iteration 256, Loss: 0.6837942004203796
Epoch 14, Iteration 257, Loss: 0.5877158641815186
Epoch 14, Iteration 258, Loss: 0.8337875604629517
Epoch 14, Iteration 259, Loss: 0.5414684414863586
Epoch 14, Iteration 260, Loss: 0.6666642427444458
Epoch 14, Iteration 261, Loss: 0.6128965020179749
Epoch 14, Iteration 262, Loss: 0.5700690150260925
Epoch 14, Iteration 263, Loss: 0.7359674572944641
Epoch 14, Iteration 264, Loss: 0.6223708391189575
Epoch 14, Iteration 265, Loss: 0.6473710536956787
Epoch 14, Iteration 266, Loss: 0.7085281610488892
Epoch 14, Iteration 267, Loss: 0.5997295379638672
Epoch 14, Iteration 268, Loss: 0.5991355180740356
Epoch 14, Iteration 269, Loss: 0.641214907169342
Epoch 14, Iteration 270, Loss: 0.6304333806037903
Epoch 14, Iteration 271, Loss: 0.653393566608429
Epoch 14, Iteration 272, Loss: 0.5860128998756409
Epoch 14, Iteration 273, Loss: 0.7527735829353333
Epoch 14, Iteration 274, Loss: 0.6573354005813599
Epoch 14, Iteration 275, Loss: 0.5229412317276001
Epoch 14, Iteration 276, Loss: 0.5610924363136292
Epoch 14, Iteration 277, Loss: 0.5509462356567383
Epoch 14, Iteration 278, Loss: 0.8243198990821838
Epoch 14, Iteration 279, Loss: 0.5546190142631531
Epoch 14, Iteration 280, Loss: 0.470173180103302
Epoch 14, Iteration 281, Loss: 0.6214513182640076
Epoch 14, Iteration 282, Loss: 0.5577471852302551
Epoch 14, Iteration 283, Loss: 0.6121353507041931
Epoch 14, Iteration 284, Loss: 0.5456141829490662
Epoch 14, Iteration 285, Loss: 0.5175022482872009
Epoch 14, Iteration 286, Loss: 0.7202721238136292
Epoch 14, Iteration 287, Loss: 0.8068439364433289
Epoch 14, Iteration 288, Loss: 0.49012646079063416
Epoch 14, Iteration 289, Loss: 0.5144026279449463
Epoch 14, Iteration 290, Loss: 0.6996760964393616
Epoch 14, Iteration 291, Loss: 0.42148104310035706
Epoch 14, Iteration 292, Loss: 0.6977176070213318
Epoch 14, Iteration 293, Loss: 0.7223997712135315
Epoch 14, Iteration 294, Loss: 0.588953971862793
Epoch 14, Iteration 295, Loss: 0.4689693748950958
Epoch 14, Iteration 296, Loss: 0.7474380135536194
Epoch 14, Iteration 297, Loss: 0.6966290473937988
Epoch 14, Iteration 298, Loss: 0.6713414192199707
Epoch 14, Iteration 299, Loss: 0.5390381813049316
Epoch 14, Iteration 300, Loss: 0.4777832627296448
Epoch 14, Iteration 300, Valid Loss: 0.556969940662384
Epoch 14, Iteration 301, Loss: 0.5622046589851379
Epoch 14, Iteration 302, Loss: 0.5832691788673401
Epoch 14, Iteration 303, Loss: 0.7290912866592407
Epoch 14, Iteration 304, Loss: 0.5907501578330994
Epoch 14, Iteration 305, Loss: 0.5676746368408203
Epoch 14, Iteration 306, Loss: 0.5643795132637024
Epoch 14, Iteration 307, Loss: 0.571269154548645
Epoch 14, Iteration 308, Loss: 0.5755190253257751
Epoch 14, Iteration 309, Loss: 0.6739599108695984
Epoch 14, Iteration 310, Loss: 0.6442617177963257
Epoch 14, Iteration 311, Loss: 0.6272590756416321
Epoch 14, Iteration 312, Loss: 0.4842514991760254
Epoch 14, Iteration 313, Loss: 0.6864299178123474
Epoch 14, Iteration 314, Loss: 0.6153287291526794
Epoch 14, Iteration 315, Loss: 0.7465932965278625
Epoch 14, Iteration 316, Loss: 0.8259593844413757
Epoch 14, Iteration 317, Loss: 0.6302006840705872
Epoch 14, Iteration 318, Loss: 0.5446757078170776
Epoch 14, Iteration 319, Loss: 0.7844739556312561
Epoch 14, Iteration 320, Loss: 0.44046440720558167
Epoch 14, Iteration 321, Loss: 0.6651378870010376
Epoch 14, Iteration 322, Loss: 0.5873116254806519
Epoch 14, Iteration 323, Loss: 0.6334177255630493
Epoch 14, Iteration 324, Loss: 0.6252709627151489
Epoch 14, Iteration 325, Loss: 0.6175390481948853
Epoch 14, Iteration 326, Loss: 0.5635004043579102
Epoch 14, Iteration 327, Loss: 0.4923475384712219
Epoch 14, Iteration 328, Loss: 0.5868098735809326
Epoch 14, Iteration 329, Loss: 0.7179847955703735
Epoch 14, Iteration 330, Loss: 0.5451033711433411
Epoch 14, Iteration 331, Loss: 0.6221243739128113
Epoch 14, Iteration 332, Loss: 0.5824337005615234
Epoch 14, Iteration 333, Loss: 0.7882437109947205
Epoch 14, Iteration 334, Loss: 0.5671252012252808
Epoch 14, Iteration 335, Loss: 0.8014878630638123
Epoch 14, Iteration 336, Loss: 0.6549918055534363
Epoch 14, Iteration 337, Loss: 0.5691003799438477
Epoch 14, Iteration 338, Loss: 0.7254765629768372
Epoch 14, Iteration 339, Loss: 0.5917448401451111
Epoch 14, Iteration 340, Loss: 0.5065962076187134
Epoch 14, Iteration 341, Loss: 0.6374509930610657
Epoch 14, Iteration 342, Loss: 0.6174458861351013
Epoch 14, Iteration 343, Loss: 0.569108784198761
Epoch 14, Iteration 344, Loss: 0.7145522236824036
Epoch 14, Iteration 345, Loss: 0.5942813158035278
Epoch 14, Iteration 346, Loss: 0.5270751118659973
Epoch 14, Iteration 347, Loss: 0.46178004145622253
Epoch 14, Iteration 348, Loss: 0.49353665113449097
Epoch 14, Iteration 349, Loss: 0.6756992340087891
Epoch 14, Iteration 350, Loss: 0.5342204570770264
Epoch 14, Iteration 350, Valid Loss: 0.5740734338760376
Epoch 14, Iteration 351, Loss: 0.6610851287841797
Epoch 14, Iteration 352, Loss: 0.5364763736724854
Epoch 14, Iteration 353, Loss: 0.604708731174469
Epoch 14, Iteration 354, Loss: 0.48500052094459534
Epoch 14, Iteration 355, Loss: 0.5672560930252075
Epoch 14, Iteration 356, Loss: 0.6837770938873291
Epoch 14, Iteration 357, Loss: 0.4965064525604248
Epoch 14, Iteration 358, Loss: 0.5248560309410095
Epoch 14, Iteration 359, Loss: 0.6506376266479492
Epoch 14, Iteration 360, Loss: 0.5922913551330566
Epoch 14, Iteration 361, Loss: 0.6887060403823853
Epoch 14, Iteration 362, Loss: 0.5828639268875122
Epoch 14, Iteration 363, Loss: 0.5541618466377258
Epoch 14, Iteration 364, Loss: 0.6418083310127258
Epoch 14, Iteration 365, Loss: 0.7320284247398376
Epoch 14, Iteration 366, Loss: 0.7109942436218262
Epoch 14, Iteration 367, Loss: 0.5572710037231445
Epoch 14, Iteration 368, Loss: 0.5640605092048645
Epoch 14, Iteration 369, Loss: 0.6047599911689758
Epoch 14, Iteration 370, Loss: 0.6522803902626038
Epoch 14, Iteration 371, Loss: 0.6099329590797424
Epoch 14, Iteration 372, Loss: 0.5074207782745361
Epoch 14, Iteration 373, Loss: 0.5977193117141724
Epoch 14, Iteration 374, Loss: 0.8016809821128845
Epoch 14, Iteration 375, Loss: 0.579961359500885
Epoch 14, Iteration 376, Loss: 0.6793494820594788
Epoch 14, Iteration 377, Loss: 0.5821374654769897
Epoch 14, Iteration 378, Loss: 0.5027801394462585
Epoch 14, Iteration 379, Loss: 0.5250089764595032
Epoch 14, Iteration 380, Loss: 0.5753220915794373
Epoch 14, Iteration 381, Loss: 0.5200352668762207
Epoch 14, Iteration 382, Loss: 0.4058254659175873
Epoch 14, Iteration 383, Loss: 0.5597006678581238
Epoch 14, Iteration 384, Loss: 0.5849471092224121
Epoch 14, Iteration 385, Loss: 0.5235757827758789
Epoch 14, Iteration 386, Loss: 0.531743049621582
Epoch 14, Iteration 387, Loss: 0.9005934000015259
Epoch 14, Iteration 388, Loss: 0.6104074716567993
Epoch 14, Iteration 389, Loss: 0.6115405559539795
Epoch 14, Iteration 390, Loss: 0.7072548866271973
Epoch 14, Iteration 391, Loss: 0.5183640122413635
Epoch 15/15, Loss: 0.6449628796960626
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<br/> <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br/><div class="wandb-row"><div class="wandb-col"><h3>Run history:</h3><br><table class="wandb"><tr><td>head_tagging_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>unlabeled_attachment_score</td><td></td></tr><tr><td>valid_loss</td><td></td></tr></table><br/></br></div><div class="wandb-col"><h3>Run summary:</h3><br><table class="wandb"><tr><td>head_tagging_accuracy</td><td>0.8514</td></tr><tr><td>train_loss</td><td>0.51836</td></tr><tr><td>unlabeled_attachment_score</td><td>0.86436</td></tr><tr><td>valid_loss</td><td>0.57407</td></tr></table><br/></br></div></div>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View run <strong style="color:#cdcd00">dependency-parsing</strong> at: <a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/iidfj4nz" target="_blank">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/iidfj4nz</a><br/> View project at: <a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing" target="_blank">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing</a><br/>Synced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Find logs at: <code>.\wandb\run-20250113_214744-iidfj4nz\logs</code>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=45155b67b1b56e51">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ufal.chu_liu_edmonds</span> <span class="kn">import</span> <span class="n">chu_liu_edmonds</span>

<span class="c1"># Evaluation on the test set</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DependencyParserModel</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"dependency_parser.pth"</span><span class="p">))</span>

<span class="c1"># # Extra: Pro model UAS Accuracy: 0.8642811603442779</span>
<span class="c1"># model = DependencyParserProModel().to(device)</span>
<span class="c1"># model.load_state_dict(torch.load("dependency_parser_edgedim_1024.pth"))</span>

<span class="c1"># Evaluate Head prediction</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">total_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"total batch: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">head</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"head"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="n">L_dep</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">edge_scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score_edges</span><span class="p">(</span><span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, seq_len)</span>

        <span class="c1"># MST parsing UAS evaluation</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">edge_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># apply chu_liu_edmons</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">sent_scores</span> <span class="ow">in</span> <span class="n">log_probs</span><span class="p">:</span>
            <span class="c1"># convert to numpy</span>
            <span class="n">scores_np</span> <span class="o">=</span> <span class="n">sent_scores</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">mst_heads</span> <span class="o">=</span> <span class="n">chu_liu_edmonds</span><span class="p">(</span><span class="n">scores_np</span><span class="p">)</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mst_heads</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Batch </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="c1"># convert predictions and gold heads into tensors</span>
        <span class="n">uas_predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len)</span>
        <span class="n">valid_positions</span> <span class="o">=</span> <span class="p">(</span><span class="n">head</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Mask for valid positions # Shape: (batch_size, seq_len)</span>

        <span class="c1"># calculate correct predictions and total count</span>
        <span class="n">total_correct</span> <span class="o">+=</span> <span class="p">((</span><span class="n">uas_predictions</span> <span class="o">==</span> <span class="n">head</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">valid_positions</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">total_count</span> <span class="o">+=</span> <span class="n">valid_positions</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"UAS Accuracy: </span><span class="si">{</span><span class="n">total_correct</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">total_count</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
C:\Users\WangEntang\AppData\Local\Temp\ipykernel_41496\1613024923.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load("dependency_parser_edgedim_1024.pth"))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>total batch: 65
Batch 0
Batch 1
Batch 2
Batch 3
Batch 4
Batch 5
Batch 6
Batch 7
Batch 8
Batch 9
Batch 10
Batch 11
Batch 12
Batch 13
Batch 14
Batch 15
Batch 16
Batch 17
Batch 18
Batch 19
Batch 20
Batch 21
Batch 22
Batch 23
Batch 24
Batch 25
Batch 26
Batch 27
Batch 28
Batch 29
Batch 30
Batch 31
Batch 32
Batch 33
Batch 34
Batch 35
Batch 36
Batch 37
Batch 38
Batch 39
Batch 40
Batch 41
Batch 42
Batch 43
Batch 44
Batch 45
Batch 46
Batch 47
Batch 48
Batch 49
Batch 50
Batch 51
Batch 52
Batch 53
Batch 54
Batch 55
Batch 56
Batch 57
Batch 58
Batch 59
Batch 60
Batch 61
Batch 62
Batch 63
Batch 64
UAS Accuracy: 0.8635639145680587
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=26f806e4e936831d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Extra: training label predicting and evaluation</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DependencyParserModel</span><span class="p">(</span><span class="n">label_predicting</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">valid_dataloader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Evaluate Head prediction</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">total_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"total batch: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">head</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"head"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">deprel</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"deprel_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">,</span> <span class="n">L_head</span><span class="p">,</span> <span class="n">L_dep</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">edge_scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score_edges</span><span class="p">(</span><span class="n">H_head</span><span class="p">,</span> <span class="n">H_dep</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, seq_len)</span>
        <span class="n">label_scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score_labels</span><span class="p">(</span><span class="n">L_head</span><span class="p">,</span> <span class="n">L_dep</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, seq_len, seq_len, num_labels)</span>

        <span class="c1"># best score heads</span>
        <span class="n">predicted_heads</span> <span class="o">=</span> <span class="n">edge_scores</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len)</span>

        <span class="c1"># Shape: (batch_size, seq_len, seq_len, num_labels)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">num_labels</span> <span class="o">=</span> <span class="n">label_scores</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># batch_indices: (batch_size, 1)</span>
        <span class="c1"># dep_indices:   (1,   n)</span>
        <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># batch index</span>
        <span class="n">dep_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># dep index</span>

        <span class="c1"># gather label_scores according to the gold head</span>
        <span class="c1"># Shape: (batch_size, seq_len, num_labels)</span>
        <span class="n">label_scores_for_gold_edge</span> <span class="o">=</span> <span class="n">label_scores</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">dep_indices</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># best score labels</span>
        <span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">label_scores_for_gold_edge</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Batch </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="n">valid_positions</span> <span class="o">=</span> <span class="p">(</span><span class="n">head</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Mask for valid positions # Shape: (batch_size, seq_len)</span>

        <span class="c1"># calculate correct predictions and total count</span>
        <span class="n">total_correct</span> <span class="o">+=</span> <span class="p">(((</span><span class="n">predicted_heads</span> <span class="o">==</span> <span class="n">head</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">predicted_labels</span> <span class="o">==</span> <span class="n">deprel</span><span class="p">))</span> <span class="o">&amp;</span> <span class="n">valid_positions</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">total_count</span> <span class="o">+=</span> <span class="n">valid_positions</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"LAS Accuracy: </span><span class="si">{</span><span class="n">total_correct</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">total_count</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\WangEntang\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
C:\Users\WangEntang\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
wandb: Currently logged in as: maxewang10 (maxewang10-saarland-informatics-campus). Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Tracking run with wandb version 0.19.2
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Run data is saved locally in <code>C:\Users\WangEntang\Desktop\code\CL\a5\wandb\run-20250113_225223-er1khkx3</code>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Syncing run <strong><a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/er1khkx3" target="_blank">dependency-parsing</a></strong> to <a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/developer-guide" target="_blank">docs</a>)<br/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View project at <a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing" target="_blank">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View run at <a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/er1khkx3" target="_blank">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/er1khkx3</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>batch num 392
Epoch 0, Iteration 0, Loss: 9.847716331481934
Epoch 0, Iteration 1, Loss: 9.546501159667969
Epoch 0, Iteration 2, Loss: 9.301563262939453
Epoch 0, Iteration 3, Loss: 9.073440551757812
Epoch 0, Iteration 4, Loss: 8.827247619628906
Epoch 0, Iteration 5, Loss: 8.644109725952148
Epoch 0, Iteration 6, Loss: 8.412551879882812
Epoch 0, Iteration 7, Loss: 8.29134464263916
Epoch 0, Iteration 8, Loss: 8.082523345947266
Epoch 0, Iteration 9, Loss: 7.994370460510254
Epoch 0, Iteration 10, Loss: 7.656954765319824
Epoch 0, Iteration 11, Loss: 7.419976234436035
Epoch 0, Iteration 12, Loss: 7.286379814147949
Epoch 0, Iteration 13, Loss: 7.291024684906006
Epoch 0, Iteration 14, Loss: 7.00642728805542
Epoch 0, Iteration 15, Loss: 6.925200462341309
Epoch 0, Iteration 16, Loss: 6.721271514892578
Epoch 0, Iteration 17, Loss: 6.732643127441406
Epoch 0, Iteration 18, Loss: 6.394735813140869
Epoch 0, Iteration 19, Loss: 6.45346736907959
Epoch 0, Iteration 20, Loss: 6.432611465454102
Epoch 0, Iteration 21, Loss: 6.348763465881348
Epoch 0, Iteration 22, Loss: 6.085209846496582
Epoch 0, Iteration 23, Loss: 6.124578952789307
Epoch 0, Iteration 24, Loss: 6.039186477661133
Epoch 0, Iteration 25, Loss: 5.9354400634765625
Epoch 0, Iteration 26, Loss: 6.050265312194824
Epoch 0, Iteration 27, Loss: 5.909564971923828
Epoch 0, Iteration 28, Loss: 5.789445877075195
Epoch 0, Iteration 29, Loss: 5.6460065841674805
Epoch 0, Iteration 30, Loss: 5.536341667175293
Epoch 0, Iteration 31, Loss: 5.399195194244385
Epoch 0, Iteration 32, Loss: 5.497375011444092
Epoch 0, Iteration 33, Loss: 5.18115234375
Epoch 0, Iteration 34, Loss: 5.42244815826416
Epoch 0, Iteration 35, Loss: 5.413187026977539
Epoch 0, Iteration 36, Loss: 5.208505630493164
Epoch 0, Iteration 37, Loss: 5.149444103240967
Epoch 0, Iteration 38, Loss: 5.211514472961426
Epoch 0, Iteration 39, Loss: 5.101093292236328
Epoch 0, Iteration 40, Loss: 5.235589027404785
Epoch 0, Iteration 41, Loss: 5.145758628845215
Epoch 0, Iteration 42, Loss: 5.469198226928711
Epoch 0, Iteration 43, Loss: 5.334802627563477
Epoch 0, Iteration 44, Loss: 5.375179290771484
Epoch 0, Iteration 45, Loss: 5.194720268249512
Epoch 0, Iteration 46, Loss: 5.145501136779785
Epoch 0, Iteration 47, Loss: 5.133576393127441
Epoch 0, Iteration 48, Loss: 5.137571334838867
Epoch 0, Iteration 49, Loss: 5.060364246368408
Epoch 0, Iteration 50, Loss: 5.216657638549805
Epoch 0, Iteration 50, Valid Loss: 2.7567293643951416
Epoch 0, Iteration 51, Loss: 4.903803825378418
Epoch 0, Iteration 52, Loss: 5.343389511108398
Epoch 0, Iteration 53, Loss: 4.872424125671387
Epoch 0, Iteration 54, Loss: 4.987217426300049
Epoch 0, Iteration 55, Loss: 5.031269073486328
Epoch 0, Iteration 56, Loss: 4.49389123916626
Epoch 0, Iteration 57, Loss: 4.964474678039551
Epoch 0, Iteration 58, Loss: 4.588923454284668
Epoch 0, Iteration 59, Loss: 4.949513912200928
Epoch 0, Iteration 60, Loss: 4.995227336883545
Epoch 0, Iteration 61, Loss: 4.3268022537231445
Epoch 0, Iteration 62, Loss: 4.652259826660156
Epoch 0, Iteration 63, Loss: 4.610742092132568
Epoch 0, Iteration 64, Loss: 4.3084306716918945
Epoch 0, Iteration 65, Loss: 4.593693256378174
Epoch 0, Iteration 66, Loss: 4.762104511260986
Epoch 0, Iteration 67, Loss: 4.510526180267334
Epoch 0, Iteration 68, Loss: 5.016168594360352
Epoch 0, Iteration 69, Loss: 4.627368927001953
Epoch 0, Iteration 70, Loss: 4.898713111877441
Epoch 0, Iteration 71, Loss: 4.538634777069092
Epoch 0, Iteration 72, Loss: 4.858819007873535
Epoch 0, Iteration 73, Loss: 4.464206218719482
Epoch 0, Iteration 74, Loss: 4.695379734039307
Epoch 0, Iteration 75, Loss: 4.382760524749756
Epoch 0, Iteration 76, Loss: 4.283947944641113
Epoch 0, Iteration 77, Loss: 4.222935199737549
Epoch 0, Iteration 78, Loss: 4.63950777053833
Epoch 0, Iteration 79, Loss: 4.522343158721924
Epoch 0, Iteration 80, Loss: 4.199439525604248
Epoch 0, Iteration 81, Loss: 4.3742899894714355
Epoch 0, Iteration 82, Loss: 4.285918712615967
Epoch 0, Iteration 83, Loss: 4.284214973449707
Epoch 0, Iteration 84, Loss: 4.512027263641357
Epoch 0, Iteration 85, Loss: 4.361198425292969
Epoch 0, Iteration 86, Loss: 4.247093200683594
Epoch 0, Iteration 87, Loss: 4.1361260414123535
Epoch 0, Iteration 88, Loss: 4.14699649810791
Epoch 0, Iteration 89, Loss: 4.265470504760742
Epoch 0, Iteration 90, Loss: 4.054080963134766
Epoch 0, Iteration 91, Loss: 4.103646278381348
Epoch 0, Iteration 92, Loss: 4.038068771362305
Epoch 0, Iteration 93, Loss: 4.072612762451172
Epoch 0, Iteration 94, Loss: 4.564094066619873
Epoch 0, Iteration 95, Loss: 4.252722263336182
Epoch 0, Iteration 96, Loss: 4.327775001525879
Epoch 0, Iteration 97, Loss: 4.093064785003662
Epoch 0, Iteration 98, Loss: 4.328907012939453
Epoch 0, Iteration 99, Loss: 4.072683334350586
Epoch 0, Iteration 100, Loss: 4.058050632476807
Epoch 0, Iteration 100, Valid Loss: 2.6010122299194336
Epoch 0, Iteration 101, Loss: 4.281209468841553
Epoch 0, Iteration 102, Loss: 4.248579025268555
Epoch 0, Iteration 103, Loss: 4.088962554931641
Epoch 0, Iteration 104, Loss: 4.568479537963867
Epoch 0, Iteration 105, Loss: 4.20873498916626
Epoch 0, Iteration 106, Loss: 4.153979778289795
Epoch 0, Iteration 107, Loss: 5.06034517288208
Epoch 0, Iteration 108, Loss: 3.897047996520996
Epoch 0, Iteration 109, Loss: 4.135614395141602
Epoch 0, Iteration 110, Loss: 4.407773017883301
Epoch 0, Iteration 111, Loss: 4.092238903045654
Epoch 0, Iteration 112, Loss: 4.064296722412109
Epoch 0, Iteration 113, Loss: 3.8016483783721924
Epoch 0, Iteration 114, Loss: 4.406543731689453
Epoch 0, Iteration 115, Loss: 4.303811550140381
Epoch 0, Iteration 116, Loss: 4.160530090332031
Epoch 0, Iteration 117, Loss: 3.8493471145629883
Epoch 0, Iteration 118, Loss: 4.26616907119751
Epoch 0, Iteration 119, Loss: 4.278938293457031
Epoch 0, Iteration 120, Loss: 3.8935303688049316
Epoch 0, Iteration 121, Loss: 3.909630298614502
Epoch 0, Iteration 122, Loss: 3.924140214920044
Epoch 0, Iteration 123, Loss: 3.8484206199645996
Epoch 0, Iteration 124, Loss: 3.930314540863037
Epoch 0, Iteration 125, Loss: 3.6281046867370605
Epoch 0, Iteration 126, Loss: 3.780555248260498
Epoch 0, Iteration 127, Loss: 4.071514129638672
Epoch 0, Iteration 128, Loss: 4.015175819396973
Epoch 0, Iteration 129, Loss: 3.880694627761841
Epoch 0, Iteration 130, Loss: 3.952240467071533
Epoch 0, Iteration 131, Loss: 3.933094024658203
Epoch 0, Iteration 132, Loss: 3.8009700775146484
Epoch 0, Iteration 133, Loss: 3.8316550254821777
Epoch 0, Iteration 134, Loss: 3.471909523010254
Epoch 0, Iteration 135, Loss: 3.6614160537719727
Epoch 0, Iteration 136, Loss: 3.6766915321350098
Epoch 0, Iteration 137, Loss: 3.674656867980957
Epoch 0, Iteration 138, Loss: 3.532376766204834
Epoch 0, Iteration 139, Loss: 3.503699779510498
Epoch 0, Iteration 140, Loss: 3.7813942432403564
Epoch 0, Iteration 141, Loss: 3.968003273010254
Epoch 0, Iteration 142, Loss: 3.4633002281188965
Epoch 0, Iteration 143, Loss: 3.7714452743530273
Epoch 0, Iteration 144, Loss: 3.647156000137329
Epoch 0, Iteration 145, Loss: 3.695132255554199
Epoch 0, Iteration 146, Loss: 3.955199718475342
Epoch 0, Iteration 147, Loss: 3.7446413040161133
Epoch 0, Iteration 148, Loss: 3.3614439964294434
Epoch 0, Iteration 149, Loss: 3.620366334915161
Epoch 0, Iteration 150, Loss: 3.578016519546509
Epoch 0, Iteration 150, Valid Loss: 2.5227532386779785
Epoch 0, Iteration 151, Loss: 3.8323464393615723
Epoch 0, Iteration 152, Loss: 3.76686954498291
Epoch 0, Iteration 153, Loss: 3.427994728088379
Epoch 0, Iteration 154, Loss: 3.547128200531006
Epoch 0, Iteration 155, Loss: 3.598629951477051
Epoch 0, Iteration 156, Loss: 3.7767210006713867
Epoch 0, Iteration 157, Loss: 4.4557318687438965
Epoch 0, Iteration 158, Loss: 3.748856544494629
Epoch 0, Iteration 159, Loss: 3.887603759765625
Epoch 0, Iteration 160, Loss: 3.736206531524658
Epoch 0, Iteration 161, Loss: 3.681645631790161
Epoch 0, Iteration 162, Loss: 3.929394245147705
Epoch 0, Iteration 163, Loss: 3.531088352203369
Epoch 0, Iteration 164, Loss: 3.248706817626953
Epoch 0, Iteration 165, Loss: 3.447147846221924
Epoch 0, Iteration 166, Loss: 3.4634389877319336
Epoch 0, Iteration 167, Loss: 3.954288959503174
Epoch 0, Iteration 168, Loss: 4.055214881896973
Epoch 0, Iteration 169, Loss: 3.7507617473602295
Epoch 0, Iteration 170, Loss: 3.736267566680908
Epoch 0, Iteration 171, Loss: 3.3912806510925293
Epoch 0, Iteration 172, Loss: 3.353628396987915
Epoch 0, Iteration 173, Loss: 3.7273426055908203
Epoch 0, Iteration 174, Loss: 3.734616994857788
Epoch 0, Iteration 175, Loss: 3.7554707527160645
Epoch 0, Iteration 176, Loss: 3.447787046432495
Epoch 0, Iteration 177, Loss: 3.7519593238830566
Epoch 0, Iteration 178, Loss: 3.683361530303955
Epoch 0, Iteration 179, Loss: 3.6507482528686523
Epoch 0, Iteration 180, Loss: 3.91841983795166
Epoch 0, Iteration 181, Loss: 3.7679176330566406
Epoch 0, Iteration 182, Loss: 3.7534079551696777
Epoch 0, Iteration 183, Loss: 3.4155571460723877
Epoch 0, Iteration 184, Loss: 3.320107936859131
Epoch 0, Iteration 185, Loss: 3.692577838897705
Epoch 0, Iteration 186, Loss: 3.4144287109375
Epoch 0, Iteration 187, Loss: 3.5345802307128906
Epoch 0, Iteration 188, Loss: 3.512542963027954
Epoch 0, Iteration 189, Loss: 3.569976329803467
Epoch 0, Iteration 190, Loss: 3.941693067550659
Epoch 0, Iteration 191, Loss: 3.6970701217651367
Epoch 0, Iteration 192, Loss: 3.631720542907715
Epoch 0, Iteration 193, Loss: 3.4810075759887695
Epoch 0, Iteration 194, Loss: 3.2687642574310303
Epoch 0, Iteration 195, Loss: 3.3984241485595703
Epoch 0, Iteration 196, Loss: 3.5122625827789307
Epoch 0, Iteration 197, Loss: 3.4501469135284424
Epoch 0, Iteration 198, Loss: 3.4164490699768066
Epoch 0, Iteration 199, Loss: 3.6957316398620605
Epoch 0, Iteration 200, Loss: 3.494436502456665
Epoch 0, Iteration 200, Valid Loss: 2.3782942295074463
Epoch 0, Iteration 201, Loss: 3.384122610092163
Epoch 0, Iteration 202, Loss: 3.506242275238037
Epoch 0, Iteration 203, Loss: 3.498509407043457
Epoch 0, Iteration 204, Loss: 3.4828453063964844
Epoch 0, Iteration 205, Loss: 3.1275980472564697
Epoch 0, Iteration 206, Loss: 3.135586738586426
Epoch 0, Iteration 207, Loss: 3.4551923274993896
Epoch 0, Iteration 208, Loss: 3.9012317657470703
Epoch 0, Iteration 209, Loss: 4.278946876525879
Epoch 0, Iteration 210, Loss: 3.953396797180176
Epoch 0, Iteration 211, Loss: 4.16502571105957
Epoch 0, Iteration 212, Loss: 3.976043224334717
Epoch 0, Iteration 213, Loss: 3.9038751125335693
Epoch 0, Iteration 214, Loss: 3.531233310699463
Epoch 0, Iteration 215, Loss: 3.3809006214141846
Epoch 0, Iteration 216, Loss: 3.3256490230560303
Epoch 0, Iteration 217, Loss: 3.6661274433135986
Epoch 0, Iteration 218, Loss: 3.778454542160034
Epoch 0, Iteration 219, Loss: 3.7397139072418213
Epoch 0, Iteration 220, Loss: 3.642124652862549
Epoch 0, Iteration 221, Loss: 3.543823719024658
Epoch 0, Iteration 222, Loss: 3.522270679473877
Epoch 0, Iteration 223, Loss: 3.603985071182251
Epoch 0, Iteration 224, Loss: 3.348815441131592
Epoch 0, Iteration 225, Loss: 3.6569294929504395
Epoch 0, Iteration 226, Loss: 3.5514349937438965
Epoch 0, Iteration 227, Loss: 3.4053359031677246
Epoch 0, Iteration 228, Loss: 3.3436203002929688
Epoch 0, Iteration 229, Loss: 3.1811766624450684
Epoch 0, Iteration 230, Loss: 3.6392405033111572
Epoch 0, Iteration 231, Loss: 3.389244556427002
Epoch 0, Iteration 232, Loss: 3.488523006439209
Epoch 0, Iteration 233, Loss: 3.9203543663024902
Epoch 0, Iteration 234, Loss: 3.6974544525146484
Epoch 0, Iteration 235, Loss: 3.1045353412628174
Epoch 0, Iteration 236, Loss: 3.388026237487793
Epoch 0, Iteration 237, Loss: 3.437232494354248
Epoch 0, Iteration 238, Loss: 3.101548194885254
Epoch 0, Iteration 239, Loss: 3.2691245079040527
Epoch 0, Iteration 240, Loss: 3.019469738006592
Epoch 0, Iteration 241, Loss: 3.112483501434326
Epoch 0, Iteration 242, Loss: 2.957745313644409
Epoch 0, Iteration 243, Loss: 3.120914936065674
Epoch 0, Iteration 244, Loss: 3.1154966354370117
Epoch 0, Iteration 245, Loss: 3.011948585510254
Epoch 0, Iteration 246, Loss: 3.2363176345825195
Epoch 0, Iteration 247, Loss: 3.0341367721557617
Epoch 0, Iteration 248, Loss: 3.391435146331787
Epoch 0, Iteration 249, Loss: 2.992872714996338
Epoch 0, Iteration 250, Loss: 3.291639804840088
Epoch 0, Iteration 250, Valid Loss: 2.294922113418579
Epoch 0, Iteration 251, Loss: 3.2376084327697754
Epoch 0, Iteration 252, Loss: 3.163817882537842
Epoch 0, Iteration 253, Loss: 3.3494248390197754
Epoch 0, Iteration 254, Loss: 3.229332685470581
Epoch 0, Iteration 255, Loss: 3.3718204498291016
Epoch 0, Iteration 256, Loss: 3.460712432861328
Epoch 0, Iteration 257, Loss: 3.256852865219116
Epoch 0, Iteration 258, Loss: 3.563138008117676
Epoch 0, Iteration 259, Loss: 3.0314793586730957
Epoch 0, Iteration 260, Loss: 3.227077007293701
Epoch 0, Iteration 261, Loss: 3.3119606971740723
Epoch 0, Iteration 262, Loss: 3.0377368927001953
Epoch 0, Iteration 263, Loss: 3.298053741455078
Epoch 0, Iteration 264, Loss: 3.3239855766296387
Epoch 0, Iteration 265, Loss: 3.2758970260620117
Epoch 0, Iteration 266, Loss: 3.3802247047424316
Epoch 0, Iteration 267, Loss: 3.305941581726074
Epoch 0, Iteration 268, Loss: 3.2057762145996094
Epoch 0, Iteration 269, Loss: 3.099949836730957
Epoch 0, Iteration 270, Loss: 3.019279956817627
Epoch 0, Iteration 271, Loss: 3.190762996673584
Epoch 0, Iteration 272, Loss: 3.08166766166687
Epoch 0, Iteration 273, Loss: 3.159611225128174
Epoch 0, Iteration 274, Loss: 3.0366032123565674
Epoch 0, Iteration 275, Loss: 2.9583640098571777
Epoch 0, Iteration 276, Loss: 2.9852094650268555
Epoch 0, Iteration 277, Loss: 2.914320468902588
Epoch 0, Iteration 278, Loss: 3.0117030143737793
Epoch 0, Iteration 279, Loss: 2.9118754863739014
Epoch 0, Iteration 280, Loss: 2.766672134399414
Epoch 0, Iteration 281, Loss: 2.67610502243042
Epoch 0, Iteration 282, Loss: 3.1962027549743652
Epoch 0, Iteration 283, Loss: 3.154083251953125
Epoch 0, Iteration 284, Loss: 3.0029783248901367
Epoch 0, Iteration 285, Loss: 3.0706372261047363
Epoch 0, Iteration 286, Loss: 2.9380156993865967
Epoch 0, Iteration 287, Loss: 2.94240140914917
Epoch 0, Iteration 288, Loss: 2.7780942916870117
Epoch 0, Iteration 289, Loss: 3.0559120178222656
Epoch 0, Iteration 290, Loss: 2.838951587677002
Epoch 0, Iteration 291, Loss: 3.0171122550964355
Epoch 0, Iteration 292, Loss: 3.4577763080596924
Epoch 0, Iteration 293, Loss: 3.143867015838623
Epoch 0, Iteration 294, Loss: 2.835533618927002
Epoch 0, Iteration 295, Loss: 2.9257826805114746
Epoch 0, Iteration 296, Loss: 3.3973464965820312
Epoch 0, Iteration 297, Loss: 3.3526244163513184
Epoch 0, Iteration 298, Loss: 3.0655784606933594
Epoch 0, Iteration 299, Loss: 2.70121431350708
Epoch 0, Iteration 300, Loss: 2.85284423828125
Epoch 0, Iteration 300, Valid Loss: 2.1937718391418457
Epoch 0, Iteration 301, Loss: 2.9772768020629883
Epoch 0, Iteration 302, Loss: 2.8815865516662598
Epoch 0, Iteration 303, Loss: 3.4632625579833984
Epoch 0, Iteration 304, Loss: 2.8057823181152344
Epoch 0, Iteration 305, Loss: 2.908209800720215
Epoch 0, Iteration 306, Loss: 2.878495216369629
Epoch 0, Iteration 307, Loss: 2.779040813446045
Epoch 0, Iteration 308, Loss: 2.893857955932617
Epoch 0, Iteration 309, Loss: 2.774521827697754
Epoch 0, Iteration 310, Loss: 3.184093475341797
Epoch 0, Iteration 311, Loss: 2.7823829650878906
Epoch 0, Iteration 312, Loss: 2.703989028930664
Epoch 0, Iteration 313, Loss: 3.0016660690307617
Epoch 0, Iteration 314, Loss: 2.988527297973633
Epoch 0, Iteration 315, Loss: 2.7406654357910156
Epoch 0, Iteration 316, Loss: 2.767595052719116
Epoch 0, Iteration 317, Loss: 2.7468929290771484
Epoch 0, Iteration 318, Loss: 2.8384597301483154
Epoch 0, Iteration 319, Loss: 2.9592981338500977
Epoch 0, Iteration 320, Loss: 2.7644994258880615
Epoch 0, Iteration 321, Loss: 2.868340492248535
Epoch 0, Iteration 322, Loss: 2.669663906097412
Epoch 0, Iteration 323, Loss: 2.7342095375061035
Epoch 0, Iteration 324, Loss: 2.950629472732544
Epoch 0, Iteration 325, Loss: 2.7911365032196045
Epoch 0, Iteration 326, Loss: 2.9352779388427734
Epoch 0, Iteration 327, Loss: 2.8059194087982178
Epoch 0, Iteration 328, Loss: 2.682185649871826
Epoch 0, Iteration 329, Loss: 3.044206142425537
Epoch 0, Iteration 330, Loss: 2.5739288330078125
Epoch 0, Iteration 331, Loss: 2.7095510959625244
Epoch 0, Iteration 332, Loss: 2.9699816703796387
Epoch 0, Iteration 333, Loss: 3.0616369247436523
Epoch 0, Iteration 334, Loss: 2.701838254928589
Epoch 0, Iteration 335, Loss: 2.9443092346191406
Epoch 0, Iteration 336, Loss: 2.9674010276794434
Epoch 0, Iteration 337, Loss: 2.6864845752716064
Epoch 0, Iteration 338, Loss: 2.9434125423431396
Epoch 0, Iteration 339, Loss: 2.823716163635254
Epoch 0, Iteration 340, Loss: 2.6120057106018066
Epoch 0, Iteration 341, Loss: 2.872532367706299
Epoch 0, Iteration 342, Loss: 2.677905559539795
Epoch 0, Iteration 343, Loss: 2.601562976837158
Epoch 0, Iteration 344, Loss: 2.9277474880218506
Epoch 0, Iteration 345, Loss: 2.986400604248047
Epoch 0, Iteration 346, Loss: 2.7535345554351807
Epoch 0, Iteration 347, Loss: 2.6225433349609375
Epoch 0, Iteration 348, Loss: 2.637385129928589
Epoch 0, Iteration 349, Loss: 2.9535601139068604
Epoch 0, Iteration 350, Loss: 2.5958423614501953
Epoch 0, Iteration 350, Valid Loss: 2.1282708644866943
Epoch 0, Iteration 351, Loss: 2.805795192718506
Epoch 0, Iteration 352, Loss: 2.6815176010131836
Epoch 0, Iteration 353, Loss: 3.0265634059906006
Epoch 0, Iteration 354, Loss: 2.74658465385437
Epoch 0, Iteration 355, Loss: 2.797865152359009
Epoch 0, Iteration 356, Loss: 2.836045742034912
Epoch 0, Iteration 357, Loss: 2.819453716278076
Epoch 0, Iteration 358, Loss: 2.8677399158477783
Epoch 0, Iteration 359, Loss: 2.7354698181152344
Epoch 0, Iteration 360, Loss: 2.4977173805236816
Epoch 0, Iteration 361, Loss: 2.634648323059082
Epoch 0, Iteration 362, Loss: 2.6301333904266357
Epoch 0, Iteration 363, Loss: 2.8233461380004883
Epoch 0, Iteration 364, Loss: 2.977174758911133
Epoch 0, Iteration 365, Loss: 2.96736478805542
Epoch 0, Iteration 366, Loss: 3.0962162017822266
Epoch 0, Iteration 367, Loss: 2.914863109588623
Epoch 0, Iteration 368, Loss: 2.739309072494507
Epoch 0, Iteration 369, Loss: 3.0763838291168213
Epoch 0, Iteration 370, Loss: 2.961979866027832
Epoch 0, Iteration 371, Loss: 2.9392738342285156
Epoch 0, Iteration 372, Loss: 2.434478521347046
Epoch 0, Iteration 373, Loss: 2.6810829639434814
Epoch 0, Iteration 374, Loss: 2.992007255554199
Epoch 0, Iteration 375, Loss: 2.6845245361328125
Epoch 0, Iteration 376, Loss: 3.063183069229126
Epoch 0, Iteration 377, Loss: 2.886951208114624
Epoch 0, Iteration 378, Loss: 2.6641807556152344
Epoch 0, Iteration 379, Loss: 2.7994678020477295
Epoch 0, Iteration 380, Loss: 2.748642921447754
Epoch 0, Iteration 381, Loss: 2.5439846515655518
Epoch 0, Iteration 382, Loss: 2.5802547931671143
Epoch 0, Iteration 383, Loss: 2.7239553928375244
Epoch 0, Iteration 384, Loss: 2.7361702919006348
Epoch 0, Iteration 385, Loss: 2.6632509231567383
Epoch 0, Iteration 386, Loss: 2.6112096309661865
Epoch 0, Iteration 387, Loss: 2.9717981815338135
Epoch 0, Iteration 388, Loss: 2.7765095233917236
Epoch 0, Iteration 389, Loss: 2.8183541297912598
Epoch 0, Iteration 390, Loss: 2.9897451400756836
Epoch 0, Iteration 391, Loss: 2.7932586669921875
Epoch 1/10, Loss: 3.870835578563262
Epoch 1, Iteration 0, Loss: 2.929718494415283
Epoch 1, Iteration 1, Loss: 3.2561216354370117
Epoch 1, Iteration 2, Loss: 3.1509432792663574
Epoch 1, Iteration 3, Loss: 3.0441513061523438
Epoch 1, Iteration 4, Loss: 2.966804027557373
Epoch 1, Iteration 5, Loss: 3.2198596000671387
Epoch 1, Iteration 6, Loss: 2.822037935256958
Epoch 1, Iteration 7, Loss: 2.9864470958709717
Epoch 1, Iteration 8, Loss: 3.2148003578186035
Epoch 1, Iteration 9, Loss: 3.3268485069274902
Epoch 1, Iteration 10, Loss: 3.104628562927246
Epoch 1, Iteration 11, Loss: 2.6856932640075684
Epoch 1, Iteration 12, Loss: 2.802534580230713
Epoch 1, Iteration 13, Loss: 3.0433216094970703
Epoch 1, Iteration 14, Loss: 2.7401890754699707
Epoch 1, Iteration 15, Loss: 2.9608383178710938
Epoch 1, Iteration 16, Loss: 2.7410316467285156
Epoch 1, Iteration 17, Loss: 3.0958080291748047
Epoch 1, Iteration 18, Loss: 3.0232625007629395
Epoch 1, Iteration 19, Loss: 3.059680223464966
Epoch 1, Iteration 20, Loss: 2.9939215183258057
Epoch 1, Iteration 21, Loss: 3.1133174896240234
Epoch 1, Iteration 22, Loss: 2.767305374145508
Epoch 1, Iteration 23, Loss: 2.8227672576904297
Epoch 1, Iteration 24, Loss: 2.662497043609619
Epoch 1, Iteration 25, Loss: 2.6886305809020996
Epoch 1, Iteration 26, Loss: 3.2243032455444336
Epoch 1, Iteration 27, Loss: 2.7773919105529785
Epoch 1, Iteration 28, Loss: 2.893660545349121
Epoch 1, Iteration 29, Loss: 2.9350647926330566
Epoch 1, Iteration 30, Loss: 2.872114419937134
Epoch 1, Iteration 31, Loss: 2.7804126739501953
Epoch 1, Iteration 32, Loss: 2.624100685119629
Epoch 1, Iteration 33, Loss: 2.712965488433838
Epoch 1, Iteration 34, Loss: 2.78085994720459
Epoch 1, Iteration 35, Loss: 2.911715030670166
Epoch 1, Iteration 36, Loss: 2.6039206981658936
Epoch 1, Iteration 37, Loss: 2.636409044265747
Epoch 1, Iteration 38, Loss: 2.7570693492889404
Epoch 1, Iteration 39, Loss: 2.644141435623169
Epoch 1, Iteration 40, Loss: 2.795302391052246
Epoch 1, Iteration 41, Loss: 2.882899522781372
Epoch 1, Iteration 42, Loss: 3.060317277908325
Epoch 1, Iteration 43, Loss: 2.7741219997406006
Epoch 1, Iteration 44, Loss: 2.94028902053833
Epoch 1, Iteration 45, Loss: 2.796900510787964
Epoch 1, Iteration 46, Loss: 2.878718852996826
Epoch 1, Iteration 47, Loss: 2.8957977294921875
Epoch 1, Iteration 48, Loss: 3.0578763484954834
Epoch 1, Iteration 49, Loss: 2.9502921104431152
Epoch 1, Iteration 50, Loss: 2.858139753341675
Epoch 1, Iteration 50, Valid Loss: 1.9666019678115845
Epoch 1, Iteration 51, Loss: 2.7851085662841797
Epoch 1, Iteration 52, Loss: 2.997345447540283
Epoch 1, Iteration 53, Loss: 2.7082157135009766
Epoch 1, Iteration 54, Loss: 2.926750421524048
Epoch 1, Iteration 55, Loss: 3.011016368865967
Epoch 1, Iteration 56, Loss: 2.5122487545013428
Epoch 1, Iteration 57, Loss: 2.812964916229248
Epoch 1, Iteration 58, Loss: 2.450106620788574
Epoch 1, Iteration 59, Loss: 2.909555673599243
Epoch 1, Iteration 60, Loss: 2.9468023777008057
Epoch 1, Iteration 61, Loss: 2.3072948455810547
Epoch 1, Iteration 62, Loss: 2.548079013824463
Epoch 1, Iteration 63, Loss: 2.405674457550049
Epoch 1, Iteration 64, Loss: 3.1688709259033203
Epoch 1, Iteration 65, Loss: 2.819506883621216
Epoch 1, Iteration 66, Loss: 2.823728084564209
Epoch 1, Iteration 67, Loss: 2.512380599975586
Epoch 1, Iteration 68, Loss: 2.807260036468506
Epoch 1, Iteration 69, Loss: 2.7136073112487793
Epoch 1, Iteration 70, Loss: 2.902906656265259
Epoch 1, Iteration 71, Loss: 2.867306709289551
Epoch 1, Iteration 72, Loss: 3.0494818687438965
Epoch 1, Iteration 73, Loss: 2.355048656463623
Epoch 1, Iteration 74, Loss: 2.8432528972625732
Epoch 1, Iteration 75, Loss: 2.4165096282958984
Epoch 1, Iteration 76, Loss: 2.5069260597229004
Epoch 1, Iteration 77, Loss: 2.424684524536133
Epoch 1, Iteration 78, Loss: 3.0372862815856934
Epoch 1, Iteration 79, Loss: 2.987328052520752
Epoch 1, Iteration 80, Loss: 2.346406936645508
Epoch 1, Iteration 81, Loss: 2.8334836959838867
Epoch 1, Iteration 82, Loss: 2.529773473739624
Epoch 1, Iteration 83, Loss: 2.3993780612945557
Epoch 1, Iteration 84, Loss: 2.6648712158203125
Epoch 1, Iteration 85, Loss: 2.745170831680298
Epoch 1, Iteration 86, Loss: 2.5569651126861572
Epoch 1, Iteration 87, Loss: 2.429582118988037
Epoch 1, Iteration 88, Loss: 2.544119358062744
Epoch 1, Iteration 89, Loss: 2.609938621520996
Epoch 1, Iteration 90, Loss: 2.2164387702941895
Epoch 1, Iteration 91, Loss: 2.5206949710845947
Epoch 1, Iteration 92, Loss: 2.1905784606933594
Epoch 1, Iteration 93, Loss: 2.5010790824890137
Epoch 1, Iteration 94, Loss: 3.0153677463531494
Epoch 1, Iteration 95, Loss: 2.6962056159973145
Epoch 1, Iteration 96, Loss: 2.601996898651123
Epoch 1, Iteration 97, Loss: 2.3799147605895996
Epoch 1, Iteration 98, Loss: 2.6112074851989746
Epoch 1, Iteration 99, Loss: 2.6229066848754883
Epoch 1, Iteration 100, Loss: 2.4185631275177
Epoch 1, Iteration 100, Valid Loss: 1.9101494550704956
Epoch 1, Iteration 101, Loss: 2.8278017044067383
Epoch 1, Iteration 102, Loss: 2.6063194274902344
Epoch 1, Iteration 103, Loss: 2.478332042694092
Epoch 1, Iteration 104, Loss: 3.14284086227417
Epoch 1, Iteration 105, Loss: 2.735675096511841
Epoch 1, Iteration 106, Loss: 2.6115188598632812
Epoch 1, Iteration 107, Loss: 3.472679853439331
Epoch 1, Iteration 108, Loss: 2.5908114910125732
Epoch 1, Iteration 109, Loss: 2.914862632751465
Epoch 1, Iteration 110, Loss: 3.028566598892212
Epoch 1, Iteration 111, Loss: 2.69329571723938
Epoch 1, Iteration 112, Loss: 2.638523817062378
Epoch 1, Iteration 113, Loss: 2.368875503540039
Epoch 1, Iteration 114, Loss: 3.1091830730438232
Epoch 1, Iteration 115, Loss: 3.0127921104431152
Epoch 1, Iteration 116, Loss: 2.7085485458374023
Epoch 1, Iteration 117, Loss: 2.3850600719451904
Epoch 1, Iteration 118, Loss: 2.7669014930725098
Epoch 1, Iteration 119, Loss: 2.6632113456726074
Epoch 1, Iteration 120, Loss: 2.568939208984375
Epoch 1, Iteration 121, Loss: 2.5103979110717773
Epoch 1, Iteration 122, Loss: 2.602283239364624
Epoch 1, Iteration 123, Loss: 2.4584782123565674
Epoch 1, Iteration 124, Loss: 2.579658031463623
Epoch 1, Iteration 125, Loss: 2.276992082595825
Epoch 1, Iteration 126, Loss: 2.5218026638031006
Epoch 1, Iteration 127, Loss: 2.8691017627716064
Epoch 1, Iteration 128, Loss: 2.849830389022827
Epoch 1, Iteration 129, Loss: 2.723959445953369
Epoch 1, Iteration 130, Loss: 3.007742404937744
Epoch 1, Iteration 131, Loss: 2.5338141918182373
Epoch 1, Iteration 132, Loss: 2.516120433807373
Epoch 1, Iteration 133, Loss: 2.746861457824707
Epoch 1, Iteration 134, Loss: 2.381638288497925
Epoch 1, Iteration 135, Loss: 2.3638134002685547
Epoch 1, Iteration 136, Loss: 2.5450241565704346
Epoch 1, Iteration 137, Loss: 2.4853262901306152
Epoch 1, Iteration 138, Loss: 2.348574161529541
Epoch 1, Iteration 139, Loss: 2.377197265625
Epoch 1, Iteration 140, Loss: 2.331813335418701
Epoch 1, Iteration 141, Loss: 2.6626241207122803
Epoch 1, Iteration 142, Loss: 2.246328353881836
Epoch 1, Iteration 143, Loss: 2.594076156616211
Epoch 1, Iteration 144, Loss: 2.431610345840454
Epoch 1, Iteration 145, Loss: 2.681023597717285
Epoch 1, Iteration 146, Loss: 2.9591221809387207
Epoch 1, Iteration 147, Loss: 2.5458357334136963
Epoch 1, Iteration 148, Loss: 2.0877654552459717
Epoch 1, Iteration 149, Loss: 2.260444164276123
Epoch 1, Iteration 150, Loss: 2.622335910797119
Epoch 1, Iteration 150, Valid Loss: 1.8300867080688477
Epoch 1, Iteration 151, Loss: 2.719947099685669
Epoch 1, Iteration 152, Loss: 2.5234878063201904
Epoch 1, Iteration 153, Loss: 2.594094753265381
Epoch 1, Iteration 154, Loss: 2.3481295108795166
Epoch 1, Iteration 155, Loss: 2.488255500793457
Epoch 1, Iteration 156, Loss: 2.5492169857025146
Epoch 1, Iteration 157, Loss: 3.0227015018463135
Epoch 1, Iteration 158, Loss: 2.678347110748291
Epoch 1, Iteration 159, Loss: 2.657527208328247
Epoch 1, Iteration 160, Loss: 2.4666953086853027
Epoch 1, Iteration 161, Loss: 2.446859836578369
Epoch 1, Iteration 162, Loss: 2.853679656982422
Epoch 1, Iteration 163, Loss: 2.3126211166381836
Epoch 1, Iteration 164, Loss: 2.161681652069092
Epoch 1, Iteration 165, Loss: 2.3011436462402344
Epoch 1, Iteration 166, Loss: 2.3050525188446045
Epoch 1, Iteration 167, Loss: 2.8827528953552246
Epoch 1, Iteration 168, Loss: 2.7398979663848877
Epoch 1, Iteration 169, Loss: 2.66719388961792
Epoch 1, Iteration 170, Loss: 2.6387219429016113
Epoch 1, Iteration 171, Loss: 2.8514881134033203
Epoch 1, Iteration 172, Loss: 2.3874597549438477
Epoch 1, Iteration 173, Loss: 2.548152446746826
Epoch 1, Iteration 174, Loss: 2.7036831378936768
Epoch 1, Iteration 175, Loss: 2.59183406829834
Epoch 1, Iteration 176, Loss: 2.2608559131622314
Epoch 1, Iteration 177, Loss: 2.8548898696899414
Epoch 1, Iteration 178, Loss: 2.7187507152557373
Epoch 1, Iteration 179, Loss: 2.6329803466796875
Epoch 1, Iteration 180, Loss: 2.4543182849884033
Epoch 1, Iteration 181, Loss: 2.8478171825408936
Epoch 1, Iteration 182, Loss: 2.7359471321105957
Epoch 1, Iteration 183, Loss: 2.311985969543457
Epoch 1, Iteration 184, Loss: 2.3338398933410645
Epoch 1, Iteration 185, Loss: 2.5089516639709473
Epoch 1, Iteration 186, Loss: 2.4026618003845215
Epoch 1, Iteration 187, Loss: 2.3630547523498535
Epoch 1, Iteration 188, Loss: 2.4718127250671387
Epoch 1, Iteration 189, Loss: 2.4324283599853516
Epoch 1, Iteration 190, Loss: 2.8206276893615723
Epoch 1, Iteration 191, Loss: 2.6017861366271973
Epoch 1, Iteration 192, Loss: 2.551754951477051
Epoch 1, Iteration 193, Loss: 2.4984207153320312
Epoch 1, Iteration 194, Loss: 2.2547950744628906
Epoch 1, Iteration 195, Loss: 2.310366153717041
Epoch 1, Iteration 196, Loss: 2.5423195362091064
Epoch 1, Iteration 197, Loss: 2.5195939540863037
Epoch 1, Iteration 198, Loss: 2.4872071743011475
Epoch 1, Iteration 199, Loss: 2.655186891555786
Epoch 1, Iteration 200, Loss: 2.381539821624756
Epoch 1, Iteration 200, Valid Loss: 1.765332579612732
Epoch 1, Iteration 201, Loss: 2.4440059661865234
Epoch 1, Iteration 202, Loss: 2.622429847717285
Epoch 1, Iteration 203, Loss: 2.3156964778900146
Epoch 1, Iteration 204, Loss: 2.4730515480041504
Epoch 1, Iteration 205, Loss: 2.182501792907715
Epoch 1, Iteration 206, Loss: 2.269002914428711
Epoch 1, Iteration 207, Loss: 2.673776388168335
Epoch 1, Iteration 208, Loss: 3.020709991455078
Epoch 1, Iteration 209, Loss: 3.3633275032043457
Epoch 1, Iteration 210, Loss: 2.983981132507324
Epoch 1, Iteration 211, Loss: 3.209458827972412
Epoch 1, Iteration 212, Loss: 2.981060028076172
Epoch 1, Iteration 213, Loss: 2.947214126586914
Epoch 1, Iteration 214, Loss: 2.4936132431030273
Epoch 1, Iteration 215, Loss: 2.49436354637146
Epoch 1, Iteration 216, Loss: 2.3953919410705566
Epoch 1, Iteration 217, Loss: 2.6458687782287598
Epoch 1, Iteration 218, Loss: 2.816016912460327
Epoch 1, Iteration 219, Loss: 2.8142762184143066
Epoch 1, Iteration 220, Loss: 2.670410394668579
Epoch 1, Iteration 221, Loss: 2.6329727172851562
Epoch 1, Iteration 222, Loss: 2.5261390209198
Epoch 1, Iteration 223, Loss: 2.5003604888916016
Epoch 1, Iteration 224, Loss: 2.3236725330352783
Epoch 1, Iteration 225, Loss: 2.6488590240478516
Epoch 1, Iteration 226, Loss: 2.670156478881836
Epoch 1, Iteration 227, Loss: 2.4873852729797363
Epoch 1, Iteration 228, Loss: 2.2820401191711426
Epoch 1, Iteration 229, Loss: 2.311140775680542
Epoch 1, Iteration 230, Loss: 2.636190414428711
Epoch 1, Iteration 231, Loss: 2.530656337738037
Epoch 1, Iteration 232, Loss: 2.5059800148010254
Epoch 1, Iteration 233, Loss: 2.9906015396118164
Epoch 1, Iteration 234, Loss: 2.721343517303467
Epoch 1, Iteration 235, Loss: 2.1888437271118164
Epoch 1, Iteration 236, Loss: 2.5676398277282715
Epoch 1, Iteration 237, Loss: 2.5557446479797363
Epoch 1, Iteration 238, Loss: 2.236215591430664
Epoch 1, Iteration 239, Loss: 2.573184013366699
Epoch 1, Iteration 240, Loss: 2.102567195892334
Epoch 1, Iteration 241, Loss: 2.206817150115967
Epoch 1, Iteration 242, Loss: 2.1450085639953613
Epoch 1, Iteration 243, Loss: 2.3601021766662598
Epoch 1, Iteration 244, Loss: 2.382343292236328
Epoch 1, Iteration 245, Loss: 2.1201744079589844
Epoch 1, Iteration 246, Loss: 2.421661376953125
Epoch 1, Iteration 247, Loss: 2.2631704807281494
Epoch 1, Iteration 248, Loss: 2.4826433658599854
Epoch 1, Iteration 249, Loss: 2.177189350128174
Epoch 1, Iteration 250, Loss: 2.5035276412963867
Epoch 1, Iteration 250, Valid Loss: 1.68952214717865
Epoch 1, Iteration 251, Loss: 2.365046977996826
Epoch 1, Iteration 252, Loss: 2.4270150661468506
Epoch 1, Iteration 253, Loss: 2.5271384716033936
Epoch 1, Iteration 254, Loss: 2.4443721771240234
Epoch 1, Iteration 255, Loss: 2.49369478225708
Epoch 1, Iteration 256, Loss: 2.6767992973327637
Epoch 1, Iteration 257, Loss: 2.210475444793701
Epoch 1, Iteration 258, Loss: 2.712580919265747
Epoch 1, Iteration 259, Loss: 2.1237823963165283
Epoch 1, Iteration 260, Loss: 2.41736102104187
Epoch 1, Iteration 261, Loss: 2.522493839263916
Epoch 1, Iteration 262, Loss: 2.1298024654388428
Epoch 1, Iteration 263, Loss: 2.648317337036133
Epoch 1, Iteration 264, Loss: 2.525376319885254
Epoch 1, Iteration 265, Loss: 2.452493190765381
Epoch 1, Iteration 266, Loss: 2.4815707206726074
Epoch 1, Iteration 267, Loss: 2.3857083320617676
Epoch 1, Iteration 268, Loss: 2.308835983276367
Epoch 1, Iteration 269, Loss: 2.304215669631958
Epoch 1, Iteration 270, Loss: 2.288381576538086
Epoch 1, Iteration 271, Loss: 2.4869730472564697
Epoch 1, Iteration 272, Loss: 2.22098970413208
Epoch 1, Iteration 273, Loss: 2.120967149734497
Epoch 1, Iteration 274, Loss: 2.165925979614258
Epoch 1, Iteration 275, Loss: 2.1176483631134033
Epoch 1, Iteration 276, Loss: 2.1733551025390625
Epoch 1, Iteration 277, Loss: 2.1782917976379395
Epoch 1, Iteration 278, Loss: 2.4260427951812744
Epoch 1, Iteration 279, Loss: 2.2003870010375977
Epoch 1, Iteration 280, Loss: 2.0047311782836914
Epoch 1, Iteration 281, Loss: 2.081037759780884
Epoch 1, Iteration 282, Loss: 2.390122890472412
Epoch 1, Iteration 283, Loss: 2.3455092906951904
Epoch 1, Iteration 284, Loss: 2.5062098503112793
Epoch 1, Iteration 285, Loss: 2.2450950145721436
Epoch 1, Iteration 286, Loss: 2.2038962841033936
Epoch 1, Iteration 287, Loss: 2.179431200027466
Epoch 1, Iteration 288, Loss: 2.088042736053467
Epoch 1, Iteration 289, Loss: 2.270559787750244
Epoch 1, Iteration 290, Loss: 2.112051248550415
Epoch 1, Iteration 291, Loss: 2.307587146759033
Epoch 1, Iteration 292, Loss: 2.6125452518463135
Epoch 1, Iteration 293, Loss: 2.5254201889038086
Epoch 1, Iteration 294, Loss: 2.123588800430298
Epoch 1, Iteration 295, Loss: 2.160126209259033
Epoch 1, Iteration 296, Loss: 2.6193137168884277
Epoch 1, Iteration 297, Loss: 2.474640369415283
Epoch 1, Iteration 298, Loss: 2.3382205963134766
Epoch 1, Iteration 299, Loss: 1.9627776145935059
Epoch 1, Iteration 300, Loss: 2.0814061164855957
Epoch 1, Iteration 300, Valid Loss: 1.6283998489379883
Epoch 1, Iteration 301, Loss: 2.3136165142059326
Epoch 1, Iteration 302, Loss: 2.230719804763794
Epoch 1, Iteration 303, Loss: 2.6891419887542725
Epoch 1, Iteration 304, Loss: 2.0816075801849365
Epoch 1, Iteration 305, Loss: 2.340463161468506
Epoch 1, Iteration 306, Loss: 2.3947696685791016
Epoch 1, Iteration 307, Loss: 2.093515634536743
Epoch 1, Iteration 308, Loss: 2.166707992553711
Epoch 1, Iteration 309, Loss: 2.1189637184143066
Epoch 1, Iteration 310, Loss: 2.5303659439086914
Epoch 1, Iteration 311, Loss: 2.018118381500244
Epoch 1, Iteration 312, Loss: 2.0426554679870605
Epoch 1, Iteration 313, Loss: 2.283726692199707
Epoch 1, Iteration 314, Loss: 2.1380627155303955
Epoch 1, Iteration 315, Loss: 2.0876429080963135
Epoch 1, Iteration 316, Loss: 2.1197471618652344
Epoch 1, Iteration 317, Loss: 2.132847309112549
Epoch 1, Iteration 318, Loss: 2.1174373626708984
Epoch 1, Iteration 319, Loss: 2.2444114685058594
Epoch 1, Iteration 320, Loss: 1.9696483612060547
Epoch 1, Iteration 321, Loss: 2.069786310195923
Epoch 1, Iteration 322, Loss: 2.0206265449523926
Epoch 1, Iteration 323, Loss: 2.152468204498291
Epoch 1, Iteration 324, Loss: 2.3122990131378174
Epoch 1, Iteration 325, Loss: 2.1322150230407715
Epoch 1, Iteration 326, Loss: 2.1725566387176514
Epoch 1, Iteration 327, Loss: 2.224168062210083
Epoch 1, Iteration 328, Loss: 2.1094627380371094
Epoch 1, Iteration 329, Loss: 2.4502780437469482
Epoch 1, Iteration 330, Loss: 1.9047563076019287
Epoch 1, Iteration 331, Loss: 1.9482529163360596
Epoch 1, Iteration 332, Loss: 2.207941770553589
Epoch 1, Iteration 333, Loss: 2.4555821418762207
Epoch 1, Iteration 334, Loss: 2.228618621826172
Epoch 1, Iteration 335, Loss: 2.341294765472412
Epoch 1, Iteration 336, Loss: 2.2027230262756348
Epoch 1, Iteration 337, Loss: 1.9787497520446777
Epoch 1, Iteration 338, Loss: 2.2882418632507324
Epoch 1, Iteration 339, Loss: 2.171067237854004
Epoch 1, Iteration 340, Loss: 1.971839189529419
Epoch 1, Iteration 341, Loss: 2.1621549129486084
Epoch 1, Iteration 342, Loss: 1.9842110872268677
Epoch 1, Iteration 343, Loss: 2.103533983230591
Epoch 1, Iteration 344, Loss: 2.264626979827881
Epoch 1, Iteration 345, Loss: 2.267334461212158
Epoch 1, Iteration 346, Loss: 2.152834892272949
Epoch 1, Iteration 347, Loss: 1.8919835090637207
Epoch 1, Iteration 348, Loss: 1.9597171545028687
Epoch 1, Iteration 349, Loss: 2.3375935554504395
Epoch 1, Iteration 350, Loss: 1.9475200176239014
Epoch 1, Iteration 350, Valid Loss: 1.5681591033935547
Epoch 1, Iteration 351, Loss: 2.2800803184509277
Epoch 1, Iteration 352, Loss: 2.022433280944824
Epoch 1, Iteration 353, Loss: 2.3380930423736572
Epoch 1, Iteration 354, Loss: 2.2082972526550293
Epoch 1, Iteration 355, Loss: 2.011723518371582
Epoch 1, Iteration 356, Loss: 2.172292947769165
Epoch 1, Iteration 357, Loss: 2.1731116771698
Epoch 1, Iteration 358, Loss: 2.116807699203491
Epoch 1, Iteration 359, Loss: 2.0247178077697754
Epoch 1, Iteration 360, Loss: 1.9530994892120361
Epoch 1, Iteration 361, Loss: 2.0484204292297363
Epoch 1, Iteration 362, Loss: 2.0422732830047607
Epoch 1, Iteration 363, Loss: 2.1841890811920166
Epoch 1, Iteration 364, Loss: 2.077766180038452
Epoch 1, Iteration 365, Loss: 2.5500409603118896
Epoch 1, Iteration 366, Loss: 2.45626163482666
Epoch 1, Iteration 367, Loss: 2.2972538471221924
Epoch 1, Iteration 368, Loss: 2.1800365447998047
Epoch 1, Iteration 369, Loss: 2.341439723968506
Epoch 1, Iteration 370, Loss: 2.264253616333008
Epoch 1, Iteration 371, Loss: 2.2345190048217773
Epoch 1, Iteration 372, Loss: 1.8584774732589722
Epoch 1, Iteration 373, Loss: 2.0580334663391113
Epoch 1, Iteration 374, Loss: 2.4043009281158447
Epoch 1, Iteration 375, Loss: 2.0695583820343018
Epoch 1, Iteration 376, Loss: 2.3634438514709473
Epoch 1, Iteration 377, Loss: 2.277512550354004
Epoch 1, Iteration 378, Loss: 2.0501742362976074
Epoch 1, Iteration 379, Loss: 2.1528944969177246
Epoch 1, Iteration 380, Loss: 2.096625328063965
Epoch 1, Iteration 381, Loss: 1.9787874221801758
Epoch 1, Iteration 382, Loss: 1.922613501548767
Epoch 1, Iteration 383, Loss: 2.088258743286133
Epoch 1, Iteration 384, Loss: 2.1069037914276123
Epoch 1, Iteration 385, Loss: 1.9982714653015137
Epoch 1, Iteration 386, Loss: 1.9964567422866821
Epoch 1, Iteration 387, Loss: 2.650114059448242
Epoch 1, Iteration 388, Loss: 2.1228854656219482
Epoch 1, Iteration 389, Loss: 2.2169735431671143
Epoch 1, Iteration 390, Loss: 2.320500135421753
Epoch 1, Iteration 391, Loss: 2.0929336547851562
Epoch 2/10, Loss: 2.501201459643792
Epoch 2, Iteration 0, Loss: 2.286557674407959
Epoch 2, Iteration 1, Loss: 2.614622116088867
Epoch 2, Iteration 2, Loss: 2.4487786293029785
Epoch 2, Iteration 3, Loss: 2.3690881729125977
Epoch 2, Iteration 4, Loss: 2.3142921924591064
Epoch 2, Iteration 5, Loss: 2.382401704788208
Epoch 2, Iteration 6, Loss: 2.190772294998169
Epoch 2, Iteration 7, Loss: 2.346938371658325
Epoch 2, Iteration 8, Loss: 2.4393463134765625
Epoch 2, Iteration 9, Loss: 2.6392405033111572
Epoch 2, Iteration 10, Loss: 2.2874512672424316
Epoch 2, Iteration 11, Loss: 2.157971143722534
Epoch 2, Iteration 12, Loss: 2.2852015495300293
Epoch 2, Iteration 13, Loss: 2.485421657562256
Epoch 2, Iteration 14, Loss: 2.1346280574798584
Epoch 2, Iteration 15, Loss: 2.393770694732666
Epoch 2, Iteration 16, Loss: 2.1718499660491943
Epoch 2, Iteration 17, Loss: 2.432003974914551
Epoch 2, Iteration 18, Loss: 2.3318567276000977
Epoch 2, Iteration 19, Loss: 2.3828635215759277
Epoch 2, Iteration 20, Loss: 2.2084734439849854
Epoch 2, Iteration 21, Loss: 2.2867214679718018
Epoch 2, Iteration 22, Loss: 2.278371810913086
Epoch 2, Iteration 23, Loss: 2.269719123840332
Epoch 2, Iteration 24, Loss: 2.086919069290161
Epoch 2, Iteration 25, Loss: 2.007523536682129
Epoch 2, Iteration 26, Loss: 2.584179639816284
Epoch 2, Iteration 27, Loss: 2.1719720363616943
Epoch 2, Iteration 28, Loss: 2.194897174835205
Epoch 2, Iteration 29, Loss: 2.4409170150756836
Epoch 2, Iteration 30, Loss: 2.276454210281372
Epoch 2, Iteration 31, Loss: 2.25176739692688
Epoch 2, Iteration 32, Loss: 2.2687740325927734
Epoch 2, Iteration 33, Loss: 2.1357016563415527
Epoch 2, Iteration 34, Loss: 2.175964832305908
Epoch 2, Iteration 35, Loss: 2.2780418395996094
Epoch 2, Iteration 36, Loss: 1.948418140411377
Epoch 2, Iteration 37, Loss: 1.9796761274337769
Epoch 2, Iteration 38, Loss: 2.085378646850586
Epoch 2, Iteration 39, Loss: 2.048222780227661
Epoch 2, Iteration 40, Loss: 2.221381187438965
Epoch 2, Iteration 41, Loss: 2.194794178009033
Epoch 2, Iteration 42, Loss: 2.3149428367614746
Epoch 2, Iteration 43, Loss: 2.1628475189208984
Epoch 2, Iteration 44, Loss: 2.284600257873535
Epoch 2, Iteration 45, Loss: 2.120267152786255
Epoch 2, Iteration 46, Loss: 2.2481346130371094
Epoch 2, Iteration 47, Loss: 2.304439067840576
Epoch 2, Iteration 48, Loss: 2.4795522689819336
Epoch 2, Iteration 49, Loss: 2.244668483734131
Epoch 2, Iteration 50, Loss: 2.162907600402832
Epoch 2, Iteration 50, Valid Loss: 1.4213221073150635
Epoch 2, Iteration 51, Loss: 2.0878076553344727
Epoch 2, Iteration 52, Loss: 2.2632603645324707
Epoch 2, Iteration 53, Loss: 2.108198404312134
Epoch 2, Iteration 54, Loss: 4.600985527038574
Epoch 2, Iteration 55, Loss: 2.2870593070983887
Epoch 2, Iteration 56, Loss: 1.9840397834777832
Epoch 2, Iteration 57, Loss: 2.1654469966888428
Epoch 2, Iteration 58, Loss: 1.908090353012085
Epoch 2, Iteration 59, Loss: 2.1883792877197266
Epoch 2, Iteration 60, Loss: 2.6085245609283447
Epoch 2, Iteration 61, Loss: 1.8684709072113037
Epoch 2, Iteration 62, Loss: 2.101689338684082
Epoch 2, Iteration 63, Loss: 1.969943642616272
Epoch 2, Iteration 64, Loss: 1.9301307201385498
Epoch 2, Iteration 65, Loss: 2.2052161693573
Epoch 2, Iteration 66, Loss: 2.1853466033935547
Epoch 2, Iteration 67, Loss: 1.8913639783859253
Epoch 2, Iteration 68, Loss: 2.4045915603637695
Epoch 2, Iteration 69, Loss: 2.384411096572876
Epoch 2, Iteration 70, Loss: 2.2909042835235596
Epoch 2, Iteration 71, Loss: 2.2285218238830566
Epoch 2, Iteration 72, Loss: 2.469818353652954
Epoch 2, Iteration 73, Loss: 1.7943761348724365
Epoch 2, Iteration 74, Loss: 2.350026845932007
Epoch 2, Iteration 75, Loss: 2.044023275375366
Epoch 2, Iteration 76, Loss: 2.080824375152588
Epoch 2, Iteration 77, Loss: 1.7879729270935059
Epoch 2, Iteration 78, Loss: 2.49055552482605
Epoch 2, Iteration 79, Loss: 2.4874320030212402
Epoch 2, Iteration 80, Loss: 2.0746617317199707
Epoch 2, Iteration 81, Loss: 2.358713150024414
Epoch 2, Iteration 82, Loss: 2.2870123386383057
Epoch 2, Iteration 83, Loss: 1.8669553995132446
Epoch 2, Iteration 84, Loss: 2.282327651977539
Epoch 2, Iteration 85, Loss: 2.162407875061035
Epoch 2, Iteration 86, Loss: 2.164499282836914
Epoch 2, Iteration 87, Loss: 2.0206618309020996
Epoch 2, Iteration 88, Loss: 2.1260759830474854
Epoch 2, Iteration 89, Loss: 2.1906001567840576
Epoch 2, Iteration 90, Loss: 1.639358401298523
Epoch 2, Iteration 91, Loss: 2.035695791244507
Epoch 2, Iteration 92, Loss: 1.6431154012680054
Epoch 2, Iteration 93, Loss: 2.0404434204101562
Epoch 2, Iteration 94, Loss: 2.5114688873291016
Epoch 2, Iteration 95, Loss: 2.067798137664795
Epoch 2, Iteration 96, Loss: 2.119192600250244
Epoch 2, Iteration 97, Loss: 1.841003179550171
Epoch 2, Iteration 98, Loss: 2.2519705295562744
Epoch 2, Iteration 99, Loss: 2.0609309673309326
Epoch 2, Iteration 100, Loss: 1.9864476919174194
Epoch 2, Iteration 100, Valid Loss: 1.4327645301818848
Epoch 2, Iteration 101, Loss: 2.433574676513672
Epoch 2, Iteration 102, Loss: 2.2646055221557617
Epoch 2, Iteration 103, Loss: 2.0611140727996826
Epoch 2, Iteration 104, Loss: 2.56742525100708
Epoch 2, Iteration 105, Loss: 2.2528176307678223
Epoch 2, Iteration 106, Loss: 2.098330497741699
Epoch 2, Iteration 107, Loss: 2.988760232925415
Epoch 2, Iteration 108, Loss: 2.1841814517974854
Epoch 2, Iteration 109, Loss: 2.3368499279022217
Epoch 2, Iteration 110, Loss: 2.3855717182159424
Epoch 2, Iteration 111, Loss: 2.2859115600585938
Epoch 2, Iteration 112, Loss: 2.1882638931274414
Epoch 2, Iteration 113, Loss: 1.8568365573883057
Epoch 2, Iteration 114, Loss: 2.662118434906006
Epoch 2, Iteration 115, Loss: 2.4008641242980957
Epoch 2, Iteration 116, Loss: 2.1028356552124023
Epoch 2, Iteration 117, Loss: 1.8301891088485718
Epoch 2, Iteration 118, Loss: 2.270049810409546
Epoch 2, Iteration 119, Loss: 2.1909773349761963
Epoch 2, Iteration 120, Loss: 1.9862655401229858
Epoch 2, Iteration 121, Loss: 1.989842414855957
Epoch 2, Iteration 122, Loss: 2.065607786178589
Epoch 2, Iteration 123, Loss: 1.8876595497131348
Epoch 2, Iteration 124, Loss: 2.0782902240753174
Epoch 2, Iteration 125, Loss: 1.894863486289978
Epoch 2, Iteration 126, Loss: 1.8142839670181274
Epoch 2, Iteration 127, Loss: 2.5226454734802246
Epoch 2, Iteration 128, Loss: 2.306884288787842
Epoch 2, Iteration 129, Loss: 2.178489923477173
Epoch 2, Iteration 130, Loss: 2.4563405513763428
Epoch 2, Iteration 131, Loss: 1.9575656652450562
Epoch 2, Iteration 132, Loss: 1.9815584421157837
Epoch 2, Iteration 133, Loss: 2.463592767715454
Epoch 2, Iteration 134, Loss: 1.8528800010681152
Epoch 2, Iteration 135, Loss: 2.1706197261810303
Epoch 2, Iteration 136, Loss: 1.9732637405395508
Epoch 2, Iteration 137, Loss: 1.9684380292892456
Epoch 2, Iteration 138, Loss: 1.9924395084381104
Epoch 2, Iteration 139, Loss: 1.9029819965362549
Epoch 2, Iteration 140, Loss: 2.0194528102874756
Epoch 2, Iteration 141, Loss: 2.517256736755371
Epoch 2, Iteration 142, Loss: 1.8961538076400757
Epoch 2, Iteration 143, Loss: 2.097055196762085
Epoch 2, Iteration 144, Loss: 1.9962385892868042
Epoch 2, Iteration 145, Loss: 2.2070603370666504
Epoch 2, Iteration 146, Loss: 2.262887954711914
Epoch 2, Iteration 147, Loss: 1.9534482955932617
Epoch 2, Iteration 148, Loss: 1.715044617652893
Epoch 2, Iteration 149, Loss: 1.9373698234558105
Epoch 2, Iteration 150, Loss: 2.183907985687256
Epoch 2, Iteration 150, Valid Loss: 1.3856003284454346
Epoch 2, Iteration 151, Loss: 2.125739574432373
Epoch 2, Iteration 152, Loss: 1.9286315441131592
Epoch 2, Iteration 153, Loss: 2.1813530921936035
Epoch 2, Iteration 154, Loss: 1.9181668758392334
Epoch 2, Iteration 155, Loss: 2.22383975982666
Epoch 2, Iteration 156, Loss: 1.9474704265594482
Epoch 2, Iteration 157, Loss: 2.4764509201049805
Epoch 2, Iteration 158, Loss: 2.1043734550476074
Epoch 2, Iteration 159, Loss: 2.118313789367676
Epoch 2, Iteration 160, Loss: 1.9635858535766602
Epoch 2, Iteration 161, Loss: 2.0313117504119873
Epoch 2, Iteration 162, Loss: 2.4122838973999023
Epoch 2, Iteration 163, Loss: 1.861215591430664
Epoch 2, Iteration 164, Loss: 1.9887120723724365
Epoch 2, Iteration 165, Loss: 1.9791063070297241
Epoch 2, Iteration 166, Loss: 1.9717398881912231
Epoch 2, Iteration 167, Loss: 2.418872117996216
Epoch 2, Iteration 168, Loss: 2.246973991394043
Epoch 2, Iteration 169, Loss: 2.1547436714172363
Epoch 2, Iteration 170, Loss: 2.230612277984619
Epoch 2, Iteration 171, Loss: 2.136319875717163
Epoch 2, Iteration 172, Loss: 1.9036263227462769
Epoch 2, Iteration 173, Loss: 2.0495996475219727
Epoch 2, Iteration 174, Loss: 2.1942877769470215
Epoch 2, Iteration 175, Loss: 2.229475498199463
Epoch 2, Iteration 176, Loss: 1.846193790435791
Epoch 2, Iteration 177, Loss: 2.551244020462036
Epoch 2, Iteration 178, Loss: 2.340250253677368
Epoch 2, Iteration 179, Loss: 2.1640238761901855
Epoch 2, Iteration 180, Loss: 2.035254955291748
Epoch 2, Iteration 181, Loss: 2.455217123031616
Epoch 2, Iteration 182, Loss: 2.3375377655029297
Epoch 2, Iteration 183, Loss: 1.8851567506790161
Epoch 2, Iteration 184, Loss: 1.9357702732086182
Epoch 2, Iteration 185, Loss: 1.9999479055404663
Epoch 2, Iteration 186, Loss: 1.9416370391845703
Epoch 2, Iteration 187, Loss: 1.9533491134643555
Epoch 2, Iteration 188, Loss: 2.025179624557495
Epoch 2, Iteration 189, Loss: 2.0295631885528564
Epoch 2, Iteration 190, Loss: 2.387248992919922
Epoch 2, Iteration 191, Loss: 2.2403266429901123
Epoch 2, Iteration 192, Loss: 2.171095848083496
Epoch 2, Iteration 193, Loss: 2.080294609069824
Epoch 2, Iteration 194, Loss: 2.048478841781616
Epoch 2, Iteration 195, Loss: 1.802522897720337
Epoch 2, Iteration 196, Loss: 2.0860238075256348
Epoch 2, Iteration 197, Loss: 2.090195894241333
Epoch 2, Iteration 198, Loss: 2.0340468883514404
Epoch 2, Iteration 199, Loss: 2.171344041824341
Epoch 2, Iteration 200, Loss: 1.9438915252685547
Epoch 2, Iteration 200, Valid Loss: 1.3462024927139282
Epoch 2, Iteration 201, Loss: 1.9121882915496826
Epoch 2, Iteration 202, Loss: 2.148733139038086
Epoch 2, Iteration 203, Loss: 1.8877822160720825
Epoch 2, Iteration 204, Loss: 2.0536205768585205
Epoch 2, Iteration 205, Loss: 1.729576587677002
Epoch 2, Iteration 206, Loss: 1.8970060348510742
Epoch 2, Iteration 207, Loss: 2.20676326751709
Epoch 2, Iteration 208, Loss: 2.6616551876068115
Epoch 2, Iteration 209, Loss: 3.0502357482910156
Epoch 2, Iteration 210, Loss: 2.5110857486724854
Epoch 2, Iteration 211, Loss: 2.770214557647705
Epoch 2, Iteration 212, Loss: 2.4686129093170166
Epoch 2, Iteration 213, Loss: 2.590967893600464
Epoch 2, Iteration 214, Loss: 2.1375529766082764
Epoch 2, Iteration 215, Loss: 1.9817966222763062
Epoch 2, Iteration 216, Loss: 1.9952163696289062
Epoch 2, Iteration 217, Loss: 2.445341110229492
Epoch 2, Iteration 218, Loss: 2.3014204502105713
Epoch 2, Iteration 219, Loss: 2.2331719398498535
Epoch 2, Iteration 220, Loss: 2.177527904510498
Epoch 2, Iteration 221, Loss: 2.1814441680908203
Epoch 2, Iteration 222, Loss: 2.0641705989837646
Epoch 2, Iteration 223, Loss: 2.1376614570617676
Epoch 2, Iteration 224, Loss: 2.080907106399536
Epoch 2, Iteration 225, Loss: 2.1908156871795654
Epoch 2, Iteration 226, Loss: 2.295016050338745
Epoch 2, Iteration 227, Loss: 2.128798484802246
Epoch 2, Iteration 228, Loss: 1.9457005262374878
Epoch 2, Iteration 229, Loss: 2.0115182399749756
Epoch 2, Iteration 230, Loss: 2.226147174835205
Epoch 2, Iteration 231, Loss: 2.0994460582733154
Epoch 2, Iteration 232, Loss: 2.23634934425354
Epoch 2, Iteration 233, Loss: 2.599374532699585
Epoch 2, Iteration 234, Loss: 2.3429858684539795
Epoch 2, Iteration 235, Loss: 1.8240832090377808
Epoch 2, Iteration 236, Loss: 2.1357641220092773
Epoch 2, Iteration 237, Loss: 2.0123982429504395
Epoch 2, Iteration 238, Loss: 1.8875737190246582
Epoch 2, Iteration 239, Loss: 2.1969351768493652
Epoch 2, Iteration 240, Loss: 1.7195281982421875
Epoch 2, Iteration 241, Loss: 1.8051626682281494
Epoch 2, Iteration 242, Loss: 1.811455249786377
Epoch 2, Iteration 243, Loss: 1.8903343677520752
Epoch 2, Iteration 244, Loss: 1.885507583618164
Epoch 2, Iteration 245, Loss: 1.6984268426895142
Epoch 2, Iteration 246, Loss: 2.0087549686431885
Epoch 2, Iteration 247, Loss: 1.8542749881744385
Epoch 2, Iteration 248, Loss: 2.075773239135742
Epoch 2, Iteration 249, Loss: 2.006000280380249
Epoch 2, Iteration 250, Loss: 2.162698745727539
Epoch 2, Iteration 250, Valid Loss: 1.3049259185791016
Epoch 2, Iteration 251, Loss: 1.9742088317871094
Epoch 2, Iteration 252, Loss: 2.1591460704803467
Epoch 2, Iteration 253, Loss: 2.2223827838897705
Epoch 2, Iteration 254, Loss: 2.072493553161621
Epoch 2, Iteration 255, Loss: 2.1565940380096436
Epoch 2, Iteration 256, Loss: 2.3472816944122314
Epoch 2, Iteration 257, Loss: 2.024179220199585
Epoch 2, Iteration 258, Loss: 2.3193531036376953
Epoch 2, Iteration 259, Loss: 1.7896747589111328
Epoch 2, Iteration 260, Loss: 2.1168811321258545
Epoch 2, Iteration 261, Loss: 2.248678207397461
Epoch 2, Iteration 262, Loss: 1.8478422164916992
Epoch 2, Iteration 263, Loss: 2.2681431770324707
Epoch 2, Iteration 264, Loss: 2.0776829719543457
Epoch 2, Iteration 265, Loss: 2.0884451866149902
Epoch 2, Iteration 266, Loss: 2.1199557781219482
Epoch 2, Iteration 267, Loss: 2.0518977642059326
Epoch 2, Iteration 268, Loss: 2.076777458190918
Epoch 2, Iteration 269, Loss: 1.978698968887329
Epoch 2, Iteration 270, Loss: 1.8976309299468994
Epoch 2, Iteration 271, Loss: 2.1241865158081055
Epoch 2, Iteration 272, Loss: 1.919883131980896
Epoch 2, Iteration 273, Loss: 2.154454231262207
Epoch 2, Iteration 274, Loss: 1.7748363018035889
Epoch 2, Iteration 275, Loss: 1.7188624143600464
Epoch 2, Iteration 276, Loss: 1.8906900882720947
Epoch 2, Iteration 277, Loss: 1.900101661682129
Epoch 2, Iteration 278, Loss: 2.137911319732666
Epoch 2, Iteration 279, Loss: 1.8481857776641846
Epoch 2, Iteration 280, Loss: 1.637709379196167
Epoch 2, Iteration 281, Loss: 1.7629077434539795
Epoch 2, Iteration 282, Loss: 2.0265326499938965
Epoch 2, Iteration 283, Loss: 1.9493041038513184
Epoch 2, Iteration 284, Loss: 2.0075225830078125
Epoch 2, Iteration 285, Loss: 1.8201749324798584
Epoch 2, Iteration 286, Loss: 1.9577265977859497
Epoch 2, Iteration 287, Loss: 1.9044930934906006
Epoch 2, Iteration 288, Loss: 1.818145751953125
Epoch 2, Iteration 289, Loss: 1.9698574542999268
Epoch 2, Iteration 290, Loss: 1.8352595567703247
Epoch 2, Iteration 291, Loss: 1.8840535879135132
Epoch 2, Iteration 292, Loss: 2.252629518508911
Epoch 2, Iteration 293, Loss: 2.1793670654296875
Epoch 2, Iteration 294, Loss: 1.865281581878662
Epoch 2, Iteration 295, Loss: 1.9119187593460083
Epoch 2, Iteration 296, Loss: 2.2657687664031982
Epoch 2, Iteration 297, Loss: 2.1381402015686035
Epoch 2, Iteration 298, Loss: 2.03503680229187
Epoch 2, Iteration 299, Loss: 1.663654088973999
Epoch 2, Iteration 300, Loss: 1.7111482620239258
Epoch 2, Iteration 300, Valid Loss: 1.2681950330734253
Epoch 2, Iteration 301, Loss: 1.9531176090240479
Epoch 2, Iteration 302, Loss: 1.6202366352081299
Epoch 2, Iteration 303, Loss: 2.353825569152832
Epoch 2, Iteration 304, Loss: 1.8361741304397583
Epoch 2, Iteration 305, Loss: 1.869588851928711
Epoch 2, Iteration 306, Loss: 1.9020874500274658
Epoch 2, Iteration 307, Loss: 1.7567496299743652
Epoch 2, Iteration 308, Loss: 1.8369019031524658
Epoch 2, Iteration 309, Loss: 1.8667778968811035
Epoch 2, Iteration 310, Loss: 2.197144031524658
Epoch 2, Iteration 311, Loss: 1.7186299562454224
Epoch 2, Iteration 312, Loss: 1.7634892463684082
Epoch 2, Iteration 313, Loss: 2.065945863723755
Epoch 2, Iteration 314, Loss: 1.7706518173217773
Epoch 2, Iteration 315, Loss: 1.636157751083374
Epoch 2, Iteration 316, Loss: 1.924253225326538
Epoch 2, Iteration 317, Loss: 1.8084675073623657
Epoch 2, Iteration 318, Loss: 1.7598590850830078
Epoch 2, Iteration 319, Loss: 2.036818742752075
Epoch 2, Iteration 320, Loss: 1.6256771087646484
Epoch 2, Iteration 321, Loss: 1.860236406326294
Epoch 2, Iteration 322, Loss: 1.7952277660369873
Epoch 2, Iteration 323, Loss: 1.8297789096832275
Epoch 2, Iteration 324, Loss: 1.8872783184051514
Epoch 2, Iteration 325, Loss: 1.8373396396636963
Epoch 2, Iteration 326, Loss: 1.8843228816986084
Epoch 2, Iteration 327, Loss: 1.8274425268173218
Epoch 2, Iteration 328, Loss: 1.7584458589553833
Epoch 2, Iteration 329, Loss: 1.9948744773864746
Epoch 2, Iteration 330, Loss: 1.5826058387756348
Epoch 2, Iteration 331, Loss: 1.7651195526123047
Epoch 2, Iteration 332, Loss: 1.8706790208816528
Epoch 2, Iteration 333, Loss: 2.1401162147521973
Epoch 2, Iteration 334, Loss: 1.936783790588379
Epoch 2, Iteration 335, Loss: 2.018296957015991
Epoch 2, Iteration 336, Loss: 1.9182751178741455
Epoch 2, Iteration 337, Loss: 1.660406470298767
Epoch 2, Iteration 338, Loss: 1.9610058069229126
Epoch 2, Iteration 339, Loss: 1.8439879417419434
Epoch 2, Iteration 340, Loss: 1.5922224521636963
Epoch 2, Iteration 341, Loss: 1.7403309345245361
Epoch 2, Iteration 342, Loss: 1.8345891237258911
Epoch 2, Iteration 343, Loss: 1.6946364641189575
Epoch 2, Iteration 344, Loss: 1.983027458190918
Epoch 2, Iteration 345, Loss: 1.885607123374939
Epoch 2, Iteration 346, Loss: 1.8350143432617188
Epoch 2, Iteration 347, Loss: 1.6449010372161865
Epoch 2, Iteration 348, Loss: 1.6676836013793945
Epoch 2, Iteration 349, Loss: 1.9327459335327148
Epoch 2, Iteration 350, Loss: 1.7833105325698853
Epoch 2, Iteration 350, Valid Loss: 1.2611229419708252
Epoch 2, Iteration 351, Loss: 1.9248390197753906
Epoch 2, Iteration 352, Loss: 1.7204817533493042
Epoch 2, Iteration 353, Loss: 1.961958408355713
Epoch 2, Iteration 354, Loss: 1.8910620212554932
Epoch 2, Iteration 355, Loss: 1.765541911125183
Epoch 2, Iteration 356, Loss: 2.1174516677856445
Epoch 2, Iteration 357, Loss: 1.8061655759811401
Epoch 2, Iteration 358, Loss: 1.8119597434997559
Epoch 2, Iteration 359, Loss: 1.7856932878494263
Epoch 2, Iteration 360, Loss: 1.683517575263977
Epoch 2, Iteration 361, Loss: 1.6589863300323486
Epoch 2, Iteration 362, Loss: 1.7694993019104004
Epoch 2, Iteration 363, Loss: 1.7494306564331055
Epoch 2, Iteration 364, Loss: 1.8723132610321045
Epoch 2, Iteration 365, Loss: 2.034930944442749
Epoch 2, Iteration 366, Loss: 2.1757054328918457
Epoch 2, Iteration 367, Loss: 1.96094810962677
Epoch 2, Iteration 368, Loss: 1.9410173892974854
Epoch 2, Iteration 369, Loss: 2.0222625732421875
Epoch 2, Iteration 370, Loss: 2.0003435611724854
Epoch 2, Iteration 371, Loss: 1.8532301187515259
Epoch 2, Iteration 372, Loss: 1.5402729511260986
Epoch 2, Iteration 373, Loss: 1.7063719034194946
Epoch 2, Iteration 374, Loss: 2.079665422439575
Epoch 2, Iteration 375, Loss: 1.7901332378387451
Epoch 2, Iteration 376, Loss: 2.0376358032226562
Epoch 2, Iteration 377, Loss: 1.829662799835205
Epoch 2, Iteration 378, Loss: 1.6777729988098145
Epoch 2, Iteration 379, Loss: 1.835367202758789
Epoch 2, Iteration 380, Loss: 1.7454185485839844
Epoch 2, Iteration 381, Loss: 1.814885139465332
Epoch 2, Iteration 382, Loss: 1.5941115617752075
Epoch 2, Iteration 383, Loss: 1.735875129699707
Epoch 2, Iteration 384, Loss: 1.8937029838562012
Epoch 2, Iteration 385, Loss: 1.7100956439971924
Epoch 2, Iteration 386, Loss: 1.682816505432129
Epoch 2, Iteration 387, Loss: 2.180328607559204
Epoch 2, Iteration 388, Loss: 1.839030146598816
Epoch 2, Iteration 389, Loss: 1.8997719287872314
Epoch 2, Iteration 390, Loss: 2.1287457942962646
Epoch 2, Iteration 391, Loss: 1.778011679649353
Epoch 3/10, Loss: 2.072728917306783
Epoch 3, Iteration 0, Loss: 1.9927942752838135
Epoch 3, Iteration 1, Loss: 2.313610076904297
Epoch 3, Iteration 2, Loss: 2.131511926651001
Epoch 3, Iteration 3, Loss: 2.0919196605682373
Epoch 3, Iteration 4, Loss: 2.000335454940796
Epoch 3, Iteration 5, Loss: 2.1329307556152344
Epoch 3, Iteration 6, Loss: 1.8926513195037842
Epoch 3, Iteration 7, Loss: 1.9635159969329834
Epoch 3, Iteration 8, Loss: 2.0300393104553223
Epoch 3, Iteration 9, Loss: 2.2980282306671143
Epoch 3, Iteration 10, Loss: 1.973040223121643
Epoch 3, Iteration 11, Loss: 1.769657850265503
Epoch 3, Iteration 12, Loss: 1.7769505977630615
Epoch 3, Iteration 13, Loss: 2.066627264022827
Epoch 3, Iteration 14, Loss: 1.918713927268982
Epoch 3, Iteration 15, Loss: 2.0777628421783447
Epoch 3, Iteration 16, Loss: 1.8931849002838135
Epoch 3, Iteration 17, Loss: 2.09836745262146
Epoch 3, Iteration 18, Loss: 2.0840163230895996
Epoch 3, Iteration 19, Loss: 2.1429989337921143
Epoch 3, Iteration 20, Loss: 1.9330710172653198
Epoch 3, Iteration 21, Loss: 2.02394962310791
Epoch 3, Iteration 22, Loss: 1.8961416482925415
Epoch 3, Iteration 23, Loss: 1.917567253112793
Epoch 3, Iteration 24, Loss: 1.9065064191818237
Epoch 3, Iteration 25, Loss: 1.7520986795425415
Epoch 3, Iteration 26, Loss: 2.37825345993042
Epoch 3, Iteration 27, Loss: 1.9772354364395142
Epoch 3, Iteration 28, Loss: 1.946837067604065
Epoch 3, Iteration 29, Loss: 2.223017930984497
Epoch 3, Iteration 30, Loss: 1.9763761758804321
Epoch 3, Iteration 31, Loss: 1.9637715816497803
Epoch 3, Iteration 32, Loss: 1.8484781980514526
Epoch 3, Iteration 33, Loss: 1.8776886463165283
Epoch 3, Iteration 34, Loss: 1.8578386306762695
Epoch 3, Iteration 35, Loss: 2.028228759765625
Epoch 3, Iteration 36, Loss: 1.7140347957611084
Epoch 3, Iteration 37, Loss: 1.6924439668655396
Epoch 3, Iteration 38, Loss: 1.769873857498169
Epoch 3, Iteration 39, Loss: 1.7443163394927979
Epoch 3, Iteration 40, Loss: 1.9133431911468506
Epoch 3, Iteration 41, Loss: 1.9798930883407593
Epoch 3, Iteration 42, Loss: 2.050356388092041
Epoch 3, Iteration 43, Loss: 1.9092060327529907
Epoch 3, Iteration 44, Loss: 1.984686017036438
Epoch 3, Iteration 45, Loss: 1.9179705381393433
Epoch 3, Iteration 46, Loss: 1.9729692935943604
Epoch 3, Iteration 47, Loss: 1.9160693883895874
Epoch 3, Iteration 48, Loss: 2.2450814247131348
Epoch 3, Iteration 49, Loss: 1.8909412622451782
Epoch 3, Iteration 50, Loss: 1.956740140914917
Epoch 3, Iteration 50, Valid Loss: 1.156657099723816
Epoch 3, Iteration 51, Loss: 1.9632585048675537
Epoch 3, Iteration 52, Loss: 1.9456977844238281
Epoch 3, Iteration 53, Loss: 2.0356333255767822
Epoch 3, Iteration 54, Loss: 2.201798439025879
Epoch 3, Iteration 55, Loss: 2.013700008392334
Epoch 3, Iteration 56, Loss: 1.7581605911254883
Epoch 3, Iteration 57, Loss: 1.9946010112762451
Epoch 3, Iteration 58, Loss: 1.8252058029174805
Epoch 3, Iteration 59, Loss: 1.9493588209152222
Epoch 3, Iteration 60, Loss: 1.9458057880401611
Epoch 3, Iteration 61, Loss: 1.6145615577697754
Epoch 3, Iteration 62, Loss: 1.8724726438522339
Epoch 3, Iteration 63, Loss: 1.6819642782211304
Epoch 3, Iteration 64, Loss: 1.6801173686981201
Epoch 3, Iteration 65, Loss: 2.1819262504577637
Epoch 3, Iteration 66, Loss: 1.94134521484375
Epoch 3, Iteration 67, Loss: 1.5605683326721191
Epoch 3, Iteration 68, Loss: 2.10931396484375
Epoch 3, Iteration 69, Loss: 2.0747015476226807
Epoch 3, Iteration 70, Loss: 1.9840757846832275
Epoch 3, Iteration 71, Loss: 1.9145814180374146
Epoch 3, Iteration 72, Loss: 2.0205135345458984
Epoch 3, Iteration 73, Loss: 1.6122467517852783
Epoch 3, Iteration 74, Loss: 2.1141624450683594
Epoch 3, Iteration 75, Loss: 1.658858299255371
Epoch 3, Iteration 76, Loss: 1.7542123794555664
Epoch 3, Iteration 77, Loss: 1.50408935546875
Epoch 3, Iteration 78, Loss: 2.273167848587036
Epoch 3, Iteration 79, Loss: 2.1843271255493164
Epoch 3, Iteration 80, Loss: 1.771363377571106
Epoch 3, Iteration 81, Loss: 2.0994865894317627
Epoch 3, Iteration 82, Loss: 2.08545184135437
Epoch 3, Iteration 83, Loss: 1.6327458620071411
Epoch 3, Iteration 84, Loss: 2.009229898452759
Epoch 3, Iteration 85, Loss: 1.882491946220398
Epoch 3, Iteration 86, Loss: 1.8411049842834473
Epoch 3, Iteration 87, Loss: 1.761361837387085
Epoch 3, Iteration 88, Loss: 1.8891770839691162
Epoch 3, Iteration 89, Loss: 1.9336358308792114
Epoch 3, Iteration 90, Loss: 1.5776596069335938
Epoch 3, Iteration 91, Loss: 1.6835534572601318
Epoch 3, Iteration 92, Loss: 1.4483628273010254
Epoch 3, Iteration 93, Loss: 1.891270637512207
Epoch 3, Iteration 94, Loss: 2.2009010314941406
Epoch 3, Iteration 95, Loss: 1.7164616584777832
Epoch 3, Iteration 96, Loss: 1.8232002258300781
Epoch 3, Iteration 97, Loss: 1.6458890438079834
Epoch 3, Iteration 98, Loss: 1.8398593664169312
Epoch 3, Iteration 99, Loss: 1.8209967613220215
Epoch 3, Iteration 100, Loss: 1.7787151336669922
Epoch 3, Iteration 100, Valid Loss: 1.2064343690872192
Epoch 3, Iteration 101, Loss: 2.174315929412842
Epoch 3, Iteration 102, Loss: 1.9011406898498535
Epoch 3, Iteration 103, Loss: 1.9683557748794556
Epoch 3, Iteration 104, Loss: 2.617352247238159
Epoch 3, Iteration 105, Loss: 1.9488072395324707
Epoch 3, Iteration 106, Loss: 1.8975231647491455
Epoch 3, Iteration 107, Loss: 2.427664041519165
Epoch 3, Iteration 108, Loss: 1.779694676399231
Epoch 3, Iteration 109, Loss: 2.1239733695983887
Epoch 3, Iteration 110, Loss: 2.1016132831573486
Epoch 3, Iteration 111, Loss: 2.247767686843872
Epoch 3, Iteration 112, Loss: 1.8715368509292603
Epoch 3, Iteration 113, Loss: 1.7820894718170166
Epoch 3, Iteration 114, Loss: 2.252917766571045
Epoch 3, Iteration 115, Loss: 2.1543338298797607
Epoch 3, Iteration 116, Loss: 1.8294790983200073
Epoch 3, Iteration 117, Loss: 1.554114580154419
Epoch 3, Iteration 118, Loss: 1.966754674911499
Epoch 3, Iteration 119, Loss: 1.9371412992477417
Epoch 3, Iteration 120, Loss: 1.8997135162353516
Epoch 3, Iteration 121, Loss: 1.9326725006103516
Epoch 3, Iteration 122, Loss: 1.8119189739227295
Epoch 3, Iteration 123, Loss: 1.8161064386367798
Epoch 3, Iteration 124, Loss: 1.712233066558838
Epoch 3, Iteration 125, Loss: 1.6075600385665894
Epoch 3, Iteration 126, Loss: 2.430988311767578
Epoch 3, Iteration 127, Loss: 2.361602783203125
Epoch 3, Iteration 128, Loss: 1.7772935628890991
Epoch 3, Iteration 129, Loss: 2.077261447906494
Epoch 3, Iteration 130, Loss: 2.4234282970428467
Epoch 3, Iteration 131, Loss: 1.6969358921051025
Epoch 3, Iteration 132, Loss: 1.9033358097076416
Epoch 3, Iteration 133, Loss: 2.255793571472168
Epoch 3, Iteration 134, Loss: 1.6954063177108765
Epoch 3, Iteration 135, Loss: 1.7891523838043213
Epoch 3, Iteration 136, Loss: 1.8199973106384277
Epoch 3, Iteration 137, Loss: 1.7587182521820068
Epoch 3, Iteration 138, Loss: 1.668592929840088
Epoch 3, Iteration 139, Loss: 1.7983640432357788
Epoch 3, Iteration 140, Loss: 1.6356751918792725
Epoch 3, Iteration 141, Loss: 2.092153549194336
Epoch 3, Iteration 142, Loss: 1.934700846672058
Epoch 3, Iteration 143, Loss: 1.901759147644043
Epoch 3, Iteration 144, Loss: 1.7490874528884888
Epoch 3, Iteration 145, Loss: 1.9153971672058105
Epoch 3, Iteration 146, Loss: 2.140746831893921
Epoch 3, Iteration 147, Loss: 1.822947382926941
Epoch 3, Iteration 148, Loss: 1.5458335876464844
Epoch 3, Iteration 149, Loss: 1.7252391576766968
Epoch 3, Iteration 150, Loss: 2.024901866912842
Epoch 3, Iteration 150, Valid Loss: 1.1686341762542725
Epoch 3, Iteration 151, Loss: 1.98897385597229
Epoch 3, Iteration 152, Loss: 2.1994898319244385
Epoch 3, Iteration 153, Loss: 2.4468441009521484
Epoch 3, Iteration 154, Loss: 1.6552664041519165
Epoch 3, Iteration 155, Loss: 1.9403407573699951
Epoch 3, Iteration 156, Loss: 1.7612380981445312
Epoch 3, Iteration 157, Loss: 2.4807322025299072
Epoch 3, Iteration 158, Loss: 1.7725906372070312
Epoch 3, Iteration 159, Loss: 1.943507194519043
Epoch 3, Iteration 160, Loss: 1.6937416791915894
Epoch 3, Iteration 161, Loss: 1.815675139427185
Epoch 3, Iteration 162, Loss: 2.0941052436828613
Epoch 3, Iteration 163, Loss: 1.5442545413970947
Epoch 3, Iteration 164, Loss: 1.8085620403289795
Epoch 3, Iteration 165, Loss: 1.6505725383758545
Epoch 3, Iteration 166, Loss: 1.651850700378418
Epoch 3, Iteration 167, Loss: 2.116209030151367
Epoch 3, Iteration 168, Loss: 1.9851937294006348
Epoch 3, Iteration 169, Loss: 1.9768589735031128
Epoch 3, Iteration 170, Loss: 2.0325241088867188
Epoch 3, Iteration 171, Loss: 1.808887243270874
Epoch 3, Iteration 172, Loss: 1.8627369403839111
Epoch 3, Iteration 173, Loss: 1.8078782558441162
Epoch 3, Iteration 174, Loss: 2.0543951988220215
Epoch 3, Iteration 175, Loss: 2.0715389251708984
Epoch 3, Iteration 176, Loss: 1.618523359298706
Epoch 3, Iteration 177, Loss: 2.3048911094665527
Epoch 3, Iteration 178, Loss: 2.062981605529785
Epoch 3, Iteration 179, Loss: 1.911369800567627
Epoch 3, Iteration 180, Loss: 1.8138242959976196
Epoch 3, Iteration 181, Loss: 2.1670467853546143
Epoch 3, Iteration 182, Loss: 2.111783027648926
Epoch 3, Iteration 183, Loss: 1.6068415641784668
Epoch 3, Iteration 184, Loss: 1.6763821840286255
Epoch 3, Iteration 185, Loss: 1.7085886001586914
Epoch 3, Iteration 186, Loss: 1.79966139793396
Epoch 3, Iteration 187, Loss: 1.7152342796325684
Epoch 3, Iteration 188, Loss: 1.801344394683838
Epoch 3, Iteration 189, Loss: 1.7636334896087646
Epoch 3, Iteration 190, Loss: 2.072964668273926
Epoch 3, Iteration 191, Loss: 1.8992570638656616
Epoch 3, Iteration 192, Loss: 1.907185435295105
Epoch 3, Iteration 193, Loss: 1.787203311920166
Epoch 3, Iteration 194, Loss: 1.737977385520935
Epoch 3, Iteration 195, Loss: 1.659181833267212
Epoch 3, Iteration 196, Loss: 1.966025471687317
Epoch 3, Iteration 197, Loss: 1.8447505235671997
Epoch 3, Iteration 198, Loss: 1.7759265899658203
Epoch 3, Iteration 199, Loss: 2.0650887489318848
Epoch 3, Iteration 200, Loss: 1.7492036819458008
Epoch 3, Iteration 200, Valid Loss: 1.13078773021698
Epoch 3, Iteration 201, Loss: 1.7438043355941772
Epoch 3, Iteration 202, Loss: 1.845731496810913
Epoch 3, Iteration 203, Loss: 1.7594127655029297
Epoch 3, Iteration 204, Loss: 1.7547423839569092
Epoch 3, Iteration 205, Loss: 1.47288978099823
Epoch 3, Iteration 206, Loss: 1.7409740686416626
Epoch 3, Iteration 207, Loss: 2.177246332168579
Epoch 3, Iteration 208, Loss: 2.4528470039367676
Epoch 3, Iteration 209, Loss: 2.7070791721343994
Epoch 3, Iteration 210, Loss: 2.3394827842712402
Epoch 3, Iteration 211, Loss: 2.4127988815307617
Epoch 3, Iteration 212, Loss: 2.196847677230835
Epoch 3, Iteration 213, Loss: 2.3568155765533447
Epoch 3, Iteration 214, Loss: 1.950029969215393
Epoch 3, Iteration 215, Loss: 1.7670464515686035
Epoch 3, Iteration 216, Loss: 1.7748216390609741
Epoch 3, Iteration 217, Loss: 1.8885133266448975
Epoch 3, Iteration 218, Loss: 1.9876344203948975
Epoch 3, Iteration 219, Loss: 1.9864122867584229
Epoch 3, Iteration 220, Loss: 1.8293099403381348
Epoch 3, Iteration 221, Loss: 2.0432357788085938
Epoch 3, Iteration 222, Loss: 1.8334472179412842
Epoch 3, Iteration 223, Loss: 1.852443814277649
Epoch 3, Iteration 224, Loss: 1.6536130905151367
Epoch 3, Iteration 225, Loss: 2.077669620513916
Epoch 3, Iteration 226, Loss: 2.0321075916290283
Epoch 3, Iteration 227, Loss: 1.9233558177947998
Epoch 3, Iteration 228, Loss: 1.777366042137146
Epoch 3, Iteration 229, Loss: 1.692683219909668
Epoch 3, Iteration 230, Loss: 1.942657709121704
Epoch 3, Iteration 231, Loss: 1.9337459802627563
Epoch 3, Iteration 232, Loss: 1.9106109142303467
Epoch 3, Iteration 233, Loss: 2.397695541381836
Epoch 3, Iteration 234, Loss: 2.106095790863037
Epoch 3, Iteration 235, Loss: 1.7115200757980347
Epoch 3, Iteration 236, Loss: 1.9264174699783325
Epoch 3, Iteration 237, Loss: 1.838548183441162
Epoch 3, Iteration 238, Loss: 1.6443569660186768
Epoch 3, Iteration 239, Loss: 1.996193289756775
Epoch 3, Iteration 240, Loss: 1.578702449798584
Epoch 3, Iteration 241, Loss: 1.7308874130249023
Epoch 3, Iteration 242, Loss: 1.7123968601226807
Epoch 3, Iteration 243, Loss: 1.6697251796722412
Epoch 3, Iteration 244, Loss: 1.722550868988037
Epoch 3, Iteration 245, Loss: 1.45345139503479
Epoch 3, Iteration 246, Loss: 1.8810354471206665
Epoch 3, Iteration 247, Loss: 1.6394472122192383
Epoch 3, Iteration 248, Loss: 1.7942233085632324
Epoch 3, Iteration 249, Loss: 1.6379371881484985
Epoch 3, Iteration 250, Loss: 1.994032859802246
Epoch 3, Iteration 250, Valid Loss: 1.0955371856689453
Epoch 3, Iteration 251, Loss: 1.736013412475586
Epoch 3, Iteration 252, Loss: 1.920964241027832
Epoch 3, Iteration 253, Loss: 1.8854565620422363
Epoch 3, Iteration 254, Loss: 1.872876763343811
Epoch 3, Iteration 255, Loss: 1.9241784811019897
Epoch 3, Iteration 256, Loss: 2.0432963371276855
Epoch 3, Iteration 257, Loss: 1.738599181175232
Epoch 3, Iteration 258, Loss: 2.249565839767456
Epoch 3, Iteration 259, Loss: 1.6037826538085938
Epoch 3, Iteration 260, Loss: 2.012678623199463
Epoch 3, Iteration 261, Loss: 2.0508532524108887
Epoch 3, Iteration 262, Loss: 1.5914626121520996
Epoch 3, Iteration 263, Loss: 2.0654337406158447
Epoch 3, Iteration 264, Loss: 1.8663495779037476
Epoch 3, Iteration 265, Loss: 1.8947787284851074
Epoch 3, Iteration 266, Loss: 2.042623996734619
Epoch 3, Iteration 267, Loss: 1.799491286277771
Epoch 3, Iteration 268, Loss: 1.804900884628296
Epoch 3, Iteration 269, Loss: 1.7735953330993652
Epoch 3, Iteration 270, Loss: 1.7778894901275635
Epoch 3, Iteration 271, Loss: 1.84098482131958
Epoch 3, Iteration 272, Loss: 1.6569342613220215
Epoch 3, Iteration 273, Loss: 1.6504113674163818
Epoch 3, Iteration 274, Loss: 1.6654481887817383
Epoch 3, Iteration 275, Loss: 1.660343885421753
Epoch 3, Iteration 276, Loss: 1.6480900049209595
Epoch 3, Iteration 277, Loss: 1.6728366613388062
Epoch 3, Iteration 278, Loss: 1.8827431201934814
Epoch 3, Iteration 279, Loss: 1.6187487840652466
Epoch 3, Iteration 280, Loss: 1.4718244075775146
Epoch 3, Iteration 281, Loss: 1.601762294769287
Epoch 3, Iteration 282, Loss: 1.877235770225525
Epoch 3, Iteration 283, Loss: 1.7786920070648193
Epoch 3, Iteration 284, Loss: 1.732607364654541
Epoch 3, Iteration 285, Loss: 1.6622684001922607
Epoch 3, Iteration 286, Loss: 1.701936960220337
Epoch 3, Iteration 287, Loss: 1.6179049015045166
Epoch 3, Iteration 288, Loss: 1.6014642715454102
Epoch 3, Iteration 289, Loss: 1.721198558807373
Epoch 3, Iteration 290, Loss: 1.5729647874832153
Epoch 3, Iteration 291, Loss: 1.6347360610961914
Epoch 3, Iteration 292, Loss: 2.042713165283203
Epoch 3, Iteration 293, Loss: 2.1021862030029297
Epoch 3, Iteration 294, Loss: 1.6528122425079346
Epoch 3, Iteration 295, Loss: 1.7258021831512451
Epoch 3, Iteration 296, Loss: 2.141329765319824
Epoch 3, Iteration 297, Loss: 2.0481176376342773
Epoch 3, Iteration 298, Loss: 1.785968542098999
Epoch 3, Iteration 299, Loss: 1.6430717706680298
Epoch 3, Iteration 300, Loss: 1.5627635717391968
Epoch 3, Iteration 300, Valid Loss: 1.0983868837356567
Epoch 3, Iteration 301, Loss: 1.7577723264694214
Epoch 3, Iteration 302, Loss: 1.602607250213623
Epoch 3, Iteration 303, Loss: 2.058422803878784
Epoch 3, Iteration 304, Loss: 1.6647844314575195
Epoch 3, Iteration 305, Loss: 1.5975656509399414
Epoch 3, Iteration 306, Loss: 1.6623892784118652
Epoch 3, Iteration 307, Loss: 1.6264479160308838
Epoch 3, Iteration 308, Loss: 1.6394144296646118
Epoch 3, Iteration 309, Loss: 1.6448880434036255
Epoch 3, Iteration 310, Loss: 1.9471473693847656
Epoch 3, Iteration 311, Loss: 1.7078747749328613
Epoch 3, Iteration 312, Loss: 1.5132126808166504
Epoch 3, Iteration 313, Loss: 1.862917423248291
Epoch 3, Iteration 314, Loss: 1.6553610563278198
Epoch 3, Iteration 315, Loss: 1.4707709550857544
Epoch 3, Iteration 316, Loss: 1.8139252662658691
Epoch 3, Iteration 317, Loss: 1.619817852973938
Epoch 3, Iteration 318, Loss: 1.6532204151153564
Epoch 3, Iteration 319, Loss: 1.8999017477035522
Epoch 3, Iteration 320, Loss: 1.417925238609314
Epoch 3, Iteration 321, Loss: 1.6736903190612793
Epoch 3, Iteration 322, Loss: 1.5988843441009521
Epoch 3, Iteration 323, Loss: 1.7035328149795532
Epoch 3, Iteration 324, Loss: 1.679767370223999
Epoch 3, Iteration 325, Loss: 2.1046552658081055
Epoch 3, Iteration 326, Loss: 1.7618987560272217
Epoch 3, Iteration 327, Loss: 1.5847413539886475
Epoch 3, Iteration 328, Loss: 1.602769136428833
Epoch 3, Iteration 329, Loss: 1.8733530044555664
Epoch 3, Iteration 330, Loss: 1.3941093683242798
Epoch 3, Iteration 331, Loss: 1.7306504249572754
Epoch 3, Iteration 332, Loss: 1.7671396732330322
Epoch 3, Iteration 333, Loss: 1.9647632837295532
Epoch 3, Iteration 334, Loss: 1.6935429573059082
Epoch 3, Iteration 335, Loss: 1.9206628799438477
Epoch 3, Iteration 336, Loss: 1.8068559169769287
Epoch 3, Iteration 337, Loss: 1.5576162338256836
Epoch 3, Iteration 338, Loss: 1.7372061014175415
Epoch 3, Iteration 339, Loss: 1.6559929847717285
Epoch 3, Iteration 340, Loss: 1.5465463399887085
Epoch 3, Iteration 341, Loss: 1.6479169130325317
Epoch 3, Iteration 342, Loss: 1.594854474067688
Epoch 3, Iteration 343, Loss: 1.6851208209991455
Epoch 3, Iteration 344, Loss: 1.9209098815917969
Epoch 3, Iteration 345, Loss: 1.8148581981658936
Epoch 3, Iteration 346, Loss: 1.7150917053222656
Epoch 3, Iteration 347, Loss: 1.5511120557785034
Epoch 3, Iteration 348, Loss: 1.4644110202789307
Epoch 3, Iteration 349, Loss: 1.7501939535140991
Epoch 3, Iteration 350, Loss: 1.5646159648895264
Epoch 3, Iteration 350, Valid Loss: 1.0996520519256592
Epoch 3, Iteration 351, Loss: 1.752777338027954
Epoch 3, Iteration 352, Loss: 1.5229406356811523
Epoch 3, Iteration 353, Loss: 1.7696460485458374
Epoch 3, Iteration 354, Loss: 1.6334785223007202
Epoch 3, Iteration 355, Loss: 1.6130483150482178
Epoch 3, Iteration 356, Loss: 1.7121529579162598
Epoch 3, Iteration 357, Loss: 1.6076858043670654
Epoch 3, Iteration 358, Loss: 1.679269790649414
Epoch 3, Iteration 359, Loss: 1.6190524101257324
Epoch 3, Iteration 360, Loss: 1.5148509740829468
Epoch 3, Iteration 361, Loss: 1.6494715213775635
Epoch 3, Iteration 362, Loss: 1.6699659824371338
Epoch 3, Iteration 363, Loss: 1.5537974834442139
Epoch 3, Iteration 364, Loss: 1.6513886451721191
Epoch 3, Iteration 365, Loss: 1.902747392654419
Epoch 3, Iteration 366, Loss: 1.906885027885437
Epoch 3, Iteration 367, Loss: 1.8057219982147217
Epoch 3, Iteration 368, Loss: 1.6478490829467773
Epoch 3, Iteration 369, Loss: 1.8169952630996704
Epoch 3, Iteration 370, Loss: 1.8315672874450684
Epoch 3, Iteration 371, Loss: 1.7081458568572998
Epoch 3, Iteration 372, Loss: 1.4391511678695679
Epoch 3, Iteration 373, Loss: 1.4813532829284668
Epoch 3, Iteration 374, Loss: 1.9271938800811768
Epoch 3, Iteration 375, Loss: 1.673292875289917
Epoch 3, Iteration 376, Loss: 1.9085577726364136
Epoch 3, Iteration 377, Loss: 1.6818389892578125
Epoch 3, Iteration 378, Loss: 1.863849401473999
Epoch 3, Iteration 379, Loss: 1.5877892971038818
Epoch 3, Iteration 380, Loss: 1.5354117155075073
Epoch 3, Iteration 381, Loss: 1.6729965209960938
Epoch 3, Iteration 382, Loss: 1.403804063796997
Epoch 3, Iteration 383, Loss: 1.6546334028244019
Epoch 3, Iteration 384, Loss: 1.6615684032440186
Epoch 3, Iteration 385, Loss: 1.602301836013794
Epoch 3, Iteration 386, Loss: 1.581606388092041
Epoch 3, Iteration 387, Loss: 2.0073485374450684
Epoch 3, Iteration 388, Loss: 1.6509928703308105
Epoch 3, Iteration 389, Loss: 1.7343432903289795
Epoch 3, Iteration 390, Loss: 1.847362995147705
Epoch 3, Iteration 391, Loss: 1.5823321342468262
Epoch 4/10, Loss: 1.8502658295388124
Epoch 4, Iteration 0, Loss: 1.7371091842651367
Epoch 4, Iteration 1, Loss: 2.126358985900879
Epoch 4, Iteration 2, Loss: 1.9478178024291992
Epoch 4, Iteration 3, Loss: 1.9553124904632568
Epoch 4, Iteration 4, Loss: 1.8464140892028809
Epoch 4, Iteration 5, Loss: 1.904242753982544
Epoch 4, Iteration 6, Loss: 1.641303539276123
Epoch 4, Iteration 7, Loss: 1.7755777835845947
Epoch 4, Iteration 8, Loss: 1.8391495943069458
Epoch 4, Iteration 9, Loss: 2.134030818939209
Epoch 4, Iteration 10, Loss: 1.8715999126434326
Epoch 4, Iteration 11, Loss: 1.6389050483703613
Epoch 4, Iteration 12, Loss: 1.7212762832641602
Epoch 4, Iteration 13, Loss: 1.9223878383636475
Epoch 4, Iteration 14, Loss: 1.615178108215332
Epoch 4, Iteration 15, Loss: 1.9018248319625854
Epoch 4, Iteration 16, Loss: 1.756269097328186
Epoch 4, Iteration 17, Loss: 1.9803504943847656
Epoch 4, Iteration 18, Loss: 1.8195065259933472
Epoch 4, Iteration 19, Loss: 1.9197276830673218
Epoch 4, Iteration 20, Loss: 1.8515079021453857
Epoch 4, Iteration 21, Loss: 1.773347020149231
Epoch 4, Iteration 22, Loss: 1.8061922788619995
Epoch 4, Iteration 23, Loss: 1.7466647624969482
Epoch 4, Iteration 24, Loss: 1.6074597835540771
Epoch 4, Iteration 25, Loss: 1.6351577043533325
Epoch 4, Iteration 26, Loss: 2.130108594894409
Epoch 4, Iteration 27, Loss: 1.7454354763031006
Epoch 4, Iteration 28, Loss: 1.86592435836792
Epoch 4, Iteration 29, Loss: 2.0464627742767334
Epoch 4, Iteration 30, Loss: 1.8483660221099854
Epoch 4, Iteration 31, Loss: 1.8773621320724487
Epoch 4, Iteration 32, Loss: 1.794525146484375
Epoch 4, Iteration 33, Loss: 1.744555950164795
Epoch 4, Iteration 34, Loss: 1.8396472930908203
Epoch 4, Iteration 35, Loss: 1.819650411605835
Epoch 4, Iteration 36, Loss: 1.5689764022827148
Epoch 4, Iteration 37, Loss: 1.6146910190582275
Epoch 4, Iteration 38, Loss: 1.6479768753051758
Epoch 4, Iteration 39, Loss: 1.606965184211731
Epoch 4, Iteration 40, Loss: 1.7317010164260864
Epoch 4, Iteration 41, Loss: 1.8417527675628662
Epoch 4, Iteration 42, Loss: 1.817557692527771
Epoch 4, Iteration 43, Loss: 1.6959985494613647
Epoch 4, Iteration 44, Loss: 1.7360278367996216
Epoch 4, Iteration 45, Loss: 1.7643296718597412
Epoch 4, Iteration 46, Loss: 1.802947998046875
Epoch 4, Iteration 47, Loss: 1.8732471466064453
Epoch 4, Iteration 48, Loss: 1.8964965343475342
Epoch 4, Iteration 49, Loss: 1.7432219982147217
Epoch 4, Iteration 50, Loss: 1.7584269046783447
Epoch 4, Iteration 50, Valid Loss: 1.025460124015808
Epoch 4, Iteration 51, Loss: 1.7537527084350586
Epoch 4, Iteration 52, Loss: 1.7641921043395996
Epoch 4, Iteration 53, Loss: 1.7476274967193604
Epoch 4, Iteration 54, Loss: 2.1407766342163086
Epoch 4, Iteration 55, Loss: 2.019041061401367
Epoch 4, Iteration 56, Loss: 1.6371092796325684
Epoch 4, Iteration 57, Loss: 1.7951836585998535
Epoch 4, Iteration 58, Loss: 1.6893545389175415
Epoch 4, Iteration 59, Loss: 1.6823606491088867
Epoch 4, Iteration 60, Loss: 1.803578495979309
Epoch 4, Iteration 61, Loss: 1.4514050483703613
Epoch 4, Iteration 62, Loss: 1.6577417850494385
Epoch 4, Iteration 63, Loss: 1.437455177307129
Epoch 4, Iteration 64, Loss: 1.5461342334747314
Epoch 4, Iteration 65, Loss: 1.9184707403182983
Epoch 4, Iteration 66, Loss: 1.7889940738677979
Epoch 4, Iteration 67, Loss: 1.3864378929138184
Epoch 4, Iteration 68, Loss: 1.9736331701278687
Epoch 4, Iteration 69, Loss: 2.023160934448242
Epoch 4, Iteration 70, Loss: 2.065265655517578
Epoch 4, Iteration 71, Loss: 1.76806640625
Epoch 4, Iteration 72, Loss: 1.9230971336364746
Epoch 4, Iteration 73, Loss: 1.577357292175293
Epoch 4, Iteration 74, Loss: 1.9433890581130981
Epoch 4, Iteration 75, Loss: 1.7922616004943848
Epoch 4, Iteration 76, Loss: 1.5923516750335693
Epoch 4, Iteration 77, Loss: 1.5384975671768188
Epoch 4, Iteration 78, Loss: 2.1912941932678223
Epoch 4, Iteration 79, Loss: 2.0321242809295654
Epoch 4, Iteration 80, Loss: 1.530853509902954
Epoch 4, Iteration 81, Loss: 1.972400426864624
Epoch 4, Iteration 82, Loss: 1.6734037399291992
Epoch 4, Iteration 83, Loss: 1.4528169631958008
Epoch 4, Iteration 84, Loss: 1.902522087097168
Epoch 4, Iteration 85, Loss: 1.802321434020996
Epoch 4, Iteration 86, Loss: 1.8693828582763672
Epoch 4, Iteration 87, Loss: 1.911532998085022
Epoch 4, Iteration 88, Loss: 1.9102160930633545
Epoch 4, Iteration 89, Loss: 1.7146742343902588
Epoch 4, Iteration 90, Loss: 1.5078102350234985
Epoch 4, Iteration 91, Loss: 1.666885495185852
Epoch 4, Iteration 92, Loss: 1.4355181455612183
Epoch 4, Iteration 93, Loss: 1.7170125246047974
Epoch 4, Iteration 94, Loss: 2.0884218215942383
Epoch 4, Iteration 95, Loss: 1.6274406909942627
Epoch 4, Iteration 96, Loss: 1.7362534999847412
Epoch 4, Iteration 97, Loss: 1.5422587394714355
Epoch 4, Iteration 98, Loss: 1.624557375907898
Epoch 4, Iteration 99, Loss: 1.64383864402771
Epoch 4, Iteration 100, Loss: 1.5723657608032227
Epoch 4, Iteration 100, Valid Loss: 1.0552277565002441
Epoch 4, Iteration 101, Loss: 2.166410207748413
Epoch 4, Iteration 102, Loss: 1.6943670511245728
Epoch 4, Iteration 103, Loss: 1.6125187873840332
Epoch 4, Iteration 104, Loss: 2.090446949005127
Epoch 4, Iteration 105, Loss: 1.7587512731552124
Epoch 4, Iteration 106, Loss: 1.6900293827056885
Epoch 4, Iteration 107, Loss: 2.458348274230957
Epoch 4, Iteration 108, Loss: 1.7048673629760742
Epoch 4, Iteration 109, Loss: 1.9642841815948486
Epoch 4, Iteration 110, Loss: 1.9624030590057373
Epoch 4, Iteration 111, Loss: 1.8025643825531006
Epoch 4, Iteration 112, Loss: 1.7327003479003906
Epoch 4, Iteration 113, Loss: 1.4991014003753662
Epoch 4, Iteration 114, Loss: 2.283367872238159
Epoch 4, Iteration 115, Loss: 2.0514168739318848
Epoch 4, Iteration 116, Loss: 1.664703130722046
Epoch 4, Iteration 117, Loss: 1.4540058374404907
Epoch 4, Iteration 118, Loss: 1.8044593334197998
Epoch 4, Iteration 119, Loss: 1.776540756225586
Epoch 4, Iteration 120, Loss: 1.6901110410690308
Epoch 4, Iteration 121, Loss: 1.566795825958252
Epoch 4, Iteration 122, Loss: 1.6825312376022339
Epoch 4, Iteration 123, Loss: 1.626997709274292
Epoch 4, Iteration 124, Loss: 1.580359697341919
Epoch 4, Iteration 125, Loss: 1.4601476192474365
Epoch 4, Iteration 126, Loss: 1.5331989526748657
Epoch 4, Iteration 127, Loss: 1.8953083753585815
Epoch 4, Iteration 128, Loss: 2.035344123840332
Epoch 4, Iteration 129, Loss: 1.7623176574707031
Epoch 4, Iteration 130, Loss: 2.153329610824585
Epoch 4, Iteration 131, Loss: 1.5995389223098755
Epoch 4, Iteration 132, Loss: 1.6052497625350952
Epoch 4, Iteration 133, Loss: 2.1817502975463867
Epoch 4, Iteration 134, Loss: 1.7860727310180664
Epoch 4, Iteration 135, Loss: 1.9269909858703613
Epoch 4, Iteration 136, Loss: 1.775665044784546
Epoch 4, Iteration 137, Loss: 1.6644901037216187
Epoch 4, Iteration 138, Loss: 1.4536241292953491
Epoch 4, Iteration 139, Loss: 1.5600838661193848
Epoch 4, Iteration 140, Loss: 1.6655861139297485
Epoch 4, Iteration 141, Loss: 2.028593063354492
Epoch 4, Iteration 142, Loss: 1.6444640159606934
Epoch 4, Iteration 143, Loss: 1.7683095932006836
Epoch 4, Iteration 144, Loss: 1.624572515487671
Epoch 4, Iteration 145, Loss: 1.7715866565704346
Epoch 4, Iteration 146, Loss: 2.010902166366577
Epoch 4, Iteration 147, Loss: 1.6404815912246704
Epoch 4, Iteration 148, Loss: 1.4366565942764282
Epoch 4, Iteration 149, Loss: 1.5941251516342163
Epoch 4, Iteration 150, Loss: 1.8402225971221924
Epoch 4, Iteration 150, Valid Loss: 1.0464338064193726
Epoch 4, Iteration 151, Loss: 1.8037869930267334
Epoch 4, Iteration 152, Loss: 1.6312655210494995
Epoch 4, Iteration 153, Loss: 1.937288522720337
Epoch 4, Iteration 154, Loss: 1.5540857315063477
Epoch 4, Iteration 155, Loss: 1.6899232864379883
Epoch 4, Iteration 156, Loss: 1.7266898155212402
Epoch 4, Iteration 157, Loss: 2.2633845806121826
Epoch 4, Iteration 158, Loss: 1.7873810529708862
Epoch 4, Iteration 159, Loss: 1.8333075046539307
Epoch 4, Iteration 160, Loss: 1.6348901987075806
Epoch 4, Iteration 161, Loss: 1.5844477415084839
Epoch 4, Iteration 162, Loss: 1.9682466983795166
Epoch 4, Iteration 163, Loss: 1.3898885250091553
Epoch 4, Iteration 164, Loss: 1.7451919317245483
Epoch 4, Iteration 165, Loss: 1.640782356262207
Epoch 4, Iteration 166, Loss: 1.4676682949066162
Epoch 4, Iteration 167, Loss: 2.031811475753784
Epoch 4, Iteration 168, Loss: 1.8574652671813965
Epoch 4, Iteration 169, Loss: 1.8002036809921265
Epoch 4, Iteration 170, Loss: 1.8455561399459839
Epoch 4, Iteration 171, Loss: 1.7612351179122925
Epoch 4, Iteration 172, Loss: 1.6474148035049438
Epoch 4, Iteration 173, Loss: 1.7162790298461914
Epoch 4, Iteration 174, Loss: 1.8614871501922607
Epoch 4, Iteration 175, Loss: 1.8742104768753052
Epoch 4, Iteration 176, Loss: 1.4329288005828857
Epoch 4, Iteration 177, Loss: 2.2074811458587646
Epoch 4, Iteration 178, Loss: 1.9841349124908447
Epoch 4, Iteration 179, Loss: 1.7357583045959473
Epoch 4, Iteration 180, Loss: 1.6002575159072876
Epoch 4, Iteration 181, Loss: 1.9717028141021729
Epoch 4, Iteration 182, Loss: 1.9357738494873047
Epoch 4, Iteration 183, Loss: 1.5398030281066895
Epoch 4, Iteration 184, Loss: 1.5337297916412354
Epoch 4, Iteration 185, Loss: 1.5975613594055176
Epoch 4, Iteration 186, Loss: 1.5878838300704956
Epoch 4, Iteration 187, Loss: 1.5813754796981812
Epoch 4, Iteration 188, Loss: 1.5640630722045898
Epoch 4, Iteration 189, Loss: 1.6014901399612427
Epoch 4, Iteration 190, Loss: 1.8846898078918457
Epoch 4, Iteration 191, Loss: 1.8080157041549683
Epoch 4, Iteration 192, Loss: 1.8148906230926514
Epoch 4, Iteration 193, Loss: 1.748168706893921
Epoch 4, Iteration 194, Loss: 1.6580495834350586
Epoch 4, Iteration 195, Loss: 1.569737195968628
Epoch 4, Iteration 196, Loss: 1.7612353563308716
Epoch 4, Iteration 197, Loss: 1.7260738611221313
Epoch 4, Iteration 198, Loss: 1.7425241470336914
Epoch 4, Iteration 199, Loss: 1.8578791618347168
Epoch 4, Iteration 200, Loss: 1.5212819576263428
Epoch 4, Iteration 200, Valid Loss: 1.004392147064209
Epoch 4, Iteration 201, Loss: 1.6015115976333618
Epoch 4, Iteration 202, Loss: 1.7593227624893188
Epoch 4, Iteration 203, Loss: 1.6118431091308594
Epoch 4, Iteration 204, Loss: 1.6784005165100098
Epoch 4, Iteration 205, Loss: 1.4110788106918335
Epoch 4, Iteration 206, Loss: 1.6060712337493896
Epoch 4, Iteration 207, Loss: 1.93414306640625
Epoch 4, Iteration 208, Loss: 2.24261474609375
Epoch 4, Iteration 209, Loss: 2.4531965255737305
Epoch 4, Iteration 210, Loss: 2.144571304321289
Epoch 4, Iteration 211, Loss: 2.378819704055786
Epoch 4, Iteration 212, Loss: 2.083977699279785
Epoch 4, Iteration 213, Loss: 2.172849655151367
Epoch 4, Iteration 214, Loss: 1.814469575881958
Epoch 4, Iteration 215, Loss: 1.6062883138656616
Epoch 4, Iteration 216, Loss: 1.6161574125289917
Epoch 4, Iteration 217, Loss: 1.7108925580978394
Epoch 4, Iteration 218, Loss: 1.8405699729919434
Epoch 4, Iteration 219, Loss: 1.775814175605774
Epoch 4, Iteration 220, Loss: 1.7061131000518799
Epoch 4, Iteration 221, Loss: 1.7960361242294312
Epoch 4, Iteration 222, Loss: 1.6970449686050415
Epoch 4, Iteration 223, Loss: 1.6560685634613037
Epoch 4, Iteration 224, Loss: 1.5148783922195435
Epoch 4, Iteration 225, Loss: 1.9045238494873047
Epoch 4, Iteration 226, Loss: 1.9481619596481323
Epoch 4, Iteration 227, Loss: 1.7880444526672363
Epoch 4, Iteration 228, Loss: 1.5875537395477295
Epoch 4, Iteration 229, Loss: 1.538806676864624
Epoch 4, Iteration 230, Loss: 1.8740875720977783
Epoch 4, Iteration 231, Loss: 1.7760671377182007
Epoch 4, Iteration 232, Loss: 1.8892830610275269
Epoch 4, Iteration 233, Loss: 2.2258567810058594
Epoch 4, Iteration 234, Loss: 1.8941442966461182
Epoch 4, Iteration 235, Loss: 1.6737004518508911
Epoch 4, Iteration 236, Loss: 1.9178094863891602
Epoch 4, Iteration 237, Loss: 1.7086340188980103
Epoch 4, Iteration 238, Loss: 1.6256605386734009
Epoch 4, Iteration 239, Loss: 1.6723438501358032
Epoch 4, Iteration 240, Loss: 1.4530212879180908
Epoch 4, Iteration 241, Loss: 1.5028891563415527
Epoch 4, Iteration 242, Loss: 1.599487066268921
Epoch 4, Iteration 243, Loss: 1.6333656311035156
Epoch 4, Iteration 244, Loss: 1.6792707443237305
Epoch 4, Iteration 245, Loss: 1.2929209470748901
Epoch 4, Iteration 246, Loss: 1.791182518005371
Epoch 4, Iteration 247, Loss: 1.5966219902038574
Epoch 4, Iteration 248, Loss: 1.89718496799469
Epoch 4, Iteration 249, Loss: 1.666701316833496
Epoch 4, Iteration 250, Loss: 1.9048187732696533
Epoch 4, Iteration 250, Valid Loss: 1.004701018333435
Epoch 4, Iteration 251, Loss: 1.6420533657073975
Epoch 4, Iteration 252, Loss: 1.8492460250854492
Epoch 4, Iteration 253, Loss: 1.8313125371932983
Epoch 4, Iteration 254, Loss: 1.7921607494354248
Epoch 4, Iteration 255, Loss: 1.8101375102996826
Epoch 4, Iteration 256, Loss: 1.8542872667312622
Epoch 4, Iteration 257, Loss: 1.6086883544921875
Epoch 4, Iteration 258, Loss: 1.932638168334961
Epoch 4, Iteration 259, Loss: 1.504128098487854
Epoch 4, Iteration 260, Loss: 1.804335355758667
Epoch 4, Iteration 261, Loss: 1.7654438018798828
Epoch 4, Iteration 262, Loss: 1.4725558757781982
Epoch 4, Iteration 263, Loss: 2.0301694869995117
Epoch 4, Iteration 264, Loss: 1.7159290313720703
Epoch 4, Iteration 265, Loss: 1.7688121795654297
Epoch 4, Iteration 266, Loss: 1.7789411544799805
Epoch 4, Iteration 267, Loss: 1.6803373098373413
Epoch 4, Iteration 268, Loss: 1.6350362300872803
Epoch 4, Iteration 269, Loss: 1.7125533819198608
Epoch 4, Iteration 270, Loss: 1.575933575630188
Epoch 4, Iteration 271, Loss: 1.790830135345459
Epoch 4, Iteration 272, Loss: 1.5724170207977295
Epoch 4, Iteration 273, Loss: 1.707000732421875
Epoch 4, Iteration 274, Loss: 1.547121286392212
Epoch 4, Iteration 275, Loss: 1.4654598236083984
Epoch 4, Iteration 276, Loss: 1.5116251707077026
Epoch 4, Iteration 277, Loss: 1.6044139862060547
Epoch 4, Iteration 278, Loss: 1.7927157878875732
Epoch 4, Iteration 279, Loss: 1.5529897212982178
Epoch 4, Iteration 280, Loss: 1.3282767534255981
Epoch 4, Iteration 281, Loss: 1.6561501026153564
Epoch 4, Iteration 282, Loss: 1.7079397439956665
Epoch 4, Iteration 283, Loss: 1.6496586799621582
Epoch 4, Iteration 284, Loss: 1.6731865406036377
Epoch 4, Iteration 285, Loss: 1.5320916175842285
Epoch 4, Iteration 286, Loss: 1.5072442293167114
Epoch 4, Iteration 287, Loss: 1.7046982049942017
Epoch 4, Iteration 288, Loss: 1.4877521991729736
Epoch 4, Iteration 289, Loss: 1.6237916946411133
Epoch 4, Iteration 290, Loss: 1.5298078060150146
Epoch 4, Iteration 291, Loss: 1.5021257400512695
Epoch 4, Iteration 292, Loss: 1.9849563837051392
Epoch 4, Iteration 293, Loss: 2.040799379348755
Epoch 4, Iteration 294, Loss: 1.4140994548797607
Epoch 4, Iteration 295, Loss: 1.600494146347046
Epoch 4, Iteration 296, Loss: 1.928253412246704
Epoch 4, Iteration 297, Loss: 1.8607113361358643
Epoch 4, Iteration 298, Loss: 1.6247831583023071
Epoch 4, Iteration 299, Loss: 1.5832765102386475
Epoch 4, Iteration 300, Loss: 1.4042730331420898
Epoch 4, Iteration 300, Valid Loss: 0.986478328704834
Epoch 4, Iteration 301, Loss: 1.672362208366394
Epoch 4, Iteration 302, Loss: 1.6518237590789795
Epoch 4, Iteration 303, Loss: 1.9938011169433594
Epoch 4, Iteration 304, Loss: 1.572363257408142
Epoch 4, Iteration 305, Loss: 1.5815578699111938
Epoch 4, Iteration 306, Loss: 1.5854228734970093
Epoch 4, Iteration 307, Loss: 1.4990463256835938
Epoch 4, Iteration 308, Loss: 1.5339317321777344
Epoch 4, Iteration 309, Loss: 1.5008820295333862
Epoch 4, Iteration 310, Loss: 1.9270777702331543
Epoch 4, Iteration 311, Loss: 1.5199360847473145
Epoch 4, Iteration 312, Loss: 1.5261238813400269
Epoch 4, Iteration 313, Loss: 1.7912695407867432
Epoch 4, Iteration 314, Loss: 1.4734289646148682
Epoch 4, Iteration 315, Loss: 1.3145684003829956
Epoch 4, Iteration 316, Loss: 1.7872384786605835
Epoch 4, Iteration 317, Loss: 1.6325249671936035
Epoch 4, Iteration 318, Loss: 1.5154308080673218
Epoch 4, Iteration 319, Loss: 1.710349202156067
Epoch 4, Iteration 320, Loss: 1.4950320720672607
Epoch 4, Iteration 321, Loss: 1.5893913507461548
Epoch 4, Iteration 322, Loss: 1.5220527648925781
Epoch 4, Iteration 323, Loss: 1.6246157884597778
Epoch 4, Iteration 324, Loss: 1.5763312578201294
Epoch 4, Iteration 325, Loss: 1.6577962636947632
Epoch 4, Iteration 326, Loss: 1.5222772359848022
Epoch 4, Iteration 327, Loss: 1.4752886295318604
Epoch 4, Iteration 328, Loss: 1.4765735864639282
Epoch 4, Iteration 329, Loss: 1.8364938497543335
Epoch 4, Iteration 330, Loss: 1.3362257480621338
Epoch 4, Iteration 331, Loss: 1.5642666816711426
Epoch 4, Iteration 332, Loss: 1.626083254814148
Epoch 4, Iteration 333, Loss: 1.8715981245040894
Epoch 4, Iteration 334, Loss: 1.4287145137786865
Epoch 4, Iteration 335, Loss: 1.7590844631195068
Epoch 4, Iteration 336, Loss: 1.6506128311157227
Epoch 4, Iteration 337, Loss: 1.366361379623413
Epoch 4, Iteration 338, Loss: 1.6051197052001953
Epoch 4, Iteration 339, Loss: 1.6250545978546143
Epoch 4, Iteration 340, Loss: 1.4558595418930054
Epoch 4, Iteration 341, Loss: 1.5372662544250488
Epoch 4, Iteration 342, Loss: 1.6697297096252441
Epoch 4, Iteration 343, Loss: 1.4767086505889893
Epoch 4, Iteration 344, Loss: 1.6203720569610596
Epoch 4, Iteration 345, Loss: 1.6436177492141724
Epoch 4, Iteration 346, Loss: 1.6276865005493164
Epoch 4, Iteration 347, Loss: 1.3081108331680298
Epoch 4, Iteration 348, Loss: 1.3955174684524536
Epoch 4, Iteration 349, Loss: 1.6210960149765015
Epoch 4, Iteration 350, Loss: 1.4864141941070557
Epoch 4, Iteration 350, Valid Loss: 0.9985077977180481
Epoch 4, Iteration 351, Loss: 1.6482120752334595
Epoch 4, Iteration 352, Loss: 1.4681026935577393
Epoch 4, Iteration 353, Loss: 1.7654485702514648
Epoch 4, Iteration 354, Loss: 1.5032603740692139
Epoch 4, Iteration 355, Loss: 1.584737777709961
Epoch 4, Iteration 356, Loss: 1.6695059537887573
Epoch 4, Iteration 357, Loss: 1.536502718925476
Epoch 4, Iteration 358, Loss: 1.538542628288269
Epoch 4, Iteration 359, Loss: 1.5141630172729492
Epoch 4, Iteration 360, Loss: 1.4005961418151855
Epoch 4, Iteration 361, Loss: 1.5325021743774414
Epoch 4, Iteration 362, Loss: 1.5127549171447754
Epoch 4, Iteration 363, Loss: 1.458130955696106
Epoch 4, Iteration 364, Loss: 1.7111159563064575
Epoch 4, Iteration 365, Loss: 1.8508570194244385
Epoch 4, Iteration 366, Loss: 1.7768343687057495
Epoch 4, Iteration 367, Loss: 1.7582576274871826
Epoch 4, Iteration 368, Loss: 1.5194547176361084
Epoch 4, Iteration 369, Loss: 1.7112345695495605
Epoch 4, Iteration 370, Loss: 1.6362160444259644
Epoch 4, Iteration 371, Loss: 1.7528913021087646
Epoch 4, Iteration 372, Loss: 1.4156267642974854
Epoch 4, Iteration 373, Loss: 1.5308948755264282
Epoch 4, Iteration 374, Loss: 1.854848861694336
Epoch 4, Iteration 375, Loss: 1.4802041053771973
Epoch 4, Iteration 376, Loss: 1.7082582712173462
Epoch 4, Iteration 377, Loss: 1.6633937358856201
Epoch 4, Iteration 378, Loss: 1.4325897693634033
Epoch 4, Iteration 379, Loss: 1.5385715961456299
Epoch 4, Iteration 380, Loss: 1.4012792110443115
Epoch 4, Iteration 381, Loss: 1.4496872425079346
Epoch 4, Iteration 382, Loss: 1.3094700574874878
Epoch 4, Iteration 383, Loss: 1.5490217208862305
Epoch 4, Iteration 384, Loss: 1.5772429704666138
Epoch 4, Iteration 385, Loss: 1.4048466682434082
Epoch 4, Iteration 386, Loss: 1.4115619659423828
Epoch 4, Iteration 387, Loss: 1.9106844663619995
Epoch 4, Iteration 388, Loss: 1.482727289199829
Epoch 4, Iteration 389, Loss: 1.674148678779602
Epoch 4, Iteration 390, Loss: 1.7321946620941162
Epoch 4, Iteration 391, Loss: 1.4781132936477661
Epoch 5/10, Loss: 1.7147714431796754
Epoch 5, Iteration 0, Loss: 1.6241973638534546
Epoch 5, Iteration 1, Loss: 1.963212251663208
Epoch 5, Iteration 2, Loss: 1.7757047414779663
Epoch 5, Iteration 3, Loss: 1.7659999132156372
Epoch 5, Iteration 4, Loss: 1.6691088676452637
Epoch 5, Iteration 5, Loss: 1.7627533674240112
Epoch 5, Iteration 6, Loss: 1.541996717453003
Epoch 5, Iteration 7, Loss: 1.6496121883392334
Epoch 5, Iteration 8, Loss: 1.6970267295837402
Epoch 5, Iteration 9, Loss: 1.9905030727386475
Epoch 5, Iteration 10, Loss: 1.7962000370025635
Epoch 5, Iteration 11, Loss: 1.532524585723877
Epoch 5, Iteration 12, Loss: 1.5836600065231323
Epoch 5, Iteration 13, Loss: 1.8448501825332642
Epoch 5, Iteration 14, Loss: 1.591100811958313
Epoch 5, Iteration 15, Loss: 2.036252498626709
Epoch 5, Iteration 16, Loss: 1.6408731937408447
Epoch 5, Iteration 17, Loss: 1.9250812530517578
Epoch 5, Iteration 18, Loss: 1.82134211063385
Epoch 5, Iteration 19, Loss: 1.7742199897766113
Epoch 5, Iteration 20, Loss: 1.601891040802002
Epoch 5, Iteration 21, Loss: 1.6324987411499023
Epoch 5, Iteration 22, Loss: 1.6537991762161255
Epoch 5, Iteration 23, Loss: 1.6548197269439697
Epoch 5, Iteration 24, Loss: 1.570068120956421
Epoch 5, Iteration 25, Loss: 1.4799270629882812
Epoch 5, Iteration 26, Loss: 2.019129991531372
Epoch 5, Iteration 27, Loss: 1.6259310245513916
Epoch 5, Iteration 28, Loss: 1.6382135152816772
Epoch 5, Iteration 29, Loss: 1.9445935487747192
Epoch 5, Iteration 30, Loss: 1.7213358879089355
Epoch 5, Iteration 31, Loss: 1.6889522075653076
Epoch 5, Iteration 32, Loss: 1.5712909698486328
Epoch 5, Iteration 33, Loss: 1.6248365640640259
Epoch 5, Iteration 34, Loss: 1.5438814163208008
Epoch 5, Iteration 35, Loss: 1.7334164381027222
Epoch 5, Iteration 36, Loss: 1.3927234411239624
Epoch 5, Iteration 37, Loss: 1.5105963945388794
Epoch 5, Iteration 38, Loss: 1.5309550762176514
Epoch 5, Iteration 39, Loss: 1.5630650520324707
Epoch 5, Iteration 40, Loss: 1.6336628198623657
Epoch 5, Iteration 41, Loss: 1.6937111616134644
Epoch 5, Iteration 42, Loss: 1.6569857597351074
Epoch 5, Iteration 43, Loss: 1.628631591796875
Epoch 5, Iteration 44, Loss: 1.5997512340545654
Epoch 5, Iteration 45, Loss: 1.6817376613616943
Epoch 5, Iteration 46, Loss: 1.6963781118392944
Epoch 5, Iteration 47, Loss: 1.6646173000335693
Epoch 5, Iteration 48, Loss: 1.9106496572494507
Epoch 5, Iteration 49, Loss: 1.6686073541641235
Epoch 5, Iteration 50, Loss: 1.5482323169708252
Epoch 5, Iteration 50, Valid Loss: 0.9428172707557678
Epoch 5, Iteration 51, Loss: 1.5370311737060547
Epoch 5, Iteration 52, Loss: 1.625463604927063
Epoch 5, Iteration 53, Loss: 1.6707054376602173
Epoch 5, Iteration 54, Loss: 1.9300870895385742
Epoch 5, Iteration 55, Loss: 1.8402020931243896
Epoch 5, Iteration 56, Loss: 1.7051985263824463
Epoch 5, Iteration 57, Loss: 1.6634278297424316
Epoch 5, Iteration 58, Loss: 1.439271330833435
Epoch 5, Iteration 59, Loss: 1.6041339635849
Epoch 5, Iteration 60, Loss: 1.7814266681671143
Epoch 5, Iteration 61, Loss: 1.3284647464752197
Epoch 5, Iteration 62, Loss: 1.61202073097229
Epoch 5, Iteration 63, Loss: 1.3924486637115479
Epoch 5, Iteration 64, Loss: 1.6407774686813354
Epoch 5, Iteration 65, Loss: 1.725877046585083
Epoch 5, Iteration 66, Loss: 1.7210885286331177
Epoch 5, Iteration 67, Loss: 1.3523669242858887
Epoch 5, Iteration 68, Loss: 1.8087865114212036
Epoch 5, Iteration 69, Loss: 1.7957288026809692
Epoch 5, Iteration 70, Loss: 1.7048250436782837
Epoch 5, Iteration 71, Loss: 1.6886308193206787
Epoch 5, Iteration 72, Loss: 1.671466588973999
Epoch 5, Iteration 73, Loss: 1.416710615158081
Epoch 5, Iteration 74, Loss: 1.8038126230239868
Epoch 5, Iteration 75, Loss: 1.5792748928070068
Epoch 5, Iteration 76, Loss: 1.4819995164871216
Epoch 5, Iteration 77, Loss: 1.3582398891448975
Epoch 5, Iteration 78, Loss: 1.9351550340652466
Epoch 5, Iteration 79, Loss: 1.9541382789611816
Epoch 5, Iteration 80, Loss: 1.392984390258789
Epoch 5, Iteration 81, Loss: 1.7917760610580444
Epoch 5, Iteration 82, Loss: 1.634090781211853
Epoch 5, Iteration 83, Loss: 1.3501005172729492
Epoch 5, Iteration 84, Loss: 1.800550937652588
Epoch 5, Iteration 85, Loss: 1.5991591215133667
Epoch 5, Iteration 86, Loss: 1.7859070301055908
Epoch 5, Iteration 87, Loss: 1.541868805885315
Epoch 5, Iteration 88, Loss: 1.6675137281417847
Epoch 5, Iteration 89, Loss: 1.6866745948791504
Epoch 5, Iteration 90, Loss: 1.2260140180587769
Epoch 5, Iteration 91, Loss: 1.4662858247756958
Epoch 5, Iteration 92, Loss: 1.2153279781341553
Epoch 5, Iteration 93, Loss: 1.5963877439498901
Epoch 5, Iteration 94, Loss: 1.890172004699707
Epoch 5, Iteration 95, Loss: 1.570389747619629
Epoch 5, Iteration 96, Loss: 1.5415947437286377
Epoch 5, Iteration 97, Loss: 1.4527525901794434
Epoch 5, Iteration 98, Loss: 1.5589313507080078
Epoch 5, Iteration 99, Loss: 1.5037752389907837
Epoch 5, Iteration 100, Loss: 1.4567193984985352
Epoch 5, Iteration 100, Valid Loss: 0.9849002957344055
Epoch 5, Iteration 101, Loss: 1.999730110168457
Epoch 5, Iteration 102, Loss: 1.5697041749954224
Epoch 5, Iteration 103, Loss: 1.546654224395752
Epoch 5, Iteration 104, Loss: 2.0828795433044434
Epoch 5, Iteration 105, Loss: 1.737540364265442
Epoch 5, Iteration 106, Loss: 1.583606481552124
Epoch 5, Iteration 107, Loss: 2.0096566677093506
Epoch 5, Iteration 108, Loss: 1.7096853256225586
Epoch 5, Iteration 109, Loss: 1.861893653869629
Epoch 5, Iteration 110, Loss: 1.8252159357070923
Epoch 5, Iteration 111, Loss: 1.7453417778015137
Epoch 5, Iteration 112, Loss: 1.7058573961257935
Epoch 5, Iteration 113, Loss: 1.4712061882019043
Epoch 5, Iteration 114, Loss: 2.1340112686157227
Epoch 5, Iteration 115, Loss: 1.8171031475067139
Epoch 5, Iteration 116, Loss: 1.6143287420272827
Epoch 5, Iteration 117, Loss: 1.3517941236495972
Epoch 5, Iteration 118, Loss: 1.7236747741699219
Epoch 5, Iteration 119, Loss: 1.6015905141830444
Epoch 5, Iteration 120, Loss: 1.5429933071136475
Epoch 5, Iteration 121, Loss: 1.6988325119018555
Epoch 5, Iteration 122, Loss: 1.4729992151260376
Epoch 5, Iteration 123, Loss: 1.5411944389343262
Epoch 5, Iteration 124, Loss: 1.4468953609466553
Epoch 5, Iteration 125, Loss: 1.7801587581634521
Epoch 5, Iteration 126, Loss: 1.4491528272628784
Epoch 5, Iteration 127, Loss: 1.7853271961212158
Epoch 5, Iteration 128, Loss: 1.7998851537704468
Epoch 5, Iteration 129, Loss: 2.0125153064727783
Epoch 5, Iteration 130, Loss: 2.2243523597717285
Epoch 5, Iteration 131, Loss: 1.4905214309692383
Epoch 5, Iteration 132, Loss: 1.6286505460739136
Epoch 5, Iteration 133, Loss: 1.9832286834716797
Epoch 5, Iteration 134, Loss: 1.5174517631530762
Epoch 5, Iteration 135, Loss: 1.7723411321640015
Epoch 5, Iteration 136, Loss: 1.5006742477416992
Epoch 5, Iteration 137, Loss: 1.5744872093200684
Epoch 5, Iteration 138, Loss: 1.4604886770248413
Epoch 5, Iteration 139, Loss: 1.4429399967193604
Epoch 5, Iteration 140, Loss: 1.5391626358032227
Epoch 5, Iteration 141, Loss: 1.8798168897628784
Epoch 5, Iteration 142, Loss: 1.6000351905822754
Epoch 5, Iteration 143, Loss: 1.644348382949829
Epoch 5, Iteration 144, Loss: 1.4327524900436401
Epoch 5, Iteration 145, Loss: 1.6659119129180908
Epoch 5, Iteration 146, Loss: 1.8432841300964355
Epoch 5, Iteration 147, Loss: 1.5099421739578247
Epoch 5, Iteration 148, Loss: 1.250675916671753
Epoch 5, Iteration 149, Loss: 1.4180662631988525
Epoch 5, Iteration 150, Loss: 1.698464274406433
Epoch 5, Iteration 150, Valid Loss: 0.9587625861167908
Epoch 5, Iteration 151, Loss: 1.729225993156433
Epoch 5, Iteration 152, Loss: 1.526397705078125
Epoch 5, Iteration 153, Loss: 1.650132179260254
Epoch 5, Iteration 154, Loss: 1.2905577421188354
Epoch 5, Iteration 155, Loss: 1.6471542119979858
Epoch 5, Iteration 156, Loss: 1.5161149501800537
Epoch 5, Iteration 157, Loss: 2.1085798740386963
Epoch 5, Iteration 158, Loss: 1.661340594291687
Epoch 5, Iteration 159, Loss: 1.7068297863006592
Epoch 5, Iteration 160, Loss: 1.4286911487579346
Epoch 5, Iteration 161, Loss: 1.508253812789917
Epoch 5, Iteration 162, Loss: 1.9515732526779175
Epoch 5, Iteration 163, Loss: 1.3428151607513428
Epoch 5, Iteration 164, Loss: 1.4895045757293701
Epoch 5, Iteration 165, Loss: 1.4436308145523071
Epoch 5, Iteration 166, Loss: 1.5094791650772095
Epoch 5, Iteration 167, Loss: 1.922035813331604
Epoch 5, Iteration 168, Loss: 1.7542234659194946
Epoch 5, Iteration 169, Loss: 1.6911371946334839
Epoch 5, Iteration 170, Loss: 1.8578126430511475
Epoch 5, Iteration 171, Loss: 1.6885727643966675
Epoch 5, Iteration 172, Loss: 1.747322678565979
Epoch 5, Iteration 173, Loss: 1.4921616315841675
Epoch 5, Iteration 174, Loss: 1.6713054180145264
Epoch 5, Iteration 175, Loss: 1.7299220561981201
Epoch 5, Iteration 176, Loss: 1.3603038787841797
Epoch 5, Iteration 177, Loss: 2.063389778137207
Epoch 5, Iteration 178, Loss: 1.827544093132019
Epoch 5, Iteration 179, Loss: 1.6133270263671875
Epoch 5, Iteration 180, Loss: 1.5445525646209717
Epoch 5, Iteration 181, Loss: 2.077639579772949
Epoch 5, Iteration 182, Loss: 1.8234347105026245
Epoch 5, Iteration 183, Loss: 1.3829070329666138
Epoch 5, Iteration 184, Loss: 1.4374535083770752
Epoch 5, Iteration 185, Loss: 1.445854902267456
Epoch 5, Iteration 186, Loss: 1.5209345817565918
Epoch 5, Iteration 187, Loss: 1.5064618587493896
Epoch 5, Iteration 188, Loss: 1.4045917987823486
Epoch 5, Iteration 189, Loss: 1.4765143394470215
Epoch 5, Iteration 190, Loss: 1.7641825675964355
Epoch 5, Iteration 191, Loss: 1.7319798469543457
Epoch 5, Iteration 192, Loss: 1.6670129299163818
Epoch 5, Iteration 193, Loss: 1.637037754058838
Epoch 5, Iteration 194, Loss: 1.5400385856628418
Epoch 5, Iteration 195, Loss: 1.4451817274093628
Epoch 5, Iteration 196, Loss: 1.6577129364013672
Epoch 5, Iteration 197, Loss: 1.5556946992874146
Epoch 5, Iteration 198, Loss: 1.623181939125061
Epoch 5, Iteration 199, Loss: 1.752817988395691
Epoch 5, Iteration 200, Loss: 1.5511069297790527
Epoch 5, Iteration 200, Valid Loss: 0.9488080143928528
Epoch 5, Iteration 201, Loss: 1.450032353401184
Epoch 5, Iteration 202, Loss: 1.6190922260284424
Epoch 5, Iteration 203, Loss: 1.5305896997451782
Epoch 5, Iteration 204, Loss: 1.5388567447662354
Epoch 5, Iteration 205, Loss: 1.369377613067627
Epoch 5, Iteration 206, Loss: 1.5636882781982422
Epoch 5, Iteration 207, Loss: 1.7714130878448486
Epoch 5, Iteration 208, Loss: 2.1974287033081055
Epoch 5, Iteration 209, Loss: 2.4718918800354004
Epoch 5, Iteration 210, Loss: 2.0881121158599854
Epoch 5, Iteration 211, Loss: 2.2081806659698486
Epoch 5, Iteration 212, Loss: 1.9703913927078247
Epoch 5, Iteration 213, Loss: 2.01556396484375
Epoch 5, Iteration 214, Loss: 1.6103750467300415
Epoch 5, Iteration 215, Loss: 1.523792028427124
Epoch 5, Iteration 216, Loss: 1.4556301832199097
Epoch 5, Iteration 217, Loss: 1.584694743156433
Epoch 5, Iteration 218, Loss: 1.680633306503296
Epoch 5, Iteration 219, Loss: 1.6826974153518677
Epoch 5, Iteration 220, Loss: 1.559553623199463
Epoch 5, Iteration 221, Loss: 1.6265040636062622
Epoch 5, Iteration 222, Loss: 1.6638909578323364
Epoch 5, Iteration 223, Loss: 1.5563740730285645
Epoch 5, Iteration 224, Loss: 1.4225319623947144
Epoch 5, Iteration 225, Loss: 1.8290632963180542
Epoch 5, Iteration 226, Loss: 1.7514188289642334
Epoch 5, Iteration 227, Loss: 1.7081576585769653
Epoch 5, Iteration 228, Loss: 1.4905881881713867
Epoch 5, Iteration 229, Loss: 1.8251848220825195
Epoch 5, Iteration 230, Loss: 1.6662051677703857
Epoch 5, Iteration 231, Loss: 1.6864131689071655
Epoch 5, Iteration 232, Loss: 1.7253621816635132
Epoch 5, Iteration 233, Loss: 2.160684585571289
Epoch 5, Iteration 234, Loss: 1.8220239877700806
Epoch 5, Iteration 235, Loss: 1.496199369430542
Epoch 5, Iteration 236, Loss: 1.71034836769104
Epoch 5, Iteration 237, Loss: 1.5125619173049927
Epoch 5, Iteration 238, Loss: 1.4374662637710571
Epoch 5, Iteration 239, Loss: 1.7302665710449219
Epoch 5, Iteration 240, Loss: 1.4237899780273438
Epoch 5, Iteration 241, Loss: 1.5157196521759033
Epoch 5, Iteration 242, Loss: 1.5390409231185913
Epoch 5, Iteration 243, Loss: 1.598778247833252
Epoch 5, Iteration 244, Loss: 1.5074180364608765
Epoch 5, Iteration 245, Loss: 1.275437355041504
Epoch 5, Iteration 246, Loss: 1.6183373928070068
Epoch 5, Iteration 247, Loss: 1.498618483543396
Epoch 5, Iteration 248, Loss: 1.6181854009628296
Epoch 5, Iteration 249, Loss: 1.4479094743728638
Epoch 5, Iteration 250, Loss: 1.6858798265457153
Epoch 5, Iteration 250, Valid Loss: 0.9225060939788818
Epoch 5, Iteration 251, Loss: 1.6025629043579102
Epoch 5, Iteration 252, Loss: 1.7668412923812866
Epoch 5, Iteration 253, Loss: 1.6798031330108643
Epoch 5, Iteration 254, Loss: 1.6857531070709229
Epoch 5, Iteration 255, Loss: 1.8060219287872314
Epoch 5, Iteration 256, Loss: 1.9080426692962646
Epoch 5, Iteration 257, Loss: 1.4903898239135742
Epoch 5, Iteration 258, Loss: 2.049928665161133
Epoch 5, Iteration 259, Loss: 1.3544212579727173
Epoch 5, Iteration 260, Loss: 1.8829609155654907
Epoch 5, Iteration 261, Loss: 1.7180018424987793
Epoch 5, Iteration 262, Loss: 1.365537166595459
Epoch 5, Iteration 263, Loss: 1.8241095542907715
Epoch 5, Iteration 264, Loss: 1.5782525539398193
Epoch 5, Iteration 265, Loss: 1.6463662385940552
Epoch 5, Iteration 266, Loss: 1.6888396739959717
Epoch 5, Iteration 267, Loss: 1.581978678703308
Epoch 5, Iteration 268, Loss: 1.570966124534607
Epoch 5, Iteration 269, Loss: 1.5449402332305908
Epoch 5, Iteration 270, Loss: 1.6477500200271606
Epoch 5, Iteration 271, Loss: 1.7313425540924072
Epoch 5, Iteration 272, Loss: 1.4217016696929932
Epoch 5, Iteration 273, Loss: 1.7530094385147095
Epoch 5, Iteration 274, Loss: 1.5008939504623413
Epoch 5, Iteration 275, Loss: 1.438976764678955
Epoch 5, Iteration 276, Loss: 1.4305530786514282
Epoch 5, Iteration 277, Loss: 1.4764915704727173
Epoch 5, Iteration 278, Loss: 1.7396612167358398
Epoch 5, Iteration 279, Loss: 1.3985188007354736
Epoch 5, Iteration 280, Loss: 1.3488212823867798
Epoch 5, Iteration 281, Loss: 1.4411587715148926
Epoch 5, Iteration 282, Loss: 1.67073655128479
Epoch 5, Iteration 283, Loss: 1.6278018951416016
Epoch 5, Iteration 284, Loss: 1.546694278717041
Epoch 5, Iteration 285, Loss: 1.5157146453857422
Epoch 5, Iteration 286, Loss: 1.4832850694656372
Epoch 5, Iteration 287, Loss: 1.588942289352417
Epoch 5, Iteration 288, Loss: 1.4132490158081055
Epoch 5, Iteration 289, Loss: 1.4577360153198242
Epoch 5, Iteration 290, Loss: 1.4472326040267944
Epoch 5, Iteration 291, Loss: 1.4432587623596191
Epoch 5, Iteration 292, Loss: 1.806030511856079
Epoch 5, Iteration 293, Loss: 1.851209282875061
Epoch 5, Iteration 294, Loss: 1.373218297958374
Epoch 5, Iteration 295, Loss: 1.4433236122131348
Epoch 5, Iteration 296, Loss: 1.8701752424240112
Epoch 5, Iteration 297, Loss: 1.7918519973754883
Epoch 5, Iteration 298, Loss: 1.6074343919754028
Epoch 5, Iteration 299, Loss: 1.271769642829895
Epoch 5, Iteration 300, Loss: 1.3090121746063232
Epoch 5, Iteration 300, Valid Loss: 0.9176764488220215
Epoch 5, Iteration 301, Loss: 1.549169898033142
Epoch 5, Iteration 302, Loss: 1.4348331689834595
Epoch 5, Iteration 303, Loss: 1.8500018119812012
Epoch 5, Iteration 304, Loss: 1.451108694076538
Epoch 5, Iteration 305, Loss: 1.4190924167633057
Epoch 5, Iteration 306, Loss: 1.457453727722168
Epoch 5, Iteration 307, Loss: 1.4100300073623657
Epoch 5, Iteration 308, Loss: 1.4512654542922974
Epoch 5, Iteration 309, Loss: 1.6157324314117432
Epoch 5, Iteration 310, Loss: 1.854191780090332
Epoch 5, Iteration 311, Loss: 1.5261479616165161
Epoch 5, Iteration 312, Loss: 1.2916206121444702
Epoch 5, Iteration 313, Loss: 1.7311545610427856
Epoch 5, Iteration 314, Loss: 1.4270477294921875
Epoch 5, Iteration 315, Loss: 1.2871392965316772
Epoch 5, Iteration 316, Loss: 1.6991900205612183
Epoch 5, Iteration 317, Loss: 1.497772216796875
Epoch 5, Iteration 318, Loss: 1.4348313808441162
Epoch 5, Iteration 319, Loss: 1.6371804475784302
Epoch 5, Iteration 320, Loss: 1.256373643875122
Epoch 5, Iteration 321, Loss: 1.5885767936706543
Epoch 5, Iteration 322, Loss: 1.478087067604065
Epoch 5, Iteration 323, Loss: 1.5652291774749756
Epoch 5, Iteration 324, Loss: 1.5709517002105713
Epoch 5, Iteration 325, Loss: 1.5193835496902466
Epoch 5, Iteration 326, Loss: 1.5100078582763672
Epoch 5, Iteration 327, Loss: 1.457986831665039
Epoch 5, Iteration 328, Loss: 1.5297892093658447
Epoch 5, Iteration 329, Loss: 1.584195852279663
Epoch 5, Iteration 330, Loss: 1.2303707599639893
Epoch 5, Iteration 331, Loss: 1.4284725189208984
Epoch 5, Iteration 332, Loss: 1.5218212604522705
Epoch 5, Iteration 333, Loss: 1.7115068435668945
Epoch 5, Iteration 334, Loss: 1.4609233140945435
Epoch 5, Iteration 335, Loss: 1.5551788806915283
Epoch 5, Iteration 336, Loss: 1.593967080116272
Epoch 5, Iteration 337, Loss: 1.2515661716461182
Epoch 5, Iteration 338, Loss: 1.6314823627471924
Epoch 5, Iteration 339, Loss: 1.439786672592163
Epoch 5, Iteration 340, Loss: 1.362583875656128
Epoch 5, Iteration 341, Loss: 1.4734538793563843
Epoch 5, Iteration 342, Loss: 1.4099359512329102
Epoch 5, Iteration 343, Loss: 1.3991708755493164
Epoch 5, Iteration 344, Loss: 1.650292992591858
Epoch 5, Iteration 345, Loss: 1.6740976572036743
Epoch 5, Iteration 346, Loss: 1.5752472877502441
Epoch 5, Iteration 347, Loss: 1.2533700466156006
Epoch 5, Iteration 348, Loss: 1.34756338596344
Epoch 5, Iteration 349, Loss: 1.5934185981750488
Epoch 5, Iteration 350, Loss: 1.4331225156784058
Epoch 5, Iteration 350, Valid Loss: 0.9296104907989502
Epoch 5, Iteration 351, Loss: 1.5181796550750732
Epoch 5, Iteration 352, Loss: 1.4747213125228882
Epoch 5, Iteration 353, Loss: 1.5352963209152222
Epoch 5, Iteration 354, Loss: 1.3996578454971313
Epoch 5, Iteration 355, Loss: 1.5225749015808105
Epoch 5, Iteration 356, Loss: 1.5642588138580322
Epoch 5, Iteration 357, Loss: 1.3757166862487793
Epoch 5, Iteration 358, Loss: 1.4726537466049194
Epoch 5, Iteration 359, Loss: 1.5025755167007446
Epoch 5, Iteration 360, Loss: 1.4207279682159424
Epoch 5, Iteration 361, Loss: 1.450935959815979
Epoch 5, Iteration 362, Loss: 1.446498155593872
Epoch 5, Iteration 363, Loss: 1.465457797050476
Epoch 5, Iteration 364, Loss: 1.489612340927124
Epoch 5, Iteration 365, Loss: 1.6371926069259644
Epoch 5, Iteration 366, Loss: 1.6911771297454834
Epoch 5, Iteration 367, Loss: 1.5758883953094482
Epoch 5, Iteration 368, Loss: 1.5197315216064453
Epoch 5, Iteration 369, Loss: 1.6710612773895264
Epoch 5, Iteration 370, Loss: 1.6399503946304321
Epoch 5, Iteration 371, Loss: 1.5268973112106323
Epoch 5, Iteration 372, Loss: 1.3054159879684448
Epoch 5, Iteration 373, Loss: 1.3553645610809326
Epoch 5, Iteration 374, Loss: 1.7647312879562378
Epoch 5, Iteration 375, Loss: 1.5279526710510254
Epoch 5, Iteration 376, Loss: 1.6568537950515747
Epoch 5, Iteration 377, Loss: 1.5262727737426758
Epoch 5, Iteration 378, Loss: 1.394017219543457
Epoch 5, Iteration 379, Loss: 1.4247462749481201
Epoch 5, Iteration 380, Loss: 1.4443881511688232
Epoch 5, Iteration 381, Loss: 1.3863593339920044
Epoch 5, Iteration 382, Loss: 1.139230489730835
Epoch 5, Iteration 383, Loss: 1.4424571990966797
Epoch 5, Iteration 384, Loss: 1.4292678833007812
Epoch 5, Iteration 385, Loss: 1.365984320640564
Epoch 5, Iteration 386, Loss: 1.3070930242538452
Epoch 5, Iteration 387, Loss: 1.7878165245056152
Epoch 5, Iteration 388, Loss: 1.4664231538772583
Epoch 5, Iteration 389, Loss: 1.589698314666748
Epoch 5, Iteration 390, Loss: 1.764993667602539
Epoch 5, Iteration 391, Loss: 1.3075919151306152
Epoch 6/10, Loss: 1.610075089700368
Epoch 6, Iteration 0, Loss: 1.6199774742126465
Epoch 6, Iteration 1, Loss: 1.9752928018569946
Epoch 6, Iteration 2, Loss: 1.6939550638198853
Epoch 6, Iteration 3, Loss: 1.722096562385559
Epoch 6, Iteration 4, Loss: 1.57614004611969
Epoch 6, Iteration 5, Loss: 1.701351284980774
Epoch 6, Iteration 6, Loss: 1.5209085941314697
Epoch 6, Iteration 7, Loss: 1.5276198387145996
Epoch 6, Iteration 8, Loss: 1.5916011333465576
Epoch 6, Iteration 9, Loss: 1.8126264810562134
Epoch 6, Iteration 10, Loss: 1.6002862453460693
Epoch 6, Iteration 11, Loss: 1.4392764568328857
Epoch 6, Iteration 12, Loss: 1.4769902229309082
Epoch 6, Iteration 13, Loss: 1.6957480907440186
Epoch 6, Iteration 14, Loss: 1.5315653085708618
Epoch 6, Iteration 15, Loss: 1.6574445962905884
Epoch 6, Iteration 16, Loss: 1.585288405418396
Epoch 6, Iteration 17, Loss: 1.7523553371429443
Epoch 6, Iteration 18, Loss: 1.6332430839538574
Epoch 6, Iteration 19, Loss: 1.7007032632827759
Epoch 6, Iteration 20, Loss: 1.5388457775115967
Epoch 6, Iteration 21, Loss: 1.6277046203613281
Epoch 6, Iteration 22, Loss: 1.5918887853622437
Epoch 6, Iteration 23, Loss: 1.6447951793670654
Epoch 6, Iteration 24, Loss: 1.5157431364059448
Epoch 6, Iteration 25, Loss: 1.3695793151855469
Epoch 6, Iteration 26, Loss: 1.917839527130127
Epoch 6, Iteration 27, Loss: 1.5510177612304688
Epoch 6, Iteration 28, Loss: 1.5950958728790283
Epoch 6, Iteration 29, Loss: 1.774978518486023
Epoch 6, Iteration 30, Loss: 1.653422236442566
Epoch 6, Iteration 31, Loss: 1.611724615097046
Epoch 6, Iteration 32, Loss: 1.6420629024505615
Epoch 6, Iteration 33, Loss: 1.5167860984802246
Epoch 6, Iteration 34, Loss: 1.518810510635376
Epoch 6, Iteration 35, Loss: 1.7654571533203125
Epoch 6, Iteration 36, Loss: 1.348781943321228
Epoch 6, Iteration 37, Loss: 1.4671165943145752
Epoch 6, Iteration 38, Loss: 1.5064862966537476
Epoch 6, Iteration 39, Loss: 1.4275003671646118
Epoch 6, Iteration 40, Loss: 1.4698586463928223
Epoch 6, Iteration 41, Loss: 1.630218267440796
Epoch 6, Iteration 42, Loss: 1.519860029220581
Epoch 6, Iteration 43, Loss: 1.5286273956298828
Epoch 6, Iteration 44, Loss: 1.508428692817688
Epoch 6, Iteration 45, Loss: 1.5316461324691772
Epoch 6, Iteration 46, Loss: 1.6069648265838623
Epoch 6, Iteration 47, Loss: 1.6345535516738892
Epoch 6, Iteration 48, Loss: 1.7910077571868896
Epoch 6, Iteration 49, Loss: 1.6145765781402588
Epoch 6, Iteration 50, Loss: 1.5419031381607056
Epoch 6, Iteration 50, Valid Loss: 0.8801066875457764
Epoch 6, Iteration 51, Loss: 1.5248162746429443
Epoch 6, Iteration 52, Loss: 1.4592008590698242
Epoch 6, Iteration 53, Loss: 1.8820598125457764
Epoch 6, Iteration 54, Loss: 1.8643913269042969
Epoch 6, Iteration 55, Loss: 1.8089256286621094
Epoch 6, Iteration 56, Loss: 1.4671275615692139
Epoch 6, Iteration 57, Loss: 1.8304493427276611
Epoch 6, Iteration 58, Loss: 1.43105149269104
Epoch 6, Iteration 59, Loss: 1.5173838138580322
Epoch 6, Iteration 60, Loss: 1.595081090927124
Epoch 6, Iteration 61, Loss: 1.268319010734558
Epoch 6, Iteration 62, Loss: 1.5163404941558838
Epoch 6, Iteration 63, Loss: 1.375815510749817
Epoch 6, Iteration 64, Loss: 1.4243367910385132
Epoch 6, Iteration 65, Loss: 1.834952712059021
Epoch 6, Iteration 66, Loss: 1.4665634632110596
Epoch 6, Iteration 67, Loss: 1.2455077171325684
Epoch 6, Iteration 68, Loss: 1.797236680984497
Epoch 6, Iteration 69, Loss: 1.9584975242614746
Epoch 6, Iteration 70, Loss: 1.7993382215499878
Epoch 6, Iteration 71, Loss: 1.6500751972198486
Epoch 6, Iteration 72, Loss: 1.7802550792694092
Epoch 6, Iteration 73, Loss: 1.38499116897583
Epoch 6, Iteration 74, Loss: 1.8708425760269165
Epoch 6, Iteration 75, Loss: 1.639788269996643
Epoch 6, Iteration 76, Loss: 1.4574744701385498
Epoch 6, Iteration 77, Loss: 1.2456490993499756
Epoch 6, Iteration 78, Loss: 1.8526241779327393
Epoch 6, Iteration 79, Loss: 1.8638877868652344
Epoch 6, Iteration 80, Loss: 1.3100886344909668
Epoch 6, Iteration 81, Loss: 1.8885821104049683
Epoch 6, Iteration 82, Loss: 1.4150880575180054
Epoch 6, Iteration 83, Loss: 1.2991806268692017
Epoch 6, Iteration 84, Loss: 1.768306851387024
Epoch 6, Iteration 85, Loss: 1.6327557563781738
Epoch 6, Iteration 86, Loss: 1.7086223363876343
Epoch 6, Iteration 87, Loss: 1.4709006547927856
Epoch 6, Iteration 88, Loss: 1.6663978099822998
Epoch 6, Iteration 89, Loss: 1.6517815589904785
Epoch 6, Iteration 90, Loss: 1.2594797611236572
Epoch 6, Iteration 91, Loss: 1.5023152828216553
Epoch 6, Iteration 92, Loss: 1.2363882064819336
Epoch 6, Iteration 93, Loss: 1.660900592803955
Epoch 6, Iteration 94, Loss: 1.9668269157409668
Epoch 6, Iteration 95, Loss: 1.342360496520996
Epoch 6, Iteration 96, Loss: 1.586355209350586
Epoch 6, Iteration 97, Loss: 1.3347020149230957
Epoch 6, Iteration 98, Loss: 1.5126726627349854
Epoch 6, Iteration 99, Loss: 1.431624412536621
Epoch 6, Iteration 100, Loss: 1.349648356437683
Epoch 6, Iteration 100, Valid Loss: 0.9077703952789307
Epoch 6, Iteration 101, Loss: 1.9472973346710205
Epoch 6, Iteration 102, Loss: 1.4677927494049072
Epoch 6, Iteration 103, Loss: 1.6908758878707886
Epoch 6, Iteration 104, Loss: 1.9260406494140625
Epoch 6, Iteration 105, Loss: 1.6798138618469238
Epoch 6, Iteration 106, Loss: 1.5099396705627441
Epoch 6, Iteration 107, Loss: 1.9842350482940674
Epoch 6, Iteration 108, Loss: 1.5039970874786377
Epoch 6, Iteration 109, Loss: 1.8017916679382324
Epoch 6, Iteration 110, Loss: 1.6946724653244019
Epoch 6, Iteration 111, Loss: 1.7168257236480713
Epoch 6, Iteration 112, Loss: 1.7467677593231201
Epoch 6, Iteration 113, Loss: 1.6069656610488892
Epoch 6, Iteration 114, Loss: 1.8182008266448975
Epoch 6, Iteration 115, Loss: 1.7952830791473389
Epoch 6, Iteration 116, Loss: 1.507143259048462
Epoch 6, Iteration 117, Loss: 1.2312302589416504
Epoch 6, Iteration 118, Loss: 1.7613332271575928
Epoch 6, Iteration 119, Loss: 1.5167096853256226
Epoch 6, Iteration 120, Loss: 1.539642095565796
Epoch 6, Iteration 121, Loss: 1.437103509902954
Epoch 6, Iteration 122, Loss: 1.504991888999939
Epoch 6, Iteration 123, Loss: 1.4682892560958862
Epoch 6, Iteration 124, Loss: 1.4118834733963013
Epoch 6, Iteration 125, Loss: 1.4099677801132202
Epoch 6, Iteration 126, Loss: 1.6281824111938477
Epoch 6, Iteration 127, Loss: 1.7660911083221436
Epoch 6, Iteration 128, Loss: 1.6210098266601562
Epoch 6, Iteration 129, Loss: 1.6805026531219482
Epoch 6, Iteration 130, Loss: 2.0623703002929688
Epoch 6, Iteration 131, Loss: 1.3849362134933472
Epoch 6, Iteration 132, Loss: 1.3292641639709473
Epoch 6, Iteration 133, Loss: 2.0101428031921387
Epoch 6, Iteration 134, Loss: 1.4277002811431885
Epoch 6, Iteration 135, Loss: 1.7068655490875244
Epoch 6, Iteration 136, Loss: 1.4823964834213257
Epoch 6, Iteration 137, Loss: 1.5091005563735962
Epoch 6, Iteration 138, Loss: 1.294032096862793
Epoch 6, Iteration 139, Loss: 1.5137851238250732
Epoch 6, Iteration 140, Loss: 1.5053246021270752
Epoch 6, Iteration 141, Loss: 1.9657225608825684
Epoch 6, Iteration 142, Loss: 1.3840432167053223
Epoch 6, Iteration 143, Loss: 1.621026635169983
Epoch 6, Iteration 144, Loss: 1.3804835081100464
Epoch 6, Iteration 145, Loss: 1.5712579488754272
Epoch 6, Iteration 146, Loss: 1.822765827178955
Epoch 6, Iteration 147, Loss: 1.5091968774795532
Epoch 6, Iteration 148, Loss: 1.3077669143676758
Epoch 6, Iteration 149, Loss: 1.5717713832855225
Epoch 6, Iteration 150, Loss: 1.6728101968765259
Epoch 6, Iteration 150, Valid Loss: 0.8900068402290344
Epoch 6, Iteration 151, Loss: 1.6467156410217285
Epoch 6, Iteration 152, Loss: 1.4174944162368774
Epoch 6, Iteration 153, Loss: 1.5098551511764526
Epoch 6, Iteration 154, Loss: 1.2643811702728271
Epoch 6, Iteration 155, Loss: 1.460545539855957
Epoch 6, Iteration 156, Loss: 1.4667834043502808
Epoch 6, Iteration 157, Loss: 2.0621538162231445
Epoch 6, Iteration 158, Loss: 1.5493929386138916
Epoch 6, Iteration 159, Loss: 1.6079069375991821
Epoch 6, Iteration 160, Loss: 1.3924989700317383
Epoch 6, Iteration 161, Loss: 1.4226629734039307
Epoch 6, Iteration 162, Loss: 1.966302752494812
Epoch 6, Iteration 163, Loss: 1.3185315132141113
Epoch 6, Iteration 164, Loss: 1.4695051908493042
Epoch 6, Iteration 165, Loss: 1.3623703718185425
Epoch 6, Iteration 166, Loss: 1.430393934249878
Epoch 6, Iteration 167, Loss: 1.7870765924453735
Epoch 6, Iteration 168, Loss: 1.672603964805603
Epoch 6, Iteration 169, Loss: 1.5805009603500366
Epoch 6, Iteration 170, Loss: 1.8130018711090088
Epoch 6, Iteration 171, Loss: 1.6844273805618286
Epoch 6, Iteration 172, Loss: 1.681754469871521
Epoch 6, Iteration 173, Loss: 1.4997889995574951
Epoch 6, Iteration 174, Loss: 1.6853007078170776
Epoch 6, Iteration 175, Loss: 1.7438760995864868
Epoch 6, Iteration 176, Loss: 1.2812011241912842
Epoch 6, Iteration 177, Loss: 1.9700212478637695
Epoch 6, Iteration 178, Loss: 1.7303378582000732
Epoch 6, Iteration 179, Loss: 1.5392810106277466
Epoch 6, Iteration 180, Loss: 1.4631752967834473
Epoch 6, Iteration 181, Loss: 1.8209939002990723
Epoch 6, Iteration 182, Loss: 1.6672279834747314
Epoch 6, Iteration 183, Loss: 1.3285036087036133
Epoch 6, Iteration 184, Loss: 1.4732544422149658
Epoch 6, Iteration 185, Loss: 1.390442132949829
Epoch 6, Iteration 186, Loss: 1.4472277164459229
Epoch 6, Iteration 187, Loss: 1.4445841312408447
Epoch 6, Iteration 188, Loss: 1.432904839515686
Epoch 6, Iteration 189, Loss: 1.4102249145507812
Epoch 6, Iteration 190, Loss: 1.6740726232528687
Epoch 6, Iteration 191, Loss: 1.6596860885620117
Epoch 6, Iteration 192, Loss: 1.6061774492263794
Epoch 6, Iteration 193, Loss: 1.5020365715026855
Epoch 6, Iteration 194, Loss: 1.6505844593048096
Epoch 6, Iteration 195, Loss: 1.4477304220199585
Epoch 6, Iteration 196, Loss: 1.6577105522155762
Epoch 6, Iteration 197, Loss: 1.4947516918182373
Epoch 6, Iteration 198, Loss: 1.5032143592834473
Epoch 6, Iteration 199, Loss: 1.6648792028427124
Epoch 6, Iteration 200, Loss: 1.374205470085144
Epoch 6, Iteration 200, Valid Loss: 0.8835152387619019
Epoch 6, Iteration 201, Loss: 1.3960838317871094
Epoch 6, Iteration 202, Loss: 1.6325286626815796
Epoch 6, Iteration 203, Loss: 1.3655840158462524
Epoch 6, Iteration 204, Loss: 1.3991329669952393
Epoch 6, Iteration 205, Loss: 1.2249974012374878
Epoch 6, Iteration 206, Loss: 1.537030577659607
Epoch 6, Iteration 207, Loss: 1.756842017173767
Epoch 6, Iteration 208, Loss: 2.237668752670288
Epoch 6, Iteration 209, Loss: 2.2318427562713623
Epoch 6, Iteration 210, Loss: 1.9342153072357178
Epoch 6, Iteration 211, Loss: 2.07342529296875
Epoch 6, Iteration 212, Loss: 1.8778975009918213
Epoch 6, Iteration 213, Loss: 1.924814224243164
Epoch 6, Iteration 214, Loss: 1.6485130786895752
Epoch 6, Iteration 215, Loss: 1.4320183992385864
Epoch 6, Iteration 216, Loss: 1.4425418376922607
Epoch 6, Iteration 217, Loss: 1.4405066967010498
Epoch 6, Iteration 218, Loss: 1.5139672756195068
Epoch 6, Iteration 219, Loss: 1.574571132659912
Epoch 6, Iteration 220, Loss: 1.498403549194336
Epoch 6, Iteration 221, Loss: 1.5495511293411255
Epoch 6, Iteration 222, Loss: 1.5290782451629639
Epoch 6, Iteration 223, Loss: 1.4733175039291382
Epoch 6, Iteration 224, Loss: 1.3704843521118164
Epoch 6, Iteration 225, Loss: 1.6893842220306396
Epoch 6, Iteration 226, Loss: 1.7678959369659424
Epoch 6, Iteration 227, Loss: 1.5492870807647705
Epoch 6, Iteration 228, Loss: 1.5126879215240479
Epoch 6, Iteration 229, Loss: 1.6243575811386108
Epoch 6, Iteration 230, Loss: 1.7354897260665894
Epoch 6, Iteration 231, Loss: 1.5668001174926758
Epoch 6, Iteration 232, Loss: 1.833617925643921
Epoch 6, Iteration 233, Loss: 2.0582361221313477
Epoch 6, Iteration 234, Loss: 1.756669044494629
Epoch 6, Iteration 235, Loss: 1.3859987258911133
Epoch 6, Iteration 236, Loss: 1.7243491411209106
Epoch 6, Iteration 237, Loss: 1.620750069618225
Epoch 6, Iteration 238, Loss: 1.5057439804077148
Epoch 6, Iteration 239, Loss: 1.703249216079712
Epoch 6, Iteration 240, Loss: 1.4177664518356323
Epoch 6, Iteration 241, Loss: 1.4284911155700684
Epoch 6, Iteration 242, Loss: 1.4465550184249878
Epoch 6, Iteration 243, Loss: 1.5001354217529297
Epoch 6, Iteration 244, Loss: 1.446664571762085
Epoch 6, Iteration 245, Loss: 1.3101412057876587
Epoch 6, Iteration 246, Loss: 1.5314809083938599
Epoch 6, Iteration 247, Loss: 1.4800751209259033
Epoch 6, Iteration 248, Loss: 1.615035891532898
Epoch 6, Iteration 249, Loss: 1.6256024837493896
Epoch 6, Iteration 250, Loss: 1.6550995111465454
Epoch 6, Iteration 250, Valid Loss: 0.8846786022186279
Epoch 6, Iteration 251, Loss: 1.4536815881729126
Epoch 6, Iteration 252, Loss: 1.7094496488571167
Epoch 6, Iteration 253, Loss: 1.6200472116470337
Epoch 6, Iteration 254, Loss: 1.624190092086792
Epoch 6, Iteration 255, Loss: 1.675429344177246
Epoch 6, Iteration 256, Loss: 1.768024206161499
Epoch 6, Iteration 257, Loss: 1.4466201066970825
Epoch 6, Iteration 258, Loss: 1.866119384765625
Epoch 6, Iteration 259, Loss: 1.5152325630187988
Epoch 6, Iteration 260, Loss: 1.7421846389770508
Epoch 6, Iteration 261, Loss: 1.6729971170425415
Epoch 6, Iteration 262, Loss: 1.3566968441009521
Epoch 6, Iteration 263, Loss: 1.794919729232788
Epoch 6, Iteration 264, Loss: 1.617527961730957
Epoch 6, Iteration 265, Loss: 1.5097886323928833
Epoch 6, Iteration 266, Loss: 1.6631226539611816
Epoch 6, Iteration 267, Loss: 1.524803638458252
Epoch 6, Iteration 268, Loss: 1.5041667222976685
Epoch 6, Iteration 269, Loss: 1.6495808362960815
Epoch 6, Iteration 270, Loss: 1.533400297164917
Epoch 6, Iteration 271, Loss: 1.6865164041519165
Epoch 6, Iteration 272, Loss: 1.4459902048110962
Epoch 6, Iteration 273, Loss: 1.3200833797454834
Epoch 6, Iteration 274, Loss: 1.6013141870498657
Epoch 6, Iteration 275, Loss: 1.37417733669281
Epoch 6, Iteration 276, Loss: 1.5154613256454468
Epoch 6, Iteration 277, Loss: 1.3789782524108887
Epoch 6, Iteration 278, Loss: 1.6475672721862793
Epoch 6, Iteration 279, Loss: 1.3184257745742798
Epoch 6, Iteration 280, Loss: 1.1966501474380493
Epoch 6, Iteration 281, Loss: 1.3146158456802368
Epoch 6, Iteration 282, Loss: 1.6328426599502563
Epoch 6, Iteration 283, Loss: 1.5473178625106812
Epoch 6, Iteration 284, Loss: 1.5762349367141724
Epoch 6, Iteration 285, Loss: 1.4658817052841187
Epoch 6, Iteration 286, Loss: 1.4126553535461426
Epoch 6, Iteration 287, Loss: 1.5005011558532715
Epoch 6, Iteration 288, Loss: 1.364957332611084
Epoch 6, Iteration 289, Loss: 1.545977234840393
Epoch 6, Iteration 290, Loss: 1.3789973258972168
Epoch 6, Iteration 291, Loss: 1.4217307567596436
Epoch 6, Iteration 292, Loss: 1.7549631595611572
Epoch 6, Iteration 293, Loss: 1.7469594478607178
Epoch 6, Iteration 294, Loss: 1.4498441219329834
Epoch 6, Iteration 295, Loss: 1.4749081134796143
Epoch 6, Iteration 296, Loss: 1.7957133054733276
Epoch 6, Iteration 297, Loss: 1.7204983234405518
Epoch 6, Iteration 298, Loss: 1.476863145828247
Epoch 6, Iteration 299, Loss: 1.3488072156906128
Epoch 6, Iteration 300, Loss: 1.3291943073272705
Epoch 6, Iteration 300, Valid Loss: 0.8756405711174011
Epoch 6, Iteration 301, Loss: 1.4513570070266724
Epoch 6, Iteration 302, Loss: 1.2998287677764893
Epoch 6, Iteration 303, Loss: 1.7815475463867188
Epoch 6, Iteration 304, Loss: 1.379940390586853
Epoch 6, Iteration 305, Loss: 1.422245979309082
Epoch 6, Iteration 306, Loss: 1.3752119541168213
Epoch 6, Iteration 307, Loss: 1.4798264503479004
Epoch 6, Iteration 308, Loss: 1.3930978775024414
Epoch 6, Iteration 309, Loss: 1.4156060218811035
Epoch 6, Iteration 310, Loss: 1.7274553775787354
Epoch 6, Iteration 311, Loss: 1.3887453079223633
Epoch 6, Iteration 312, Loss: 1.3895567655563354
Epoch 6, Iteration 313, Loss: 1.7248284816741943
Epoch 6, Iteration 314, Loss: 1.417992115020752
Epoch 6, Iteration 315, Loss: 1.3605964183807373
Epoch 6, Iteration 316, Loss: 1.6675742864608765
Epoch 6, Iteration 317, Loss: 1.4778106212615967
Epoch 6, Iteration 318, Loss: 1.3206040859222412
Epoch 6, Iteration 319, Loss: 1.8175328969955444
Epoch 6, Iteration 320, Loss: 1.2203521728515625
Epoch 6, Iteration 321, Loss: 1.4299731254577637
Epoch 6, Iteration 322, Loss: 1.3866820335388184
Epoch 6, Iteration 323, Loss: 1.4304001331329346
Epoch 6, Iteration 324, Loss: 1.438102126121521
Epoch 6, Iteration 325, Loss: 1.3719102144241333
Epoch 6, Iteration 326, Loss: 1.4943572282791138
Epoch 6, Iteration 327, Loss: 1.3884832859039307
Epoch 6, Iteration 328, Loss: 1.2918565273284912
Epoch 6, Iteration 329, Loss: 1.5959079265594482
Epoch 6, Iteration 330, Loss: 1.1310347318649292
Epoch 6, Iteration 331, Loss: 1.3569104671478271
Epoch 6, Iteration 332, Loss: 1.514693260192871
Epoch 6, Iteration 333, Loss: 1.757453203201294
Epoch 6, Iteration 334, Loss: 1.411563515663147
Epoch 6, Iteration 335, Loss: 1.7097175121307373
Epoch 6, Iteration 336, Loss: 1.5805790424346924
Epoch 6, Iteration 337, Loss: 1.2158348560333252
Epoch 6, Iteration 338, Loss: 1.5348010063171387
Epoch 6, Iteration 339, Loss: 1.550653100013733
Epoch 6, Iteration 340, Loss: 1.3498198986053467
Epoch 6, Iteration 341, Loss: 1.3559339046478271
Epoch 6, Iteration 342, Loss: 1.4237744808197021
Epoch 6, Iteration 343, Loss: 1.3745876550674438
Epoch 6, Iteration 344, Loss: 1.4811877012252808
Epoch 6, Iteration 345, Loss: 1.4941662549972534
Epoch 6, Iteration 346, Loss: 1.4697924852371216
Epoch 6, Iteration 347, Loss: 1.3302743434906006
Epoch 6, Iteration 348, Loss: 1.2765297889709473
Epoch 6, Iteration 349, Loss: 1.575301170349121
Epoch 6, Iteration 350, Loss: 1.313553810119629
Epoch 6, Iteration 350, Valid Loss: 0.8903225660324097
Epoch 6, Iteration 351, Loss: 1.5969749689102173
Epoch 6, Iteration 352, Loss: 1.271857500076294
Epoch 6, Iteration 353, Loss: 1.5514142513275146
Epoch 6, Iteration 354, Loss: 1.4968317747116089
Epoch 6, Iteration 355, Loss: 1.4342539310455322
Epoch 6, Iteration 356, Loss: 1.4962825775146484
Epoch 6, Iteration 357, Loss: 1.4084193706512451
Epoch 6, Iteration 358, Loss: 1.4336577653884888
Epoch 6, Iteration 359, Loss: 1.4181315898895264
Epoch 6, Iteration 360, Loss: 1.2694729566574097
Epoch 6, Iteration 361, Loss: 1.4546488523483276
Epoch 6, Iteration 362, Loss: 1.384979486465454
Epoch 6, Iteration 363, Loss: 1.3814640045166016
Epoch 6, Iteration 364, Loss: 1.4989975690841675
Epoch 6, Iteration 365, Loss: 1.6247286796569824
Epoch 6, Iteration 366, Loss: 1.640358805656433
Epoch 6, Iteration 367, Loss: 1.6232037544250488
Epoch 6, Iteration 368, Loss: 1.378652572631836
Epoch 6, Iteration 369, Loss: 1.643567681312561
Epoch 6, Iteration 370, Loss: 1.5579239130020142
Epoch 6, Iteration 371, Loss: 1.404334545135498
Epoch 6, Iteration 372, Loss: 1.2610243558883667
Epoch 6, Iteration 373, Loss: 1.3536263704299927
Epoch 6, Iteration 374, Loss: 1.7440602779388428
Epoch 6, Iteration 375, Loss: 1.4313160181045532
Epoch 6, Iteration 376, Loss: 1.5415066480636597
Epoch 6, Iteration 377, Loss: 1.5006128549575806
Epoch 6, Iteration 378, Loss: 1.3354153633117676
Epoch 6, Iteration 379, Loss: 1.3872712850570679
Epoch 6, Iteration 380, Loss: 1.334791898727417
Epoch 6, Iteration 381, Loss: 1.3750548362731934
Epoch 6, Iteration 382, Loss: 1.1354877948760986
Epoch 6, Iteration 383, Loss: 1.3043265342712402
Epoch 6, Iteration 384, Loss: 1.4615356922149658
Epoch 6, Iteration 385, Loss: 1.334808588027954
Epoch 6, Iteration 386, Loss: 1.320015788078308
Epoch 6, Iteration 387, Loss: 1.8003954887390137
Epoch 6, Iteration 388, Loss: 1.408442497253418
Epoch 6, Iteration 389, Loss: 1.4810298681259155
Epoch 6, Iteration 390, Loss: 1.5447837114334106
Epoch 6, Iteration 391, Loss: 1.3616070747375488
Epoch 7/10, Loss: 1.552569690407539
Epoch 7, Iteration 0, Loss: 1.5364042520523071
Epoch 7, Iteration 1, Loss: 1.8069727420806885
Epoch 7, Iteration 2, Loss: 1.5839309692382812
Epoch 7, Iteration 3, Loss: 1.6996214389801025
Epoch 7, Iteration 4, Loss: 1.5201318264007568
Epoch 7, Iteration 5, Loss: 1.772413969039917
Epoch 7, Iteration 6, Loss: 1.4731824398040771
Epoch 7, Iteration 7, Loss: 1.5280439853668213
Epoch 7, Iteration 8, Loss: 1.535683274269104
Epoch 7, Iteration 9, Loss: 1.834026575088501
Epoch 7, Iteration 10, Loss: 1.612262487411499
Epoch 7, Iteration 11, Loss: 1.3830910921096802
Epoch 7, Iteration 12, Loss: 1.4403996467590332
Epoch 7, Iteration 13, Loss: 1.636704683303833
Epoch 7, Iteration 14, Loss: 1.522611379623413
Epoch 7, Iteration 15, Loss: 1.8805503845214844
Epoch 7, Iteration 16, Loss: 1.4223806858062744
Epoch 7, Iteration 17, Loss: 1.6550254821777344
Epoch 7, Iteration 18, Loss: 1.5299267768859863
Epoch 7, Iteration 19, Loss: 1.6456334590911865
Epoch 7, Iteration 20, Loss: 1.4915447235107422
Epoch 7, Iteration 21, Loss: 1.4573338031768799
Epoch 7, Iteration 22, Loss: 1.4851800203323364
Epoch 7, Iteration 23, Loss: 1.565545916557312
Epoch 7, Iteration 24, Loss: 1.3903381824493408
Epoch 7, Iteration 25, Loss: 1.3031795024871826
Epoch 7, Iteration 26, Loss: 1.859615445137024
Epoch 7, Iteration 27, Loss: 1.4108502864837646
Epoch 7, Iteration 28, Loss: 1.5862480401992798
Epoch 7, Iteration 29, Loss: 1.6826770305633545
Epoch 7, Iteration 30, Loss: 1.524187684059143
Epoch 7, Iteration 31, Loss: 1.7087258100509644
Epoch 7, Iteration 32, Loss: 1.4362863302230835
Epoch 7, Iteration 33, Loss: 1.5465748310089111
Epoch 7, Iteration 34, Loss: 1.4138437509536743
Epoch 7, Iteration 35, Loss: 1.5235779285430908
Epoch 7, Iteration 36, Loss: 1.2702566385269165
Epoch 7, Iteration 37, Loss: 1.3949508666992188
Epoch 7, Iteration 38, Loss: 1.443190336227417
Epoch 7, Iteration 39, Loss: 1.4515658617019653
Epoch 7, Iteration 40, Loss: 1.4742549657821655
Epoch 7, Iteration 41, Loss: 1.5856633186340332
Epoch 7, Iteration 42, Loss: 1.5358591079711914
Epoch 7, Iteration 43, Loss: 1.4563450813293457
Epoch 7, Iteration 44, Loss: 1.4677205085754395
Epoch 7, Iteration 45, Loss: 1.5384495258331299
Epoch 7, Iteration 46, Loss: 1.512183427810669
Epoch 7, Iteration 47, Loss: 1.5485618114471436
Epoch 7, Iteration 48, Loss: 1.6368772983551025
Epoch 7, Iteration 49, Loss: 1.505608081817627
Epoch 7, Iteration 50, Loss: 1.4159218072891235
Epoch 7, Iteration 50, Valid Loss: 0.83391273021698
Epoch 7, Iteration 51, Loss: 1.3667972087860107
Epoch 7, Iteration 52, Loss: 1.4188916683197021
Epoch 7, Iteration 53, Loss: 1.5639739036560059
Epoch 7, Iteration 54, Loss: 1.7462739944458008
Epoch 7, Iteration 55, Loss: 1.6693553924560547
Epoch 7, Iteration 56, Loss: 1.5015265941619873
Epoch 7, Iteration 57, Loss: 1.879530668258667
Epoch 7, Iteration 58, Loss: 1.4212104082107544
Epoch 7, Iteration 59, Loss: 1.5024888515472412
Epoch 7, Iteration 60, Loss: 1.5320227146148682
Epoch 7, Iteration 61, Loss: 1.1462322473526
Epoch 7, Iteration 62, Loss: 1.505871057510376
Epoch 7, Iteration 63, Loss: 1.3024036884307861
Epoch 7, Iteration 64, Loss: 1.319401502609253
Epoch 7, Iteration 65, Loss: 1.6046713590621948
Epoch 7, Iteration 66, Loss: 1.4976153373718262
Epoch 7, Iteration 67, Loss: 1.1966201066970825
Epoch 7, Iteration 68, Loss: 1.6865694522857666
Epoch 7, Iteration 69, Loss: 1.695443868637085
Epoch 7, Iteration 70, Loss: 1.5221256017684937
Epoch 7, Iteration 71, Loss: 1.665388822555542
Epoch 7, Iteration 72, Loss: 1.6800636053085327
Epoch 7, Iteration 73, Loss: 1.313042163848877
Epoch 7, Iteration 74, Loss: 1.7743771076202393
Epoch 7, Iteration 75, Loss: 1.4799778461456299
Epoch 7, Iteration 76, Loss: 1.400665283203125
Epoch 7, Iteration 77, Loss: 1.2386932373046875
Epoch 7, Iteration 78, Loss: 1.8199423551559448
Epoch 7, Iteration 79, Loss: 1.753056287765503
Epoch 7, Iteration 80, Loss: 1.3100736141204834
Epoch 7, Iteration 81, Loss: 1.6508173942565918
Epoch 7, Iteration 82, Loss: 1.7525875568389893
Epoch 7, Iteration 83, Loss: 1.2975974082946777
Epoch 7, Iteration 84, Loss: 1.6318672895431519
Epoch 7, Iteration 85, Loss: 1.5775004625320435
Epoch 7, Iteration 86, Loss: 1.590164303779602
Epoch 7, Iteration 87, Loss: 1.4440494775772095
Epoch 7, Iteration 88, Loss: 1.5936675071716309
Epoch 7, Iteration 89, Loss: 1.4662792682647705
Epoch 7, Iteration 90, Loss: 1.173667073249817
Epoch 7, Iteration 91, Loss: 1.3596712350845337
Epoch 7, Iteration 92, Loss: 1.1211730241775513
Epoch 7, Iteration 93, Loss: 1.4592901468276978
Epoch 7, Iteration 94, Loss: 2.029233694076538
Epoch 7, Iteration 95, Loss: 1.3324891328811646
Epoch 7, Iteration 96, Loss: 1.5255156755447388
Epoch 7, Iteration 97, Loss: 1.3092734813690186
Epoch 7, Iteration 98, Loss: 1.4298205375671387
Epoch 7, Iteration 99, Loss: 1.513092279434204
Epoch 7, Iteration 100, Loss: 1.3181430101394653
Epoch 7, Iteration 100, Valid Loss: 0.8532846570014954
Epoch 7, Iteration 101, Loss: 1.8426382541656494
Epoch 7, Iteration 102, Loss: 1.5977833271026611
Epoch 7, Iteration 103, Loss: 1.4208849668502808
Epoch 7, Iteration 104, Loss: 1.8742687702178955
Epoch 7, Iteration 105, Loss: 1.65004301071167
Epoch 7, Iteration 106, Loss: 1.382920265197754
Epoch 7, Iteration 107, Loss: 1.8511983156204224
Epoch 7, Iteration 108, Loss: 1.4457041025161743
Epoch 7, Iteration 109, Loss: 1.757643461227417
Epoch 7, Iteration 110, Loss: 1.7592123746871948
Epoch 7, Iteration 111, Loss: 1.6111232042312622
Epoch 7, Iteration 112, Loss: 1.512965440750122
Epoch 7, Iteration 113, Loss: 1.343505859375
Epoch 7, Iteration 114, Loss: 1.8154540061950684
Epoch 7, Iteration 115, Loss: 1.6921472549438477
Epoch 7, Iteration 116, Loss: 1.4661509990692139
Epoch 7, Iteration 117, Loss: 1.1301360130310059
Epoch 7, Iteration 118, Loss: 1.562126874923706
Epoch 7, Iteration 119, Loss: 1.5060672760009766
Epoch 7, Iteration 120, Loss: 1.888845682144165
Epoch 7, Iteration 121, Loss: 1.4274879693984985
Epoch 7, Iteration 122, Loss: 1.3917781114578247
Epoch 7, Iteration 123, Loss: 1.6006063222885132
Epoch 7, Iteration 124, Loss: 1.2917358875274658
Epoch 7, Iteration 125, Loss: 1.323639988899231
Epoch 7, Iteration 126, Loss: 1.2359473705291748
Epoch 7, Iteration 127, Loss: 1.6675896644592285
Epoch 7, Iteration 128, Loss: 1.6345332860946655
Epoch 7, Iteration 129, Loss: 1.5754061937332153
Epoch 7, Iteration 130, Loss: 1.8727376461029053
Epoch 7, Iteration 131, Loss: 1.3276910781860352
Epoch 7, Iteration 132, Loss: 1.3883315324783325
Epoch 7, Iteration 133, Loss: 1.956460952758789
Epoch 7, Iteration 134, Loss: 1.3473459482192993
Epoch 7, Iteration 135, Loss: 1.663020372390747
Epoch 7, Iteration 136, Loss: 1.7486552000045776
Epoch 7, Iteration 137, Loss: 1.4985401630401611
Epoch 7, Iteration 138, Loss: 1.4357879161834717
Epoch 7, Iteration 139, Loss: 1.3256046772003174
Epoch 7, Iteration 140, Loss: 1.4099270105361938
Epoch 7, Iteration 141, Loss: 1.7452785968780518
Epoch 7, Iteration 142, Loss: 1.530281662940979
Epoch 7, Iteration 143, Loss: 1.5285375118255615
Epoch 7, Iteration 144, Loss: 1.3158942461013794
Epoch 7, Iteration 145, Loss: 1.480312466621399
Epoch 7, Iteration 146, Loss: 1.6985737085342407
Epoch 7, Iteration 147, Loss: 1.5178812742233276
Epoch 7, Iteration 148, Loss: 1.1991907358169556
Epoch 7, Iteration 149, Loss: 1.423799753189087
Epoch 7, Iteration 150, Loss: 1.620507001876831
Epoch 7, Iteration 150, Valid Loss: 0.860694944858551
Epoch 7, Iteration 151, Loss: 1.539823055267334
Epoch 7, Iteration 152, Loss: 1.4677436351776123
Epoch 7, Iteration 153, Loss: 1.44652259349823
Epoch 7, Iteration 154, Loss: 1.3046598434448242
Epoch 7, Iteration 155, Loss: 1.33828866481781
Epoch 7, Iteration 156, Loss: 1.5588719844818115
Epoch 7, Iteration 157, Loss: 2.223271131515503
Epoch 7, Iteration 158, Loss: 1.515028476715088
Epoch 7, Iteration 159, Loss: 1.6467763185501099
Epoch 7, Iteration 160, Loss: 1.3063749074935913
Epoch 7, Iteration 161, Loss: 1.4073184728622437
Epoch 7, Iteration 162, Loss: 1.8064128160476685
Epoch 7, Iteration 163, Loss: 1.1900001764297485
Epoch 7, Iteration 164, Loss: 1.345703125
Epoch 7, Iteration 165, Loss: 1.2959198951721191
Epoch 7, Iteration 166, Loss: 1.3821094036102295
Epoch 7, Iteration 167, Loss: 1.6840463876724243
Epoch 7, Iteration 168, Loss: 1.5880907773971558
Epoch 7, Iteration 169, Loss: 1.5326006412506104
Epoch 7, Iteration 170, Loss: 1.719942569732666
Epoch 7, Iteration 171, Loss: 1.661402702331543
Epoch 7, Iteration 172, Loss: 1.471421241760254
Epoch 7, Iteration 173, Loss: 1.4497238397598267
Epoch 7, Iteration 174, Loss: 1.528917908668518
Epoch 7, Iteration 175, Loss: 1.7608438730239868
Epoch 7, Iteration 176, Loss: 1.211942434310913
Epoch 7, Iteration 177, Loss: 1.931076169013977
Epoch 7, Iteration 178, Loss: 1.7882360219955444
Epoch 7, Iteration 179, Loss: 1.4212496280670166
Epoch 7, Iteration 180, Loss: 1.4209967851638794
Epoch 7, Iteration 181, Loss: 1.7749449014663696
Epoch 7, Iteration 182, Loss: 1.723534345626831
Epoch 7, Iteration 183, Loss: 1.3300647735595703
Epoch 7, Iteration 184, Loss: 1.4241092205047607
Epoch 7, Iteration 185, Loss: 1.297806739807129
Epoch 7, Iteration 186, Loss: 1.4223074913024902
Epoch 7, Iteration 187, Loss: 1.4744025468826294
Epoch 7, Iteration 188, Loss: 1.367459774017334
Epoch 7, Iteration 189, Loss: 1.4432966709136963
Epoch 7, Iteration 190, Loss: 1.6838912963867188
Epoch 7, Iteration 191, Loss: 1.6596890687942505
Epoch 7, Iteration 192, Loss: 1.5636622905731201
Epoch 7, Iteration 193, Loss: 1.4855843782424927
Epoch 7, Iteration 194, Loss: 1.41678786277771
Epoch 7, Iteration 195, Loss: 1.3968698978424072
Epoch 7, Iteration 196, Loss: 1.5871665477752686
Epoch 7, Iteration 197, Loss: 1.4205816984176636
Epoch 7, Iteration 198, Loss: 1.5193690061569214
Epoch 7, Iteration 199, Loss: 1.5808026790618896
Epoch 7, Iteration 200, Loss: 1.3629164695739746
Epoch 7, Iteration 200, Valid Loss: 0.8607769012451172
Epoch 7, Iteration 201, Loss: 1.3785291910171509
Epoch 7, Iteration 202, Loss: 1.5226616859436035
Epoch 7, Iteration 203, Loss: 1.3506578207015991
Epoch 7, Iteration 204, Loss: 1.392716407775879
Epoch 7, Iteration 205, Loss: 1.1865379810333252
Epoch 7, Iteration 206, Loss: 1.3191202878952026
Epoch 7, Iteration 207, Loss: 1.752992033958435
Epoch 7, Iteration 208, Loss: 2.0793964862823486
Epoch 7, Iteration 209, Loss: 2.217776298522949
Epoch 7, Iteration 210, Loss: 1.900684118270874
Epoch 7, Iteration 211, Loss: 1.985004186630249
Epoch 7, Iteration 212, Loss: 1.7656002044677734
Epoch 7, Iteration 213, Loss: 1.8681000471115112
Epoch 7, Iteration 214, Loss: 1.5166524648666382
Epoch 7, Iteration 215, Loss: 1.3472480773925781
Epoch 7, Iteration 216, Loss: 1.3077459335327148
Epoch 7, Iteration 217, Loss: 1.4729052782058716
Epoch 7, Iteration 218, Loss: 1.5044605731964111
Epoch 7, Iteration 219, Loss: 1.6277565956115723
Epoch 7, Iteration 220, Loss: 1.38688325881958
Epoch 7, Iteration 221, Loss: 1.562766432762146
Epoch 7, Iteration 222, Loss: 1.5341863632202148
Epoch 7, Iteration 223, Loss: 1.4911425113677979
Epoch 7, Iteration 224, Loss: 1.361806035041809
Epoch 7, Iteration 225, Loss: 1.619752287864685
Epoch 7, Iteration 226, Loss: 1.7849459648132324
Epoch 7, Iteration 227, Loss: 1.4326069355010986
Epoch 7, Iteration 228, Loss: 1.4383165836334229
Epoch 7, Iteration 229, Loss: 1.4270853996276855
Epoch 7, Iteration 230, Loss: 1.631999135017395
Epoch 7, Iteration 231, Loss: 1.510425329208374
Epoch 7, Iteration 232, Loss: 1.6339576244354248
Epoch 7, Iteration 233, Loss: 1.9589160680770874
Epoch 7, Iteration 234, Loss: 1.6949964761734009
Epoch 7, Iteration 235, Loss: 1.4066003561019897
Epoch 7, Iteration 236, Loss: 1.6314820051193237
Epoch 7, Iteration 237, Loss: 1.4407709836959839
Epoch 7, Iteration 238, Loss: 1.3717234134674072
Epoch 7, Iteration 239, Loss: 1.7211480140686035
Epoch 7, Iteration 240, Loss: 1.2572238445281982
Epoch 7, Iteration 241, Loss: 1.3879342079162598
Epoch 7, Iteration 242, Loss: 1.433213233947754
Epoch 7, Iteration 243, Loss: 1.499159336090088
Epoch 7, Iteration 244, Loss: 1.3115283250808716
Epoch 7, Iteration 245, Loss: 1.2269222736358643
Epoch 7, Iteration 246, Loss: 1.6066133975982666
Epoch 7, Iteration 247, Loss: 1.425289511680603
Epoch 7, Iteration 248, Loss: 1.4713006019592285
Epoch 7, Iteration 249, Loss: 1.4296848773956299
Epoch 7, Iteration 250, Loss: 1.5912766456604004
Epoch 7, Iteration 250, Valid Loss: 0.8594619035720825
Epoch 7, Iteration 251, Loss: 1.4305832386016846
Epoch 7, Iteration 252, Loss: 1.733934998512268
Epoch 7, Iteration 253, Loss: 1.630379557609558
Epoch 7, Iteration 254, Loss: 1.6707360744476318
Epoch 7, Iteration 255, Loss: 1.640630841255188
Epoch 7, Iteration 256, Loss: 1.721746802330017
Epoch 7, Iteration 257, Loss: 1.552176833152771
Epoch 7, Iteration 258, Loss: 1.7994579076766968
Epoch 7, Iteration 259, Loss: 1.276687741279602
Epoch 7, Iteration 260, Loss: 1.6748861074447632
Epoch 7, Iteration 261, Loss: 1.6760450601577759
Epoch 7, Iteration 262, Loss: 1.3635364770889282
Epoch 7, Iteration 263, Loss: 1.7043724060058594
Epoch 7, Iteration 264, Loss: 1.5922389030456543
Epoch 7, Iteration 265, Loss: 1.5649546384811401
Epoch 7, Iteration 266, Loss: 1.6091581583023071
Epoch 7, Iteration 267, Loss: 1.5414801836013794
Epoch 7, Iteration 268, Loss: 1.4567064046859741
Epoch 7, Iteration 269, Loss: 1.480970025062561
Epoch 7, Iteration 270, Loss: 1.5141252279281616
Epoch 7, Iteration 271, Loss: 1.5396217107772827
Epoch 7, Iteration 272, Loss: 1.340606451034546
Epoch 7, Iteration 273, Loss: 1.430370807647705
Epoch 7, Iteration 274, Loss: 1.3962594270706177
Epoch 7, Iteration 275, Loss: 1.4324934482574463
Epoch 7, Iteration 276, Loss: 1.4026076793670654
Epoch 7, Iteration 277, Loss: 1.4414656162261963
Epoch 7, Iteration 278, Loss: 1.628364086151123
Epoch 7, Iteration 279, Loss: 1.315476655960083
Epoch 7, Iteration 280, Loss: 1.121764063835144
Epoch 7, Iteration 281, Loss: 1.3478070497512817
Epoch 7, Iteration 282, Loss: 1.4567859172821045
Epoch 7, Iteration 283, Loss: 1.482600212097168
Epoch 7, Iteration 284, Loss: 1.5350918769836426
Epoch 7, Iteration 285, Loss: 1.4178235530853271
Epoch 7, Iteration 286, Loss: 1.4640103578567505
Epoch 7, Iteration 287, Loss: 1.48512601852417
Epoch 7, Iteration 288, Loss: 1.2622572183609009
Epoch 7, Iteration 289, Loss: 1.4218385219573975
Epoch 7, Iteration 290, Loss: 1.3407418727874756
Epoch 7, Iteration 291, Loss: 1.1485275030136108
Epoch 7, Iteration 292, Loss: 1.6877105236053467
Epoch 7, Iteration 293, Loss: 1.778075933456421
Epoch 7, Iteration 294, Loss: 1.3780243396759033
Epoch 7, Iteration 295, Loss: 1.3348298072814941
Epoch 7, Iteration 296, Loss: 1.7831591367721558
Epoch 7, Iteration 297, Loss: 1.6726912260055542
Epoch 7, Iteration 298, Loss: 1.4546699523925781
Epoch 7, Iteration 299, Loss: 1.3145053386688232
Epoch 7, Iteration 300, Loss: 1.2116385698318481
Epoch 7, Iteration 300, Valid Loss: 0.8372480869293213
Epoch 7, Iteration 301, Loss: 1.5077885389328003
Epoch 7, Iteration 302, Loss: 1.3403589725494385
Epoch 7, Iteration 303, Loss: 1.757131814956665
Epoch 7, Iteration 304, Loss: 1.400667428970337
Epoch 7, Iteration 305, Loss: 1.2768350839614868
Epoch 7, Iteration 306, Loss: 1.3736684322357178
Epoch 7, Iteration 307, Loss: 1.3842341899871826
Epoch 7, Iteration 308, Loss: 1.4099645614624023
Epoch 7, Iteration 309, Loss: 1.369411587715149
Epoch 7, Iteration 310, Loss: 1.6874823570251465
Epoch 7, Iteration 311, Loss: 1.2992111444473267
Epoch 7, Iteration 312, Loss: 1.2526355981826782
Epoch 7, Iteration 313, Loss: 1.6189556121826172
Epoch 7, Iteration 314, Loss: 1.2962532043457031
Epoch 7, Iteration 315, Loss: 1.2348759174346924
Epoch 7, Iteration 316, Loss: 1.4935319423675537
Epoch 7, Iteration 317, Loss: 1.4165706634521484
Epoch 7, Iteration 318, Loss: 1.4370167255401611
Epoch 7, Iteration 319, Loss: 1.6696996688842773
Epoch 7, Iteration 320, Loss: 1.2322158813476562
Epoch 7, Iteration 321, Loss: 1.4784128665924072
Epoch 7, Iteration 322, Loss: 1.2901453971862793
Epoch 7, Iteration 323, Loss: 1.3501096963882446
Epoch 7, Iteration 324, Loss: 1.4763202667236328
Epoch 7, Iteration 325, Loss: 1.358790397644043
Epoch 7, Iteration 326, Loss: 1.41684889793396
Epoch 7, Iteration 327, Loss: 1.3331732749938965
Epoch 7, Iteration 328, Loss: 1.2312146425247192
Epoch 7, Iteration 329, Loss: 1.5873734951019287
Epoch 7, Iteration 330, Loss: 1.2047247886657715
Epoch 7, Iteration 331, Loss: 1.3280105590820312
Epoch 7, Iteration 332, Loss: 1.5391995906829834
Epoch 7, Iteration 333, Loss: 1.7032504081726074
Epoch 7, Iteration 334, Loss: 1.5606976747512817
Epoch 7, Iteration 335, Loss: 1.6208271980285645
Epoch 7, Iteration 336, Loss: 1.4094313383102417
Epoch 7, Iteration 337, Loss: 1.3146817684173584
Epoch 7, Iteration 338, Loss: 1.3967971801757812
Epoch 7, Iteration 339, Loss: 1.3928180932998657
Epoch 7, Iteration 340, Loss: 1.2907458543777466
Epoch 7, Iteration 341, Loss: 1.379958152770996
Epoch 7, Iteration 342, Loss: 1.2487354278564453
Epoch 7, Iteration 343, Loss: 1.299283504486084
Epoch 7, Iteration 344, Loss: 1.481992244720459
Epoch 7, Iteration 345, Loss: 1.5135042667388916
Epoch 7, Iteration 346, Loss: 1.3945543766021729
Epoch 7, Iteration 347, Loss: 1.2520854473114014
Epoch 7, Iteration 348, Loss: 1.250410795211792
Epoch 7, Iteration 349, Loss: 1.3433477878570557
Epoch 7, Iteration 350, Loss: 1.293210506439209
Epoch 7, Iteration 350, Valid Loss: 0.8525695204734802
Epoch 7, Iteration 351, Loss: 1.648637294769287
Epoch 7, Iteration 352, Loss: 1.2877535820007324
Epoch 7, Iteration 353, Loss: 1.515759825706482
Epoch 7, Iteration 354, Loss: 1.234400987625122
Epoch 7, Iteration 355, Loss: 1.3510807752609253
Epoch 7, Iteration 356, Loss: 1.390845537185669
Epoch 7, Iteration 357, Loss: 1.3133699893951416
Epoch 7, Iteration 358, Loss: 1.3347254991531372
Epoch 7, Iteration 359, Loss: 1.3623448610305786
Epoch 7, Iteration 360, Loss: 1.3388794660568237
Epoch 7, Iteration 361, Loss: 1.3688161373138428
Epoch 7, Iteration 362, Loss: 1.3684812784194946
Epoch 7, Iteration 363, Loss: 1.325979232788086
Epoch 7, Iteration 364, Loss: 1.352188229560852
Epoch 7, Iteration 365, Loss: 1.5552858114242554
Epoch 7, Iteration 366, Loss: 1.5967679023742676
Epoch 7, Iteration 367, Loss: 1.4464483261108398
Epoch 7, Iteration 368, Loss: 1.4300975799560547
Epoch 7, Iteration 369, Loss: 1.49965500831604
Epoch 7, Iteration 370, Loss: 1.4779882431030273
Epoch 7, Iteration 371, Loss: 1.4459489583969116
Epoch 7, Iteration 372, Loss: 1.1498998403549194
Epoch 7, Iteration 373, Loss: 1.2484537363052368
Epoch 7, Iteration 374, Loss: 1.6576699018478394
Epoch 7, Iteration 375, Loss: 1.3904669284820557
Epoch 7, Iteration 376, Loss: 1.477550983428955
Epoch 7, Iteration 377, Loss: 1.4823521375656128
Epoch 7, Iteration 378, Loss: 1.2617837190628052
Epoch 7, Iteration 379, Loss: 1.333933711051941
Epoch 7, Iteration 380, Loss: 1.276319980621338
Epoch 7, Iteration 381, Loss: 1.2719659805297852
Epoch 7, Iteration 382, Loss: 1.0900455713272095
Epoch 7, Iteration 383, Loss: 1.3700652122497559
Epoch 7, Iteration 384, Loss: 1.3247740268707275
Epoch 7, Iteration 385, Loss: 1.390204668045044
Epoch 7, Iteration 386, Loss: 1.3264415264129639
Epoch 7, Iteration 387, Loss: 1.6846771240234375
Epoch 7, Iteration 388, Loss: 1.302365779876709
Epoch 7, Iteration 389, Loss: 1.4911448955535889
Epoch 7, Iteration 390, Loss: 1.5014781951904297
Epoch 7, Iteration 391, Loss: 1.366663932800293
Epoch 8/10, Loss: 1.4948714831653906
Epoch 8, Iteration 0, Loss: 1.4393656253814697
Epoch 8, Iteration 1, Loss: 1.749307632446289
Epoch 8, Iteration 2, Loss: 1.619559407234192
Epoch 8, Iteration 3, Loss: 1.6098167896270752
Epoch 8, Iteration 4, Loss: 1.445092797279358
Epoch 8, Iteration 5, Loss: 1.6445002555847168
Epoch 8, Iteration 6, Loss: 1.3997108936309814
Epoch 8, Iteration 7, Loss: 1.4300384521484375
Epoch 8, Iteration 8, Loss: 1.4215648174285889
Epoch 8, Iteration 9, Loss: 1.7518935203552246
Epoch 8, Iteration 10, Loss: 1.5694869756698608
Epoch 8, Iteration 11, Loss: 1.3507561683654785
Epoch 8, Iteration 12, Loss: 1.3842344284057617
Epoch 8, Iteration 13, Loss: 1.6089829206466675
Epoch 8, Iteration 14, Loss: 1.4212747812271118
Epoch 8, Iteration 15, Loss: 1.5174354314804077
Epoch 8, Iteration 16, Loss: 1.4856479167938232
Epoch 8, Iteration 17, Loss: 1.6202976703643799
Epoch 8, Iteration 18, Loss: 1.5395195484161377
Epoch 8, Iteration 19, Loss: 1.587661862373352
Epoch 8, Iteration 20, Loss: 1.390952706336975
Epoch 8, Iteration 21, Loss: 1.4542813301086426
Epoch 8, Iteration 22, Loss: 1.4128124713897705
Epoch 8, Iteration 23, Loss: 1.4754559993743896
Epoch 8, Iteration 24, Loss: 1.3225899934768677
Epoch 8, Iteration 25, Loss: 1.2508559226989746
Epoch 8, Iteration 26, Loss: 1.735918402671814
Epoch 8, Iteration 27, Loss: 1.4011512994766235
Epoch 8, Iteration 28, Loss: 1.4844341278076172
Epoch 8, Iteration 29, Loss: 1.7016100883483887
Epoch 8, Iteration 30, Loss: 1.4806609153747559
Epoch 8, Iteration 31, Loss: 1.5237555503845215
Epoch 8, Iteration 32, Loss: 1.4608161449432373
Epoch 8, Iteration 33, Loss: 1.4671906232833862
Epoch 8, Iteration 34, Loss: 1.312432050704956
Epoch 8, Iteration 35, Loss: 1.5441200733184814
Epoch 8, Iteration 36, Loss: 1.2542717456817627
Epoch 8, Iteration 37, Loss: 1.3586506843566895
Epoch 8, Iteration 38, Loss: 1.4289895296096802
Epoch 8, Iteration 39, Loss: 1.377084732055664
Epoch 8, Iteration 40, Loss: 1.4122065305709839
Epoch 8, Iteration 41, Loss: 1.5219085216522217
Epoch 8, Iteration 42, Loss: 1.503842830657959
Epoch 8, Iteration 43, Loss: 1.3867956399917603
Epoch 8, Iteration 44, Loss: 1.4417328834533691
Epoch 8, Iteration 45, Loss: 1.527442216873169
Epoch 8, Iteration 46, Loss: 1.4141278266906738
Epoch 8, Iteration 47, Loss: 1.4786288738250732
Epoch 8, Iteration 48, Loss: 1.70689058303833
Epoch 8, Iteration 49, Loss: 1.4417004585266113
Epoch 8, Iteration 50, Loss: 1.3697521686553955
Epoch 8, Iteration 50, Valid Loss: 0.8142073154449463
Epoch 8, Iteration 51, Loss: 1.327920913696289
Epoch 8, Iteration 52, Loss: 1.3846054077148438
Epoch 8, Iteration 53, Loss: 1.4683520793914795
Epoch 8, Iteration 54, Loss: 1.905142903327942
Epoch 8, Iteration 55, Loss: 1.559916377067566
Epoch 8, Iteration 56, Loss: 1.4018763303756714
Epoch 8, Iteration 57, Loss: 1.703188419342041
Epoch 8, Iteration 58, Loss: 1.3453912734985352
Epoch 8, Iteration 59, Loss: 1.4615615606307983
Epoch 8, Iteration 60, Loss: 1.4819310903549194
Epoch 8, Iteration 61, Loss: 1.2399073839187622
Epoch 8, Iteration 62, Loss: 1.4502267837524414
Epoch 8, Iteration 63, Loss: 1.190110683441162
Epoch 8, Iteration 64, Loss: 1.428169846534729
Epoch 8, Iteration 65, Loss: 1.52140212059021
Epoch 8, Iteration 66, Loss: 1.489134430885315
Epoch 8, Iteration 67, Loss: 1.1362155675888062
Epoch 8, Iteration 68, Loss: 1.7221672534942627
Epoch 8, Iteration 69, Loss: 1.6757407188415527
Epoch 8, Iteration 70, Loss: 1.642999529838562
Epoch 8, Iteration 71, Loss: 1.3782751560211182
Epoch 8, Iteration 72, Loss: 1.5857903957366943
Epoch 8, Iteration 73, Loss: 1.2198508977890015
Epoch 8, Iteration 74, Loss: 1.5751879215240479
Epoch 8, Iteration 75, Loss: 1.506572961807251
Epoch 8, Iteration 76, Loss: 1.3940833806991577
Epoch 8, Iteration 77, Loss: 1.250809907913208
Epoch 8, Iteration 78, Loss: 1.683335781097412
Epoch 8, Iteration 79, Loss: 1.764510154724121
Epoch 8, Iteration 80, Loss: 1.2985246181488037
Epoch 8, Iteration 81, Loss: 1.7511258125305176
Epoch 8, Iteration 82, Loss: 1.4603161811828613
Epoch 8, Iteration 83, Loss: 1.2765015363693237
Epoch 8, Iteration 84, Loss: 1.6057634353637695
Epoch 8, Iteration 85, Loss: 1.4768340587615967
Epoch 8, Iteration 86, Loss: 1.4635337591171265
Epoch 8, Iteration 87, Loss: 1.4716765880584717
Epoch 8, Iteration 88, Loss: 1.5998427867889404
Epoch 8, Iteration 89, Loss: 1.4176008701324463
Epoch 8, Iteration 90, Loss: 1.124244213104248
Epoch 8, Iteration 91, Loss: 1.5466810464859009
Epoch 8, Iteration 92, Loss: 1.1251397132873535
Epoch 8, Iteration 93, Loss: 1.5199849605560303
Epoch 8, Iteration 94, Loss: 1.712134838104248
Epoch 8, Iteration 95, Loss: 1.3028590679168701
Epoch 8, Iteration 96, Loss: 1.4864552021026611
Epoch 8, Iteration 97, Loss: 1.285569190979004
Epoch 8, Iteration 98, Loss: 1.3698313236236572
Epoch 8, Iteration 99, Loss: 1.3162766695022583
Epoch 8, Iteration 100, Loss: 1.255401372909546
Epoch 8, Iteration 100, Valid Loss: 0.8328921794891357
Epoch 8, Iteration 101, Loss: 1.9012844562530518
Epoch 8, Iteration 102, Loss: 1.4711846113204956
Epoch 8, Iteration 103, Loss: 1.54902982711792
Epoch 8, Iteration 104, Loss: 1.6159932613372803
Epoch 8, Iteration 105, Loss: 1.4966495037078857
Epoch 8, Iteration 106, Loss: 1.4268882274627686
Epoch 8, Iteration 107, Loss: 2.1595935821533203
Epoch 8, Iteration 108, Loss: 1.4506843090057373
Epoch 8, Iteration 109, Loss: 1.6761221885681152
Epoch 8, Iteration 110, Loss: 1.7237591743469238
Epoch 8, Iteration 111, Loss: 1.6180825233459473
Epoch 8, Iteration 112, Loss: 1.5635175704956055
Epoch 8, Iteration 113, Loss: 1.3622355461120605
Epoch 8, Iteration 114, Loss: 1.7846330404281616
Epoch 8, Iteration 115, Loss: 1.729926586151123
Epoch 8, Iteration 116, Loss: 1.4229888916015625
Epoch 8, Iteration 117, Loss: 1.119889259338379
Epoch 8, Iteration 118, Loss: 1.4335529804229736
Epoch 8, Iteration 119, Loss: 1.5178775787353516
Epoch 8, Iteration 120, Loss: 1.423566460609436
Epoch 8, Iteration 121, Loss: 1.3452779054641724
Epoch 8, Iteration 122, Loss: 1.3426618576049805
Epoch 8, Iteration 123, Loss: 1.3955308198928833
Epoch 8, Iteration 124, Loss: 1.3595011234283447
Epoch 8, Iteration 125, Loss: 1.2509636878967285
Epoch 8, Iteration 126, Loss: 1.2911298274993896
Epoch 8, Iteration 127, Loss: 1.4781854152679443
Epoch 8, Iteration 128, Loss: 1.6847320795059204
Epoch 8, Iteration 129, Loss: 1.6362817287445068
Epoch 8, Iteration 130, Loss: 1.9033818244934082
Epoch 8, Iteration 131, Loss: 1.2914756536483765
Epoch 8, Iteration 132, Loss: 1.3270022869110107
Epoch 8, Iteration 133, Loss: 1.7974798679351807
Epoch 8, Iteration 134, Loss: 1.2722746133804321
Epoch 8, Iteration 135, Loss: 1.3472386598587036
Epoch 8, Iteration 136, Loss: 1.3197059631347656
Epoch 8, Iteration 137, Loss: 1.3179067373275757
Epoch 8, Iteration 138, Loss: 1.342419981956482
Epoch 8, Iteration 139, Loss: 1.3281686305999756
Epoch 8, Iteration 140, Loss: 1.339903712272644
Epoch 8, Iteration 141, Loss: 1.7370318174362183
Epoch 8, Iteration 142, Loss: 1.3032565116882324
Epoch 8, Iteration 143, Loss: 1.492177963256836
Epoch 8, Iteration 144, Loss: 1.4045606851577759
Epoch 8, Iteration 145, Loss: 1.5269674062728882
Epoch 8, Iteration 146, Loss: 1.5436789989471436
Epoch 8, Iteration 147, Loss: 1.4678387641906738
Epoch 8, Iteration 148, Loss: 1.1284492015838623
Epoch 8, Iteration 149, Loss: 1.3140754699707031
Epoch 8, Iteration 150, Loss: 1.420262336730957
Epoch 8, Iteration 150, Valid Loss: 0.8365446925163269
Epoch 8, Iteration 151, Loss: 1.5658708810806274
Epoch 8, Iteration 152, Loss: 1.3772087097167969
Epoch 8, Iteration 153, Loss: 1.6380558013916016
Epoch 8, Iteration 154, Loss: 1.1398038864135742
Epoch 8, Iteration 155, Loss: 1.3853527307510376
Epoch 8, Iteration 156, Loss: 1.3150633573532104
Epoch 8, Iteration 157, Loss: 1.999753475189209
Epoch 8, Iteration 158, Loss: 1.502272367477417
Epoch 8, Iteration 159, Loss: 1.5365874767303467
Epoch 8, Iteration 160, Loss: 1.4063152074813843
Epoch 8, Iteration 161, Loss: 1.473973035812378
Epoch 8, Iteration 162, Loss: 1.760183334350586
Epoch 8, Iteration 163, Loss: 1.1503713130950928
Epoch 8, Iteration 164, Loss: 1.4243618249893188
Epoch 8, Iteration 165, Loss: 1.3183990716934204
Epoch 8, Iteration 166, Loss: 2.1079976558685303
Epoch 8, Iteration 167, Loss: 1.6974807977676392
Epoch 8, Iteration 168, Loss: 1.5609649419784546
Epoch 8, Iteration 169, Loss: 1.3858206272125244
Epoch 8, Iteration 170, Loss: 1.716569185256958
Epoch 8, Iteration 171, Loss: 1.4928340911865234
Epoch 8, Iteration 172, Loss: 1.4374504089355469
Epoch 8, Iteration 173, Loss: 1.3223860263824463
Epoch 8, Iteration 174, Loss: 1.5515375137329102
Epoch 8, Iteration 175, Loss: 1.6635844707489014
Epoch 8, Iteration 176, Loss: 1.185982346534729
Epoch 8, Iteration 177, Loss: 1.9051772356033325
Epoch 8, Iteration 178, Loss: 1.6359890699386597
Epoch 8, Iteration 179, Loss: 1.4315474033355713
Epoch 8, Iteration 180, Loss: 1.4180126190185547
Epoch 8, Iteration 181, Loss: 1.6520657539367676
Epoch 8, Iteration 182, Loss: 1.589085578918457
Epoch 8, Iteration 183, Loss: 1.2963566780090332
Epoch 8, Iteration 184, Loss: 1.3298134803771973
Epoch 8, Iteration 185, Loss: 1.235674262046814
Epoch 8, Iteration 186, Loss: 1.3702569007873535
Epoch 8, Iteration 187, Loss: 1.3678654432296753
Epoch 8, Iteration 188, Loss: 1.33292818069458
Epoch 8, Iteration 189, Loss: 1.3165028095245361
Epoch 8, Iteration 190, Loss: 1.6609970331192017
Epoch 8, Iteration 191, Loss: 1.4856085777282715
Epoch 8, Iteration 192, Loss: 1.510599136352539
Epoch 8, Iteration 193, Loss: 1.4616944789886475
Epoch 8, Iteration 194, Loss: 1.4290618896484375
Epoch 8, Iteration 195, Loss: 1.382369875907898
Epoch 8, Iteration 196, Loss: 1.598785400390625
Epoch 8, Iteration 197, Loss: 1.365872859954834
Epoch 8, Iteration 198, Loss: 1.5219300985336304
Epoch 8, Iteration 199, Loss: 1.4938350915908813
Epoch 8, Iteration 200, Loss: 1.356231927871704
Epoch 8, Iteration 200, Valid Loss: 0.8136895298957825
Epoch 8, Iteration 201, Loss: 1.380087971687317
Epoch 8, Iteration 202, Loss: 1.5078399181365967
Epoch 8, Iteration 203, Loss: 1.2740119695663452
Epoch 8, Iteration 204, Loss: 1.5201716423034668
Epoch 8, Iteration 205, Loss: 1.1465911865234375
Epoch 8, Iteration 206, Loss: 1.3958700895309448
Epoch 8, Iteration 207, Loss: 1.6810295581817627
Epoch 8, Iteration 208, Loss: 1.976500153541565
Epoch 8, Iteration 209, Loss: 2.1842174530029297
Epoch 8, Iteration 210, Loss: 1.8045377731323242
Epoch 8, Iteration 211, Loss: 1.9806667566299438
Epoch 8, Iteration 212, Loss: 1.669787049293518
Epoch 8, Iteration 213, Loss: 1.7943029403686523
Epoch 8, Iteration 214, Loss: 1.5752582550048828
Epoch 8, Iteration 215, Loss: 1.281163215637207
Epoch 8, Iteration 216, Loss: 1.3518118858337402
Epoch 8, Iteration 217, Loss: 1.4186139106750488
Epoch 8, Iteration 218, Loss: 1.3963794708251953
Epoch 8, Iteration 219, Loss: 1.5260313749313354
Epoch 8, Iteration 220, Loss: 1.364668369293213
Epoch 8, Iteration 221, Loss: 1.369138240814209
Epoch 8, Iteration 222, Loss: 1.4474046230316162
Epoch 8, Iteration 223, Loss: 1.3310123682022095
Epoch 8, Iteration 224, Loss: 1.3062105178833008
Epoch 8, Iteration 225, Loss: 1.706632375717163
Epoch 8, Iteration 226, Loss: 1.6176680326461792
Epoch 8, Iteration 227, Loss: 1.6180717945098877
Epoch 8, Iteration 228, Loss: 1.3636350631713867
Epoch 8, Iteration 229, Loss: 1.418032169342041
Epoch 8, Iteration 230, Loss: 1.4355151653289795
Epoch 8, Iteration 231, Loss: 1.5273534059524536
Epoch 8, Iteration 232, Loss: 1.5685410499572754
Epoch 8, Iteration 233, Loss: 1.8433127403259277
Epoch 8, Iteration 234, Loss: 1.7894924879074097
Epoch 8, Iteration 235, Loss: 1.2781779766082764
Epoch 8, Iteration 236, Loss: 1.6115330457687378
Epoch 8, Iteration 237, Loss: 1.3914077281951904
Epoch 8, Iteration 238, Loss: 1.2664414644241333
Epoch 8, Iteration 239, Loss: 1.7323219776153564
Epoch 8, Iteration 240, Loss: 1.2715274095535278
Epoch 8, Iteration 241, Loss: 1.3328114748001099
Epoch 8, Iteration 242, Loss: 1.3214268684387207
Epoch 8, Iteration 243, Loss: 1.4016236066818237
Epoch 8, Iteration 244, Loss: 1.3166565895080566
Epoch 8, Iteration 245, Loss: 1.1092288494110107
Epoch 8, Iteration 246, Loss: 1.5040010213851929
Epoch 8, Iteration 247, Loss: 1.3571982383728027
Epoch 8, Iteration 248, Loss: 1.4038856029510498
Epoch 8, Iteration 249, Loss: 1.3785985708236694
Epoch 8, Iteration 250, Loss: 1.5818884372711182
Epoch 8, Iteration 250, Valid Loss: 0.818796694278717
Epoch 8, Iteration 251, Loss: 1.3336812257766724
Epoch 8, Iteration 252, Loss: 1.5830342769622803
Epoch 8, Iteration 253, Loss: 1.609312653541565
Epoch 8, Iteration 254, Loss: 1.565932273864746
Epoch 8, Iteration 255, Loss: 1.5759886503219604
Epoch 8, Iteration 256, Loss: 1.6925079822540283
Epoch 8, Iteration 257, Loss: 1.3768601417541504
Epoch 8, Iteration 258, Loss: 1.6672887802124023
Epoch 8, Iteration 259, Loss: 1.191219687461853
Epoch 8, Iteration 260, Loss: 1.7063173055648804
Epoch 8, Iteration 261, Loss: 1.4853249788284302
Epoch 8, Iteration 262, Loss: 1.3375139236450195
Epoch 8, Iteration 263, Loss: 1.7756961584091187
Epoch 8, Iteration 264, Loss: 1.4887639284133911
Epoch 8, Iteration 265, Loss: 1.4984776973724365
Epoch 8, Iteration 266, Loss: 1.574129581451416
Epoch 8, Iteration 267, Loss: 1.4836547374725342
Epoch 8, Iteration 268, Loss: 1.3672380447387695
Epoch 8, Iteration 269, Loss: 1.3942408561706543
Epoch 8, Iteration 270, Loss: 1.4789018630981445
Epoch 8, Iteration 271, Loss: 1.7812353372573853
Epoch 8, Iteration 272, Loss: 1.3330246210098267
Epoch 8, Iteration 273, Loss: 1.33529794216156
Epoch 8, Iteration 274, Loss: 1.3749226331710815
Epoch 8, Iteration 275, Loss: 1.353347659111023
Epoch 8, Iteration 276, Loss: 1.329105019569397
Epoch 8, Iteration 277, Loss: 1.302438497543335
Epoch 8, Iteration 278, Loss: 1.6135163307189941
Epoch 8, Iteration 279, Loss: 1.2673285007476807
Epoch 8, Iteration 280, Loss: 1.081084966659546
Epoch 8, Iteration 281, Loss: 1.2946090698242188
Epoch 8, Iteration 282, Loss: 1.384164571762085
Epoch 8, Iteration 283, Loss: 1.409653902053833
Epoch 8, Iteration 284, Loss: 1.5127785205841064
Epoch 8, Iteration 285, Loss: 1.3284696340560913
Epoch 8, Iteration 286, Loss: 1.2922934293746948
Epoch 8, Iteration 287, Loss: 1.5543532371520996
Epoch 8, Iteration 288, Loss: 1.2384096384048462
Epoch 8, Iteration 289, Loss: 1.2936992645263672
Epoch 8, Iteration 290, Loss: 1.2740414142608643
Epoch 8, Iteration 291, Loss: 1.2369639873504639
Epoch 8, Iteration 292, Loss: 1.7208929061889648
Epoch 8, Iteration 293, Loss: 1.7672300338745117
Epoch 8, Iteration 294, Loss: 1.3467220067977905
Epoch 8, Iteration 295, Loss: 1.3362317085266113
Epoch 8, Iteration 296, Loss: 1.6673945188522339
Epoch 8, Iteration 297, Loss: 1.6823673248291016
Epoch 8, Iteration 298, Loss: 1.4619226455688477
Epoch 8, Iteration 299, Loss: 1.3258981704711914
Epoch 8, Iteration 300, Loss: 1.215512990951538
Epoch 8, Iteration 300, Valid Loss: 0.8014295697212219
Epoch 8, Iteration 301, Loss: 1.37364661693573
Epoch 8, Iteration 302, Loss: 1.2319819927215576
Epoch 8, Iteration 303, Loss: 1.6969226598739624
Epoch 8, Iteration 304, Loss: 1.273331642150879
Epoch 8, Iteration 305, Loss: 1.2675058841705322
Epoch 8, Iteration 306, Loss: 1.358703851699829
Epoch 8, Iteration 307, Loss: 1.3040680885314941
Epoch 8, Iteration 308, Loss: 1.228568434715271
Epoch 8, Iteration 309, Loss: 1.3408762216567993
Epoch 8, Iteration 310, Loss: 1.6419155597686768
Epoch 8, Iteration 311, Loss: 1.2638887166976929
Epoch 8, Iteration 312, Loss: 1.2610951662063599
Epoch 8, Iteration 313, Loss: 1.657314419746399
Epoch 8, Iteration 314, Loss: 1.3921769857406616
Epoch 8, Iteration 315, Loss: 1.2208983898162842
Epoch 8, Iteration 316, Loss: 1.5785413980484009
Epoch 8, Iteration 317, Loss: 1.5096721649169922
Epoch 8, Iteration 318, Loss: 1.228377103805542
Epoch 8, Iteration 319, Loss: 1.5814241170883179
Epoch 8, Iteration 320, Loss: 1.2059462070465088
Epoch 8, Iteration 321, Loss: 1.4600269794464111
Epoch 8, Iteration 322, Loss: 1.3228093385696411
Epoch 8, Iteration 323, Loss: 1.3948911428451538
Epoch 8, Iteration 324, Loss: 1.418845534324646
Epoch 8, Iteration 325, Loss: 1.3993473052978516
Epoch 8, Iteration 326, Loss: 1.333662509918213
Epoch 8, Iteration 327, Loss: 1.2963999509811401
Epoch 8, Iteration 328, Loss: 1.1970399618148804
Epoch 8, Iteration 329, Loss: 1.5907562971115112
Epoch 8, Iteration 330, Loss: 1.1401777267456055
Epoch 8, Iteration 331, Loss: 1.2814931869506836
Epoch 8, Iteration 332, Loss: 1.3689743280410767
Epoch 8, Iteration 333, Loss: 1.552196979522705
Epoch 8, Iteration 334, Loss: 1.2402868270874023
Epoch 8, Iteration 335, Loss: 1.6287086009979248
Epoch 8, Iteration 336, Loss: 1.4322322607040405
Epoch 8, Iteration 337, Loss: 1.2011687755584717
Epoch 8, Iteration 338, Loss: 1.5510497093200684
Epoch 8, Iteration 339, Loss: 1.475487470626831
Epoch 8, Iteration 340, Loss: 1.2206122875213623
Epoch 8, Iteration 341, Loss: 1.4118802547454834
Epoch 8, Iteration 342, Loss: 1.2807338237762451
Epoch 8, Iteration 343, Loss: 1.3128540515899658
Epoch 8, Iteration 344, Loss: 1.4593578577041626
Epoch 8, Iteration 345, Loss: 1.4253696203231812
Epoch 8, Iteration 346, Loss: 1.309037446975708
Epoch 8, Iteration 347, Loss: 1.2492514848709106
Epoch 8, Iteration 348, Loss: 1.234269618988037
Epoch 8, Iteration 349, Loss: 1.3828213214874268
Epoch 8, Iteration 350, Loss: 1.3575525283813477
Epoch 8, Iteration 350, Valid Loss: 0.830708920955658
Epoch 8, Iteration 351, Loss: 1.511652946472168
Epoch 8, Iteration 352, Loss: 1.3057756423950195
Epoch 8, Iteration 353, Loss: 1.429250717163086
Epoch 8, Iteration 354, Loss: 1.3560984134674072
Epoch 8, Iteration 355, Loss: 1.2704479694366455
Epoch 8, Iteration 356, Loss: 1.2711501121520996
Epoch 8, Iteration 357, Loss: 1.2344800233840942
Epoch 8, Iteration 358, Loss: 1.3544447422027588
Epoch 8, Iteration 359, Loss: 1.3995583057403564
Epoch 8, Iteration 360, Loss: 1.201523780822754
Epoch 8, Iteration 361, Loss: 1.1661036014556885
Epoch 8, Iteration 362, Loss: 1.2919906377792358
Epoch 8, Iteration 363, Loss: 1.2997891902923584
Epoch 8, Iteration 364, Loss: 1.5171459913253784
Epoch 8, Iteration 365, Loss: 1.5732061862945557
Epoch 8, Iteration 366, Loss: 1.5418556928634644
Epoch 8, Iteration 367, Loss: 1.4914216995239258
Epoch 8, Iteration 368, Loss: 1.348759651184082
Epoch 8, Iteration 369, Loss: 1.525402545928955
Epoch 8, Iteration 370, Loss: 1.4487409591674805
Epoch 8, Iteration 371, Loss: 1.395687460899353
Epoch 8, Iteration 372, Loss: 1.2660819292068481
Epoch 8, Iteration 373, Loss: 1.22538161277771
Epoch 8, Iteration 374, Loss: 1.5359010696411133
Epoch 8, Iteration 375, Loss: 1.405929684638977
Epoch 8, Iteration 376, Loss: 1.5663930177688599
Epoch 8, Iteration 377, Loss: 1.3881818056106567
Epoch 8, Iteration 378, Loss: 1.2151463031768799
Epoch 8, Iteration 379, Loss: 1.2234786748886108
Epoch 8, Iteration 380, Loss: 1.2484883069992065
Epoch 8, Iteration 381, Loss: 1.2789976596832275
Epoch 8, Iteration 382, Loss: 1.110854983329773
Epoch 8, Iteration 383, Loss: 1.2658593654632568
Epoch 8, Iteration 384, Loss: 1.3611443042755127
Epoch 8, Iteration 385, Loss: 1.2153513431549072
Epoch 8, Iteration 386, Loss: 1.197295069694519
Epoch 8, Iteration 387, Loss: 1.7118903398513794
Epoch 8, Iteration 388, Loss: 1.2375954389572144
Epoch 8, Iteration 389, Loss: 1.370826005935669
Epoch 8, Iteration 390, Loss: 1.6258788108825684
Epoch 8, Iteration 391, Loss: 1.267673373222351
Epoch 9/10, Loss: 1.4493662754491883
Epoch 9, Iteration 0, Loss: 1.4190019369125366
Epoch 9, Iteration 1, Loss: 1.6678229570388794
Epoch 9, Iteration 2, Loss: 1.5624536275863647
Epoch 9, Iteration 3, Loss: 1.6376588344573975
Epoch 9, Iteration 4, Loss: 1.3865503072738647
Epoch 9, Iteration 5, Loss: 1.501921534538269
Epoch 9, Iteration 6, Loss: 1.3373445272445679
Epoch 9, Iteration 7, Loss: 1.3903498649597168
Epoch 9, Iteration 8, Loss: 1.3458861112594604
Epoch 9, Iteration 9, Loss: 1.6689820289611816
Epoch 9, Iteration 10, Loss: 1.4673335552215576
Epoch 9, Iteration 11, Loss: 1.274507761001587
Epoch 9, Iteration 12, Loss: 1.3109683990478516
Epoch 9, Iteration 13, Loss: 1.5159189701080322
Epoch 9, Iteration 14, Loss: 1.3909316062927246
Epoch 9, Iteration 15, Loss: 1.4832422733306885
Epoch 9, Iteration 16, Loss: 1.2988488674163818
Epoch 9, Iteration 17, Loss: 1.58353853225708
Epoch 9, Iteration 18, Loss: 1.4418280124664307
Epoch 9, Iteration 19, Loss: 1.5768640041351318
Epoch 9, Iteration 20, Loss: 1.393585443496704
Epoch 9, Iteration 21, Loss: 1.4125055074691772
Epoch 9, Iteration 22, Loss: 1.4289298057556152
Epoch 9, Iteration 23, Loss: 1.5695505142211914
Epoch 9, Iteration 24, Loss: 1.262903094291687
Epoch 9, Iteration 25, Loss: 1.1742537021636963
Epoch 9, Iteration 26, Loss: 1.7576768398284912
Epoch 9, Iteration 27, Loss: 1.3321698904037476
Epoch 9, Iteration 28, Loss: 1.4999444484710693
Epoch 9, Iteration 29, Loss: 1.6928555965423584
Epoch 9, Iteration 30, Loss: 1.46009361743927
Epoch 9, Iteration 31, Loss: 1.4277325868606567
Epoch 9, Iteration 32, Loss: 1.3933098316192627
Epoch 9, Iteration 33, Loss: 1.3823776245117188
Epoch 9, Iteration 34, Loss: 1.3488729000091553
Epoch 9, Iteration 35, Loss: 1.5763962268829346
Epoch 9, Iteration 36, Loss: 1.1922333240509033
Epoch 9, Iteration 37, Loss: 1.2769726514816284
Epoch 9, Iteration 38, Loss: 1.2868902683258057
Epoch 9, Iteration 39, Loss: 1.3184149265289307
Epoch 9, Iteration 40, Loss: 1.3710565567016602
Epoch 9, Iteration 41, Loss: 1.5256539583206177
Epoch 9, Iteration 42, Loss: 1.3979108333587646
Epoch 9, Iteration 43, Loss: 1.411049485206604
Epoch 9, Iteration 44, Loss: 1.3796111345291138
Epoch 9, Iteration 45, Loss: 1.3918449878692627
Epoch 9, Iteration 46, Loss: 1.4331579208374023
Epoch 9, Iteration 47, Loss: 1.5099258422851562
Epoch 9, Iteration 48, Loss: 1.5577951669692993
Epoch 9, Iteration 49, Loss: 1.3896297216415405
Epoch 9, Iteration 50, Loss: 1.3005259037017822
Epoch 9, Iteration 50, Valid Loss: 0.7849835753440857
Epoch 9, Iteration 51, Loss: 1.4431955814361572
Epoch 9, Iteration 52, Loss: 1.2651309967041016
Epoch 9, Iteration 53, Loss: 1.4650492668151855
Epoch 9, Iteration 54, Loss: 1.9141803979873657
Epoch 9, Iteration 55, Loss: 1.5673633813858032
Epoch 9, Iteration 56, Loss: 1.462011694908142
Epoch 9, Iteration 57, Loss: 1.5916647911071777
Epoch 9, Iteration 58, Loss: 1.2627547979354858
Epoch 9, Iteration 59, Loss: 1.2665127515792847
Epoch 9, Iteration 60, Loss: 1.424279808998108
Epoch 9, Iteration 61, Loss: 1.2091315984725952
Epoch 9, Iteration 62, Loss: 1.3397941589355469
Epoch 9, Iteration 63, Loss: 1.142366886138916
Epoch 9, Iteration 64, Loss: 1.395816683769226
Epoch 9, Iteration 65, Loss: 1.7463512420654297
Epoch 9, Iteration 66, Loss: 1.4525737762451172
Epoch 9, Iteration 67, Loss: 1.066644549369812
Epoch 9, Iteration 68, Loss: 1.8254789113998413
Epoch 9, Iteration 69, Loss: 1.5741422176361084
Epoch 9, Iteration 70, Loss: 1.495194911956787
Epoch 9, Iteration 71, Loss: 1.4407414197921753
Epoch 9, Iteration 72, Loss: 1.4747889041900635
Epoch 9, Iteration 73, Loss: 1.2637864351272583
Epoch 9, Iteration 74, Loss: 1.654082179069519
Epoch 9, Iteration 75, Loss: 1.331043004989624
Epoch 9, Iteration 76, Loss: 1.3922600746154785
Epoch 9, Iteration 77, Loss: 2.6368472576141357
Epoch 9, Iteration 78, Loss: 1.689424753189087
Epoch 9, Iteration 79, Loss: 1.6141750812530518
Epoch 9, Iteration 80, Loss: 1.1870262622833252
Epoch 9, Iteration 81, Loss: 1.652380347251892
Epoch 9, Iteration 82, Loss: 1.4608736038208008
Epoch 9, Iteration 83, Loss: 1.2275687456130981
Epoch 9, Iteration 84, Loss: 1.55076265335083
Epoch 9, Iteration 85, Loss: 1.3575410842895508
Epoch 9, Iteration 86, Loss: 1.5002729892730713
Epoch 9, Iteration 87, Loss: 1.3782978057861328
Epoch 9, Iteration 88, Loss: 1.46652352809906
Epoch 9, Iteration 89, Loss: 1.4991540908813477
Epoch 9, Iteration 90, Loss: 1.0839512348175049
Epoch 9, Iteration 91, Loss: 1.2965677976608276
Epoch 9, Iteration 92, Loss: 1.1759670972824097
Epoch 9, Iteration 93, Loss: 1.388593316078186
Epoch 9, Iteration 94, Loss: 1.8111412525177002
Epoch 9, Iteration 95, Loss: 1.1998732089996338
Epoch 9, Iteration 96, Loss: 1.4266644716262817
Epoch 9, Iteration 97, Loss: 1.2327173948287964
Epoch 9, Iteration 98, Loss: 1.5148429870605469
Epoch 9, Iteration 99, Loss: 1.3451381921768188
Epoch 9, Iteration 100, Loss: 1.3132104873657227
Epoch 9, Iteration 100, Valid Loss: 0.8280003666877747
Epoch 9, Iteration 101, Loss: 1.8089020252227783
Epoch 9, Iteration 102, Loss: 1.3842121362686157
Epoch 9, Iteration 103, Loss: 1.3909515142440796
Epoch 9, Iteration 104, Loss: 1.9482202529907227
Epoch 9, Iteration 105, Loss: 1.5218716859817505
Epoch 9, Iteration 106, Loss: 1.3924609422683716
Epoch 9, Iteration 107, Loss: 1.5056670904159546
Epoch 9, Iteration 108, Loss: 1.3861420154571533
Epoch 9, Iteration 109, Loss: 1.7374829053878784
Epoch 9, Iteration 110, Loss: 1.6782736778259277
Epoch 9, Iteration 111, Loss: 1.6536738872528076
Epoch 9, Iteration 112, Loss: 1.4890046119689941
Epoch 9, Iteration 113, Loss: 1.2781367301940918
Epoch 9, Iteration 114, Loss: 1.7813427448272705
Epoch 9, Iteration 115, Loss: 1.6448886394500732
Epoch 9, Iteration 116, Loss: 1.3103582859039307
Epoch 9, Iteration 117, Loss: 1.0626662969589233
Epoch 9, Iteration 118, Loss: 1.4866437911987305
Epoch 9, Iteration 119, Loss: 1.3932693004608154
Epoch 9, Iteration 120, Loss: 1.3984849452972412
Epoch 9, Iteration 121, Loss: 1.278580904006958
Epoch 9, Iteration 122, Loss: 1.632873773574829
Epoch 9, Iteration 123, Loss: 1.3822617530822754
Epoch 9, Iteration 124, Loss: 1.3198797702789307
Epoch 9, Iteration 125, Loss: 1.2305052280426025
Epoch 9, Iteration 126, Loss: 1.1562451124191284
Epoch 9, Iteration 127, Loss: 1.5675194263458252
Epoch 9, Iteration 128, Loss: 1.450805425643921
Epoch 9, Iteration 129, Loss: 1.6761071681976318
Epoch 9, Iteration 130, Loss: 1.8835705518722534
Epoch 9, Iteration 131, Loss: 1.2749435901641846
Epoch 9, Iteration 132, Loss: 1.3326234817504883
Epoch 9, Iteration 133, Loss: 1.7904927730560303
Epoch 9, Iteration 134, Loss: 1.2449692487716675
Epoch 9, Iteration 135, Loss: 1.5083760023117065
Epoch 9, Iteration 136, Loss: 1.3153557777404785
Epoch 9, Iteration 137, Loss: 1.3637171983718872
Epoch 9, Iteration 138, Loss: 1.4730933904647827
Epoch 9, Iteration 139, Loss: 1.2949639558792114
Epoch 9, Iteration 140, Loss: 1.2995529174804688
Epoch 9, Iteration 141, Loss: 1.5435734987258911
Epoch 9, Iteration 142, Loss: 1.3894681930541992
Epoch 9, Iteration 143, Loss: 1.4384011030197144
Epoch 9, Iteration 144, Loss: 1.2931286096572876
Epoch 9, Iteration 145, Loss: 1.4219043254852295
Epoch 9, Iteration 146, Loss: 1.5924078226089478
Epoch 9, Iteration 147, Loss: 1.43877375125885
Epoch 9, Iteration 148, Loss: 1.1683177947998047
Epoch 9, Iteration 149, Loss: 1.2747983932495117
Epoch 9, Iteration 150, Loss: 1.54348886013031
Epoch 9, Iteration 150, Valid Loss: 0.8128789067268372
Epoch 9, Iteration 151, Loss: 1.5017849206924438
Epoch 9, Iteration 152, Loss: 1.2742891311645508
Epoch 9, Iteration 153, Loss: 1.3283048868179321
Epoch 9, Iteration 154, Loss: 1.077863097190857
Epoch 9, Iteration 155, Loss: 1.3917696475982666
Epoch 9, Iteration 156, Loss: 1.3980902433395386
Epoch 9, Iteration 157, Loss: 1.9668841361999512
Epoch 9, Iteration 158, Loss: 1.4268035888671875
Epoch 9, Iteration 159, Loss: 1.4096120595932007
Epoch 9, Iteration 160, Loss: 1.3086421489715576
Epoch 9, Iteration 161, Loss: 1.4094302654266357
Epoch 9, Iteration 162, Loss: 1.7684775590896606
Epoch 9, Iteration 163, Loss: 1.1567333936691284
Epoch 9, Iteration 164, Loss: 2.542574882507324
Epoch 9, Iteration 165, Loss: 1.316420078277588
Epoch 9, Iteration 166, Loss: 1.265669345855713
Epoch 9, Iteration 167, Loss: 1.7348363399505615
Epoch 9, Iteration 168, Loss: 1.5332651138305664
Epoch 9, Iteration 169, Loss: 1.3951289653778076
Epoch 9, Iteration 170, Loss: 1.6752887964248657
Epoch 9, Iteration 171, Loss: 1.4704086780548096
Epoch 9, Iteration 172, Loss: 1.456965446472168
Epoch 9, Iteration 173, Loss: 1.2828093767166138
Epoch 9, Iteration 174, Loss: 1.4910417795181274
Epoch 9, Iteration 175, Loss: 1.6175440549850464
Epoch 9, Iteration 176, Loss: 1.162591576576233
Epoch 9, Iteration 177, Loss: 1.7588715553283691
Epoch 9, Iteration 178, Loss: 1.562511920928955
Epoch 9, Iteration 179, Loss: 1.4710365533828735
Epoch 9, Iteration 180, Loss: 1.3461543321609497
Epoch 9, Iteration 181, Loss: 1.7562358379364014
Epoch 9, Iteration 182, Loss: 1.6179008483886719
Epoch 9, Iteration 183, Loss: 1.28679621219635
Epoch 9, Iteration 184, Loss: 1.346968412399292
Epoch 9, Iteration 185, Loss: 1.2824108600616455
Epoch 9, Iteration 186, Loss: 1.3958839178085327
Epoch 9, Iteration 187, Loss: 1.333539366722107
Epoch 9, Iteration 188, Loss: 1.3044579029083252
Epoch 9, Iteration 189, Loss: 1.2841517925262451
Epoch 9, Iteration 190, Loss: 1.528052806854248
Epoch 9, Iteration 191, Loss: 1.4950495958328247
Epoch 9, Iteration 192, Loss: 1.54655921459198
Epoch 9, Iteration 193, Loss: 1.3348572254180908
Epoch 9, Iteration 194, Loss: 1.4162429571151733
Epoch 9, Iteration 195, Loss: 1.3726085424423218
Epoch 9, Iteration 196, Loss: 1.5571889877319336
Epoch 9, Iteration 197, Loss: 1.3745536804199219
Epoch 9, Iteration 198, Loss: 1.398815393447876
Epoch 9, Iteration 199, Loss: 1.501975655555725
Epoch 9, Iteration 200, Loss: 1.2963647842407227
Epoch 9, Iteration 200, Valid Loss: 0.8030509948730469
Epoch 9, Iteration 201, Loss: 1.2804629802703857
Epoch 9, Iteration 202, Loss: 1.494346022605896
Epoch 9, Iteration 203, Loss: 1.314789056777954
Epoch 9, Iteration 204, Loss: 1.5140466690063477
Epoch 9, Iteration 205, Loss: 1.2023165225982666
Epoch 9, Iteration 206, Loss: 1.3440556526184082
Epoch 9, Iteration 207, Loss: 1.6732187271118164
Epoch 9, Iteration 208, Loss: 1.9684853553771973
Epoch 9, Iteration 209, Loss: 2.1286208629608154
Epoch 9, Iteration 210, Loss: 1.7884618043899536
Epoch 9, Iteration 211, Loss: 1.9467988014221191
Epoch 9, Iteration 212, Loss: 1.7111070156097412
Epoch 9, Iteration 213, Loss: 1.779000997543335
Epoch 9, Iteration 214, Loss: 1.5318877696990967
Epoch 9, Iteration 215, Loss: 1.2701929807662964
Epoch 9, Iteration 216, Loss: 1.2501310110092163
Epoch 9, Iteration 217, Loss: 1.399592638015747
Epoch 9, Iteration 218, Loss: 1.3931922912597656
Epoch 9, Iteration 219, Loss: 1.622056484222412
Epoch 9, Iteration 220, Loss: 1.289209008216858
Epoch 9, Iteration 221, Loss: 1.384772777557373
Epoch 9, Iteration 222, Loss: 1.42800772190094
Epoch 9, Iteration 223, Loss: 1.3452988862991333
Epoch 9, Iteration 224, Loss: 1.1864007711410522
Epoch 9, Iteration 225, Loss: 1.602111577987671
Epoch 9, Iteration 226, Loss: 1.5325839519500732
Epoch 9, Iteration 227, Loss: 1.5406993627548218
Epoch 9, Iteration 228, Loss: 1.265689492225647
Epoch 9, Iteration 229, Loss: 1.377777338027954
Epoch 9, Iteration 230, Loss: 1.4940121173858643
Epoch 9, Iteration 231, Loss: 1.438992977142334
Epoch 9, Iteration 232, Loss: 1.5263736248016357
Epoch 9, Iteration 233, Loss: 1.8519511222839355
Epoch 9, Iteration 234, Loss: 1.6092109680175781
Epoch 9, Iteration 235, Loss: 1.2497249841690063
Epoch 9, Iteration 236, Loss: 1.5269708633422852
Epoch 9, Iteration 237, Loss: 1.379347801208496
Epoch 9, Iteration 238, Loss: 1.221745491027832
Epoch 9, Iteration 239, Loss: 1.3724308013916016
Epoch 9, Iteration 240, Loss: 1.2168596982955933
Epoch 9, Iteration 241, Loss: 1.2439018487930298
Epoch 9, Iteration 242, Loss: 1.3814566135406494
Epoch 9, Iteration 243, Loss: 1.3204550743103027
Epoch 9, Iteration 244, Loss: 1.356824517250061
Epoch 9, Iteration 245, Loss: 1.0416145324707031
Epoch 9, Iteration 246, Loss: 1.4764769077301025
Epoch 9, Iteration 247, Loss: 1.3640680313110352
Epoch 9, Iteration 248, Loss: 1.7447030544281006
Epoch 9, Iteration 249, Loss: 1.3857622146606445
Epoch 9, Iteration 250, Loss: 1.4681951999664307
Epoch 9, Iteration 250, Valid Loss: 0.7900123596191406
Epoch 9, Iteration 251, Loss: 1.3727232217788696
Epoch 9, Iteration 252, Loss: 1.5906593799591064
Epoch 9, Iteration 253, Loss: 1.5250396728515625
Epoch 9, Iteration 254, Loss: 1.5672903060913086
Epoch 9, Iteration 255, Loss: 1.5739502906799316
Epoch 9, Iteration 256, Loss: 1.6524009704589844
Epoch 9, Iteration 257, Loss: 1.370086431503296
Epoch 9, Iteration 258, Loss: 1.777796745300293
Epoch 9, Iteration 259, Loss: 1.2884572744369507
Epoch 9, Iteration 260, Loss: 1.4895884990692139
Epoch 9, Iteration 261, Loss: 1.4452033042907715
Epoch 9, Iteration 262, Loss: 1.2259595394134521
Epoch 9, Iteration 263, Loss: 1.645209550857544
Epoch 9, Iteration 264, Loss: 1.496997356414795
Epoch 9, Iteration 265, Loss: 1.4791933298110962
Epoch 9, Iteration 266, Loss: 1.556034803390503
Epoch 9, Iteration 267, Loss: 1.3982244729995728
Epoch 9, Iteration 268, Loss: 1.3608936071395874
Epoch 9, Iteration 269, Loss: 1.2973467111587524
Epoch 9, Iteration 270, Loss: 1.4126930236816406
Epoch 9, Iteration 271, Loss: 1.4439764022827148
Epoch 9, Iteration 272, Loss: 1.316035270690918
Epoch 9, Iteration 273, Loss: 1.4264492988586426
Epoch 9, Iteration 274, Loss: 1.3504571914672852
Epoch 9, Iteration 275, Loss: 1.2250850200653076
Epoch 9, Iteration 276, Loss: 1.4178094863891602
Epoch 9, Iteration 277, Loss: 1.340114951133728
Epoch 9, Iteration 278, Loss: 1.6817773580551147
Epoch 9, Iteration 279, Loss: 1.2447001934051514
Epoch 9, Iteration 280, Loss: 1.121953010559082
Epoch 9, Iteration 281, Loss: 1.3751417398452759
Epoch 9, Iteration 282, Loss: 1.3604165315628052
Epoch 9, Iteration 283, Loss: 1.4023836851119995
Epoch 9, Iteration 284, Loss: 1.363822340965271
Epoch 9, Iteration 285, Loss: 1.278154969215393
Epoch 9, Iteration 286, Loss: 1.3132747411727905
Epoch 9, Iteration 287, Loss: 1.457972764968872
Epoch 9, Iteration 288, Loss: 1.2142549753189087
Epoch 9, Iteration 289, Loss: 1.2920889854431152
Epoch 9, Iteration 290, Loss: 1.3218201398849487
Epoch 9, Iteration 291, Loss: 1.1988680362701416
Epoch 9, Iteration 292, Loss: 1.5647292137145996
Epoch 9, Iteration 293, Loss: 1.594014048576355
Epoch 9, Iteration 294, Loss: 1.2580474615097046
Epoch 9, Iteration 295, Loss: 1.2466771602630615
Epoch 9, Iteration 296, Loss: 1.5991408824920654
Epoch 9, Iteration 297, Loss: 1.6060692071914673
Epoch 9, Iteration 298, Loss: 1.4174120426177979
Epoch 9, Iteration 299, Loss: 1.2325971126556396
Epoch 9, Iteration 300, Loss: 1.1692687273025513
Epoch 9, Iteration 300, Valid Loss: 0.7829958200454712
Epoch 9, Iteration 301, Loss: 1.363530158996582
Epoch 9, Iteration 302, Loss: 1.2792961597442627
Epoch 9, Iteration 303, Loss: 1.7194998264312744
Epoch 9, Iteration 304, Loss: 1.318787932395935
Epoch 9, Iteration 305, Loss: 1.2382304668426514
Epoch 9, Iteration 306, Loss: 1.3760796785354614
Epoch 9, Iteration 307, Loss: 1.2310556173324585
Epoch 9, Iteration 308, Loss: 1.2611351013183594
Epoch 9, Iteration 309, Loss: 1.3756368160247803
Epoch 9, Iteration 310, Loss: 1.6826181411743164
Epoch 9, Iteration 311, Loss: 1.2766928672790527
Epoch 9, Iteration 312, Loss: 1.217157006263733
Epoch 9, Iteration 313, Loss: 1.6197330951690674
Epoch 9, Iteration 314, Loss: 1.238403558731079
Epoch 9, Iteration 315, Loss: 1.1635477542877197
Epoch 9, Iteration 316, Loss: 1.4682676792144775
Epoch 9, Iteration 317, Loss: 1.238741159439087
Epoch 9, Iteration 318, Loss: 1.3183268308639526
Epoch 9, Iteration 319, Loss: 1.531143069267273
Epoch 9, Iteration 320, Loss: 1.1308104991912842
Epoch 9, Iteration 321, Loss: 1.3986748456954956
Epoch 9, Iteration 322, Loss: 1.2713452577590942
Epoch 9, Iteration 323, Loss: 1.3544834852218628
Epoch 9, Iteration 324, Loss: 1.3651328086853027
Epoch 9, Iteration 325, Loss: 1.3014825582504272
Epoch 9, Iteration 326, Loss: 1.3630539178848267
Epoch 9, Iteration 327, Loss: 1.2223440408706665
Epoch 9, Iteration 328, Loss: 1.3118140697479248
Epoch 9, Iteration 329, Loss: 1.491673231124878
Epoch 9, Iteration 330, Loss: 1.1141217947006226
Epoch 9, Iteration 331, Loss: 1.3674430847167969
Epoch 9, Iteration 332, Loss: 1.4625592231750488
Epoch 9, Iteration 333, Loss: 1.577138900756836
Epoch 9, Iteration 334, Loss: 1.257855772972107
Epoch 9, Iteration 335, Loss: 1.582724928855896
Epoch 9, Iteration 336, Loss: 1.344641089439392
Epoch 9, Iteration 337, Loss: 1.2347785234451294
Epoch 9, Iteration 338, Loss: 1.491646647453308
Epoch 9, Iteration 339, Loss: 1.342214584350586
Epoch 9, Iteration 340, Loss: 1.1733399629592896
Epoch 9, Iteration 341, Loss: 1.3009965419769287
Epoch 9, Iteration 342, Loss: 1.2873454093933105
Epoch 9, Iteration 343, Loss: 1.2621715068817139
Epoch 9, Iteration 344, Loss: 1.385965347290039
Epoch 9, Iteration 345, Loss: 1.404354453086853
Epoch 9, Iteration 346, Loss: 1.3246417045593262
Epoch 9, Iteration 347, Loss: 1.1077098846435547
Epoch 9, Iteration 348, Loss: 1.2453962564468384
Epoch 9, Iteration 349, Loss: 1.300994634628296
Epoch 9, Iteration 350, Loss: 1.2715940475463867
Epoch 9, Iteration 350, Valid Loss: 0.800444483757019
Epoch 9, Iteration 351, Loss: 1.4597824811935425
Epoch 9, Iteration 352, Loss: 1.2731527090072632
Epoch 9, Iteration 353, Loss: 1.4373162984848022
Epoch 9, Iteration 354, Loss: 1.451431393623352
Epoch 9, Iteration 355, Loss: 1.2932658195495605
Epoch 9, Iteration 356, Loss: 1.3680179119110107
Epoch 9, Iteration 357, Loss: 1.2089743614196777
Epoch 9, Iteration 358, Loss: 1.2266782522201538
Epoch 9, Iteration 359, Loss: 1.221789836883545
Epoch 9, Iteration 360, Loss: 1.1673219203948975
Epoch 9, Iteration 361, Loss: 1.4138561487197876
Epoch 9, Iteration 362, Loss: 1.2037696838378906
Epoch 9, Iteration 363, Loss: 1.2910650968551636
Epoch 9, Iteration 364, Loss: 1.3591971397399902
Epoch 9, Iteration 365, Loss: 1.4860423803329468
Epoch 9, Iteration 366, Loss: 1.5768814086914062
Epoch 9, Iteration 367, Loss: 1.3832780122756958
Epoch 9, Iteration 368, Loss: 1.4255926609039307
Epoch 9, Iteration 369, Loss: 1.485979676246643
Epoch 9, Iteration 370, Loss: 1.4631835222244263
Epoch 9, Iteration 371, Loss: 1.3195242881774902
Epoch 9, Iteration 372, Loss: 1.1398322582244873
Epoch 9, Iteration 373, Loss: 1.2332851886749268
Epoch 9, Iteration 374, Loss: 1.6524927616119385
Epoch 9, Iteration 375, Loss: 1.4044651985168457
Epoch 9, Iteration 376, Loss: 1.399505615234375
Epoch 9, Iteration 377, Loss: 1.4422587156295776
Epoch 9, Iteration 378, Loss: 1.173766016960144
Epoch 9, Iteration 379, Loss: 1.3219201564788818
Epoch 9, Iteration 380, Loss: 1.1788979768753052
Epoch 9, Iteration 381, Loss: 1.243704080581665
Epoch 9, Iteration 382, Loss: 1.0883216857910156
Epoch 9, Iteration 383, Loss: 1.203870177268982
Epoch 9, Iteration 384, Loss: 1.3974683284759521
Epoch 9, Iteration 385, Loss: 1.2745839357376099
Epoch 9, Iteration 386, Loss: 1.2142212390899658
Epoch 9, Iteration 387, Loss: 1.5820064544677734
Epoch 9, Iteration 388, Loss: 1.318082571029663
Epoch 9, Iteration 389, Loss: 1.3726059198379517
Epoch 9, Iteration 390, Loss: 1.4738065004348755
Epoch 9, Iteration 391, Loss: 1.2505515813827515
Epoch 10/10, Loss: 1.42042383916524
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<br/> <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br/><div class="wandb-row"><div class="wandb-col"><h3>Run history:</h3><br><table class="wandb"><tr><td>head_tagging_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>unlabeled_attachment_score</td><td></td></tr><tr><td>valid_loss</td><td></td></tr></table><br/></br></div><div class="wandb-col"><h3>Run summary:</h3><br><table class="wandb"><tr><td>head_tagging_accuracy</td><td>0.75891</td></tr><tr><td>train_loss</td><td>1.25055</td></tr><tr><td>unlabeled_attachment_score</td><td>0.8013</td></tr><tr><td>valid_loss</td><td>0.80044</td></tr></table><br/></br></div></div>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View run <strong style="color:#cdcd00">dependency-parsing</strong> at: <a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/er1khkx3" target="_blank">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/er1khkx3</a><br/> View project at: <a href="https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing" target="_blank">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing</a><br/>Synced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Find logs at: <code>.\wandb\run-20250113_225223-er1khkx3\logs</code>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>total batch: 65
Batch 0
Batch 1
Batch 2
Batch 3
Batch 4
Batch 5
Batch 6
Batch 7
Batch 8
Batch 9
Batch 10
Batch 11
Batch 12
Batch 13
Batch 14
Batch 15
Batch 16
Batch 17
Batch 18
Batch 19
Batch 20
Batch 21
Batch 22
Batch 23
Batch 24
Batch 25
Batch 26
Batch 27
Batch 28
Batch 29
Batch 30
Batch 31
Batch 32
Batch 33
Batch 34
Batch 35
Batch 36
Batch 37
Batch 38
Batch 39
Batch 40
Batch 41
Batch 42
Batch 43
Batch 44
Batch 45
Batch 46
Batch 47
Batch 48
Batch 49
Batch 50
Batch 51
Batch 52
Batch 53
Batch 54
Batch 55
Batch 56
Batch 57
Batch 58
Batch 59
Batch 60
Batch 61
Batch 62
Batch 63
Batch 64
LAS Accuracy: 0.7171660822441823
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=2892f70c2cd2df37">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Observation">Observation<a class="anchor-link" href="#Observation"></a></h1><p>I trained 2 neural network models with different hyperparameters and architectures.
The models are:</p>
<ul>
<li>Base Model: RoBERTa + Linear Layer<ul>
<li>Learning Rate: 1e-3</li>
<li>Epochs: 10</li>
<li>edge dimension: 500</li>
<li>Final UAS: 0.8089</li>
</ul>
</li>
<li>Pro Model: RoBERTa + 2 Linear Layers with residual connection and more edge dimension<ul>
<li>Learning Rate: 6e-4</li>
<li>Epochs: 15</li>
<li>edge dimension: 1024</li>
<li>Final UAS: 0.8636</li>
</ul>
</li>
</ul>
<p>All of the models are trained with a 32 batch size.</p>
<p>The Head Tagging Accuracy was achieved by computing the accuracy of the predicted heads against the gold standard annotations, while masking invalid positions (e.g., head = -100 and mask = 0). This value was computed after each training epoch on the validation set.</p>
<p>The UAS score was evaluated using the MST-parsed dependency tree, ensuring the predicted edges can form a tree. The UAS score was recorded on the validation set also after training epoch. Compared to the base model, more linear layers and more edge dimension gives more improvement. However, it also takes a longer time for a more complex model to be trained.</p>
<p>From the train loss and valid loss figures we can see the pro model has a much more better performance than the base model. The pro model has a lower loss and a higher accuracy. The pro model has a better performance on the validation set, which means it has a better generalization ability. Correspondingly, the UAS score of the pro model is higher than the base model.</p>
<h1 id="Extra:-Label-Prediction">Extra: Label Prediction<a class="anchor-link" href="#Extra:-Label-Prediction"></a></h1><p>Additionally, I trained a label prediction model by combining the edge prediction loss and the label prediction loss.
The final LAS is 0.7171660822441823.</p>
<h1 id="Training-figures">Training figures<a class="anchor-link" href="#Training-figures"></a></h1><p><img alt="train_loss" src="./figures/train_loss.png" title="train_loss"/>
<img alt="valid_loss" src="./figures/valid_loss.png" title="valid_loss"/>
<img alt="head_tagging_accuracy" src="./figures/head_tagging_accuracy.png" title="head_tagging_accuracy"/>
<img alt="unlabeled_attachment_score" src="./figures/UAS.png" title="unlabeled_attachment_score"/></p>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
