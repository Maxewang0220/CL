{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T21:51:01.843113Z",
     "start_time": "2025-01-13T21:51:01.833610Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, XLMRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246be7d70e3db82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T21:51:06.722981Z",
     "start_time": "2025-01-13T21:51:03.698270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 12543\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 2002\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 2077\n",
      "    })\n",
      "})\n",
      "['Al-Zaman : American forces killed Shaikh Abdullah al-Ani, the preacher at the mosque in the town of Qaim, near the Syrian border.', '[This killing of a respected cleric will be causing us trouble for years to come.]', 'DPA: Iraqi authorities announced that they had busted up 3 terrorist cells operating in Baghdad.', 'Two of them were being run by 2 officials of the Ministry of the Interior!', 'The MoI in Iraq is equivalent to the US FBI, so this would be like having J. Edgar Hoover unwittingly employ at a high level members of the Weathermen bombers back in the 1960s.', 'The third was being run by the head of an investment firm.', 'You wonder if he was manipulating the market with his bombing targets.', 'The cells were operating in the Ghazaliyah and al-Jihad districts of the capital.', \"Although the announcement was probably made to show progress in identifying and breaking up terror cells, I don't find the news that the Baathists continue to penetrate the Iraqi government very hopeful.\", 'It reminds me too much of the ARVN officers who were secretly working for the other side in Vietnam.']\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "dataset = datasets.load_dataset(path=\"universal_dependencies\", name=\"en_ewt\", trust_remote_code=True)\n",
    "print(dataset)\n",
    "train_dataset = dataset[\"train\"]\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[\"text\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907365ee16f491c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T21:51:08.718535Z",
     "start_time": "2025-01-13T21:51:08.708125Z"
    }
   },
   "outputs": [],
   "source": [
    "all_deprels = [\n",
    "    # these are the default UD dependency relations according to https://universaldependencies.org/u/dep/\n",
    "    \"acl\", \"acl:relcl\", \"advcl\", \"advcl:relcl\", \"advmod\", \"advmod:emph\", \"advmod:lmod\", \"amod\", \"appos\",\n",
    "    \"aux\", \"aux:pass\", \"case\", \"cc\", \"cc:preconj\", \"ccomp\", \"clf\", \"compound\", \"compound:lvc\",\n",
    "    \"compound:prt\", \"compound:redup\", \"compound:svc\", \"conj\", \"cop\", \"csubj\", \"csubj:outer\",\n",
    "    \"csubj:pass\", \"dep\", \"det\", \"det:numgov\", \"det:nummod\", \"det:poss\", \"discourse\", \"dislocated\",\n",
    "    \"expl\", \"expl:impers\", \"expl:pass\", \"expl:pv\", \"fixed\", \"flat\", \"flat:foreign\", \"flat:name\",\n",
    "    \"goeswith\", \"iobj\", \"list\", \"mark\", \"nmod\", \"nmod:poss\", \"nmod:tmod\", \"nsubj\", \"nsubj:outer\",\n",
    "    \"nsubj:pass\", \"nummod\", \"nummod:gov\", \"obj\", \"obl\", \"obl:agent\", \"obl:arg\", \"obl:lmod\",\n",
    "    \"obl:tmod\", \"orphan\", \"parataxis\", \"punct\", \"reparandum\", \"root\", \"vocative\", \"xcomp\",\n",
    "\n",
    "    # we need some more for en_ewt\n",
    "    \"det:predet\", \"obl:npmod\", \"nmod:npmod\"\n",
    "]\n",
    "\n",
    "# construct deprel to ID mapping\n",
    "deprel_to_id = {rel: idx for idx, rel in enumerate(all_deprels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3b37577b6ef9f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T21:51:11.612849Z",
     "start_time": "2025-01-13T21:51:11.598266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code for the assignment in https://github.com/coli-saar/cl/wiki/Assignment:-Dependency-parsing\n",
    "# Alexander Koller, December 2023\n",
    "\n",
    "def strip_none_heads(examples, i):\n",
    "    tokens = examples[\"tokens\"][i]\n",
    "    heads = examples[\"head\"][i]\n",
    "    deprels = examples[\"deprel\"][i]\n",
    "\n",
    "    non_none = [(t, h, d) for t, h, d in zip(tokens, heads, deprels) if h != \"None\"]\n",
    "    return zip(*non_none)\n",
    "\n",
    "\n",
    "def map_first_occurrence(nums):\n",
    "    \"\"\"\n",
    "    Maps a list of numbers to a dictionary that assigns each unique number the position of its first occurrence.\n",
    "\n",
    "    Example:\n",
    "    > map_first_occurrence([0,1,2,3,3,3,4])\n",
    "    {0: 0, 1: 1, 2: 2, 3: 3, 4: 6}\n",
    "\n",
    "    :param nums:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    return {num: i for i, num in enumerate(nums) if num is not None and num not in seen and not seen.add(num)}\n",
    "\n",
    "\n",
    "def pad_to_same_size(lists, padding_symbol):\n",
    "    maxlen = max([len(l) for l in lists])\n",
    "    return [l + (padding_symbol,) * (maxlen - len(l)) for l in lists]\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples, deprel_to_id, tokenizer, skip_index=-100):\n",
    "    # delete tokens with \"None\" head and their annotations\n",
    "    examples_tokens, examples_heads, examples_deprels = [], [], []\n",
    "    for sentence_id in range(len(examples[\"tokens\"])):\n",
    "        tt, hh, dd = strip_none_heads(examples, sentence_id)\n",
    "        examples_tokens.append(tt)\n",
    "        examples_heads.append(hh)\n",
    "        examples_deprels.append(dd)\n",
    "\n",
    "    tokenized_inputs = tokenizer(examples_tokens, truncation=True, is_split_into_words=True,\n",
    "                                 padding=True)  # get \"tokenizer\" from global variable\n",
    "    # tokenized_inputs is a dictionary with keys input_ids and attention_mask;\n",
    "    # each is a list (per sentence) of lists (per token).\n",
    "\n",
    "    remapped_heads = []  # these will be lists (per sentence) of lists (per token)\n",
    "    deprel_ids = []\n",
    "    tokens_representing_words = []\n",
    "    num_words: list[int] = []\n",
    "    maxlen_t2w = 0  # max length of a token_to_word_here list\n",
    "\n",
    "    for sentence_id, annotated_heads in enumerate(examples_heads):\n",
    "        deprels = examples_deprels[sentence_id]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=sentence_id)\n",
    "        word_pos_to_token_pos = map_first_occurrence(\n",
    "            word_ids)  # word-pos to first token-pos; both start at 0 for first word (actual) / first token (BOS)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        heads_here: list[int] = []\n",
    "        deprel_ids_here: list[int] = []\n",
    "\n",
    "        # list of token positions that map to words (first token of each word)\n",
    "        # token 0 -> word 0 (BOS)\n",
    "        tokens_representing_word_here: list[int] = [0]\n",
    "\n",
    "        for sentence_position, word_idx in enumerate(word_ids):\n",
    "            # Special tokens (BOS, EOS) have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                heads_here.append(skip_index)\n",
    "                deprel_ids_here.append(skip_index)\n",
    "\n",
    "            # We set the label for the first token of each word;\n",
    "            # subsequent tokens of the same word will have the same word_idx.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                if annotated_heads[word_idx] == \"None\":  # added by padding\n",
    "                    print(\"A 'None' head survived!\")\n",
    "                    sys.exit(0)\n",
    "                else:\n",
    "                    # Map HEAD annotation to position of first token of head word.\n",
    "                    # HEAD = 0 => map it to first token (BOS)\n",
    "                    # Otherwise, look up first token for HEAD-1 (HEAD is 1-based, word positions are 0-based)\n",
    "                    head_word_pos = int(annotated_heads[word_idx])\n",
    "                    head_token_pos = 0 if head_word_pos == 0 else word_pos_to_token_pos[head_word_pos - 1]\n",
    "\n",
    "                    heads_here.append(head_token_pos)\n",
    "                    deprel_ids_here.append(deprel_to_id[deprels[word_idx]])\n",
    "\n",
    "                    tokens_representing_word_here.append(sentence_position)  # first word is index 1; index 0 is BOS\n",
    "\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                heads_here.append(skip_index)\n",
    "                deprel_ids_here.append(skip_index)\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        remapped_heads.append(heads_here)\n",
    "        deprel_ids.append(deprel_ids_here)\n",
    "        tokens_representing_words.append(tokens_representing_word_here)\n",
    "\n",
    "        num_words.append(len(tokens_representing_word_here))\n",
    "        if len(tokens_representing_word_here) > maxlen_t2w:\n",
    "            maxlen_t2w = len(tokens_representing_word_here)\n",
    "\n",
    "    # pad t2w lists to same length\n",
    "    for t2w in tokens_representing_words:\n",
    "        t2w += [-1] * (maxlen_t2w - len(t2w))\n",
    "\n",
    "    tokenized_inputs[\"head\"] = remapped_heads\n",
    "    tokenized_inputs[\"deprel_ids\"] = deprel_ids\n",
    "    tokenized_inputs[\"tokens_representing_words\"] = tokens_representing_words\n",
    "    tokenized_inputs[\"num_words\"] = num_words\n",
    "    tokenized_inputs[\"tokenid_to_wordid\"] = [tokenized_inputs.word_ids(batch_index=i) for i in\n",
    "                                             range(len(examples_heads))]  # map token ID to word ID\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99990e156c16a774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T21:51:17.586392Z",
     "start_time": "2025-01-13T21:51:16.906751Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WangEntang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁Al            884       0         root           0              \n",
      "▁-             20        1         punct          1              \n",
      "▁Zaman         53113     1         flat           2              \n",
      "▁:             152       1         punct          3              \n",
      "▁American      15672     6         amod           4              \n",
      "▁forces        84616     7         nsubj          5              \n",
      "▁killed        152388    1         parataxis      6              \n",
      "▁Sha           7224      7         obj            7              \n",
      "ikh            41336     -100      None           7              \n",
      "▁Abdullah      34490     8         flat           8              \n",
      "▁al            144       8         flat           9              \n",
      "▁-             20        8         punct          10             \n",
      "▁Ani           32340     8         flat           11             \n",
      "▁              6         8         punct          12             \n",
      ",              4         -100      None           12             \n",
      "▁the           70        17        det            13             \n",
      "▁prea          19542     8         appos          14             \n",
      "cher           5372      -100      None           14             \n",
      "▁at            99        21        case           15             \n",
      "▁the           70        21        det            16             \n",
      "▁mos           7304      7         obl            17             \n",
      "que            944       -100      None           17             \n",
      "▁in            23        25        case           18             \n",
      "▁the           70        25        det            19             \n",
      "▁town          59444     21        nmod           20             \n",
      "▁of            111       27        case           21             \n",
      "▁Qa            16785     25        nmod           22             \n",
      "im             464       -100      None           22             \n",
      "▁              6         25        punct          23             \n",
      ",              4         -100      None           23             \n",
      "▁near          43573     35        case           24             \n",
      "▁the           70        35        det            25             \n",
      "▁Syria         51712     35        amod           26             \n",
      "n              19        -100      None           26             \n",
      "▁border        132988    25        nmod           27             \n",
      "▁              6         1         punct          28             \n",
      ".              5         -100      None           28             \n",
      "</s>           2         -100      None           None           \n",
      "Example 2\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁[             378       12        punct          0              \n",
      "▁This          3293      3         det            1              \n",
      "▁ki            200       12        nsubj          2              \n",
      "lling          30319     -100      None           2              \n",
      "▁of            111       9         case           3              \n",
      "▁a             10        9         det            4              \n",
      "▁respect       15072     9         amod           5              \n",
      "ed             297       -100      None           5              \n",
      "▁cleric        181186    3         nmod           6              \n",
      "▁will          1221      12        aux            7              \n",
      "▁be            186       12        aux            8              \n",
      "▁causing       216806    0         root           9              \n",
      "▁us            1821      12        iobj           10             \n",
      "▁trouble       63134     12        obj            11             \n",
      "▁for           100       16        case           12             \n",
      "▁years         5369      12        obl            13             \n",
      "▁to            47        18        mark           14             \n",
      "▁come          1380      16        acl            15             \n",
      "▁              6         12        punct          16             \n",
      ".              5         -100      None           16             \n",
      "▁]             10114     12        punct          17             \n",
      "</s>           2         -100      None           None           \n",
      "Example 3\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁D             391       0         root           0              \n",
      "PA             12236     -100      None           0              \n",
      "▁:             152       1         punct          1              \n",
      "▁Iraq          69496     6         amod           2              \n",
      "i              14        -100      None           2              \n",
      "▁authorities   207048    7         nsubj          3              \n",
      "▁announced     171530    1         parataxis      4              \n",
      "▁that          450       11        mark           5              \n",
      "▁they          1836      11        nsubj          6              \n",
      "▁had           1902      11        aux            7              \n",
      "▁bu            373       7         ccomp          8              \n",
      "sted           14437     -100      None           8              \n",
      "▁up            1257      11        compound:prt   9              \n",
      "▁3             138       16        nummod         10             \n",
      "▁terrorist     42998     16        amod           11             \n",
      "▁cell          38750     11        obj            12             \n",
      "s              7         -100      None           12             \n",
      "▁operating     172852    16        acl            13             \n",
      "▁in            23        20        case           14             \n",
      "▁Bagh          177161    18        obl            15             \n",
      "dad            12409     -100      None           15             \n",
      "▁              6         1         punct          16             \n",
      ".              5         -100      None           16             \n",
      "</s>           2         -100      None           None           \n",
      "Example 4\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁Two           32964     6         nsubj:pass     0              \n",
      "▁of            111       3         case           1              \n",
      "▁them          2856      1         nmod           2              \n",
      "▁were          3542      6         aux            3              \n",
      "▁being         8035      6         aux:pass       4              \n",
      "▁run           11675     0         root           5              \n",
      "▁by            390       9         case           6              \n",
      "▁2             116       9         nummod         7              \n",
      "▁official      51521     6         obl            8              \n",
      "s              7         -100      None           8              \n",
      "▁of            111       13        case           9              \n",
      "▁the           70        13        det            10             \n",
      "▁Ministry      185236    9         nmod           11             \n",
      "▁of            111       16        case           12             \n",
      "▁the           70        16        det            13             \n",
      "▁Interior      123024    13        nmod           14             \n",
      "▁!             711       6         punct          15             \n",
      "</s>           2         -100      None           None           \n",
      "Example 5\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁The           581       2         det            0              \n",
      "▁Mo            2501      7         nsubj          1              \n",
      "I              568       -100      None           1              \n",
      "▁in            23        5         case           2              \n",
      "▁Iraq          69496     2         nmod           3              \n",
      "▁is            83        7         cop            4              \n",
      "▁equivalent    183234    0         root           5              \n",
      "▁to            47        11        case           6              \n",
      "▁the           70        11        det            7              \n",
      "▁US            7082      11        compound       8              \n",
      "▁FBI           83085     7         obl            9              \n",
      "▁              6         7         punct          10             \n",
      ",              4         -100      None           10             \n",
      "▁so            221       19        advmod         11             \n",
      "▁this          903       17        nsubj          12             \n",
      "▁would         2806      17        aux            13             \n",
      "▁be            186       7         parataxis      14             \n",
      "▁like          1884      19        mark           15             \n",
      "▁having        19441     17        advcl          16             \n",
      "▁J             821       29        nsubj          17             \n",
      ".              5         -100      None           17             \n",
      "▁Edgar         111954    20        flat           18             \n",
      "▁Ho            2291      20        flat           19             \n",
      "over           5465      -100      None           19             \n",
      "▁un            51        29        advmod         20             \n",
      "wit            14817     -100      None           20             \n",
      "ting           1916      -100      None           20             \n",
      "ly             538       -100      None           20             \n",
      "▁employ        187016    19        ccomp          21             \n",
      "▁at            99        33        case           22             \n",
      "▁a             10        33        det            23             \n",
      "▁high          11192     33        amod           24             \n",
      "▁level         17366     29        obl            25             \n",
      "▁members       43032     29        obj            26             \n",
      "▁of            111       39        case           27             \n",
      "▁the           70        39        det            28             \n",
      "▁Weather       214526    39        compound       29             \n",
      "men            1055      -100      None           29             \n",
      "▁bomb          54330     34        nmod           30             \n",
      "ers            1314      -100      None           30             \n",
      "▁back          4420      44        advmod         31             \n",
      "▁in            23        44        case           32             \n",
      "▁the           70        44        det            33             \n",
      "▁1960          23936     29        obl            34             \n",
      "s              7         -100      None           34             \n",
      "▁              6         7         punct          35             \n",
      ".              5         -100      None           35             \n",
      "</s>           2         -100      None           None           \n",
      "Example 6\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁The           581       2         det            0              \n",
      "▁third         50960     5         nsubj:pass     1              \n",
      "▁was           509       5         aux            2              \n",
      "▁being         8035      5         aux:pass       3              \n",
      "▁run           11675     0         root           4              \n",
      "▁by            390       8         case           5              \n",
      "▁the           70        8         det            6              \n",
      "▁head          10336     5         obl            7              \n",
      "▁of            111       12        case           8              \n",
      "▁an            142       12        det            9              \n",
      "▁investment    77021     12        compound       10             \n",
      "▁firm          11037     8         nmod           11             \n",
      "▁              6         5         punct          12             \n",
      ".              5         -100      None           12             \n",
      "</s>           2         -100      None           None           \n",
      "Example 7\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁You           2583      2         nsubj          0              \n",
      "▁wonder        32195     0         root           1              \n",
      "▁if            2174      6         mark           2              \n",
      "▁he            764       6         nsubj          3              \n",
      "▁was           509       6         aux            4              \n",
      "▁manipula      45258     2         ccomp          5              \n",
      "ting           1916      -100      None           5              \n",
      "▁the           70        9         det            6              \n",
      "▁market        16839     6         obj            7              \n",
      "▁with          678       14        case           8              \n",
      "▁his           1919      14        nmod:poss      9              \n",
      "▁bomb          54330     14        compound       10             \n",
      "ing            214       -100      None           10             \n",
      "▁target        30388     6         obl            11             \n",
      "s              7         -100      None           11             \n",
      "▁              6         2         punct          12             \n",
      ".              5         -100      None           12             \n",
      "</s>           2         -100      None           None           \n",
      "Example 8\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁The           581       2         det            0              \n",
      "▁cell          38750     5         nsubj          1              \n",
      "s              7         -100      None           1              \n",
      "▁were          3542      5         aux            2              \n",
      "▁operating     172852    0         root           3              \n",
      "▁in            23        15        case           4              \n",
      "▁the           70        15        det            5              \n",
      "▁Ghazal        220428    15        compound       6              \n",
      "iyah           62776     -100      None           6              \n",
      "▁and           136       13        cc             7              \n",
      "▁al            144       13        compound       8              \n",
      "▁-             20        13        punct          9              \n",
      "▁Ji            3291      8         conj           10             \n",
      "had            8408      -100      None           10             \n",
      "▁district      103724    5         obl            11             \n",
      "s              7         -100      None           11             \n",
      "▁of            111       19        case           12             \n",
      "▁the           70        19        det            13             \n",
      "▁capital       10323     15        nmod           14             \n",
      "▁              6         5         punct          15             \n",
      ".              5         -100      None           15             \n",
      "</s>           2         -100      None           None           \n",
      "Example 9\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁Although      106073    8         mark           0              \n",
      "▁the           70        3         det            1              \n",
      "▁ann           3398      8         nsubj:pass     2              \n",
      "ounce          85018     -100      None           2              \n",
      "ment           674       -100      None           2              \n",
      "▁was           509       8         aux:pass       3              \n",
      "▁probably      31895     8         advmod         4              \n",
      "▁made          7228      28        advcl          5              \n",
      "▁to            47        10        mark           6              \n",
      "▁show          7639      8         xcomp          7              \n",
      "▁progress      42658     10        obj            8              \n",
      "▁in            23        13        mark           9              \n",
      "▁identify      135812    11        acl            10             \n",
      "ing            214       -100      None           10             \n",
      "▁and           136       16        cc             11             \n",
      "▁breaking      116987    13        conj           12             \n",
      "▁up            1257      16        compound:prt   13             \n",
      "▁terror        20870     19        compound       14             \n",
      "▁cell          38750     13        obj            15             \n",
      "s              7         -100      None           15             \n",
      "▁              6         28        punct          16             \n",
      ",              4         -100      None           16             \n",
      "▁I             87        28        nsubj          17             \n",
      "▁do            54        28        aux            18             \n",
      "▁n             653       28        advmod         19             \n",
      "'              25        -100      None           19             \n",
      "t              18        -100      None           19             \n",
      "▁find          7413      0         root           20             \n",
      "▁the           70        30        det            21             \n",
      "▁news          7123      28        obj            22             \n",
      "▁that          450       36        mark           23             \n",
      "▁the           70        33        det            24             \n",
      "▁Baat          160062    36        nsubj          25             \n",
      "hist           49063     -100      None           25             \n",
      "s              7         -100      None           25             \n",
      "▁continue      21342     30        acl            26             \n",
      "▁to            47        38        mark           27             \n",
      "▁penetra       75076     36        xcomp          28             \n",
      "te             67        -100      None           28             \n",
      "▁the           70        43        det            29             \n",
      "▁Iraq          69496     43        amod           30             \n",
      "i              14        -100      None           30             \n",
      "▁government    27759     38        obj            31             \n",
      "▁very          4552      45        advmod         32             \n",
      "▁hope          15673     28        xcomp          33             \n",
      "ful            7844      -100      None           33             \n",
      "▁              6         28        punct          34             \n",
      ".              5         -100      None           34             \n",
      "</s>           2         -100      None           None           \n",
      "Example 10\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁It            1650      2         nsubj          0              \n",
      "▁remind        98911     0         root           1              \n",
      "s              7         -100      None           1              \n",
      "▁me            163       2         obj            2              \n",
      "▁too           5792      6         advmod         3              \n",
      "▁much          5045      2         advmod         4              \n",
      "▁of            111       11        case           5              \n",
      "▁the           70        11        det            6              \n",
      "▁AR            13685     11        compound       7              \n",
      "VN             40711     -100      None           7              \n",
      "▁officer       93324     2         obl            8              \n",
      "s              7         -100      None           8              \n",
      "▁who           2750      17        nsubj          9              \n",
      "▁were          3542      17        aux            10             \n",
      "▁secret        23410     17        advmod         11             \n",
      "ly             538       -100      None           11             \n",
      "▁working       20697     11        acl:relcl      12             \n",
      "▁for           100       21        case           13             \n",
      "▁the           70        21        det            14             \n",
      "▁other         3789      21        amod           15             \n",
      "▁side          5609      17        obl            16             \n",
      "▁in            23        23        case           17             \n",
      "▁Vietnam       39272     17        obl            18             \n",
      "▁              6         2         punct          19             \n",
      ".              5         -100      None           19             \n",
      "</s>           2         -100      None           None           \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "# test tokenization\n",
    "tokenized_inputs = tokenize_and_align_labels(train_dataset[:10], deprel_to_id, tokenizer)\n",
    "\n",
    "for i in range(10):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][i])  # i 是句子的索引\n",
    "    word_ids = tokenized_inputs[\"tokenid_to_wordid\"][i]\n",
    "\n",
    "    print(f\"Example {i + 1}\")\n",
    "    print(f\"{'Token':<15}{'Token ID':<10}{'Head':<10}{'Deprel':<15}{'Word Mapping':<15}\")\n",
    "    for j, token in enumerate(tokens):\n",
    "        if token == tokenizer.pad_token:\n",
    "            break\n",
    "        token_id = tokenized_inputs[\"input_ids\"][i][j]\n",
    "        head = tokenized_inputs[\"head\"][i][j]\n",
    "        deprel = all_deprels[tokenized_inputs[\"deprel_ids\"][i][j]] if tokenized_inputs[\"deprel_ids\"][i][\n",
    "                                                                          j] != -100 else \"None\"\n",
    "        word_mapping = word_ids[j]\n",
    "\n",
    "        token_str = token if token else \"None\"\n",
    "        token_id = str(token_id)\n",
    "        head_str = str(head)\n",
    "        deprel_str = deprel if deprel else \"None\"\n",
    "        word_mapping_str = str(word_mapping) if word_mapping is not None else \"None\"\n",
    "\n",
    "        print(f\"{token_str:<15}{token_id:<10}{head_str:<10}{deprel_str:<15}{word_mapping_str:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb1acfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T21:51:25.338462Z",
     "start_time": "2025-01-13T21:51:20.970192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'head', 'deprel_ids', 'tokens_representing_words', 'num_words', 'tokenid_to_wordid'],\n",
      "    num_rows: 12543\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   884,    20,  ...,     1,     1,     1],\n",
       "         [    0,   378,  3293,  ...,     1,     1,     1],\n",
       "         [    0,   391, 12236,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,   581,   262,  ...,     1,     1,     1],\n",
       "         [    0, 56645, 14508,  ...,     1,     1,     1],\n",
       "         [    0,  1529,    83,  ...,     1,     1,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'head': tensor([[-100,    0,    1,  ..., -100, -100, -100],\n",
       "         [-100,   12,    3,  ..., -100, -100, -100],\n",
       "         [-100,    0, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100,    6,    6,  ..., -100, -100, -100],\n",
       "         [-100,    7, -100,  ..., -100, -100, -100],\n",
       "         [-100,   10,   10,  ..., -100, -100, -100]]),\n",
       " 'deprel_ids': tensor([[-100,   63,   61,  ..., -100, -100, -100],\n",
       "         [-100,   61,   27,  ..., -100, -100, -100],\n",
       "         [-100,   63, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100,   27,   16,  ..., -100, -100, -100],\n",
       "         [-100,   50, -100,  ..., -100, -100, -100],\n",
       "         [-100,   48,   22,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# tokenized dataset and construct dataloader\n",
    "train_tokenized_inputs = tokenize_and_align_labels(train_dataset[:], deprel_to_id, tokenizer)\n",
    "# Convert BatchEncoding to Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized_inputs.data)\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'head', 'deprel_ids'])\n",
    "print(train_dataset)\n",
    "\n",
    "valid_tokenized_inputs = tokenize_and_align_labels(valid_dataset[:], deprel_to_id, tokenizer)\n",
    "valid_dataset = Dataset.from_dict(valid_tokenized_inputs.data)\n",
    "valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'head', 'deprel_ids'])\n",
    "\n",
    "test_tokenized_inputs = tokenize_and_align_labels(test_dataset[:], deprel_to_id, tokenizer)\n",
    "test_dataset = Dataset.from_dict(test_tokenized_inputs.data)\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'head', 'deprel_ids'])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90ff586065664ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T21:51:28.750843Z",
     "start_time": "2025-01-13T21:51:28.728229Z"
    }
   },
   "outputs": [],
   "source": [
    "class DependencyParserModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size=768,  # xlm-roberta-base hidden size\n",
    "                 edge_mlp_dim=500,  # Dozat&Manning recommend 500\n",
    "                 label_mlp_dim=100,  # Dozat&Manning recommend 100\n",
    "                 num_labels=len(all_deprels),  # number of dependency labels\n",
    "                 edge_predicting=True,\n",
    "                 label_predicting=False,\n",
    "                 dropout_prob=0.33\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.edge_predicting = edge_predicting\n",
    "        self.label_predicting = label_predicting\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # load pre-trained XLM-RoBERTa model\n",
    "        self.roberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "        # freeze RoBERTa parameters\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # define MLP for edge head and dependency projections\n",
    "        if self.edge_predicting:\n",
    "            self.edge_mlp_head = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, edge_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "            self.edge_mlp_dep = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, edge_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "\n",
    "            # define U1 and u2 for edge scoring\n",
    "            self.U1 = torch.nn.Parameter(torch.empty(edge_mlp_dim, edge_mlp_dim))\n",
    "            self.u2 = torch.nn.Parameter(torch.empty(edge_mlp_dim))\n",
    "            # apply Xavier initialization\n",
    "            torch.nn.init.xavier_uniform_(self.U1)\n",
    "            torch.nn.init.xavier_uniform_(self.u2.unsqueeze(0))\n",
    "            self.u2.squeeze(0)\n",
    "\n",
    "        # Extra: Edge labels predicting\n",
    "        # define MLP for label head and dependency projections\n",
    "        if self.label_predicting:\n",
    "            self.label_mlp_head = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, label_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "            self.label_mlp_dep = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, label_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "\n",
    "            # define parameters for label scoring\n",
    "            self.U_label = torch.nn.Parameter(torch.empty(label_mlp_dim, label_mlp_dim))\n",
    "            self.W1_label = torch.nn.Parameter(torch.empty(label_mlp_dim, num_labels))\n",
    "            self.W2_label = torch.nn.Parameter(torch.empty(label_mlp_dim, num_labels))\n",
    "            self.b_label = torch.nn.Parameter(torch.empty(num_labels))\n",
    "\n",
    "            # apply Xavier initialization\n",
    "            torch.nn.init.xavier_uniform_(self.U_label)\n",
    "            torch.nn.init.xavier_uniform_(self.W1_label)\n",
    "            torch.nn.init.xavier_uniform_(self.W2_label)\n",
    "            torch.nn.init.xavier_uniform_(self.b_label.unsqueeze(0))\n",
    "            self.b_label.squeeze(0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        returns:\n",
    "          H_head (edge MLP): [batch_size, seq_len, edge_mlp_dim]\n",
    "          H_dep (edge MLP):  [batch_size, seq_len, edge_mlp_dim]\n",
    "          L_head (label MLP): [batch_size, seq_len, label_mlp_dim]\n",
    "          L_dep (label MLP):  [batch_size, seq_len, label_mlp_dim]\n",
    "        \"\"\"\n",
    "        # initialize as None\n",
    "        H_head = H_dep = L_head = L_dep = None\n",
    "\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Shape: (batch_size, seq_length, hidden_size: 768)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        if self.edge_predicting:\n",
    "            # edge MLP projections\n",
    "            H_head = self.edge_mlp_head(last_hidden_state)\n",
    "            H_dep = self.edge_mlp_dep(last_hidden_state)\n",
    "\n",
    "        if self.label_predicting:\n",
    "            # label MLP projections\n",
    "            L_head = self.label_mlp_head(last_hidden_state)\n",
    "            L_dep = self.label_mlp_dep(last_hidden_state)\n",
    "\n",
    "        return H_head, H_dep, L_head, L_dep\n",
    "\n",
    "    def score_edges(self, H_head, H_dep):\n",
    "        # score[i, j] = H_head[i] * U1 * H_dep[j].T + H_head[i] * u2\n",
    "        # b: batch_size, s: seq_len, d: edge_mlp_dim\n",
    "        H_head_U1 = torch.einsum(\"bsd,dd->bsd\", H_head, self.U1)\n",
    "        H_head_U1_H_dep = torch.einsum(\"bim,bjm->bij\", H_head_U1, H_dep)\n",
    "        H_head_u2 = torch.einsum(\"bid,d->bi\", H_head, self.u2)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # Shape: (batch_size, seq_len, seq_len) + (batch_size, seq_len, 1) broadcasting\n",
    "        scores = H_head_U1_H_dep + H_head_u2.unsqueeze(2)\n",
    "        return scores\n",
    "\n",
    "    def score_labels(self, L_head, L_dep):\n",
    "        # compute biaffine term: x1 U x2^T\n",
    "        L_head_U_L_dep = torch.einsum(\"bid,dd,bjd->bij\", L_head, self.U_label,\n",
    "                                      L_dep)  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # compute linear terms: W1 x1 and W2 x2\n",
    "        W1_L_head = torch.einsum(\"bid,dn->bin\", L_head, self.W1_label)  # Shape: (batch_size, seq_len, num_labels)\n",
    "        W2_L_dep = torch.einsum(\"bid,dn->bin\", L_dep, self.W2_label)  # Shape: (batch_size, seq_len, num_labels)\n",
    "\n",
    "        # add bias term\n",
    "        bias = self.b_label.unsqueeze(0).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 1, num_labels)\n",
    "\n",
    "        # combine all terms and expand dimensions\n",
    "        # Shape: (batch_size, seq_len, seq_len, num_labels)\n",
    "        label_scores = L_head_U_L_dep.unsqueeze(-1) + W1_L_head.unsqueeze(2) + W2_L_dep.unsqueeze(1) + bias\n",
    "\n",
    "        return label_scores\n",
    "\n",
    "\n",
    "class DependencyParserProModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size=768,  # xlm-roberta-base hidden size\n",
    "                 edge_mlp_dim=2048,  # Dozat&Manning recommend 500\n",
    "                 label_mlp_dim=100,  # Dozat&Manning recommend 100\n",
    "                 num_labels=len(all_deprels),  # number of dependency labels\n",
    "                 edge_predicting=True,\n",
    "                 label_predicting=False,\n",
    "                 dropout_prob=0.33\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.edge_predicting = edge_predicting\n",
    "        self.label_predicting = label_predicting\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # load pre-trained XLM-RoBERTa model\n",
    "        self.roberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "        # freeze RoBERTa parameters\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # define MLP for edge head and dependency projections\n",
    "        if self.edge_predicting:\n",
    "            self.edge_mlp1_head = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, edge_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "            self.edge_mlp1_dep = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, edge_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "\n",
    "            self.edge_mlp2_head = torch.nn.Sequential(\n",
    "                torch.nn.Linear(edge_mlp_dim, edge_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "\n",
    "            self.edge_mlp2_dep = torch.nn.Sequential(\n",
    "                torch.nn.Linear(edge_mlp_dim, edge_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "\n",
    "            # define U1 and u2 for edge scoring\n",
    "            self.U1 = torch.nn.Parameter(torch.empty(edge_mlp_dim, edge_mlp_dim))\n",
    "            self.u2 = torch.nn.Parameter(torch.empty(edge_mlp_dim))\n",
    "            # apply Xavier initialization\n",
    "            torch.nn.init.xavier_uniform_(self.U1)\n",
    "            torch.nn.init.xavier_uniform_(self.u2.unsqueeze(0))\n",
    "            self.u2.squeeze(0)\n",
    "\n",
    "        # Extra: Edge labels predicting\n",
    "        # define MLP for label head and dependency projections\n",
    "        if self.label_predicting:\n",
    "            self.label_mlp_head = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, label_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "            self.label_mlp_dep = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, label_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "\n",
    "            # define parameters for label scoring\n",
    "            self.U_label = torch.nn.Parameter(torch.empty(label_mlp_dim, label_mlp_dim))\n",
    "            self.W1_label = torch.nn.Parameter(torch.empty(label_mlp_dim, num_labels))\n",
    "            self.W2_label = torch.nn.Parameter(torch.empty(label_mlp_dim, num_labels))\n",
    "            self.b_label = torch.nn.Parameter(torch.empty(num_labels))\n",
    "\n",
    "            # apply Xavier initialization\n",
    "            torch.nn.init.xavier_uniform_(self.U_label)\n",
    "            torch.nn.init.xavier_uniform_(self.W1_label)\n",
    "            torch.nn.init.xavier_uniform_(self.W2_label)\n",
    "            torch.nn.init.xavier_uniform_(self.b_label.unsqueeze(0))\n",
    "            self.b_label.squeeze(0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        returns:\n",
    "          H_head (edge MLP): [batch_size, seq_len, edge_mlp_dim]\n",
    "          H_dep (edge MLP):  [batch_size, seq_len, edge_mlp_dim]\n",
    "          L_head (label MLP): [batch_size, seq_len, label_mlp_dim]\n",
    "          L_dep (label MLP):  [batch_size, seq_len, label_mlp_dim]\n",
    "        \"\"\"\n",
    "        # initialize as None\n",
    "        H_head = H_dep = L_head = L_dep = None\n",
    "\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Shape: (batch_size, seq_length, hidden_size: 768)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        if self.edge_predicting:\n",
    "            # edge MLP projections\n",
    "            H_head_1 = self.edge_mlp1_head(last_hidden_state)\n",
    "            H_dep_1 = self.edge_mlp1_dep(last_hidden_state)\n",
    "\n",
    "            # residual connection\n",
    "            H_head = self.edge_mlp2_head(H_head_1) + H_head_1\n",
    "            H_dep = self.edge_mlp2_dep(H_dep_1) + H_dep_1\n",
    "\n",
    "        if self.label_predicting:\n",
    "            # label MLP projections\n",
    "            L_head = self.label_mlp_head(last_hidden_state)\n",
    "            L_dep = self.label_mlp_dep(last_hidden_state)\n",
    "\n",
    "        return H_head, H_dep, L_head, L_dep\n",
    "\n",
    "    def score_edges(self, H_head, H_dep):\n",
    "        # score[i, j] = H_head[i] * U1 * H_dep[j].T + H_head[i] * u2\n",
    "        # b: batch_size, s: seq_len, d: edge_mlp_dim\n",
    "        H_head_U1 = torch.einsum(\"bsd,dd->bsd\", H_head, self.U1)\n",
    "        H_head_U1_H_dep = torch.einsum(\"bim,bjm->bij\", H_head_U1, H_dep)\n",
    "        H_head_u2 = torch.einsum(\"bid,d->bi\", H_head, self.u2)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # Shape: (batch_size, seq_len, seq_len) + (batch_size, seq_len, 1) broadcasting\n",
    "        scores = H_head_U1_H_dep + H_head_u2.unsqueeze(2)\n",
    "        return scores\n",
    "\n",
    "    def score_labels(self, L_head, L_dep):\n",
    "        # compute biaffine term: x1 U x2^T\n",
    "        L_head_U_L_dep = torch.einsum(\"bid,dd,bjd->bij\", L_head, self.U_label,\n",
    "                                      L_dep)  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # compute linear terms: W1 x1 and W2 x2\n",
    "        W1_L_head = torch.einsum(\"bid,dn->bin\", L_head, self.W1_label)  # Shape: (batch_size, seq_len, num_labels)\n",
    "        W2_L_dep = torch.einsum(\"bid,dn->bin\", L_dep, self.W2_label)  # Shape: (batch_size, seq_len, num_labels)\n",
    "\n",
    "        # add bias term\n",
    "        bias = self.b_label.unsqueeze(0).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 1, num_labels)\n",
    "\n",
    "        # combine all terms and expand dimensions\n",
    "        label_scores = L_head_U_L_dep.unsqueeze(-1) + W1_L_head.unsqueeze(2) + W2_L_dep.unsqueeze(\n",
    "            1) + bias  # Shape: (batch_size, seq_len, seq_len, num_labels)\n",
    "\n",
    "        return label_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a34ab69953ac7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T21:51:38.002682Z",
     "start_time": "2025-01-13T21:51:36.840617Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from ufal.chu_liu_edmonds import chu_liu_edmonds\n",
    "\n",
    "\n",
    "# train the model\n",
    "def train(model, train_dataloader, valid_dataloader, device, num_epochs=5, lr=2e-3, weight_decay=1e-2, alpha=1, beta=1):\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"dependency-parsing\", name=\"dependency-parsing\", resume=False, config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\": beta\n",
    "    })\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # ignore the padding tokens\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    valid_dataset = next(iter(valid_dataloader))\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    print(\"batch num\", len(train_dataloader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            x = data[\"input_ids\"].to(device)\n",
    "            mask = data[\"attention_mask\"].to(device)\n",
    "            head = data[\"head\"].to(device)\n",
    "\n",
    "            # Extra: Train edge labels predicting\n",
    "            if model.label_predicting:\n",
    "                label = data[\"deprel_ids\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            H_head, H_dep, L_head, L_dep = model(x, mask)\n",
    "\n",
    "            edge_scores = model.score_edges(H_head, H_dep)  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "            # Extra: Train edge labels predicting\n",
    "            if model.label_predicting:\n",
    "                label_scores = model.score_labels(L_head, L_dep)  # Shape: (batch_size, seq_len, seq_len, num_labels)\n",
    "\n",
    "            loss_edge = criterion(\n",
    "                edge_scores.view(-1, edge_scores.size(-1)),  # Shape: (batch_size * seq_len, seq_len)\n",
    "                head.view(-1)  # Shape: (batch_size, seq_len)\n",
    "            )\n",
    "\n",
    "            # Extra: Train edge labels predicting\n",
    "            loss_label = 0\n",
    "            if model.label_predicting:\n",
    "                # Shape: (batch_size, seq_len, seq_len, num_labels)\n",
    "                batch_size, n, _, num_labels = label_scores.shape\n",
    "\n",
    "                # batch_indices: (batch_size, 1)\n",
    "                # dep_indices:   (1,   n)\n",
    "                batch_indices = torch.arange(batch_size).unsqueeze(1).to(device)  # batch index\n",
    "                dep_indices = torch.arange(n).unsqueeze(0).to(device)  # dep index\n",
    "\n",
    "                # gather label_scores according to the gold head\n",
    "                # Shape: (batch_size, seq_len, num_labels)\n",
    "                label_scores_for_gold_edge = label_scores[batch_indices, dep_indices, head, :]\n",
    "\n",
    "                loss_label = criterion(label_scores_for_gold_edge.view(-1, num_labels), label.view(-1))\n",
    "\n",
    "            loss = alpha * loss_edge + beta * loss_label\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # log the loss curve\n",
    "            total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch}, Iteration {i}, Loss: {loss.item()}\")\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                # verify the model\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    x = valid_dataset[\"input_ids\"].to(device)\n",
    "                    mask = valid_dataset[\"attention_mask\"].to(device)\n",
    "                    head = valid_dataset[\"head\"].to(device)\n",
    "\n",
    "                    H_head, H_dep, L_head, L_dep = model(x, mask)\n",
    "                    edge_scores = model.score_edges(H_head, H_dep)\n",
    "\n",
    "                    valid_loss = criterion(edge_scores.view(-1, edge_scores.size(-1)), head.view(-1))\n",
    "                    print(f\"Epoch {epoch}, Iteration {i}, Valid Loss: {valid_loss}\")\n",
    "                    wandb.log({\"valid_loss\": valid_loss.item()})\n",
    "                model.train()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "        # Evaluate Head prediction\n",
    "        model.eval()\n",
    "        hta_total_count = 0  # head tagging accuracy\n",
    "        hta_total_correct = 0\n",
    "        uas_total_count = 0  # unlabeled attachment score\n",
    "        uas_total_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valid_dataloader):\n",
    "                x = data[\"input_ids\"].to(device)\n",
    "                mask = data[\"attention_mask\"].to(device)\n",
    "                head = data[\"head\"].to(device)\n",
    "\n",
    "                H_head, H_dep, L_head, L_dep = model(x, mask)\n",
    "                edge_scores = model.score_edges(H_head, H_dep)  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "                # best score heads\n",
    "                predicted_heads = edge_scores.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "\n",
    "                # construct valid mask\n",
    "                valid_positions = (head != -100) & (mask == 1)  # (batch_size, seq_len)\n",
    "\n",
    "                # count accurate predictions\n",
    "                hta_total_correct += ((predicted_heads == head) & valid_positions).sum().item()\n",
    "                hta_total_count += valid_positions.sum().item()\n",
    "\n",
    "                # MST parsing UAS evaluation\n",
    "                log_probs = torch.log_softmax(edge_scores, dim=-1)\n",
    "                # apply chu_liu_edmons\n",
    "                uas_predictions = []\n",
    "                for sent_scores in log_probs:\n",
    "                    # convert to numpy\n",
    "                    scores_np = sent_scores.cpu().double().numpy()\n",
    "                    mst_heads = chu_liu_edmonds(scores_np)\n",
    "                    uas_predictions.append(mst_heads[0])\n",
    "\n",
    "                # convert predictions and gold heads into tensors\n",
    "                uas_predictions = torch.tensor(uas_predictions, device=device)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "                # calculate correct predictions and total count\n",
    "                uas_total_correct += ((uas_predictions == head) & valid_positions).sum().item()\n",
    "                uas_total_count += valid_positions.sum().item()\n",
    "\n",
    "        model.train()\n",
    "        wandb.log({\"head_tagging_accuracy\": hta_total_correct / hta_total_count})\n",
    "        wandb.log({\"unlabeled_attachment_score\": uas_total_correct / uas_total_count})\n",
    "\n",
    "    wandb.finish()\n",
    "    # torch.save(model.state_dict(), \"dependency_parser.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d296b44d6de0f75f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T21:17:33.016575Z",
     "start_time": "2025-01-13T20:47:38.479099Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WangEntang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WangEntang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "wandb: Currently logged in as: maxewang10 (maxewang10-saarland-informatics-campus). Use `wandb login --relogin` to force relogin\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\WangEntang\\Desktop\\code\\CL\\a5\\wandb\\run-20250113_214744-iidfj4nz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/iidfj4nz' target=\"_blank\">dependency-parsing</a></strong> to <a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing' target=\"_blank\">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/iidfj4nz' target=\"_blank\">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/iidfj4nz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch num 392\n",
      "Epoch 0, Iteration 0, Loss: 5.293313026428223\n",
      "Epoch 0, Iteration 1, Loss: 4.938961505889893\n",
      "Epoch 0, Iteration 2, Loss: 4.577451705932617\n",
      "Epoch 0, Iteration 3, Loss: 4.3553547859191895\n",
      "Epoch 0, Iteration 4, Loss: 3.9377236366271973\n",
      "Epoch 0, Iteration 5, Loss: 3.726020336151123\n",
      "Epoch 0, Iteration 6, Loss: 3.594496011734009\n",
      "Epoch 0, Iteration 7, Loss: 3.61702299118042\n",
      "Epoch 0, Iteration 8, Loss: 3.552978515625\n",
      "Epoch 0, Iteration 9, Loss: 3.5524919033050537\n",
      "Epoch 0, Iteration 10, Loss: 3.2083616256713867\n",
      "Epoch 0, Iteration 11, Loss: 2.9022696018218994\n",
      "Epoch 0, Iteration 12, Loss: 3.012794256210327\n",
      "Epoch 0, Iteration 13, Loss: 3.300163984298706\n",
      "Epoch 0, Iteration 14, Loss: 3.0769410133361816\n",
      "Epoch 0, Iteration 15, Loss: 3.1394999027252197\n",
      "Epoch 0, Iteration 16, Loss: 2.8843491077423096\n",
      "Epoch 0, Iteration 17, Loss: 3.166003465652466\n",
      "Epoch 0, Iteration 18, Loss: 3.0560004711151123\n",
      "Epoch 0, Iteration 19, Loss: 3.019998073577881\n",
      "Epoch 0, Iteration 20, Loss: 3.076923370361328\n",
      "Epoch 0, Iteration 21, Loss: 3.0805788040161133\n",
      "Epoch 0, Iteration 22, Loss: 2.878321409225464\n",
      "Epoch 0, Iteration 23, Loss: 2.835618019104004\n",
      "Epoch 0, Iteration 24, Loss: 2.759072780609131\n",
      "Epoch 0, Iteration 25, Loss: 2.7681632041931152\n",
      "Epoch 0, Iteration 26, Loss: 3.0613701343536377\n",
      "Epoch 0, Iteration 27, Loss: 2.777494430541992\n",
      "Epoch 0, Iteration 28, Loss: 2.748018264770508\n",
      "Epoch 0, Iteration 29, Loss: 2.8149659633636475\n",
      "Epoch 0, Iteration 30, Loss: 2.715350389480591\n",
      "Epoch 0, Iteration 31, Loss: 2.684236764907837\n",
      "Epoch 0, Iteration 32, Loss: 2.560957193374634\n",
      "Epoch 0, Iteration 33, Loss: 2.5528969764709473\n",
      "Epoch 0, Iteration 34, Loss: 2.7786710262298584\n",
      "Epoch 0, Iteration 35, Loss: 2.8638644218444824\n",
      "Epoch 0, Iteration 36, Loss: 2.522407293319702\n",
      "Epoch 0, Iteration 37, Loss: 2.5844948291778564\n",
      "Epoch 0, Iteration 38, Loss: 2.6900365352630615\n",
      "Epoch 0, Iteration 39, Loss: 2.6801321506500244\n",
      "Epoch 0, Iteration 40, Loss: 2.8056252002716064\n",
      "Epoch 0, Iteration 41, Loss: 2.721137762069702\n",
      "Epoch 0, Iteration 42, Loss: 2.944398880004883\n",
      "Epoch 0, Iteration 43, Loss: 2.771833896636963\n",
      "Epoch 0, Iteration 44, Loss: 2.8429129123687744\n",
      "Epoch 0, Iteration 45, Loss: 2.5934054851531982\n",
      "Epoch 0, Iteration 46, Loss: 2.8014652729034424\n",
      "Epoch 0, Iteration 47, Loss: 2.780602216720581\n",
      "Epoch 0, Iteration 48, Loss: 2.7777340412139893\n",
      "Epoch 0, Iteration 49, Loss: 2.710336923599243\n",
      "Epoch 0, Iteration 50, Loss: 2.6653003692626953\n",
      "Epoch 0, Iteration 50, Valid Loss: 2.6064534187316895\n",
      "Epoch 0, Iteration 51, Loss: 2.56868314743042\n",
      "Epoch 0, Iteration 52, Loss: 2.9699039459228516\n",
      "Epoch 0, Iteration 53, Loss: 2.4082601070404053\n",
      "Epoch 0, Iteration 54, Loss: 2.6699960231781006\n",
      "Epoch 0, Iteration 55, Loss: 2.5597100257873535\n",
      "Epoch 0, Iteration 56, Loss: 2.3730883598327637\n",
      "Epoch 0, Iteration 57, Loss: 2.356968879699707\n",
      "Epoch 0, Iteration 58, Loss: 2.291525363922119\n",
      "Epoch 0, Iteration 59, Loss: 2.644200325012207\n",
      "Epoch 0, Iteration 60, Loss: 2.433483362197876\n",
      "Epoch 0, Iteration 61, Loss: 2.284102439880371\n",
      "Epoch 0, Iteration 62, Loss: 2.3244693279266357\n",
      "Epoch 0, Iteration 63, Loss: 2.4965860843658447\n",
      "Epoch 0, Iteration 64, Loss: 2.280301809310913\n",
      "Epoch 0, Iteration 65, Loss: 2.5786690711975098\n",
      "Epoch 0, Iteration 66, Loss: 2.6808342933654785\n",
      "Epoch 0, Iteration 67, Loss: 2.5278093814849854\n",
      "Epoch 0, Iteration 68, Loss: 2.424565553665161\n",
      "Epoch 0, Iteration 69, Loss: 2.439147710800171\n",
      "Epoch 0, Iteration 70, Loss: 2.6384284496307373\n",
      "Epoch 0, Iteration 71, Loss: 2.297332763671875\n",
      "Epoch 0, Iteration 72, Loss: 2.4015448093414307\n",
      "Epoch 0, Iteration 73, Loss: 2.2262609004974365\n",
      "Epoch 0, Iteration 74, Loss: 2.5135462284088135\n",
      "Epoch 0, Iteration 75, Loss: 2.2301666736602783\n",
      "Epoch 0, Iteration 76, Loss: 2.3663644790649414\n",
      "Epoch 0, Iteration 77, Loss: 2.249677896499634\n",
      "Epoch 0, Iteration 78, Loss: 2.833789110183716\n",
      "Epoch 0, Iteration 79, Loss: 2.916585683822632\n",
      "Epoch 0, Iteration 80, Loss: 2.23276424407959\n",
      "Epoch 0, Iteration 81, Loss: 2.4093916416168213\n",
      "Epoch 0, Iteration 82, Loss: 2.296265125274658\n",
      "Epoch 0, Iteration 83, Loss: 2.32222580909729\n",
      "Epoch 0, Iteration 84, Loss: 2.4565646648406982\n",
      "Epoch 0, Iteration 85, Loss: 2.5128583908081055\n",
      "Epoch 0, Iteration 86, Loss: 2.2095859050750732\n",
      "Epoch 0, Iteration 87, Loss: 2.32112979888916\n",
      "Epoch 0, Iteration 88, Loss: 2.2472739219665527\n",
      "Epoch 0, Iteration 89, Loss: 2.3849129676818848\n",
      "Epoch 0, Iteration 90, Loss: 2.256828546524048\n",
      "Epoch 0, Iteration 91, Loss: 2.211437702178955\n",
      "Epoch 0, Iteration 92, Loss: 2.269170045852661\n",
      "Epoch 0, Iteration 93, Loss: 2.2764031887054443\n",
      "Epoch 0, Iteration 94, Loss: 2.703160524368286\n",
      "Epoch 0, Iteration 95, Loss: 2.5071089267730713\n",
      "Epoch 0, Iteration 96, Loss: 2.5417723655700684\n",
      "Epoch 0, Iteration 97, Loss: 2.324132204055786\n",
      "Epoch 0, Iteration 98, Loss: 2.4947903156280518\n",
      "Epoch 0, Iteration 99, Loss: 2.3670740127563477\n",
      "Epoch 0, Iteration 100, Loss: 2.3446226119995117\n",
      "Epoch 0, Iteration 100, Valid Loss: 2.5514488220214844\n",
      "Epoch 0, Iteration 101, Loss: 2.340118885040283\n",
      "Epoch 0, Iteration 102, Loss: 2.481701612472534\n",
      "Epoch 0, Iteration 103, Loss: 2.323291778564453\n",
      "Epoch 0, Iteration 104, Loss: 2.7117552757263184\n",
      "Epoch 0, Iteration 105, Loss: 2.483217716217041\n",
      "Epoch 0, Iteration 106, Loss: 2.4380130767822266\n",
      "Epoch 0, Iteration 107, Loss: 2.6181576251983643\n",
      "Epoch 0, Iteration 108, Loss: 2.4059832096099854\n",
      "Epoch 0, Iteration 109, Loss: 2.5278470516204834\n",
      "Epoch 0, Iteration 110, Loss: 2.666189432144165\n",
      "Epoch 0, Iteration 111, Loss: 2.413231134414673\n",
      "Epoch 0, Iteration 112, Loss: 2.497483968734741\n",
      "Epoch 0, Iteration 113, Loss: 2.3996026515960693\n",
      "Epoch 0, Iteration 114, Loss: 2.7359955310821533\n",
      "Epoch 0, Iteration 115, Loss: 2.7405648231506348\n",
      "Epoch 0, Iteration 116, Loss: 2.514901876449585\n",
      "Epoch 0, Iteration 117, Loss: 2.4029345512390137\n",
      "Epoch 0, Iteration 118, Loss: 2.662109613418579\n",
      "Epoch 0, Iteration 119, Loss: 2.6180202960968018\n",
      "Epoch 0, Iteration 120, Loss: 2.2862513065338135\n",
      "Epoch 0, Iteration 121, Loss: 2.3711495399475098\n",
      "Epoch 0, Iteration 122, Loss: 2.290818452835083\n",
      "Epoch 0, Iteration 123, Loss: 2.287122964859009\n",
      "Epoch 0, Iteration 124, Loss: 2.5292773246765137\n",
      "Epoch 0, Iteration 125, Loss: 2.092794418334961\n",
      "Epoch 0, Iteration 126, Loss: 2.256470203399658\n",
      "Epoch 0, Iteration 127, Loss: 2.3715367317199707\n",
      "Epoch 0, Iteration 128, Loss: 2.160008668899536\n",
      "Epoch 0, Iteration 129, Loss: 2.330143451690674\n",
      "Epoch 0, Iteration 130, Loss: 2.4970309734344482\n",
      "Epoch 0, Iteration 131, Loss: 2.4479825496673584\n",
      "Epoch 0, Iteration 132, Loss: 2.2158830165863037\n",
      "Epoch 0, Iteration 133, Loss: 2.2479283809661865\n",
      "Epoch 0, Iteration 134, Loss: 2.0564258098602295\n",
      "Epoch 0, Iteration 135, Loss: 2.0638434886932373\n",
      "Epoch 0, Iteration 136, Loss: 2.270381212234497\n",
      "Epoch 0, Iteration 137, Loss: 2.2318074703216553\n",
      "Epoch 0, Iteration 138, Loss: 2.074496030807495\n",
      "Epoch 0, Iteration 139, Loss: 2.0984320640563965\n",
      "Epoch 0, Iteration 140, Loss: 2.2577767372131348\n",
      "Epoch 0, Iteration 141, Loss: 2.3767600059509277\n",
      "Epoch 0, Iteration 142, Loss: 2.1324074268341064\n",
      "Epoch 0, Iteration 143, Loss: 2.453594207763672\n",
      "Epoch 0, Iteration 144, Loss: 2.2677459716796875\n",
      "Epoch 0, Iteration 145, Loss: 2.2690703868865967\n",
      "Epoch 0, Iteration 146, Loss: 2.361482620239258\n",
      "Epoch 0, Iteration 147, Loss: 2.280294418334961\n",
      "Epoch 0, Iteration 148, Loss: 2.1133742332458496\n",
      "Epoch 0, Iteration 149, Loss: 2.1369235515594482\n",
      "Epoch 0, Iteration 150, Loss: 2.172049045562744\n",
      "Epoch 0, Iteration 150, Valid Loss: 2.4276912212371826\n",
      "Epoch 0, Iteration 151, Loss: 2.499953031539917\n",
      "Epoch 0, Iteration 152, Loss: 2.3839824199676514\n",
      "Epoch 0, Iteration 153, Loss: 1.9656903743743896\n",
      "Epoch 0, Iteration 154, Loss: 2.194467306137085\n",
      "Epoch 0, Iteration 155, Loss: 2.2880144119262695\n",
      "Epoch 0, Iteration 156, Loss: 2.23624587059021\n",
      "Epoch 0, Iteration 157, Loss: 2.591784715652466\n",
      "Epoch 0, Iteration 158, Loss: 2.387669563293457\n",
      "Epoch 0, Iteration 159, Loss: 2.527704954147339\n",
      "Epoch 0, Iteration 160, Loss: 2.3544490337371826\n",
      "Epoch 0, Iteration 161, Loss: 2.293950319290161\n",
      "Epoch 0, Iteration 162, Loss: 2.486067056655884\n",
      "Epoch 0, Iteration 163, Loss: 2.169045925140381\n",
      "Epoch 0, Iteration 164, Loss: 1.7828142642974854\n",
      "Epoch 0, Iteration 165, Loss: 2.172534704208374\n",
      "Epoch 0, Iteration 166, Loss: 2.2877860069274902\n",
      "Epoch 0, Iteration 167, Loss: 2.4565634727478027\n",
      "Epoch 0, Iteration 168, Loss: 2.5085878372192383\n",
      "Epoch 0, Iteration 169, Loss: 2.3206441402435303\n",
      "Epoch 0, Iteration 170, Loss: 2.2841811180114746\n",
      "Epoch 0, Iteration 171, Loss: 2.1481971740722656\n",
      "Epoch 0, Iteration 172, Loss: 2.0225608348846436\n",
      "Epoch 0, Iteration 173, Loss: 2.3883721828460693\n",
      "Epoch 0, Iteration 174, Loss: 2.3886897563934326\n",
      "Epoch 0, Iteration 175, Loss: 2.366122245788574\n",
      "Epoch 0, Iteration 176, Loss: 1.949499487876892\n",
      "Epoch 0, Iteration 177, Loss: 2.464360237121582\n",
      "Epoch 0, Iteration 178, Loss: 2.314805269241333\n",
      "Epoch 0, Iteration 179, Loss: 2.3957464694976807\n",
      "Epoch 0, Iteration 180, Loss: 2.2168140411376953\n",
      "Epoch 0, Iteration 181, Loss: 2.3578686714172363\n",
      "Epoch 0, Iteration 182, Loss: 2.338355302810669\n",
      "Epoch 0, Iteration 183, Loss: 2.137802839279175\n",
      "Epoch 0, Iteration 184, Loss: 2.0827293395996094\n",
      "Epoch 0, Iteration 185, Loss: 2.3388919830322266\n",
      "Epoch 0, Iteration 186, Loss: 2.181838035583496\n",
      "Epoch 0, Iteration 187, Loss: 2.0503289699554443\n",
      "Epoch 0, Iteration 188, Loss: 2.3367087841033936\n",
      "Epoch 0, Iteration 189, Loss: 2.261646032333374\n",
      "Epoch 0, Iteration 190, Loss: 2.6076512336730957\n",
      "Epoch 0, Iteration 191, Loss: 2.397829294204712\n",
      "Epoch 0, Iteration 192, Loss: 2.3361001014709473\n",
      "Epoch 0, Iteration 193, Loss: 2.1881871223449707\n",
      "Epoch 0, Iteration 194, Loss: 2.1382577419281006\n",
      "Epoch 0, Iteration 195, Loss: 1.918623447418213\n",
      "Epoch 0, Iteration 196, Loss: 2.2002017498016357\n",
      "Epoch 0, Iteration 197, Loss: 2.1321189403533936\n",
      "Epoch 0, Iteration 198, Loss: 2.180223226547241\n",
      "Epoch 0, Iteration 199, Loss: 2.3499999046325684\n",
      "Epoch 0, Iteration 200, Loss: 2.1889548301696777\n",
      "Epoch 0, Iteration 200, Valid Loss: 2.1419835090637207\n",
      "Epoch 0, Iteration 201, Loss: 2.205228328704834\n",
      "Epoch 0, Iteration 202, Loss: 2.232635974884033\n",
      "Epoch 0, Iteration 203, Loss: 2.0964927673339844\n",
      "Epoch 0, Iteration 204, Loss: 2.189911127090454\n",
      "Epoch 0, Iteration 205, Loss: 1.9676791429519653\n",
      "Epoch 0, Iteration 206, Loss: 1.994202971458435\n",
      "Epoch 0, Iteration 207, Loss: 2.1347320079803467\n",
      "Epoch 0, Iteration 208, Loss: 2.4803714752197266\n",
      "Epoch 0, Iteration 209, Loss: 2.442138671875\n",
      "Epoch 0, Iteration 210, Loss: 2.6462628841400146\n",
      "Epoch 0, Iteration 211, Loss: 2.8036859035491943\n",
      "Epoch 0, Iteration 212, Loss: 2.636012554168701\n",
      "Epoch 0, Iteration 213, Loss: 2.6374146938323975\n",
      "Epoch 0, Iteration 214, Loss: 2.2149617671966553\n",
      "Epoch 0, Iteration 215, Loss: 2.1822621822357178\n",
      "Epoch 0, Iteration 216, Loss: 2.1373696327209473\n",
      "Epoch 0, Iteration 217, Loss: 2.291308641433716\n",
      "Epoch 0, Iteration 218, Loss: 2.5046510696411133\n",
      "Epoch 0, Iteration 219, Loss: 2.4112002849578857\n",
      "Epoch 0, Iteration 220, Loss: 2.334226608276367\n",
      "Epoch 0, Iteration 221, Loss: 2.24833083152771\n",
      "Epoch 0, Iteration 222, Loss: 2.208850860595703\n",
      "Epoch 0, Iteration 223, Loss: 2.331615447998047\n",
      "Epoch 0, Iteration 224, Loss: 2.0270731449127197\n",
      "Epoch 0, Iteration 225, Loss: 2.2430036067962646\n",
      "Epoch 0, Iteration 226, Loss: 2.250755548477173\n",
      "Epoch 0, Iteration 227, Loss: 2.2107889652252197\n",
      "Epoch 0, Iteration 228, Loss: 1.9636038541793823\n",
      "Epoch 0, Iteration 229, Loss: 1.926635503768921\n",
      "Epoch 0, Iteration 230, Loss: 2.14142107963562\n",
      "Epoch 0, Iteration 231, Loss: 2.0160539150238037\n",
      "Epoch 0, Iteration 232, Loss: 2.1410484313964844\n",
      "Epoch 0, Iteration 233, Loss: 2.506150007247925\n",
      "Epoch 0, Iteration 234, Loss: 2.424654006958008\n",
      "Epoch 0, Iteration 235, Loss: 1.8088078498840332\n",
      "Epoch 0, Iteration 236, Loss: 2.0734500885009766\n",
      "Epoch 0, Iteration 237, Loss: 2.1672515869140625\n",
      "Epoch 0, Iteration 238, Loss: 1.8800846338272095\n",
      "Epoch 0, Iteration 239, Loss: 1.935028076171875\n",
      "Epoch 0, Iteration 240, Loss: 1.5924761295318604\n",
      "Epoch 0, Iteration 241, Loss: 1.887436866760254\n",
      "Epoch 0, Iteration 242, Loss: 1.6820483207702637\n",
      "Epoch 0, Iteration 243, Loss: 1.8892682790756226\n",
      "Epoch 0, Iteration 244, Loss: 1.8640919923782349\n",
      "Epoch 0, Iteration 245, Loss: 1.6854296922683716\n",
      "Epoch 0, Iteration 246, Loss: 2.0480403900146484\n",
      "Epoch 0, Iteration 247, Loss: 1.7402540445327759\n",
      "Epoch 0, Iteration 248, Loss: 2.0227530002593994\n",
      "Epoch 0, Iteration 249, Loss: 1.8538298606872559\n",
      "Epoch 0, Iteration 250, Loss: 2.125882148742676\n",
      "Epoch 0, Iteration 250, Valid Loss: 2.023029327392578\n",
      "Epoch 0, Iteration 251, Loss: 2.041682243347168\n",
      "Epoch 0, Iteration 252, Loss: 1.913730502128601\n",
      "Epoch 0, Iteration 253, Loss: 2.1290791034698486\n",
      "Epoch 0, Iteration 254, Loss: 2.0507595539093018\n",
      "Epoch 0, Iteration 255, Loss: 1.9358904361724854\n",
      "Epoch 0, Iteration 256, Loss: 2.2654871940612793\n",
      "Epoch 0, Iteration 257, Loss: 1.8250765800476074\n",
      "Epoch 0, Iteration 258, Loss: 2.124004602432251\n",
      "Epoch 0, Iteration 259, Loss: 1.7378073930740356\n",
      "Epoch 0, Iteration 260, Loss: 1.9023566246032715\n",
      "Epoch 0, Iteration 261, Loss: 2.116755485534668\n",
      "Epoch 0, Iteration 262, Loss: 1.7549920082092285\n",
      "Epoch 0, Iteration 263, Loss: 1.9981178045272827\n",
      "Epoch 0, Iteration 264, Loss: 2.0859603881835938\n",
      "Epoch 0, Iteration 265, Loss: 2.009338855743408\n",
      "Epoch 0, Iteration 266, Loss: 2.068153142929077\n",
      "Epoch 0, Iteration 267, Loss: 2.0442473888397217\n",
      "Epoch 0, Iteration 268, Loss: 1.986254334449768\n",
      "Epoch 0, Iteration 269, Loss: 1.9184871912002563\n",
      "Epoch 0, Iteration 270, Loss: 1.8496888875961304\n",
      "Epoch 0, Iteration 271, Loss: 1.9402034282684326\n",
      "Epoch 0, Iteration 272, Loss: 1.8875823020935059\n",
      "Epoch 0, Iteration 273, Loss: 1.7126376628875732\n",
      "Epoch 0, Iteration 274, Loss: 1.8457136154174805\n",
      "Epoch 0, Iteration 275, Loss: 1.5788633823394775\n",
      "Epoch 0, Iteration 276, Loss: 1.8142988681793213\n",
      "Epoch 0, Iteration 277, Loss: 1.7650282382965088\n",
      "Epoch 0, Iteration 278, Loss: 1.7697440385818481\n",
      "Epoch 0, Iteration 279, Loss: 1.740836501121521\n",
      "Epoch 0, Iteration 280, Loss: 1.593930721282959\n",
      "Epoch 0, Iteration 281, Loss: 1.6326850652694702\n",
      "Epoch 0, Iteration 282, Loss: 1.9510093927383423\n",
      "Epoch 0, Iteration 283, Loss: 1.8057465553283691\n",
      "Epoch 0, Iteration 284, Loss: 1.7617456912994385\n",
      "Epoch 0, Iteration 285, Loss: 1.9120334386825562\n",
      "Epoch 0, Iteration 286, Loss: 1.7078251838684082\n",
      "Epoch 0, Iteration 287, Loss: 1.7242710590362549\n",
      "Epoch 0, Iteration 288, Loss: 1.7243287563323975\n",
      "Epoch 0, Iteration 289, Loss: 1.9321365356445312\n",
      "Epoch 0, Iteration 290, Loss: 1.8245257139205933\n",
      "Epoch 0, Iteration 291, Loss: 1.9098973274230957\n",
      "Epoch 0, Iteration 292, Loss: 2.1271586418151855\n",
      "Epoch 0, Iteration 293, Loss: 1.9822949171066284\n",
      "Epoch 0, Iteration 294, Loss: 1.7233569622039795\n",
      "Epoch 0, Iteration 295, Loss: 1.8123167753219604\n",
      "Epoch 0, Iteration 296, Loss: 2.257172107696533\n",
      "Epoch 0, Iteration 297, Loss: 1.9194445610046387\n",
      "Epoch 0, Iteration 298, Loss: 1.8757659196853638\n",
      "Epoch 0, Iteration 299, Loss: 1.6949089765548706\n",
      "Epoch 0, Iteration 300, Loss: 1.704309105873108\n",
      "Epoch 0, Iteration 300, Valid Loss: 1.9237076044082642\n",
      "Epoch 0, Iteration 301, Loss: 1.8295944929122925\n",
      "Epoch 0, Iteration 302, Loss: 1.6169710159301758\n",
      "Epoch 0, Iteration 303, Loss: 2.1130688190460205\n",
      "Epoch 0, Iteration 304, Loss: 1.799018383026123\n",
      "Epoch 0, Iteration 305, Loss: 1.750558614730835\n",
      "Epoch 0, Iteration 306, Loss: 1.8114299774169922\n",
      "Epoch 0, Iteration 307, Loss: 1.656670331954956\n",
      "Epoch 0, Iteration 308, Loss: 1.7673022747039795\n",
      "Epoch 0, Iteration 309, Loss: 1.814821720123291\n",
      "Epoch 0, Iteration 310, Loss: 2.0571537017822266\n",
      "Epoch 0, Iteration 311, Loss: 1.6968293190002441\n",
      "Epoch 0, Iteration 312, Loss: 1.6636865139007568\n",
      "Epoch 0, Iteration 313, Loss: 1.7652333974838257\n",
      "Epoch 0, Iteration 314, Loss: 1.7469030618667603\n",
      "Epoch 0, Iteration 315, Loss: 1.6532225608825684\n",
      "Epoch 0, Iteration 316, Loss: 1.8241174221038818\n",
      "Epoch 0, Iteration 317, Loss: 1.7290523052215576\n",
      "Epoch 0, Iteration 318, Loss: 1.6914018392562866\n",
      "Epoch 0, Iteration 319, Loss: 1.7496005296707153\n",
      "Epoch 0, Iteration 320, Loss: 1.538475513458252\n",
      "Epoch 0, Iteration 321, Loss: 1.663192629814148\n",
      "Epoch 0, Iteration 322, Loss: 1.5933095216751099\n",
      "Epoch 0, Iteration 323, Loss: 1.7164344787597656\n",
      "Epoch 0, Iteration 324, Loss: 1.7819197177886963\n",
      "Epoch 0, Iteration 325, Loss: 1.7330840826034546\n",
      "Epoch 0, Iteration 326, Loss: 1.766784429550171\n",
      "Epoch 0, Iteration 327, Loss: 1.6357545852661133\n",
      "Epoch 0, Iteration 328, Loss: 1.6366081237792969\n",
      "Epoch 0, Iteration 329, Loss: 1.9593640565872192\n",
      "Epoch 0, Iteration 330, Loss: 1.4734503030776978\n",
      "Epoch 0, Iteration 331, Loss: 1.5703327655792236\n",
      "Epoch 0, Iteration 332, Loss: 1.86098313331604\n",
      "Epoch 0, Iteration 333, Loss: 1.8669559955596924\n",
      "Epoch 0, Iteration 334, Loss: 1.6712007522583008\n",
      "Epoch 0, Iteration 335, Loss: 1.851320505142212\n",
      "Epoch 0, Iteration 336, Loss: 1.8040916919708252\n",
      "Epoch 0, Iteration 337, Loss: 1.6060833930969238\n",
      "Epoch 0, Iteration 338, Loss: 1.895801067352295\n",
      "Epoch 0, Iteration 339, Loss: 1.5841431617736816\n",
      "Epoch 0, Iteration 340, Loss: 1.7336093187332153\n",
      "Epoch 0, Iteration 341, Loss: 1.726702094078064\n",
      "Epoch 0, Iteration 342, Loss: 1.5982176065444946\n",
      "Epoch 0, Iteration 343, Loss: 1.732932209968567\n",
      "Epoch 0, Iteration 344, Loss: 1.8160399198532104\n",
      "Epoch 0, Iteration 345, Loss: 1.8418740034103394\n",
      "Epoch 0, Iteration 346, Loss: 1.7283729314804077\n",
      "Epoch 0, Iteration 347, Loss: 1.5126830339431763\n",
      "Epoch 0, Iteration 348, Loss: 1.6131671667099\n",
      "Epoch 0, Iteration 349, Loss: 1.8045183420181274\n",
      "Epoch 0, Iteration 350, Loss: 1.5177820920944214\n",
      "Epoch 0, Iteration 350, Valid Loss: 1.8072201013565063\n",
      "Epoch 0, Iteration 351, Loss: 1.725433111190796\n",
      "Epoch 0, Iteration 352, Loss: 1.6450833082199097\n",
      "Epoch 0, Iteration 353, Loss: 1.8080049753189087\n",
      "Epoch 0, Iteration 354, Loss: 1.6027326583862305\n",
      "Epoch 0, Iteration 355, Loss: 1.6214185953140259\n",
      "Epoch 0, Iteration 356, Loss: 1.7500624656677246\n",
      "Epoch 0, Iteration 357, Loss: 1.6895995140075684\n",
      "Epoch 0, Iteration 358, Loss: 1.588383436203003\n",
      "Epoch 0, Iteration 359, Loss: 1.632638931274414\n",
      "Epoch 0, Iteration 360, Loss: 1.4760310649871826\n",
      "Epoch 0, Iteration 361, Loss: 1.5375547409057617\n",
      "Epoch 0, Iteration 362, Loss: 1.660813570022583\n",
      "Epoch 0, Iteration 363, Loss: 1.5799858570098877\n",
      "Epoch 0, Iteration 364, Loss: 1.6078345775604248\n",
      "Epoch 0, Iteration 365, Loss: 2.014436721801758\n",
      "Epoch 0, Iteration 366, Loss: 1.8757460117340088\n",
      "Epoch 0, Iteration 367, Loss: 1.8273742198944092\n",
      "Epoch 0, Iteration 368, Loss: 1.7303125858306885\n",
      "Epoch 0, Iteration 369, Loss: 1.7091752290725708\n",
      "Epoch 0, Iteration 370, Loss: 1.7201834917068481\n",
      "Epoch 0, Iteration 371, Loss: 1.765328288078308\n",
      "Epoch 0, Iteration 372, Loss: 1.4966439008712769\n",
      "Epoch 0, Iteration 373, Loss: 1.5061320066452026\n",
      "Epoch 0, Iteration 374, Loss: 1.7286475896835327\n",
      "Epoch 0, Iteration 375, Loss: 1.5949506759643555\n",
      "Epoch 0, Iteration 376, Loss: 1.8836863040924072\n",
      "Epoch 0, Iteration 377, Loss: 1.732139229774475\n",
      "Epoch 0, Iteration 378, Loss: 1.5353249311447144\n",
      "Epoch 0, Iteration 379, Loss: 1.6448931694030762\n",
      "Epoch 0, Iteration 380, Loss: 1.6106901168823242\n",
      "Epoch 0, Iteration 381, Loss: 1.5368834733963013\n",
      "Epoch 0, Iteration 382, Loss: 1.4323704242706299\n",
      "Epoch 0, Iteration 383, Loss: 1.6513986587524414\n",
      "Epoch 0, Iteration 384, Loss: 1.6585413217544556\n",
      "Epoch 0, Iteration 385, Loss: 1.5207016468048096\n",
      "Epoch 0, Iteration 386, Loss: 1.5538904666900635\n",
      "Epoch 0, Iteration 387, Loss: 1.9006428718566895\n",
      "Epoch 0, Iteration 388, Loss: 1.6571624279022217\n",
      "Epoch 0, Iteration 389, Loss: 1.8574897050857544\n",
      "Epoch 0, Iteration 390, Loss: 1.7800096273422241\n",
      "Epoch 0, Iteration 391, Loss: 1.5816171169281006\n",
      "Epoch 1/15, Loss: 2.213832358316499\n",
      "Epoch 1, Iteration 0, Loss: 1.766051173210144\n",
      "Epoch 1, Iteration 1, Loss: 1.995905876159668\n",
      "Epoch 1, Iteration 2, Loss: 2.077378511428833\n",
      "Epoch 1, Iteration 3, Loss: 1.8598264455795288\n",
      "Epoch 1, Iteration 4, Loss: 1.796717643737793\n",
      "Epoch 1, Iteration 5, Loss: 1.871799349784851\n",
      "Epoch 1, Iteration 6, Loss: 1.6857738494873047\n",
      "Epoch 1, Iteration 7, Loss: 1.843631386756897\n",
      "Epoch 1, Iteration 8, Loss: 1.9296656847000122\n",
      "Epoch 1, Iteration 9, Loss: 2.1211795806884766\n",
      "Epoch 1, Iteration 10, Loss: 1.8755935430526733\n",
      "Epoch 1, Iteration 11, Loss: 1.7603155374526978\n",
      "Epoch 1, Iteration 12, Loss: 1.5610754489898682\n",
      "Epoch 1, Iteration 13, Loss: 1.9065208435058594\n",
      "Epoch 1, Iteration 14, Loss: 1.649802327156067\n",
      "Epoch 1, Iteration 15, Loss: 1.9151707887649536\n",
      "Epoch 1, Iteration 16, Loss: 1.622831106185913\n",
      "Epoch 1, Iteration 17, Loss: 1.926804542541504\n",
      "Epoch 1, Iteration 18, Loss: 1.838024377822876\n",
      "Epoch 1, Iteration 19, Loss: 1.8419631719589233\n",
      "Epoch 1, Iteration 20, Loss: 1.7749192714691162\n",
      "Epoch 1, Iteration 21, Loss: 1.7426409721374512\n",
      "Epoch 1, Iteration 22, Loss: 1.6674745082855225\n",
      "Epoch 1, Iteration 23, Loss: 1.6598283052444458\n",
      "Epoch 1, Iteration 24, Loss: 1.6037359237670898\n",
      "Epoch 1, Iteration 25, Loss: 1.6045829057693481\n",
      "Epoch 1, Iteration 26, Loss: 2.021451950073242\n",
      "Epoch 1, Iteration 27, Loss: 1.624883770942688\n",
      "Epoch 1, Iteration 28, Loss: 1.6985481977462769\n",
      "Epoch 1, Iteration 29, Loss: 1.840247631072998\n",
      "Epoch 1, Iteration 30, Loss: 1.7929288148880005\n",
      "Epoch 1, Iteration 31, Loss: 1.692771315574646\n",
      "Epoch 1, Iteration 32, Loss: 1.6320322751998901\n",
      "Epoch 1, Iteration 33, Loss: 1.5922681093215942\n",
      "Epoch 1, Iteration 34, Loss: 1.724204421043396\n",
      "Epoch 1, Iteration 35, Loss: 1.759735107421875\n",
      "Epoch 1, Iteration 36, Loss: 1.5230857133865356\n",
      "Epoch 1, Iteration 37, Loss: 1.5826420783996582\n",
      "Epoch 1, Iteration 38, Loss: 1.5893726348876953\n",
      "Epoch 1, Iteration 39, Loss: 1.6632965803146362\n",
      "Epoch 1, Iteration 40, Loss: 1.6959290504455566\n",
      "Epoch 1, Iteration 41, Loss: 1.7598353624343872\n",
      "Epoch 1, Iteration 42, Loss: 1.882830262184143\n",
      "Epoch 1, Iteration 43, Loss: 1.623390555381775\n",
      "Epoch 1, Iteration 44, Loss: 1.7256306409835815\n",
      "Epoch 1, Iteration 45, Loss: 1.5825321674346924\n",
      "Epoch 1, Iteration 46, Loss: 1.7038308382034302\n",
      "Epoch 1, Iteration 47, Loss: 1.7513830661773682\n",
      "Epoch 1, Iteration 48, Loss: 1.8915873765945435\n",
      "Epoch 1, Iteration 49, Loss: 1.6761095523834229\n",
      "Epoch 1, Iteration 50, Loss: 1.6401169300079346\n",
      "Epoch 1, Iteration 50, Valid Loss: 1.4485353231430054\n",
      "Epoch 1, Iteration 51, Loss: 1.6952046155929565\n",
      "Epoch 1, Iteration 52, Loss: 1.8450453281402588\n",
      "Epoch 1, Iteration 53, Loss: 1.6102286577224731\n",
      "Epoch 1, Iteration 54, Loss: 1.833138108253479\n",
      "Epoch 1, Iteration 55, Loss: 1.7322819232940674\n",
      "Epoch 1, Iteration 56, Loss: 1.4699536561965942\n",
      "Epoch 1, Iteration 57, Loss: 1.527243733406067\n",
      "Epoch 1, Iteration 58, Loss: 1.4674607515335083\n",
      "Epoch 1, Iteration 59, Loss: 1.6903953552246094\n",
      "Epoch 1, Iteration 60, Loss: 1.639920711517334\n",
      "Epoch 1, Iteration 61, Loss: 1.3956018686294556\n",
      "Epoch 1, Iteration 62, Loss: 1.476341724395752\n",
      "Epoch 1, Iteration 63, Loss: 1.505836009979248\n",
      "Epoch 1, Iteration 64, Loss: 1.4305388927459717\n",
      "Epoch 1, Iteration 65, Loss: 1.6909452676773071\n",
      "Epoch 1, Iteration 66, Loss: 1.6692086458206177\n",
      "Epoch 1, Iteration 67, Loss: 1.5091278553009033\n",
      "Epoch 1, Iteration 68, Loss: 1.5649927854537964\n",
      "Epoch 1, Iteration 69, Loss: 1.8664857149124146\n",
      "Epoch 1, Iteration 70, Loss: 1.7916505336761475\n",
      "Epoch 1, Iteration 71, Loss: 1.5364677906036377\n",
      "Epoch 1, Iteration 72, Loss: 1.6780565977096558\n",
      "Epoch 1, Iteration 73, Loss: 1.382109522819519\n",
      "Epoch 1, Iteration 74, Loss: 1.7476664781570435\n",
      "Epoch 1, Iteration 75, Loss: 1.484573483467102\n",
      "Epoch 1, Iteration 76, Loss: 1.5339640378952026\n",
      "Epoch 1, Iteration 77, Loss: 1.285889983177185\n",
      "Epoch 1, Iteration 78, Loss: 1.9429113864898682\n",
      "Epoch 1, Iteration 79, Loss: 2.018778085708618\n",
      "Epoch 1, Iteration 80, Loss: 1.3581987619400024\n",
      "Epoch 1, Iteration 81, Loss: 1.7866790294647217\n",
      "Epoch 1, Iteration 82, Loss: 1.5770288705825806\n",
      "Epoch 1, Iteration 83, Loss: 1.3659193515777588\n",
      "Epoch 1, Iteration 84, Loss: 1.6160286664962769\n",
      "Epoch 1, Iteration 85, Loss: 1.7292660474777222\n",
      "Epoch 1, Iteration 86, Loss: 1.4697386026382446\n",
      "Epoch 1, Iteration 87, Loss: 1.361825942993164\n",
      "Epoch 1, Iteration 88, Loss: 1.4253780841827393\n",
      "Epoch 1, Iteration 89, Loss: 1.8314329385757446\n",
      "Epoch 1, Iteration 90, Loss: 1.4844286441802979\n",
      "Epoch 1, Iteration 91, Loss: 1.4202626943588257\n",
      "Epoch 1, Iteration 92, Loss: 1.332923412322998\n",
      "Epoch 1, Iteration 93, Loss: 1.410105586051941\n",
      "Epoch 1, Iteration 94, Loss: 1.8765732049942017\n",
      "Epoch 1, Iteration 95, Loss: 1.5829365253448486\n",
      "Epoch 1, Iteration 96, Loss: 1.6646318435668945\n",
      "Epoch 1, Iteration 97, Loss: 1.428951382637024\n",
      "Epoch 1, Iteration 98, Loss: 1.6076074838638306\n",
      "Epoch 1, Iteration 99, Loss: 1.5966817140579224\n",
      "Epoch 1, Iteration 100, Loss: 1.3975200653076172\n",
      "Epoch 1, Iteration 100, Valid Loss: 1.4718574285507202\n",
      "Epoch 1, Iteration 101, Loss: 1.5891870260238647\n",
      "Epoch 1, Iteration 102, Loss: 1.5884796380996704\n",
      "Epoch 1, Iteration 103, Loss: 1.460829496383667\n",
      "Epoch 1, Iteration 104, Loss: 1.8132630586624146\n",
      "Epoch 1, Iteration 105, Loss: 1.6424587965011597\n",
      "Epoch 1, Iteration 106, Loss: 1.6085997819900513\n",
      "Epoch 1, Iteration 107, Loss: 2.0941970348358154\n",
      "Epoch 1, Iteration 108, Loss: 1.4833672046661377\n",
      "Epoch 1, Iteration 109, Loss: 1.804589867591858\n",
      "Epoch 1, Iteration 110, Loss: 1.8855973482131958\n",
      "Epoch 1, Iteration 111, Loss: 1.6610299348831177\n",
      "Epoch 1, Iteration 112, Loss: 1.6017212867736816\n",
      "Epoch 1, Iteration 113, Loss: 1.4737682342529297\n",
      "Epoch 1, Iteration 114, Loss: 2.016113758087158\n",
      "Epoch 1, Iteration 115, Loss: 1.7362616062164307\n",
      "Epoch 1, Iteration 116, Loss: 1.494315266609192\n",
      "Epoch 1, Iteration 117, Loss: 1.3312722444534302\n",
      "Epoch 1, Iteration 118, Loss: 1.6696009635925293\n",
      "Epoch 1, Iteration 119, Loss: 1.5295803546905518\n",
      "Epoch 1, Iteration 120, Loss: 1.5091044902801514\n",
      "Epoch 1, Iteration 121, Loss: 1.4985928535461426\n",
      "Epoch 1, Iteration 122, Loss: 1.505478024482727\n",
      "Epoch 1, Iteration 123, Loss: 1.465248942375183\n",
      "Epoch 1, Iteration 124, Loss: 1.5969284772872925\n",
      "Epoch 1, Iteration 125, Loss: 1.4136368036270142\n",
      "Epoch 1, Iteration 126, Loss: 1.53896164894104\n",
      "Epoch 1, Iteration 127, Loss: 1.7722907066345215\n",
      "Epoch 1, Iteration 128, Loss: 1.5288488864898682\n",
      "Epoch 1, Iteration 129, Loss: 1.63235604763031\n",
      "Epoch 1, Iteration 130, Loss: 2.052145481109619\n",
      "Epoch 1, Iteration 131, Loss: 1.5146156549453735\n",
      "Epoch 1, Iteration 132, Loss: 1.4045555591583252\n",
      "Epoch 1, Iteration 133, Loss: 1.6433234214782715\n",
      "Epoch 1, Iteration 134, Loss: 1.4045995473861694\n",
      "Epoch 1, Iteration 135, Loss: 1.5386195182800293\n",
      "Epoch 1, Iteration 136, Loss: 1.622236967086792\n",
      "Epoch 1, Iteration 137, Loss: 1.4877113103866577\n",
      "Epoch 1, Iteration 138, Loss: 1.5546910762786865\n",
      "Epoch 1, Iteration 139, Loss: 1.4438363313674927\n",
      "Epoch 1, Iteration 140, Loss: 1.4746103286743164\n",
      "Epoch 1, Iteration 141, Loss: 1.9042840003967285\n",
      "Epoch 1, Iteration 142, Loss: 1.439428687095642\n",
      "Epoch 1, Iteration 143, Loss: 1.5879064798355103\n",
      "Epoch 1, Iteration 144, Loss: 1.4717854261398315\n",
      "Epoch 1, Iteration 145, Loss: 1.4891995191574097\n",
      "Epoch 1, Iteration 146, Loss: 1.6075427532196045\n",
      "Epoch 1, Iteration 147, Loss: 1.4039924144744873\n",
      "Epoch 1, Iteration 148, Loss: 1.3372949361801147\n",
      "Epoch 1, Iteration 149, Loss: 1.4400323629379272\n",
      "Epoch 1, Iteration 150, Loss: 1.5442500114440918\n",
      "Epoch 1, Iteration 150, Valid Loss: 1.4371472597122192\n",
      "Epoch 1, Iteration 151, Loss: 1.6685450077056885\n",
      "Epoch 1, Iteration 152, Loss: 1.5474737882614136\n",
      "Epoch 1, Iteration 153, Loss: 1.7160996198654175\n",
      "Epoch 1, Iteration 154, Loss: 1.4971872568130493\n",
      "Epoch 1, Iteration 155, Loss: 1.6425502300262451\n",
      "Epoch 1, Iteration 156, Loss: 1.4629944562911987\n",
      "Epoch 1, Iteration 157, Loss: 1.9071520566940308\n",
      "Epoch 1, Iteration 158, Loss: 1.6500698328018188\n",
      "Epoch 1, Iteration 159, Loss: 1.7259255647659302\n",
      "Epoch 1, Iteration 160, Loss: 1.4649310111999512\n",
      "Epoch 1, Iteration 161, Loss: 1.4678109884262085\n",
      "Epoch 1, Iteration 162, Loss: 1.6791781187057495\n",
      "Epoch 1, Iteration 163, Loss: 1.3076502084732056\n",
      "Epoch 1, Iteration 164, Loss: 1.3697259426116943\n",
      "Epoch 1, Iteration 165, Loss: 1.3495203256607056\n",
      "Epoch 1, Iteration 166, Loss: 1.4124130010604858\n",
      "Epoch 1, Iteration 167, Loss: 1.7203644514083862\n",
      "Epoch 1, Iteration 168, Loss: 1.650087833404541\n",
      "Epoch 1, Iteration 169, Loss: 1.4999709129333496\n",
      "Epoch 1, Iteration 170, Loss: 1.663883924484253\n",
      "Epoch 1, Iteration 171, Loss: 1.445719838142395\n",
      "Epoch 1, Iteration 172, Loss: 1.4893500804901123\n",
      "Epoch 1, Iteration 173, Loss: 1.4782377481460571\n",
      "Epoch 1, Iteration 174, Loss: 1.5503530502319336\n",
      "Epoch 1, Iteration 175, Loss: 1.4921177625656128\n",
      "Epoch 1, Iteration 176, Loss: 1.2219407558441162\n",
      "Epoch 1, Iteration 177, Loss: 1.8601067066192627\n",
      "Epoch 1, Iteration 178, Loss: 1.7211894989013672\n",
      "Epoch 1, Iteration 179, Loss: 1.5489377975463867\n",
      "Epoch 1, Iteration 180, Loss: 1.3873428106307983\n",
      "Epoch 1, Iteration 181, Loss: 1.6651339530944824\n",
      "Epoch 1, Iteration 182, Loss: 1.579970359802246\n",
      "Epoch 1, Iteration 183, Loss: 1.4689141511917114\n",
      "Epoch 1, Iteration 184, Loss: 1.2910341024398804\n",
      "Epoch 1, Iteration 185, Loss: 1.4617418050765991\n",
      "Epoch 1, Iteration 186, Loss: 1.3598332405090332\n",
      "Epoch 1, Iteration 187, Loss: 1.376189947128296\n",
      "Epoch 1, Iteration 188, Loss: 1.4719971418380737\n",
      "Epoch 1, Iteration 189, Loss: 1.3989486694335938\n",
      "Epoch 1, Iteration 190, Loss: 1.8296527862548828\n",
      "Epoch 1, Iteration 191, Loss: 1.6249594688415527\n",
      "Epoch 1, Iteration 192, Loss: 1.5391182899475098\n",
      "Epoch 1, Iteration 193, Loss: 1.5239903926849365\n",
      "Epoch 1, Iteration 194, Loss: 1.4440900087356567\n",
      "Epoch 1, Iteration 195, Loss: 1.335312843322754\n",
      "Epoch 1, Iteration 196, Loss: 1.5167062282562256\n",
      "Epoch 1, Iteration 197, Loss: 1.4719812870025635\n",
      "Epoch 1, Iteration 198, Loss: 1.5395041704177856\n",
      "Epoch 1, Iteration 199, Loss: 1.6155741214752197\n",
      "Epoch 1, Iteration 200, Loss: 1.4398860931396484\n",
      "Epoch 1, Iteration 200, Valid Loss: 1.3287177085876465\n",
      "Epoch 1, Iteration 201, Loss: 1.4564586877822876\n",
      "Epoch 1, Iteration 202, Loss: 1.554866075515747\n",
      "Epoch 1, Iteration 203, Loss: 1.3892902135849\n",
      "Epoch 1, Iteration 204, Loss: 1.3520312309265137\n",
      "Epoch 1, Iteration 205, Loss: 1.1674052476882935\n",
      "Epoch 1, Iteration 206, Loss: 1.3099141120910645\n",
      "Epoch 1, Iteration 207, Loss: 1.544344425201416\n",
      "Epoch 1, Iteration 208, Loss: 1.8318696022033691\n",
      "Epoch 1, Iteration 209, Loss: 1.8722821474075317\n",
      "Epoch 1, Iteration 210, Loss: 2.009533166885376\n",
      "Epoch 1, Iteration 211, Loss: 2.0832300186157227\n",
      "Epoch 1, Iteration 212, Loss: 1.8896580934524536\n",
      "Epoch 1, Iteration 213, Loss: 1.9372700452804565\n",
      "Epoch 1, Iteration 214, Loss: 1.4576555490493774\n",
      "Epoch 1, Iteration 215, Loss: 1.436438798904419\n",
      "Epoch 1, Iteration 216, Loss: 1.4982404708862305\n",
      "Epoch 1, Iteration 217, Loss: 1.4801888465881348\n",
      "Epoch 1, Iteration 218, Loss: 1.6848866939544678\n",
      "Epoch 1, Iteration 219, Loss: 1.5623722076416016\n",
      "Epoch 1, Iteration 220, Loss: 1.5125632286071777\n",
      "Epoch 1, Iteration 221, Loss: 1.5053707361221313\n",
      "Epoch 1, Iteration 222, Loss: 1.4483927488327026\n",
      "Epoch 1, Iteration 223, Loss: 1.4626922607421875\n",
      "Epoch 1, Iteration 224, Loss: 1.2656089067459106\n",
      "Epoch 1, Iteration 225, Loss: 1.6071704626083374\n",
      "Epoch 1, Iteration 226, Loss: 1.6016559600830078\n",
      "Epoch 1, Iteration 227, Loss: 1.6246026754379272\n",
      "Epoch 1, Iteration 228, Loss: 1.3415334224700928\n",
      "Epoch 1, Iteration 229, Loss: 1.2451943159103394\n",
      "Epoch 1, Iteration 230, Loss: 1.5020453929901123\n",
      "Epoch 1, Iteration 231, Loss: 1.4825527667999268\n",
      "Epoch 1, Iteration 232, Loss: 1.4887670278549194\n",
      "Epoch 1, Iteration 233, Loss: 1.8426868915557861\n",
      "Epoch 1, Iteration 234, Loss: 1.8202035427093506\n",
      "Epoch 1, Iteration 235, Loss: 1.2147055864334106\n",
      "Epoch 1, Iteration 236, Loss: 1.4637173414230347\n",
      "Epoch 1, Iteration 237, Loss: 1.6213706731796265\n",
      "Epoch 1, Iteration 238, Loss: 1.3079365491867065\n",
      "Epoch 1, Iteration 239, Loss: 1.4553576707839966\n",
      "Epoch 1, Iteration 240, Loss: 1.233763337135315\n",
      "Epoch 1, Iteration 241, Loss: 1.3127710819244385\n",
      "Epoch 1, Iteration 242, Loss: 1.234320044517517\n",
      "Epoch 1, Iteration 243, Loss: 1.4139107465744019\n",
      "Epoch 1, Iteration 244, Loss: 1.355732798576355\n",
      "Epoch 1, Iteration 245, Loss: 1.1162971258163452\n",
      "Epoch 1, Iteration 246, Loss: 1.3589766025543213\n",
      "Epoch 1, Iteration 247, Loss: 1.2028580904006958\n",
      "Epoch 1, Iteration 248, Loss: 1.4226646423339844\n",
      "Epoch 1, Iteration 249, Loss: 1.3340651988983154\n",
      "Epoch 1, Iteration 250, Loss: 1.516063928604126\n",
      "Epoch 1, Iteration 250, Valid Loss: 1.2595428228378296\n",
      "Epoch 1, Iteration 251, Loss: 1.5025378465652466\n",
      "Epoch 1, Iteration 252, Loss: 1.6240767240524292\n",
      "Epoch 1, Iteration 253, Loss: 1.5498967170715332\n",
      "Epoch 1, Iteration 254, Loss: 1.466488003730774\n",
      "Epoch 1, Iteration 255, Loss: 1.4294192790985107\n",
      "Epoch 1, Iteration 256, Loss: 1.6481807231903076\n",
      "Epoch 1, Iteration 257, Loss: 1.3497576713562012\n",
      "Epoch 1, Iteration 258, Loss: 1.5768221616744995\n",
      "Epoch 1, Iteration 259, Loss: 1.2375949621200562\n",
      "Epoch 1, Iteration 260, Loss: 1.5093965530395508\n",
      "Epoch 1, Iteration 261, Loss: 1.602980375289917\n",
      "Epoch 1, Iteration 262, Loss: 1.139979362487793\n",
      "Epoch 1, Iteration 263, Loss: 1.4755122661590576\n",
      "Epoch 1, Iteration 264, Loss: 1.4224188327789307\n",
      "Epoch 1, Iteration 265, Loss: 1.4788140058517456\n",
      "Epoch 1, Iteration 266, Loss: 1.5071061849594116\n",
      "Epoch 1, Iteration 267, Loss: 1.410326361656189\n",
      "Epoch 1, Iteration 268, Loss: 1.3788782358169556\n",
      "Epoch 1, Iteration 269, Loss: 1.3941408395767212\n",
      "Epoch 1, Iteration 270, Loss: 1.2820186614990234\n",
      "Epoch 1, Iteration 271, Loss: 1.4065803289413452\n",
      "Epoch 1, Iteration 272, Loss: 1.3323187828063965\n",
      "Epoch 1, Iteration 273, Loss: 1.297698974609375\n",
      "Epoch 1, Iteration 274, Loss: 1.242661476135254\n",
      "Epoch 1, Iteration 275, Loss: 1.1727683544158936\n",
      "Epoch 1, Iteration 276, Loss: 1.1950795650482178\n",
      "Epoch 1, Iteration 277, Loss: 1.2908555269241333\n",
      "Epoch 1, Iteration 278, Loss: 1.4926260709762573\n",
      "Epoch 1, Iteration 279, Loss: 1.2426693439483643\n",
      "Epoch 1, Iteration 280, Loss: 1.1517571210861206\n",
      "Epoch 1, Iteration 281, Loss: 1.1941393613815308\n",
      "Epoch 1, Iteration 282, Loss: 1.3174870014190674\n",
      "Epoch 1, Iteration 283, Loss: 1.3038002252578735\n",
      "Epoch 1, Iteration 284, Loss: 1.311618447303772\n",
      "Epoch 1, Iteration 285, Loss: 1.3152694702148438\n",
      "Epoch 1, Iteration 286, Loss: 1.2771341800689697\n",
      "Epoch 1, Iteration 287, Loss: 1.2807124853134155\n",
      "Epoch 1, Iteration 288, Loss: 1.3183140754699707\n",
      "Epoch 1, Iteration 289, Loss: 1.3771862983703613\n",
      "Epoch 1, Iteration 290, Loss: 1.2496885061264038\n",
      "Epoch 1, Iteration 291, Loss: 1.3170390129089355\n",
      "Epoch 1, Iteration 292, Loss: 1.5337350368499756\n",
      "Epoch 1, Iteration 293, Loss: 1.6001254320144653\n",
      "Epoch 1, Iteration 294, Loss: 1.3099820613861084\n",
      "Epoch 1, Iteration 295, Loss: 1.2929545640945435\n",
      "Epoch 1, Iteration 296, Loss: 1.6021901369094849\n",
      "Epoch 1, Iteration 297, Loss: 1.5571036338806152\n",
      "Epoch 1, Iteration 298, Loss: 1.429477334022522\n",
      "Epoch 1, Iteration 299, Loss: 1.1914278268814087\n",
      "Epoch 1, Iteration 300, Loss: 1.1954827308654785\n",
      "Epoch 1, Iteration 300, Valid Loss: 1.2122628688812256\n",
      "Epoch 1, Iteration 301, Loss: 1.2923136949539185\n",
      "Epoch 1, Iteration 302, Loss: 1.2036954164505005\n",
      "Epoch 1, Iteration 303, Loss: 1.6799498796463013\n",
      "Epoch 1, Iteration 304, Loss: 1.2729474306106567\n",
      "Epoch 1, Iteration 305, Loss: 1.264849066734314\n",
      "Epoch 1, Iteration 306, Loss: 1.1767719984054565\n",
      "Epoch 1, Iteration 307, Loss: 1.380623459815979\n",
      "Epoch 1, Iteration 308, Loss: 1.2801258563995361\n",
      "Epoch 1, Iteration 309, Loss: 1.3980435132980347\n",
      "Epoch 1, Iteration 310, Loss: 1.4534016847610474\n",
      "Epoch 1, Iteration 311, Loss: 1.1469119787216187\n",
      "Epoch 1, Iteration 312, Loss: 1.2580870389938354\n",
      "Epoch 1, Iteration 313, Loss: 1.4177148342132568\n",
      "Epoch 1, Iteration 314, Loss: 1.2190439701080322\n",
      "Epoch 1, Iteration 315, Loss: 1.156395435333252\n",
      "Epoch 1, Iteration 316, Loss: 1.3918241262435913\n",
      "Epoch 1, Iteration 317, Loss: 1.288510799407959\n",
      "Epoch 1, Iteration 318, Loss: 1.1542110443115234\n",
      "Epoch 1, Iteration 319, Loss: 1.446327805519104\n",
      "Epoch 1, Iteration 320, Loss: 1.096866250038147\n",
      "Epoch 1, Iteration 321, Loss: 1.2655705213546753\n",
      "Epoch 1, Iteration 322, Loss: 1.1605896949768066\n",
      "Epoch 1, Iteration 323, Loss: 1.2532464265823364\n",
      "Epoch 1, Iteration 324, Loss: 1.3412230014801025\n",
      "Epoch 1, Iteration 325, Loss: 1.307773232460022\n",
      "Epoch 1, Iteration 326, Loss: 1.250739574432373\n",
      "Epoch 1, Iteration 327, Loss: 1.2031644582748413\n",
      "Epoch 1, Iteration 328, Loss: 1.3323370218276978\n",
      "Epoch 1, Iteration 329, Loss: 1.507731318473816\n",
      "Epoch 1, Iteration 330, Loss: 1.061104416847229\n",
      "Epoch 1, Iteration 331, Loss: 1.2235201597213745\n",
      "Epoch 1, Iteration 332, Loss: 1.3565367460250854\n",
      "Epoch 1, Iteration 333, Loss: 1.4489601850509644\n",
      "Epoch 1, Iteration 334, Loss: 1.1563736200332642\n",
      "Epoch 1, Iteration 335, Loss: 1.4401212930679321\n",
      "Epoch 1, Iteration 336, Loss: 1.4102959632873535\n",
      "Epoch 1, Iteration 337, Loss: 1.2070664167404175\n",
      "Epoch 1, Iteration 338, Loss: 1.376094937324524\n",
      "Epoch 1, Iteration 339, Loss: 1.2408021688461304\n",
      "Epoch 1, Iteration 340, Loss: 1.0827453136444092\n",
      "Epoch 1, Iteration 341, Loss: 1.2344764471054077\n",
      "Epoch 1, Iteration 342, Loss: 1.1539145708084106\n",
      "Epoch 1, Iteration 343, Loss: 1.3110121488571167\n",
      "Epoch 1, Iteration 344, Loss: 1.502750039100647\n",
      "Epoch 1, Iteration 345, Loss: 1.361360788345337\n",
      "Epoch 1, Iteration 346, Loss: 1.3757387399673462\n",
      "Epoch 1, Iteration 347, Loss: 1.135177731513977\n",
      "Epoch 1, Iteration 348, Loss: 1.103747010231018\n",
      "Epoch 1, Iteration 349, Loss: 1.3434001207351685\n",
      "Epoch 1, Iteration 350, Loss: 1.1179286241531372\n",
      "Epoch 1, Iteration 350, Valid Loss: 1.2014061212539673\n",
      "Epoch 1, Iteration 351, Loss: 1.3257968425750732\n",
      "Epoch 1, Iteration 352, Loss: 1.182412028312683\n",
      "Epoch 1, Iteration 353, Loss: 1.3589540719985962\n",
      "Epoch 1, Iteration 354, Loss: 1.2019633054733276\n",
      "Epoch 1, Iteration 355, Loss: 1.283197045326233\n",
      "Epoch 1, Iteration 356, Loss: 1.2631863355636597\n",
      "Epoch 1, Iteration 357, Loss: 1.2213910818099976\n",
      "Epoch 1, Iteration 358, Loss: 1.216498613357544\n",
      "Epoch 1, Iteration 359, Loss: 1.227883219718933\n",
      "Epoch 1, Iteration 360, Loss: 1.119398593902588\n",
      "Epoch 1, Iteration 361, Loss: 1.1751484870910645\n",
      "Epoch 1, Iteration 362, Loss: 1.3469494581222534\n",
      "Epoch 1, Iteration 363, Loss: 1.132649540901184\n",
      "Epoch 1, Iteration 364, Loss: 1.2452784776687622\n",
      "Epoch 1, Iteration 365, Loss: 1.5839998722076416\n",
      "Epoch 1, Iteration 366, Loss: 1.3641456365585327\n",
      "Epoch 1, Iteration 367, Loss: 1.3855332136154175\n",
      "Epoch 1, Iteration 368, Loss: 1.3177103996276855\n",
      "Epoch 1, Iteration 369, Loss: 1.3508129119873047\n",
      "Epoch 1, Iteration 370, Loss: 1.3511488437652588\n",
      "Epoch 1, Iteration 371, Loss: 1.3325376510620117\n",
      "Epoch 1, Iteration 372, Loss: 1.1225920915603638\n",
      "Epoch 1, Iteration 373, Loss: 1.1216070652008057\n",
      "Epoch 1, Iteration 374, Loss: 1.3553204536437988\n",
      "Epoch 1, Iteration 375, Loss: 1.166293978691101\n",
      "Epoch 1, Iteration 376, Loss: 1.541292667388916\n",
      "Epoch 1, Iteration 377, Loss: 1.323535680770874\n",
      "Epoch 1, Iteration 378, Loss: 1.1273010969161987\n",
      "Epoch 1, Iteration 379, Loss: 1.1914187669754028\n",
      "Epoch 1, Iteration 380, Loss: 1.122992753982544\n",
      "Epoch 1, Iteration 381, Loss: 1.23050057888031\n",
      "Epoch 1, Iteration 382, Loss: 0.9855629205703735\n",
      "Epoch 1, Iteration 383, Loss: 1.3029712438583374\n",
      "Epoch 1, Iteration 384, Loss: 1.3223323822021484\n",
      "Epoch 1, Iteration 385, Loss: 1.1695373058319092\n",
      "Epoch 1, Iteration 386, Loss: 1.1455535888671875\n",
      "Epoch 1, Iteration 387, Loss: 1.4389551877975464\n",
      "Epoch 1, Iteration 388, Loss: 1.2447060346603394\n",
      "Epoch 1, Iteration 389, Loss: 1.4106117486953735\n",
      "Epoch 1, Iteration 390, Loss: 1.4579824209213257\n",
      "Epoch 1, Iteration 391, Loss: 1.242053747177124\n",
      "Epoch 2/15, Loss: 1.4914674971784865\n",
      "Epoch 2, Iteration 0, Loss: 1.3585050106048584\n",
      "Epoch 2, Iteration 1, Loss: 1.7052570581436157\n",
      "Epoch 2, Iteration 2, Loss: 1.6192524433135986\n",
      "Epoch 2, Iteration 3, Loss: 1.4448665380477905\n",
      "Epoch 2, Iteration 4, Loss: 1.3293166160583496\n",
      "Epoch 2, Iteration 5, Loss: 1.5260887145996094\n",
      "Epoch 2, Iteration 6, Loss: 1.275814414024353\n",
      "Epoch 2, Iteration 7, Loss: 1.3211416006088257\n",
      "Epoch 2, Iteration 8, Loss: 1.3960188627243042\n",
      "Epoch 2, Iteration 9, Loss: 1.6212043762207031\n",
      "Epoch 2, Iteration 10, Loss: 1.4986393451690674\n",
      "Epoch 2, Iteration 11, Loss: 1.2949296236038208\n",
      "Epoch 2, Iteration 12, Loss: 1.228013277053833\n",
      "Epoch 2, Iteration 13, Loss: 1.5601650476455688\n",
      "Epoch 2, Iteration 14, Loss: 1.28094482421875\n",
      "Epoch 2, Iteration 15, Loss: 1.5522674322128296\n",
      "Epoch 2, Iteration 16, Loss: 1.282633900642395\n",
      "Epoch 2, Iteration 17, Loss: 1.5558730363845825\n",
      "Epoch 2, Iteration 18, Loss: 1.4217065572738647\n",
      "Epoch 2, Iteration 19, Loss: 1.4150309562683105\n",
      "Epoch 2, Iteration 20, Loss: 1.3009984493255615\n",
      "Epoch 2, Iteration 21, Loss: 1.3310277462005615\n",
      "Epoch 2, Iteration 22, Loss: 1.3318287134170532\n",
      "Epoch 2, Iteration 23, Loss: 1.2634990215301514\n",
      "Epoch 2, Iteration 24, Loss: 1.2902685403823853\n",
      "Epoch 2, Iteration 25, Loss: 1.199089527130127\n",
      "Epoch 2, Iteration 26, Loss: 1.691246747970581\n",
      "Epoch 2, Iteration 27, Loss: 1.264173150062561\n",
      "Epoch 2, Iteration 28, Loss: 1.4678457975387573\n",
      "Epoch 2, Iteration 29, Loss: 1.5533677339553833\n",
      "Epoch 2, Iteration 30, Loss: 1.3373323678970337\n",
      "Epoch 2, Iteration 31, Loss: 1.3462896347045898\n",
      "Epoch 2, Iteration 32, Loss: 1.2520865201950073\n",
      "Epoch 2, Iteration 33, Loss: 1.290856957435608\n",
      "Epoch 2, Iteration 34, Loss: 1.305687427520752\n",
      "Epoch 2, Iteration 35, Loss: 1.425973653793335\n",
      "Epoch 2, Iteration 36, Loss: 1.1516977548599243\n",
      "Epoch 2, Iteration 37, Loss: 1.2506530284881592\n",
      "Epoch 2, Iteration 38, Loss: 1.2883967161178589\n",
      "Epoch 2, Iteration 39, Loss: 1.2893283367156982\n",
      "Epoch 2, Iteration 40, Loss: 1.4360722303390503\n",
      "Epoch 2, Iteration 41, Loss: 1.4092897176742554\n",
      "Epoch 2, Iteration 42, Loss: 1.3858414888381958\n",
      "Epoch 2, Iteration 43, Loss: 1.2088695764541626\n",
      "Epoch 2, Iteration 44, Loss: 1.3387315273284912\n",
      "Epoch 2, Iteration 45, Loss: 1.187865138053894\n",
      "Epoch 2, Iteration 46, Loss: 1.4028663635253906\n",
      "Epoch 2, Iteration 47, Loss: 1.4199872016906738\n",
      "Epoch 2, Iteration 48, Loss: 1.427333950996399\n",
      "Epoch 2, Iteration 49, Loss: 1.2379610538482666\n",
      "Epoch 2, Iteration 50, Loss: 1.2251149415969849\n",
      "Epoch 2, Iteration 50, Valid Loss: 1.0527894496917725\n",
      "Epoch 2, Iteration 51, Loss: 1.2291897535324097\n",
      "Epoch 2, Iteration 52, Loss: 1.2887953519821167\n",
      "Epoch 2, Iteration 53, Loss: 1.2500923871994019\n",
      "Epoch 2, Iteration 54, Loss: 1.549117088317871\n",
      "Epoch 2, Iteration 55, Loss: 1.436954140663147\n",
      "Epoch 2, Iteration 56, Loss: 1.2690051794052124\n",
      "Epoch 2, Iteration 57, Loss: 1.3669615983963013\n",
      "Epoch 2, Iteration 58, Loss: 1.1150248050689697\n",
      "Epoch 2, Iteration 59, Loss: 1.241655945777893\n",
      "Epoch 2, Iteration 60, Loss: 1.2999324798583984\n",
      "Epoch 2, Iteration 61, Loss: 1.1168427467346191\n",
      "Epoch 2, Iteration 62, Loss: 1.157361388206482\n",
      "Epoch 2, Iteration 63, Loss: 1.1970493793487549\n",
      "Epoch 2, Iteration 64, Loss: 1.2324485778808594\n",
      "Epoch 2, Iteration 65, Loss: 1.3852468729019165\n",
      "Epoch 2, Iteration 66, Loss: 1.1769047975540161\n",
      "Epoch 2, Iteration 67, Loss: 1.0401164293289185\n",
      "Epoch 2, Iteration 68, Loss: 1.311206340789795\n",
      "Epoch 2, Iteration 69, Loss: 1.563722848892212\n",
      "Epoch 2, Iteration 70, Loss: 1.535728096961975\n",
      "Epoch 2, Iteration 71, Loss: 1.317094087600708\n",
      "Epoch 2, Iteration 72, Loss: 1.2594369649887085\n",
      "Epoch 2, Iteration 73, Loss: 1.0540989637374878\n",
      "Epoch 2, Iteration 74, Loss: 1.5050956010818481\n",
      "Epoch 2, Iteration 75, Loss: 1.2701709270477295\n",
      "Epoch 2, Iteration 76, Loss: 1.264011025428772\n",
      "Epoch 2, Iteration 77, Loss: 1.1012470722198486\n",
      "Epoch 2, Iteration 78, Loss: 1.6387596130371094\n",
      "Epoch 2, Iteration 79, Loss: 1.5746967792510986\n",
      "Epoch 2, Iteration 80, Loss: 1.0753511190414429\n",
      "Epoch 2, Iteration 81, Loss: 1.5377706289291382\n",
      "Epoch 2, Iteration 82, Loss: 1.2506130933761597\n",
      "Epoch 2, Iteration 83, Loss: 1.1000291109085083\n",
      "Epoch 2, Iteration 84, Loss: 1.3247207403182983\n",
      "Epoch 2, Iteration 85, Loss: 1.3374651670455933\n",
      "Epoch 2, Iteration 86, Loss: 1.300841212272644\n",
      "Epoch 2, Iteration 87, Loss: 1.240937352180481\n",
      "Epoch 2, Iteration 88, Loss: 1.0999501943588257\n",
      "Epoch 2, Iteration 89, Loss: 1.3374899625778198\n",
      "Epoch 2, Iteration 90, Loss: 1.0808342695236206\n",
      "Epoch 2, Iteration 91, Loss: 1.0904529094696045\n",
      "Epoch 2, Iteration 92, Loss: 0.9275846481323242\n",
      "Epoch 2, Iteration 93, Loss: 1.1733700037002563\n",
      "Epoch 2, Iteration 94, Loss: 1.545705795288086\n",
      "Epoch 2, Iteration 95, Loss: 1.1878979206085205\n",
      "Epoch 2, Iteration 96, Loss: 1.3061805963516235\n",
      "Epoch 2, Iteration 97, Loss: 1.1048293113708496\n",
      "Epoch 2, Iteration 98, Loss: 1.2530295848846436\n",
      "Epoch 2, Iteration 99, Loss: 1.3652034997940063\n",
      "Epoch 2, Iteration 100, Loss: 1.0712153911590576\n",
      "Epoch 2, Iteration 100, Valid Loss: 1.1030675172805786\n",
      "Epoch 2, Iteration 101, Loss: 1.3891394138336182\n",
      "Epoch 2, Iteration 102, Loss: 1.3199727535247803\n",
      "Epoch 2, Iteration 103, Loss: 1.1454373598098755\n",
      "Epoch 2, Iteration 104, Loss: 1.4287923574447632\n",
      "Epoch 2, Iteration 105, Loss: 1.336130976676941\n",
      "Epoch 2, Iteration 106, Loss: 1.25489342212677\n",
      "Epoch 2, Iteration 107, Loss: 1.6456389427185059\n",
      "Epoch 2, Iteration 108, Loss: 1.2907077074050903\n",
      "Epoch 2, Iteration 109, Loss: 1.5018826723098755\n",
      "Epoch 2, Iteration 110, Loss: 1.5889348983764648\n",
      "Epoch 2, Iteration 111, Loss: 1.3761584758758545\n",
      "Epoch 2, Iteration 112, Loss: 1.2938551902770996\n",
      "Epoch 2, Iteration 113, Loss: 1.1676092147827148\n",
      "Epoch 2, Iteration 114, Loss: 1.665952205657959\n",
      "Epoch 2, Iteration 115, Loss: 1.4273051023483276\n",
      "Epoch 2, Iteration 116, Loss: 1.210695743560791\n",
      "Epoch 2, Iteration 117, Loss: 1.0649958848953247\n",
      "Epoch 2, Iteration 118, Loss: 1.4542828798294067\n",
      "Epoch 2, Iteration 119, Loss: 1.240682601928711\n",
      "Epoch 2, Iteration 120, Loss: 1.1999189853668213\n",
      "Epoch 2, Iteration 121, Loss: 1.3058868646621704\n",
      "Epoch 2, Iteration 122, Loss: 1.2172551155090332\n",
      "Epoch 2, Iteration 123, Loss: 1.160948395729065\n",
      "Epoch 2, Iteration 124, Loss: 1.2230794429779053\n",
      "Epoch 2, Iteration 125, Loss: 1.1328763961791992\n",
      "Epoch 2, Iteration 126, Loss: 1.1238386631011963\n",
      "Epoch 2, Iteration 127, Loss: 1.4741414785385132\n",
      "Epoch 2, Iteration 128, Loss: 1.184247612953186\n",
      "Epoch 2, Iteration 129, Loss: 1.4000476598739624\n",
      "Epoch 2, Iteration 130, Loss: 1.6121044158935547\n",
      "Epoch 2, Iteration 131, Loss: 1.2381064891815186\n",
      "Epoch 2, Iteration 132, Loss: 1.211764931678772\n",
      "Epoch 2, Iteration 133, Loss: 1.4387640953063965\n",
      "Epoch 2, Iteration 134, Loss: 1.238364815711975\n",
      "Epoch 2, Iteration 135, Loss: 1.40717613697052\n",
      "Epoch 2, Iteration 136, Loss: 1.2149372100830078\n",
      "Epoch 2, Iteration 137, Loss: 1.1779533624649048\n",
      "Epoch 2, Iteration 138, Loss: 1.1822452545166016\n",
      "Epoch 2, Iteration 139, Loss: 1.0853763818740845\n",
      "Epoch 2, Iteration 140, Loss: 1.1429606676101685\n",
      "Epoch 2, Iteration 141, Loss: 1.6542880535125732\n",
      "Epoch 2, Iteration 142, Loss: 1.1350570917129517\n",
      "Epoch 2, Iteration 143, Loss: 1.1943308115005493\n",
      "Epoch 2, Iteration 144, Loss: 1.1922088861465454\n",
      "Epoch 2, Iteration 145, Loss: 1.2752357721328735\n",
      "Epoch 2, Iteration 146, Loss: 1.2858319282531738\n",
      "Epoch 2, Iteration 147, Loss: 1.2370882034301758\n",
      "Epoch 2, Iteration 148, Loss: 1.0160777568817139\n",
      "Epoch 2, Iteration 149, Loss: 1.128968596458435\n",
      "Epoch 2, Iteration 150, Loss: 1.249684453010559\n",
      "Epoch 2, Iteration 150, Valid Loss: 1.0590143203735352\n",
      "Epoch 2, Iteration 151, Loss: 1.4817719459533691\n",
      "Epoch 2, Iteration 152, Loss: 1.1880853176116943\n",
      "Epoch 2, Iteration 153, Loss: 1.378589153289795\n",
      "Epoch 2, Iteration 154, Loss: 1.130679965019226\n",
      "Epoch 2, Iteration 155, Loss: 1.3513519763946533\n",
      "Epoch 2, Iteration 156, Loss: 1.0652215480804443\n",
      "Epoch 2, Iteration 157, Loss: 1.539331078529358\n",
      "Epoch 2, Iteration 158, Loss: 1.3969093561172485\n",
      "Epoch 2, Iteration 159, Loss: 1.2743374109268188\n",
      "Epoch 2, Iteration 160, Loss: 1.1231715679168701\n",
      "Epoch 2, Iteration 161, Loss: 1.2239477634429932\n",
      "Epoch 2, Iteration 162, Loss: 1.5389460325241089\n",
      "Epoch 2, Iteration 163, Loss: 0.983245849609375\n",
      "Epoch 2, Iteration 164, Loss: 1.060785174369812\n",
      "Epoch 2, Iteration 165, Loss: 1.1505430936813354\n",
      "Epoch 2, Iteration 166, Loss: 1.094492793083191\n",
      "Epoch 2, Iteration 167, Loss: 1.5016801357269287\n",
      "Epoch 2, Iteration 168, Loss: 1.263330101966858\n",
      "Epoch 2, Iteration 169, Loss: 1.2590782642364502\n",
      "Epoch 2, Iteration 170, Loss: 1.4304208755493164\n",
      "Epoch 2, Iteration 171, Loss: 1.329261064529419\n",
      "Epoch 2, Iteration 172, Loss: 1.4414761066436768\n",
      "Epoch 2, Iteration 173, Loss: 1.1687816381454468\n",
      "Epoch 2, Iteration 174, Loss: 1.3005338907241821\n",
      "Epoch 2, Iteration 175, Loss: 1.2631235122680664\n",
      "Epoch 2, Iteration 176, Loss: 0.9368302822113037\n",
      "Epoch 2, Iteration 177, Loss: 1.6147526502609253\n",
      "Epoch 2, Iteration 178, Loss: 1.451687216758728\n",
      "Epoch 2, Iteration 179, Loss: 1.3375521898269653\n",
      "Epoch 2, Iteration 180, Loss: 1.2262414693832397\n",
      "Epoch 2, Iteration 181, Loss: 1.3687846660614014\n",
      "Epoch 2, Iteration 182, Loss: 1.3614848852157593\n",
      "Epoch 2, Iteration 183, Loss: 1.0939810276031494\n",
      "Epoch 2, Iteration 184, Loss: 1.0769323110580444\n",
      "Epoch 2, Iteration 185, Loss: 1.1292591094970703\n",
      "Epoch 2, Iteration 186, Loss: 1.1368470191955566\n",
      "Epoch 2, Iteration 187, Loss: 1.2261943817138672\n",
      "Epoch 2, Iteration 188, Loss: 1.1919997930526733\n",
      "Epoch 2, Iteration 189, Loss: 1.1116359233856201\n",
      "Epoch 2, Iteration 190, Loss: 1.4385687112808228\n",
      "Epoch 2, Iteration 191, Loss: 1.3155633211135864\n",
      "Epoch 2, Iteration 192, Loss: 1.3175166845321655\n",
      "Epoch 2, Iteration 193, Loss: 1.2632704973220825\n",
      "Epoch 2, Iteration 194, Loss: 1.1043936014175415\n",
      "Epoch 2, Iteration 195, Loss: 1.1946059465408325\n",
      "Epoch 2, Iteration 196, Loss: 1.320804476737976\n",
      "Epoch 2, Iteration 197, Loss: 1.2107627391815186\n",
      "Epoch 2, Iteration 198, Loss: 1.2173362970352173\n",
      "Epoch 2, Iteration 199, Loss: 1.3772296905517578\n",
      "Epoch 2, Iteration 200, Loss: 1.1935757398605347\n",
      "Epoch 2, Iteration 200, Valid Loss: 1.0429996252059937\n",
      "Epoch 2, Iteration 201, Loss: 1.2058026790618896\n",
      "Epoch 2, Iteration 202, Loss: 1.2992796897888184\n",
      "Epoch 2, Iteration 203, Loss: 1.12651789188385\n",
      "Epoch 2, Iteration 204, Loss: 1.104512333869934\n",
      "Epoch 2, Iteration 205, Loss: 0.9533286094665527\n",
      "Epoch 2, Iteration 206, Loss: 1.0745385885238647\n",
      "Epoch 2, Iteration 207, Loss: 1.3084948062896729\n",
      "Epoch 2, Iteration 208, Loss: 1.6047544479370117\n",
      "Epoch 2, Iteration 209, Loss: 1.568177580833435\n",
      "Epoch 2, Iteration 210, Loss: 1.6548279523849487\n",
      "Epoch 2, Iteration 211, Loss: 1.7727138996124268\n",
      "Epoch 2, Iteration 212, Loss: 1.5409343242645264\n",
      "Epoch 2, Iteration 213, Loss: 1.679283618927002\n",
      "Epoch 2, Iteration 214, Loss: 1.2093660831451416\n",
      "Epoch 2, Iteration 215, Loss: 1.1539944410324097\n",
      "Epoch 2, Iteration 216, Loss: 1.1023638248443604\n",
      "Epoch 2, Iteration 217, Loss: 1.218210220336914\n",
      "Epoch 2, Iteration 218, Loss: 1.319913625717163\n",
      "Epoch 2, Iteration 219, Loss: 1.2899518013000488\n",
      "Epoch 2, Iteration 220, Loss: 1.2587751150131226\n",
      "Epoch 2, Iteration 221, Loss: 1.2567282915115356\n",
      "Epoch 2, Iteration 222, Loss: 1.1748398542404175\n",
      "Epoch 2, Iteration 223, Loss: 1.0945098400115967\n",
      "Epoch 2, Iteration 224, Loss: 1.0238933563232422\n",
      "Epoch 2, Iteration 225, Loss: 1.3052597045898438\n",
      "Epoch 2, Iteration 226, Loss: 1.4083878993988037\n",
      "Epoch 2, Iteration 227, Loss: 1.3909844160079956\n",
      "Epoch 2, Iteration 228, Loss: 1.0856198072433472\n",
      "Epoch 2, Iteration 229, Loss: 1.117658019065857\n",
      "Epoch 2, Iteration 230, Loss: 1.1797212362289429\n",
      "Epoch 2, Iteration 231, Loss: 1.3231159448623657\n",
      "Epoch 2, Iteration 232, Loss: 1.2190901041030884\n",
      "Epoch 2, Iteration 233, Loss: 1.6343528032302856\n",
      "Epoch 2, Iteration 234, Loss: 1.4636688232421875\n",
      "Epoch 2, Iteration 235, Loss: 1.0065624713897705\n",
      "Epoch 2, Iteration 236, Loss: 1.3197182416915894\n",
      "Epoch 2, Iteration 237, Loss: 1.2396130561828613\n",
      "Epoch 2, Iteration 238, Loss: 1.0741064548492432\n",
      "Epoch 2, Iteration 239, Loss: 1.271323800086975\n",
      "Epoch 2, Iteration 240, Loss: 1.09678053855896\n",
      "Epoch 2, Iteration 241, Loss: 1.1904034614562988\n",
      "Epoch 2, Iteration 242, Loss: 1.0539668798446655\n",
      "Epoch 2, Iteration 243, Loss: 1.2537822723388672\n",
      "Epoch 2, Iteration 244, Loss: 1.1632721424102783\n",
      "Epoch 2, Iteration 245, Loss: 0.9533917903900146\n",
      "Epoch 2, Iteration 246, Loss: 1.2224875688552856\n",
      "Epoch 2, Iteration 247, Loss: 1.0542608499526978\n",
      "Epoch 2, Iteration 248, Loss: 1.2136319875717163\n",
      "Epoch 2, Iteration 249, Loss: 1.1444156169891357\n",
      "Epoch 2, Iteration 250, Loss: 1.403063416481018\n",
      "Epoch 2, Iteration 250, Valid Loss: 0.985991358757019\n",
      "Epoch 2, Iteration 251, Loss: 1.2166633605957031\n",
      "Epoch 2, Iteration 252, Loss: 1.3595855236053467\n",
      "Epoch 2, Iteration 253, Loss: 1.2595386505126953\n",
      "Epoch 2, Iteration 254, Loss: 1.2551109790802002\n",
      "Epoch 2, Iteration 255, Loss: 1.2381298542022705\n",
      "Epoch 2, Iteration 256, Loss: 1.3718409538269043\n",
      "Epoch 2, Iteration 257, Loss: 1.0422435998916626\n",
      "Epoch 2, Iteration 258, Loss: 1.3500118255615234\n",
      "Epoch 2, Iteration 259, Loss: 0.9567878842353821\n",
      "Epoch 2, Iteration 260, Loss: 1.308213233947754\n",
      "Epoch 2, Iteration 261, Loss: 1.3028963804244995\n",
      "Epoch 2, Iteration 262, Loss: 1.0123953819274902\n",
      "Epoch 2, Iteration 263, Loss: 1.4050441980361938\n",
      "Epoch 2, Iteration 264, Loss: 1.247766137123108\n",
      "Epoch 2, Iteration 265, Loss: 1.3283464908599854\n",
      "Epoch 2, Iteration 266, Loss: 1.2787246704101562\n",
      "Epoch 2, Iteration 267, Loss: 1.1836777925491333\n",
      "Epoch 2, Iteration 268, Loss: 1.1920043230056763\n",
      "Epoch 2, Iteration 269, Loss: 1.1557906866073608\n",
      "Epoch 2, Iteration 270, Loss: 1.1069351434707642\n",
      "Epoch 2, Iteration 271, Loss: 1.2651410102844238\n",
      "Epoch 2, Iteration 272, Loss: 1.0737520456314087\n",
      "Epoch 2, Iteration 273, Loss: 1.1728070974349976\n",
      "Epoch 2, Iteration 274, Loss: 1.1381306648254395\n",
      "Epoch 2, Iteration 275, Loss: 0.9935009479522705\n",
      "Epoch 2, Iteration 276, Loss: 1.0807745456695557\n",
      "Epoch 2, Iteration 277, Loss: 1.0871856212615967\n",
      "Epoch 2, Iteration 278, Loss: 1.1773782968521118\n",
      "Epoch 2, Iteration 279, Loss: 1.049915075302124\n",
      "Epoch 2, Iteration 280, Loss: 0.9355392456054688\n",
      "Epoch 2, Iteration 281, Loss: 1.1026726961135864\n",
      "Epoch 2, Iteration 282, Loss: 1.1246956586837769\n",
      "Epoch 2, Iteration 283, Loss: 1.14919114112854\n",
      "Epoch 2, Iteration 284, Loss: 1.1093398332595825\n",
      "Epoch 2, Iteration 285, Loss: 1.0680955648422241\n",
      "Epoch 2, Iteration 286, Loss: 1.245273232460022\n",
      "Epoch 2, Iteration 287, Loss: 1.0806971788406372\n",
      "Epoch 2, Iteration 288, Loss: 1.0213826894760132\n",
      "Epoch 2, Iteration 289, Loss: 1.169663429260254\n",
      "Epoch 2, Iteration 290, Loss: 1.0439858436584473\n",
      "Epoch 2, Iteration 291, Loss: 1.0018000602722168\n",
      "Epoch 2, Iteration 292, Loss: 1.3471516370773315\n",
      "Epoch 2, Iteration 293, Loss: 1.359987497329712\n",
      "Epoch 2, Iteration 294, Loss: 1.1301649808883667\n",
      "Epoch 2, Iteration 295, Loss: 1.1145488023757935\n",
      "Epoch 2, Iteration 296, Loss: 1.3848927021026611\n",
      "Epoch 2, Iteration 297, Loss: 1.308872938156128\n",
      "Epoch 2, Iteration 298, Loss: 1.1648415327072144\n",
      "Epoch 2, Iteration 299, Loss: 1.0080671310424805\n",
      "Epoch 2, Iteration 300, Loss: 0.8661330938339233\n",
      "Epoch 2, Iteration 300, Valid Loss: 0.941214919090271\n",
      "Epoch 2, Iteration 301, Loss: 1.0956426858901978\n",
      "Epoch 2, Iteration 302, Loss: 1.1172891855239868\n",
      "Epoch 2, Iteration 303, Loss: 1.3110929727554321\n",
      "Epoch 2, Iteration 304, Loss: 1.062587022781372\n",
      "Epoch 2, Iteration 305, Loss: 1.0172067880630493\n",
      "Epoch 2, Iteration 306, Loss: 1.0366710424423218\n",
      "Epoch 2, Iteration 307, Loss: 1.1005442142486572\n",
      "Epoch 2, Iteration 308, Loss: 0.9919312596321106\n",
      "Epoch 2, Iteration 309, Loss: 1.159654140472412\n",
      "Epoch 2, Iteration 310, Loss: 1.3899539709091187\n",
      "Epoch 2, Iteration 311, Loss: 0.9934293031692505\n",
      "Epoch 2, Iteration 312, Loss: 1.0814578533172607\n",
      "Epoch 2, Iteration 313, Loss: 1.1922624111175537\n",
      "Epoch 2, Iteration 314, Loss: 1.044293761253357\n",
      "Epoch 2, Iteration 315, Loss: 0.9899454712867737\n",
      "Epoch 2, Iteration 316, Loss: 1.2335302829742432\n",
      "Epoch 2, Iteration 317, Loss: 1.1147055625915527\n",
      "Epoch 2, Iteration 318, Loss: 0.8958510160446167\n",
      "Epoch 2, Iteration 319, Loss: 1.1550326347351074\n",
      "Epoch 2, Iteration 320, Loss: 0.8028846979141235\n",
      "Epoch 2, Iteration 321, Loss: 1.0047028064727783\n",
      "Epoch 2, Iteration 322, Loss: 1.0193761587142944\n",
      "Epoch 2, Iteration 323, Loss: 1.1041219234466553\n",
      "Epoch 2, Iteration 324, Loss: 1.2299089431762695\n",
      "Epoch 2, Iteration 325, Loss: 1.0956635475158691\n",
      "Epoch 2, Iteration 326, Loss: 1.1071244478225708\n",
      "Epoch 2, Iteration 327, Loss: 0.9092209339141846\n",
      "Epoch 2, Iteration 328, Loss: 1.0162434577941895\n",
      "Epoch 2, Iteration 329, Loss: 1.210309624671936\n",
      "Epoch 2, Iteration 330, Loss: 0.8920063972473145\n",
      "Epoch 2, Iteration 331, Loss: 1.0307791233062744\n",
      "Epoch 2, Iteration 332, Loss: 1.2267656326293945\n",
      "Epoch 2, Iteration 333, Loss: 1.3230599164962769\n",
      "Epoch 2, Iteration 334, Loss: 0.9319519996643066\n",
      "Epoch 2, Iteration 335, Loss: 1.2995468378067017\n",
      "Epoch 2, Iteration 336, Loss: 1.1129332780838013\n",
      "Epoch 2, Iteration 337, Loss: 0.8812860250473022\n",
      "Epoch 2, Iteration 338, Loss: 1.1973434686660767\n",
      "Epoch 2, Iteration 339, Loss: 1.0048469305038452\n",
      "Epoch 2, Iteration 340, Loss: 0.9464308023452759\n",
      "Epoch 2, Iteration 341, Loss: 1.07508385181427\n",
      "Epoch 2, Iteration 342, Loss: 1.0455026626586914\n",
      "Epoch 2, Iteration 343, Loss: 1.1671037673950195\n",
      "Epoch 2, Iteration 344, Loss: 1.2346174716949463\n",
      "Epoch 2, Iteration 345, Loss: 1.24411940574646\n",
      "Epoch 2, Iteration 346, Loss: 1.1289629936218262\n",
      "Epoch 2, Iteration 347, Loss: 0.9780958294868469\n",
      "Epoch 2, Iteration 348, Loss: 0.9398738145828247\n",
      "Epoch 2, Iteration 349, Loss: 1.0260486602783203\n",
      "Epoch 2, Iteration 350, Loss: 0.9273669123649597\n",
      "Epoch 2, Iteration 350, Valid Loss: 0.9549928307533264\n",
      "Epoch 2, Iteration 351, Loss: 1.118416666984558\n",
      "Epoch 2, Iteration 352, Loss: 0.9780813455581665\n",
      "Epoch 2, Iteration 353, Loss: 1.0547291040420532\n",
      "Epoch 2, Iteration 354, Loss: 0.9888044595718384\n",
      "Epoch 2, Iteration 355, Loss: 1.0930322408676147\n",
      "Epoch 2, Iteration 356, Loss: 1.110621690750122\n",
      "Epoch 2, Iteration 357, Loss: 0.9997664093971252\n",
      "Epoch 2, Iteration 358, Loss: 1.0730623006820679\n",
      "Epoch 2, Iteration 359, Loss: 0.9565787315368652\n",
      "Epoch 2, Iteration 360, Loss: 0.9841212034225464\n",
      "Epoch 2, Iteration 361, Loss: 0.997808575630188\n",
      "Epoch 2, Iteration 362, Loss: 1.0469703674316406\n",
      "Epoch 2, Iteration 363, Loss: 0.9691178798675537\n",
      "Epoch 2, Iteration 364, Loss: 1.1092610359191895\n",
      "Epoch 2, Iteration 365, Loss: 1.319387674331665\n",
      "Epoch 2, Iteration 366, Loss: 1.2308993339538574\n",
      "Epoch 2, Iteration 367, Loss: 1.204268217086792\n",
      "Epoch 2, Iteration 368, Loss: 1.1583144664764404\n",
      "Epoch 2, Iteration 369, Loss: 1.1920385360717773\n",
      "Epoch 2, Iteration 370, Loss: 1.136528730392456\n",
      "Epoch 2, Iteration 371, Loss: 1.1459465026855469\n",
      "Epoch 2, Iteration 372, Loss: 0.9772014021873474\n",
      "Epoch 2, Iteration 373, Loss: 0.955428957939148\n",
      "Epoch 2, Iteration 374, Loss: 1.227167010307312\n",
      "Epoch 2, Iteration 375, Loss: 0.9440174102783203\n",
      "Epoch 2, Iteration 376, Loss: 1.238476276397705\n",
      "Epoch 2, Iteration 377, Loss: 1.096373438835144\n",
      "Epoch 2, Iteration 378, Loss: 0.9520246386528015\n",
      "Epoch 2, Iteration 379, Loss: 0.9661081433296204\n",
      "Epoch 2, Iteration 380, Loss: 0.9770174622535706\n",
      "Epoch 2, Iteration 381, Loss: 0.9658941626548767\n",
      "Epoch 2, Iteration 382, Loss: 0.8490253686904907\n",
      "Epoch 2, Iteration 383, Loss: 1.0020315647125244\n",
      "Epoch 2, Iteration 384, Loss: 1.0898722410202026\n",
      "Epoch 2, Iteration 385, Loss: 0.9513973593711853\n",
      "Epoch 2, Iteration 386, Loss: 1.0404490232467651\n",
      "Epoch 2, Iteration 387, Loss: 1.3068523406982422\n",
      "Epoch 2, Iteration 388, Loss: 1.0356459617614746\n",
      "Epoch 2, Iteration 389, Loss: 1.1310940980911255\n",
      "Epoch 2, Iteration 390, Loss: 1.254923939704895\n",
      "Epoch 2, Iteration 391, Loss: 1.071784496307373\n",
      "Epoch 3/15, Loss: 1.2252244298555413\n",
      "Epoch 3, Iteration 0, Loss: 1.1641385555267334\n",
      "Epoch 3, Iteration 1, Loss: 1.3637230396270752\n",
      "Epoch 3, Iteration 2, Loss: 1.3176958560943604\n",
      "Epoch 3, Iteration 3, Loss: 1.2116209268569946\n",
      "Epoch 3, Iteration 4, Loss: 1.148163080215454\n",
      "Epoch 3, Iteration 5, Loss: 1.2777574062347412\n",
      "Epoch 3, Iteration 6, Loss: 1.1102975606918335\n",
      "Epoch 3, Iteration 7, Loss: 1.1176131963729858\n",
      "Epoch 3, Iteration 8, Loss: 1.1874511241912842\n",
      "Epoch 3, Iteration 9, Loss: 1.3173099756240845\n",
      "Epoch 3, Iteration 10, Loss: 1.2071696519851685\n",
      "Epoch 3, Iteration 11, Loss: 1.1192716360092163\n",
      "Epoch 3, Iteration 12, Loss: 1.070862889289856\n",
      "Epoch 3, Iteration 13, Loss: 1.282929539680481\n",
      "Epoch 3, Iteration 14, Loss: 1.1027774810791016\n",
      "Epoch 3, Iteration 15, Loss: 1.240773320198059\n",
      "Epoch 3, Iteration 16, Loss: 1.0211865901947021\n",
      "Epoch 3, Iteration 17, Loss: 1.368262529373169\n",
      "Epoch 3, Iteration 18, Loss: 1.2020686864852905\n",
      "Epoch 3, Iteration 19, Loss: 1.2434638738632202\n",
      "Epoch 3, Iteration 20, Loss: 1.1259726285934448\n",
      "Epoch 3, Iteration 21, Loss: 1.027970314025879\n",
      "Epoch 3, Iteration 22, Loss: 1.1242753267288208\n",
      "Epoch 3, Iteration 23, Loss: 1.0981554985046387\n",
      "Epoch 3, Iteration 24, Loss: 1.062686562538147\n",
      "Epoch 3, Iteration 25, Loss: 0.996124804019928\n",
      "Epoch 3, Iteration 26, Loss: 1.4744619131088257\n",
      "Epoch 3, Iteration 27, Loss: 1.1512761116027832\n",
      "Epoch 3, Iteration 28, Loss: 1.2190465927124023\n",
      "Epoch 3, Iteration 29, Loss: 1.4248597621917725\n",
      "Epoch 3, Iteration 30, Loss: 1.2066539525985718\n",
      "Epoch 3, Iteration 31, Loss: 1.22140371799469\n",
      "Epoch 3, Iteration 32, Loss: 1.1933400630950928\n",
      "Epoch 3, Iteration 33, Loss: 1.1959046125411987\n",
      "Epoch 3, Iteration 34, Loss: 1.093108892440796\n",
      "Epoch 3, Iteration 35, Loss: 1.2333601713180542\n",
      "Epoch 3, Iteration 36, Loss: 1.056677222251892\n",
      "Epoch 3, Iteration 37, Loss: 1.0395513772964478\n",
      "Epoch 3, Iteration 38, Loss: 1.04006826877594\n",
      "Epoch 3, Iteration 39, Loss: 1.070136547088623\n",
      "Epoch 3, Iteration 40, Loss: 1.197893738746643\n",
      "Epoch 3, Iteration 41, Loss: 1.1276686191558838\n",
      "Epoch 3, Iteration 42, Loss: 1.2011077404022217\n",
      "Epoch 3, Iteration 43, Loss: 0.9644978642463684\n",
      "Epoch 3, Iteration 44, Loss: 1.1141523122787476\n",
      "Epoch 3, Iteration 45, Loss: 1.080623984336853\n",
      "Epoch 3, Iteration 46, Loss: 1.15419340133667\n",
      "Epoch 3, Iteration 47, Loss: 1.1628371477127075\n",
      "Epoch 3, Iteration 48, Loss: 1.3053350448608398\n",
      "Epoch 3, Iteration 49, Loss: 1.103352665901184\n",
      "Epoch 3, Iteration 50, Loss: 1.082261085510254\n",
      "Epoch 3, Iteration 50, Valid Loss: 0.8912926316261292\n",
      "Epoch 3, Iteration 51, Loss: 1.0361707210540771\n",
      "Epoch 3, Iteration 52, Loss: 1.0836231708526611\n",
      "Epoch 3, Iteration 53, Loss: 1.1570571660995483\n",
      "Epoch 3, Iteration 54, Loss: 1.4303617477416992\n",
      "Epoch 3, Iteration 55, Loss: 1.2243138551712036\n",
      "Epoch 3, Iteration 56, Loss: 1.0160043239593506\n",
      "Epoch 3, Iteration 57, Loss: 1.2175806760787964\n",
      "Epoch 3, Iteration 58, Loss: 1.0677040815353394\n",
      "Epoch 3, Iteration 59, Loss: 1.0727490186691284\n",
      "Epoch 3, Iteration 60, Loss: 1.0879193544387817\n",
      "Epoch 3, Iteration 61, Loss: 0.9661374688148499\n",
      "Epoch 3, Iteration 62, Loss: 1.0511915683746338\n",
      "Epoch 3, Iteration 63, Loss: 0.9377236366271973\n",
      "Epoch 3, Iteration 64, Loss: 1.0578486919403076\n",
      "Epoch 3, Iteration 65, Loss: 1.2625325918197632\n",
      "Epoch 3, Iteration 66, Loss: 1.1937541961669922\n",
      "Epoch 3, Iteration 67, Loss: 0.9604455828666687\n",
      "Epoch 3, Iteration 68, Loss: 1.2998182773590088\n",
      "Epoch 3, Iteration 69, Loss: 1.3781629800796509\n",
      "Epoch 3, Iteration 70, Loss: 1.351517677307129\n",
      "Epoch 3, Iteration 71, Loss: 1.17820405960083\n",
      "Epoch 3, Iteration 72, Loss: 1.1281676292419434\n",
      "Epoch 3, Iteration 73, Loss: 1.0633883476257324\n",
      "Epoch 3, Iteration 74, Loss: 1.5201948881149292\n",
      "Epoch 3, Iteration 75, Loss: 1.061237096786499\n",
      "Epoch 3, Iteration 76, Loss: 1.112030029296875\n",
      "Epoch 3, Iteration 77, Loss: 0.9161837697029114\n",
      "Epoch 3, Iteration 78, Loss: 1.3689908981323242\n",
      "Epoch 3, Iteration 79, Loss: 1.381077527999878\n",
      "Epoch 3, Iteration 80, Loss: 0.9920129776000977\n",
      "Epoch 3, Iteration 81, Loss: 1.290735125541687\n",
      "Epoch 3, Iteration 82, Loss: 1.0997004508972168\n",
      "Epoch 3, Iteration 83, Loss: 0.9230185151100159\n",
      "Epoch 3, Iteration 84, Loss: 1.2292217016220093\n",
      "Epoch 3, Iteration 85, Loss: 1.192324161529541\n",
      "Epoch 3, Iteration 86, Loss: 1.1614898443222046\n",
      "Epoch 3, Iteration 87, Loss: 1.1153331995010376\n",
      "Epoch 3, Iteration 88, Loss: 1.1159100532531738\n",
      "Epoch 3, Iteration 89, Loss: 1.3811321258544922\n",
      "Epoch 3, Iteration 90, Loss: 0.8688266277313232\n",
      "Epoch 3, Iteration 91, Loss: 1.0145989656448364\n",
      "Epoch 3, Iteration 92, Loss: 0.8319767117500305\n",
      "Epoch 3, Iteration 93, Loss: 0.9670761227607727\n",
      "Epoch 3, Iteration 94, Loss: 1.370466947555542\n",
      "Epoch 3, Iteration 95, Loss: 0.9565966129302979\n",
      "Epoch 3, Iteration 96, Loss: 1.1129369735717773\n",
      "Epoch 3, Iteration 97, Loss: 0.9649690985679626\n",
      "Epoch 3, Iteration 98, Loss: 1.2958723306655884\n",
      "Epoch 3, Iteration 99, Loss: 1.142082929611206\n",
      "Epoch 3, Iteration 100, Loss: 0.9332523941993713\n",
      "Epoch 3, Iteration 100, Valid Loss: 0.942785382270813\n",
      "Epoch 3, Iteration 101, Loss: 1.1824315786361694\n",
      "Epoch 3, Iteration 102, Loss: 1.121115803718567\n",
      "Epoch 3, Iteration 103, Loss: 1.0790131092071533\n",
      "Epoch 3, Iteration 104, Loss: 1.2260115146636963\n",
      "Epoch 3, Iteration 105, Loss: 1.133427381515503\n",
      "Epoch 3, Iteration 106, Loss: 1.1108438968658447\n",
      "Epoch 3, Iteration 107, Loss: 1.571319818496704\n",
      "Epoch 3, Iteration 108, Loss: 1.0042794942855835\n",
      "Epoch 3, Iteration 109, Loss: 1.3666553497314453\n",
      "Epoch 3, Iteration 110, Loss: 1.351923942565918\n",
      "Epoch 3, Iteration 111, Loss: 1.0812662839889526\n",
      "Epoch 3, Iteration 112, Loss: 1.2187719345092773\n",
      "Epoch 3, Iteration 113, Loss: 0.9845358729362488\n",
      "Epoch 3, Iteration 114, Loss: 1.4002174139022827\n",
      "Epoch 3, Iteration 115, Loss: 1.2654874324798584\n",
      "Epoch 3, Iteration 116, Loss: 1.053905963897705\n",
      "Epoch 3, Iteration 117, Loss: 0.8709203600883484\n",
      "Epoch 3, Iteration 118, Loss: 1.2245135307312012\n",
      "Epoch 3, Iteration 119, Loss: 1.0238065719604492\n",
      "Epoch 3, Iteration 120, Loss: 1.2082232236862183\n",
      "Epoch 3, Iteration 121, Loss: 0.9954643249511719\n",
      "Epoch 3, Iteration 122, Loss: 1.2030242681503296\n",
      "Epoch 3, Iteration 123, Loss: 1.0909199714660645\n",
      "Epoch 3, Iteration 124, Loss: 1.009216070175171\n",
      "Epoch 3, Iteration 125, Loss: 1.0416181087493896\n",
      "Epoch 3, Iteration 126, Loss: 1.044505000114441\n",
      "Epoch 3, Iteration 127, Loss: 1.3698982000350952\n",
      "Epoch 3, Iteration 128, Loss: 1.1695663928985596\n",
      "Epoch 3, Iteration 129, Loss: 1.1511664390563965\n",
      "Epoch 3, Iteration 130, Loss: 1.4750688076019287\n",
      "Epoch 3, Iteration 131, Loss: 1.0000066757202148\n",
      "Epoch 3, Iteration 132, Loss: 1.0541329383850098\n",
      "Epoch 3, Iteration 133, Loss: 1.3387247323989868\n",
      "Epoch 3, Iteration 134, Loss: 1.133695363998413\n",
      "Epoch 3, Iteration 135, Loss: 1.2087537050247192\n",
      "Epoch 3, Iteration 136, Loss: 1.066142201423645\n",
      "Epoch 3, Iteration 137, Loss: 1.1385409832000732\n",
      "Epoch 3, Iteration 138, Loss: 0.9070355892181396\n",
      "Epoch 3, Iteration 139, Loss: 1.0646134614944458\n",
      "Epoch 3, Iteration 140, Loss: 1.035327672958374\n",
      "Epoch 3, Iteration 141, Loss: 1.503759741783142\n",
      "Epoch 3, Iteration 142, Loss: 1.0821095705032349\n",
      "Epoch 3, Iteration 143, Loss: 1.0831917524337769\n",
      "Epoch 3, Iteration 144, Loss: 1.0703340768814087\n",
      "Epoch 3, Iteration 145, Loss: 1.0316650867462158\n",
      "Epoch 3, Iteration 146, Loss: 1.1452600955963135\n",
      "Epoch 3, Iteration 147, Loss: 1.0145771503448486\n",
      "Epoch 3, Iteration 148, Loss: 0.8990993499755859\n",
      "Epoch 3, Iteration 149, Loss: 1.04999577999115\n",
      "Epoch 3, Iteration 150, Loss: 1.0763837099075317\n",
      "Epoch 3, Iteration 150, Valid Loss: 0.8888009190559387\n",
      "Epoch 3, Iteration 151, Loss: 1.2658193111419678\n",
      "Epoch 3, Iteration 152, Loss: 1.0805376768112183\n",
      "Epoch 3, Iteration 153, Loss: 1.1270833015441895\n",
      "Epoch 3, Iteration 154, Loss: 0.9641849398612976\n",
      "Epoch 3, Iteration 155, Loss: 1.230021357536316\n",
      "Epoch 3, Iteration 156, Loss: 0.9924357533454895\n",
      "Epoch 3, Iteration 157, Loss: 1.3258895874023438\n",
      "Epoch 3, Iteration 158, Loss: 1.143977403640747\n",
      "Epoch 3, Iteration 159, Loss: 1.2297316789627075\n",
      "Epoch 3, Iteration 160, Loss: 0.9877855181694031\n",
      "Epoch 3, Iteration 161, Loss: 1.0751826763153076\n",
      "Epoch 3, Iteration 162, Loss: 1.356728196144104\n",
      "Epoch 3, Iteration 163, Loss: 0.8798678517341614\n",
      "Epoch 3, Iteration 164, Loss: 1.1009190082550049\n",
      "Epoch 3, Iteration 165, Loss: 0.9980824589729309\n",
      "Epoch 3, Iteration 166, Loss: 0.9582971334457397\n",
      "Epoch 3, Iteration 167, Loss: 1.3450957536697388\n",
      "Epoch 3, Iteration 168, Loss: 1.1459277868270874\n",
      "Epoch 3, Iteration 169, Loss: 1.1567248106002808\n",
      "Epoch 3, Iteration 170, Loss: 1.2771905660629272\n",
      "Epoch 3, Iteration 171, Loss: 1.2746353149414062\n",
      "Epoch 3, Iteration 172, Loss: 1.2062175273895264\n",
      "Epoch 3, Iteration 173, Loss: 1.0309292078018188\n",
      "Epoch 3, Iteration 174, Loss: 1.1574032306671143\n",
      "Epoch 3, Iteration 175, Loss: 1.2452576160430908\n",
      "Epoch 3, Iteration 176, Loss: 0.8963403701782227\n",
      "Epoch 3, Iteration 177, Loss: 1.4698046445846558\n",
      "Epoch 3, Iteration 178, Loss: 1.2378549575805664\n",
      "Epoch 3, Iteration 179, Loss: 1.171756625175476\n",
      "Epoch 3, Iteration 180, Loss: 1.0270260572433472\n",
      "Epoch 3, Iteration 181, Loss: 1.2656923532485962\n",
      "Epoch 3, Iteration 182, Loss: 1.2060458660125732\n",
      "Epoch 3, Iteration 183, Loss: 0.9722725749015808\n",
      "Epoch 3, Iteration 184, Loss: 1.178126335144043\n",
      "Epoch 3, Iteration 185, Loss: 0.9838789105415344\n",
      "Epoch 3, Iteration 186, Loss: 0.9888346791267395\n",
      "Epoch 3, Iteration 187, Loss: 1.0252089500427246\n",
      "Epoch 3, Iteration 188, Loss: 1.0491896867752075\n",
      "Epoch 3, Iteration 189, Loss: 1.0294642448425293\n",
      "Epoch 3, Iteration 190, Loss: 1.3679533004760742\n",
      "Epoch 3, Iteration 191, Loss: 1.1369107961654663\n",
      "Epoch 3, Iteration 192, Loss: 1.1578618288040161\n",
      "Epoch 3, Iteration 193, Loss: 1.1637741327285767\n",
      "Epoch 3, Iteration 194, Loss: 1.0835444927215576\n",
      "Epoch 3, Iteration 195, Loss: 0.9780138731002808\n",
      "Epoch 3, Iteration 196, Loss: 1.1008105278015137\n",
      "Epoch 3, Iteration 197, Loss: 1.1271520853042603\n",
      "Epoch 3, Iteration 198, Loss: 1.097278118133545\n",
      "Epoch 3, Iteration 199, Loss: 1.1458309888839722\n",
      "Epoch 3, Iteration 200, Loss: 0.9791399240493774\n",
      "Epoch 3, Iteration 200, Valid Loss: 0.8715465664863586\n",
      "Epoch 3, Iteration 201, Loss: 1.0509777069091797\n",
      "Epoch 3, Iteration 202, Loss: 1.115858554840088\n",
      "Epoch 3, Iteration 203, Loss: 0.990219235420227\n",
      "Epoch 3, Iteration 204, Loss: 1.0056238174438477\n",
      "Epoch 3, Iteration 205, Loss: 0.8537067770957947\n",
      "Epoch 3, Iteration 206, Loss: 0.9012641310691833\n",
      "Epoch 3, Iteration 207, Loss: 1.1837177276611328\n",
      "Epoch 3, Iteration 208, Loss: 1.5375564098358154\n",
      "Epoch 3, Iteration 209, Loss: 1.4673389196395874\n",
      "Epoch 3, Iteration 210, Loss: 1.4932780265808105\n",
      "Epoch 3, Iteration 211, Loss: 1.5765272378921509\n",
      "Epoch 3, Iteration 212, Loss: 1.3258380889892578\n",
      "Epoch 3, Iteration 213, Loss: 1.5202054977416992\n",
      "Epoch 3, Iteration 214, Loss: 1.1584193706512451\n",
      "Epoch 3, Iteration 215, Loss: 1.0162861347198486\n",
      "Epoch 3, Iteration 216, Loss: 1.0785199403762817\n",
      "Epoch 3, Iteration 217, Loss: 1.0474140644073486\n",
      "Epoch 3, Iteration 218, Loss: 1.1698884963989258\n",
      "Epoch 3, Iteration 219, Loss: 1.2267065048217773\n",
      "Epoch 3, Iteration 220, Loss: 1.0924490690231323\n",
      "Epoch 3, Iteration 221, Loss: 1.067989468574524\n",
      "Epoch 3, Iteration 222, Loss: 1.1484380960464478\n",
      "Epoch 3, Iteration 223, Loss: 1.0206701755523682\n",
      "Epoch 3, Iteration 224, Loss: 0.9237676858901978\n",
      "Epoch 3, Iteration 225, Loss: 1.1694320440292358\n",
      "Epoch 3, Iteration 226, Loss: 1.2708507776260376\n",
      "Epoch 3, Iteration 227, Loss: 1.2210955619812012\n",
      "Epoch 3, Iteration 228, Loss: 1.0144001245498657\n",
      "Epoch 3, Iteration 229, Loss: 1.010628581047058\n",
      "Epoch 3, Iteration 230, Loss: 1.0594536066055298\n",
      "Epoch 3, Iteration 231, Loss: 1.0013259649276733\n",
      "Epoch 3, Iteration 232, Loss: 1.2734309434890747\n",
      "Epoch 3, Iteration 233, Loss: 1.3651546239852905\n",
      "Epoch 3, Iteration 234, Loss: 1.3045594692230225\n",
      "Epoch 3, Iteration 235, Loss: 0.9043905138969421\n",
      "Epoch 3, Iteration 236, Loss: 1.1737165451049805\n",
      "Epoch 3, Iteration 237, Loss: 1.0688029527664185\n",
      "Epoch 3, Iteration 238, Loss: 0.9446855187416077\n",
      "Epoch 3, Iteration 239, Loss: 1.2719115018844604\n",
      "Epoch 3, Iteration 240, Loss: 0.892680287361145\n",
      "Epoch 3, Iteration 241, Loss: 1.0080119371414185\n",
      "Epoch 3, Iteration 242, Loss: 0.9498996734619141\n",
      "Epoch 3, Iteration 243, Loss: 1.1420079469680786\n",
      "Epoch 3, Iteration 244, Loss: 1.0173368453979492\n",
      "Epoch 3, Iteration 245, Loss: 0.8772737979888916\n",
      "Epoch 3, Iteration 246, Loss: 1.017457365989685\n",
      "Epoch 3, Iteration 247, Loss: 0.9700038433074951\n",
      "Epoch 3, Iteration 248, Loss: 1.070753812789917\n",
      "Epoch 3, Iteration 249, Loss: 0.9691004753112793\n",
      "Epoch 3, Iteration 250, Loss: 1.2310051918029785\n",
      "Epoch 3, Iteration 250, Valid Loss: 0.849982500076294\n",
      "Epoch 3, Iteration 251, Loss: 1.0805084705352783\n",
      "Epoch 3, Iteration 252, Loss: 1.249366283416748\n",
      "Epoch 3, Iteration 253, Loss: 1.2285687923431396\n",
      "Epoch 3, Iteration 254, Loss: 1.1098695993423462\n",
      "Epoch 3, Iteration 255, Loss: 1.1597620248794556\n",
      "Epoch 3, Iteration 256, Loss: 1.1866384744644165\n",
      "Epoch 3, Iteration 257, Loss: 1.0084737539291382\n",
      "Epoch 3, Iteration 258, Loss: 1.1751222610473633\n",
      "Epoch 3, Iteration 259, Loss: 0.8849813342094421\n",
      "Epoch 3, Iteration 260, Loss: 1.1103185415267944\n",
      "Epoch 3, Iteration 261, Loss: 1.0778324604034424\n",
      "Epoch 3, Iteration 262, Loss: 0.8984288573265076\n",
      "Epoch 3, Iteration 263, Loss: 1.2145118713378906\n",
      "Epoch 3, Iteration 264, Loss: 1.104931354522705\n",
      "Epoch 3, Iteration 265, Loss: 1.1422070264816284\n",
      "Epoch 3, Iteration 266, Loss: 1.1350051164627075\n",
      "Epoch 3, Iteration 267, Loss: 1.006165623664856\n",
      "Epoch 3, Iteration 268, Loss: 0.9987993836402893\n",
      "Epoch 3, Iteration 269, Loss: 0.9633957147598267\n",
      "Epoch 3, Iteration 270, Loss: 0.9597208499908447\n",
      "Epoch 3, Iteration 271, Loss: 1.070546269416809\n",
      "Epoch 3, Iteration 272, Loss: 0.9309868216514587\n",
      "Epoch 3, Iteration 273, Loss: 1.1047617197036743\n",
      "Epoch 3, Iteration 274, Loss: 0.912613034248352\n",
      "Epoch 3, Iteration 275, Loss: 0.9102969765663147\n",
      "Epoch 3, Iteration 276, Loss: 0.9403843879699707\n",
      "Epoch 3, Iteration 277, Loss: 0.9233104586601257\n",
      "Epoch 3, Iteration 278, Loss: 1.1074129343032837\n",
      "Epoch 3, Iteration 279, Loss: 0.9227139353752136\n",
      "Epoch 3, Iteration 280, Loss: 0.8607205152511597\n",
      "Epoch 3, Iteration 281, Loss: 0.9702515006065369\n",
      "Epoch 3, Iteration 282, Loss: 1.0536272525787354\n",
      "Epoch 3, Iteration 283, Loss: 0.9862913489341736\n",
      "Epoch 3, Iteration 284, Loss: 1.0107101202011108\n",
      "Epoch 3, Iteration 285, Loss: 0.9813616275787354\n",
      "Epoch 3, Iteration 286, Loss: 1.0122308731079102\n",
      "Epoch 3, Iteration 287, Loss: 0.9808397889137268\n",
      "Epoch 3, Iteration 288, Loss: 0.8916695713996887\n",
      "Epoch 3, Iteration 289, Loss: 1.0688409805297852\n",
      "Epoch 3, Iteration 290, Loss: 0.8491417765617371\n",
      "Epoch 3, Iteration 291, Loss: 0.9374788999557495\n",
      "Epoch 3, Iteration 292, Loss: 1.1233673095703125\n",
      "Epoch 3, Iteration 293, Loss: 1.2491856813430786\n",
      "Epoch 3, Iteration 294, Loss: 0.989552915096283\n",
      "Epoch 3, Iteration 295, Loss: 0.9867379069328308\n",
      "Epoch 3, Iteration 296, Loss: 1.2609144449234009\n",
      "Epoch 3, Iteration 297, Loss: 1.2031022310256958\n",
      "Epoch 3, Iteration 298, Loss: 0.9968258738517761\n",
      "Epoch 3, Iteration 299, Loss: 0.9741206765174866\n",
      "Epoch 3, Iteration 300, Loss: 0.8281399607658386\n",
      "Epoch 3, Iteration 300, Valid Loss: 0.8122333288192749\n",
      "Epoch 3, Iteration 301, Loss: 0.9889159202575684\n",
      "Epoch 3, Iteration 302, Loss: 0.9044119715690613\n",
      "Epoch 3, Iteration 303, Loss: 1.2266942262649536\n",
      "Epoch 3, Iteration 304, Loss: 0.930001437664032\n",
      "Epoch 3, Iteration 305, Loss: 0.8127192854881287\n",
      "Epoch 3, Iteration 306, Loss: 0.9039492011070251\n",
      "Epoch 3, Iteration 307, Loss: 0.9670590162277222\n",
      "Epoch 3, Iteration 308, Loss: 0.8859166502952576\n",
      "Epoch 3, Iteration 309, Loss: 0.9379065036773682\n",
      "Epoch 3, Iteration 310, Loss: 1.1248787641525269\n",
      "Epoch 3, Iteration 311, Loss: 0.8210820555686951\n",
      "Epoch 3, Iteration 312, Loss: 0.8343709111213684\n",
      "Epoch 3, Iteration 313, Loss: 1.2041070461273193\n",
      "Epoch 3, Iteration 314, Loss: 0.9703975915908813\n",
      "Epoch 3, Iteration 315, Loss: 0.7820767760276794\n",
      "Epoch 3, Iteration 316, Loss: 1.1278806924819946\n",
      "Epoch 3, Iteration 317, Loss: 0.9880719780921936\n",
      "Epoch 3, Iteration 318, Loss: 0.8414582014083862\n",
      "Epoch 3, Iteration 319, Loss: 1.0974119901657104\n",
      "Epoch 3, Iteration 320, Loss: 0.7795879244804382\n",
      "Epoch 3, Iteration 321, Loss: 0.8628724217414856\n",
      "Epoch 3, Iteration 322, Loss: 0.9061481952667236\n",
      "Epoch 3, Iteration 323, Loss: 0.998862624168396\n",
      "Epoch 3, Iteration 324, Loss: 1.0084749460220337\n",
      "Epoch 3, Iteration 325, Loss: 1.0056419372558594\n",
      "Epoch 3, Iteration 326, Loss: 0.9592318534851074\n",
      "Epoch 3, Iteration 327, Loss: 0.8717612624168396\n",
      "Epoch 3, Iteration 328, Loss: 0.9053943157196045\n",
      "Epoch 3, Iteration 329, Loss: 1.1376351118087769\n",
      "Epoch 3, Iteration 330, Loss: 0.7972564101219177\n",
      "Epoch 3, Iteration 331, Loss: 1.015381932258606\n",
      "Epoch 3, Iteration 332, Loss: 0.9913408160209656\n",
      "Epoch 3, Iteration 333, Loss: 1.1639838218688965\n",
      "Epoch 3, Iteration 334, Loss: 0.8892269730567932\n",
      "Epoch 3, Iteration 335, Loss: 1.097224473953247\n",
      "Epoch 3, Iteration 336, Loss: 0.978518545627594\n",
      "Epoch 3, Iteration 337, Loss: 0.8323197364807129\n",
      "Epoch 3, Iteration 338, Loss: 1.2018407583236694\n",
      "Epoch 3, Iteration 339, Loss: 1.063137173652649\n",
      "Epoch 3, Iteration 340, Loss: 0.8312650322914124\n",
      "Epoch 3, Iteration 341, Loss: 0.8695537447929382\n",
      "Epoch 3, Iteration 342, Loss: 0.9030860662460327\n",
      "Epoch 3, Iteration 343, Loss: 1.0115058422088623\n",
      "Epoch 3, Iteration 344, Loss: 1.1284385919570923\n",
      "Epoch 3, Iteration 345, Loss: 0.9886875152587891\n",
      "Epoch 3, Iteration 346, Loss: 0.9631791710853577\n",
      "Epoch 3, Iteration 347, Loss: 0.7962947487831116\n",
      "Epoch 3, Iteration 348, Loss: 0.8785473108291626\n",
      "Epoch 3, Iteration 349, Loss: 1.01310133934021\n",
      "Epoch 3, Iteration 350, Loss: 0.8364405035972595\n",
      "Epoch 3, Iteration 350, Valid Loss: 0.8320814371109009\n",
      "Epoch 3, Iteration 351, Loss: 1.0086307525634766\n",
      "Epoch 3, Iteration 352, Loss: 0.9517488479614258\n",
      "Epoch 3, Iteration 353, Loss: 0.9299371838569641\n",
      "Epoch 3, Iteration 354, Loss: 0.8927383422851562\n",
      "Epoch 3, Iteration 355, Loss: 0.9115539789199829\n",
      "Epoch 3, Iteration 356, Loss: 1.0307031869888306\n",
      "Epoch 3, Iteration 357, Loss: 0.9059122800827026\n",
      "Epoch 3, Iteration 358, Loss: 0.9597399830818176\n",
      "Epoch 3, Iteration 359, Loss: 0.8719119429588318\n",
      "Epoch 3, Iteration 360, Loss: 0.7880678772926331\n",
      "Epoch 3, Iteration 361, Loss: 0.8900988101959229\n",
      "Epoch 3, Iteration 362, Loss: 1.0620691776275635\n",
      "Epoch 3, Iteration 363, Loss: 0.909976601600647\n",
      "Epoch 3, Iteration 364, Loss: 0.971415638923645\n",
      "Epoch 3, Iteration 365, Loss: 1.2009754180908203\n",
      "Epoch 3, Iteration 366, Loss: 1.0253280401229858\n",
      "Epoch 3, Iteration 367, Loss: 1.069616675376892\n",
      "Epoch 3, Iteration 368, Loss: 0.9698205590248108\n",
      "Epoch 3, Iteration 369, Loss: 1.0620988607406616\n",
      "Epoch 3, Iteration 370, Loss: 1.0688802003860474\n",
      "Epoch 3, Iteration 371, Loss: 1.0139789581298828\n",
      "Epoch 3, Iteration 372, Loss: 0.8720187544822693\n",
      "Epoch 3, Iteration 373, Loss: 0.876653254032135\n",
      "Epoch 3, Iteration 374, Loss: 1.1381500959396362\n",
      "Epoch 3, Iteration 375, Loss: 0.8554465174674988\n",
      "Epoch 3, Iteration 376, Loss: 1.1245790719985962\n",
      "Epoch 3, Iteration 377, Loss: 0.9479237794876099\n",
      "Epoch 3, Iteration 378, Loss: 0.9470189213752747\n",
      "Epoch 3, Iteration 379, Loss: 0.8464686870574951\n",
      "Epoch 3, Iteration 380, Loss: 0.8465123772621155\n",
      "Epoch 3, Iteration 381, Loss: 0.8546815514564514\n",
      "Epoch 3, Iteration 382, Loss: 0.7465135455131531\n",
      "Epoch 3, Iteration 383, Loss: 0.851097047328949\n",
      "Epoch 3, Iteration 384, Loss: 0.9922703504562378\n",
      "Epoch 3, Iteration 385, Loss: 0.8724763989448547\n",
      "Epoch 3, Iteration 386, Loss: 0.8558096885681152\n",
      "Epoch 3, Iteration 387, Loss: 1.2228796482086182\n",
      "Epoch 3, Iteration 388, Loss: 0.8676925301551819\n",
      "Epoch 3, Iteration 389, Loss: 1.004194974899292\n",
      "Epoch 3, Iteration 390, Loss: 1.1677724123001099\n",
      "Epoch 3, Iteration 391, Loss: 0.9158753752708435\n",
      "Epoch 4/15, Loss: 1.0835064941523027\n",
      "Epoch 4, Iteration 0, Loss: 1.04493248462677\n",
      "Epoch 4, Iteration 1, Loss: 1.3046401739120483\n",
      "Epoch 4, Iteration 2, Loss: 1.1713558435440063\n",
      "Epoch 4, Iteration 3, Loss: 1.0851577520370483\n",
      "Epoch 4, Iteration 4, Loss: 1.0627371072769165\n",
      "Epoch 4, Iteration 5, Loss: 1.120818018913269\n",
      "Epoch 4, Iteration 6, Loss: 0.9626101851463318\n",
      "Epoch 4, Iteration 7, Loss: 0.9897845387458801\n",
      "Epoch 4, Iteration 8, Loss: 1.0065405368804932\n",
      "Epoch 4, Iteration 9, Loss: 1.2047016620635986\n",
      "Epoch 4, Iteration 10, Loss: 1.1420129537582397\n",
      "Epoch 4, Iteration 11, Loss: 1.044661283493042\n",
      "Epoch 4, Iteration 12, Loss: 0.8987705111503601\n",
      "Epoch 4, Iteration 13, Loss: 1.0800563097000122\n",
      "Epoch 4, Iteration 14, Loss: 1.0730741024017334\n",
      "Epoch 4, Iteration 15, Loss: 1.1150054931640625\n",
      "Epoch 4, Iteration 16, Loss: 1.0022896528244019\n",
      "Epoch 4, Iteration 17, Loss: 1.1185872554779053\n",
      "Epoch 4, Iteration 18, Loss: 0.9658347368240356\n",
      "Epoch 4, Iteration 19, Loss: 1.0224740505218506\n",
      "Epoch 4, Iteration 20, Loss: 0.9444252848625183\n",
      "Epoch 4, Iteration 21, Loss: 0.9879769682884216\n",
      "Epoch 4, Iteration 22, Loss: 0.9874919056892395\n",
      "Epoch 4, Iteration 23, Loss: 1.0119214057922363\n",
      "Epoch 4, Iteration 24, Loss: 0.9743785858154297\n",
      "Epoch 4, Iteration 25, Loss: 0.9252254366874695\n",
      "Epoch 4, Iteration 26, Loss: 1.3216723203659058\n",
      "Epoch 4, Iteration 27, Loss: 1.02146577835083\n",
      "Epoch 4, Iteration 28, Loss: 1.016772747039795\n",
      "Epoch 4, Iteration 29, Loss: 1.2176268100738525\n",
      "Epoch 4, Iteration 30, Loss: 1.2565345764160156\n",
      "Epoch 4, Iteration 31, Loss: 0.9946423768997192\n",
      "Epoch 4, Iteration 32, Loss: 1.0306272506713867\n",
      "Epoch 4, Iteration 33, Loss: 1.0000278949737549\n",
      "Epoch 4, Iteration 34, Loss: 0.9563770294189453\n",
      "Epoch 4, Iteration 35, Loss: 1.137046456336975\n",
      "Epoch 4, Iteration 36, Loss: 0.8737364411354065\n",
      "Epoch 4, Iteration 37, Loss: 0.8587380051612854\n",
      "Epoch 4, Iteration 38, Loss: 0.9491006731987\n",
      "Epoch 4, Iteration 39, Loss: 0.9534735679626465\n",
      "Epoch 4, Iteration 40, Loss: 0.9804425239562988\n",
      "Epoch 4, Iteration 41, Loss: 1.0616225004196167\n",
      "Epoch 4, Iteration 42, Loss: 0.9961390495300293\n",
      "Epoch 4, Iteration 43, Loss: 0.9008240699768066\n",
      "Epoch 4, Iteration 44, Loss: 1.0030969381332397\n",
      "Epoch 4, Iteration 45, Loss: 0.9670514464378357\n",
      "Epoch 4, Iteration 46, Loss: 1.0012843608856201\n",
      "Epoch 4, Iteration 47, Loss: 1.0293381214141846\n",
      "Epoch 4, Iteration 48, Loss: 1.1769049167633057\n",
      "Epoch 4, Iteration 49, Loss: 0.9655376076698303\n",
      "Epoch 4, Iteration 50, Loss: 0.9544277191162109\n",
      "Epoch 4, Iteration 50, Valid Loss: 0.789033055305481\n",
      "Epoch 4, Iteration 51, Loss: 0.9552796483039856\n",
      "Epoch 4, Iteration 52, Loss: 0.9759229421615601\n",
      "Epoch 4, Iteration 53, Loss: 1.105172038078308\n",
      "Epoch 4, Iteration 54, Loss: 1.2082394361495972\n",
      "Epoch 4, Iteration 55, Loss: 1.1055997610092163\n",
      "Epoch 4, Iteration 56, Loss: 0.9873883724212646\n",
      "Epoch 4, Iteration 57, Loss: 1.0587176084518433\n",
      "Epoch 4, Iteration 58, Loss: 0.9447943568229675\n",
      "Epoch 4, Iteration 59, Loss: 0.9233458638191223\n",
      "Epoch 4, Iteration 60, Loss: 0.9466854929924011\n",
      "Epoch 4, Iteration 61, Loss: 0.8260363936424255\n",
      "Epoch 4, Iteration 62, Loss: 0.9851475358009338\n",
      "Epoch 4, Iteration 63, Loss: 0.9184985756874084\n",
      "Epoch 4, Iteration 64, Loss: 0.9649332761764526\n",
      "Epoch 4, Iteration 65, Loss: 1.154616355895996\n",
      "Epoch 4, Iteration 66, Loss: 0.9905996918678284\n",
      "Epoch 4, Iteration 67, Loss: 0.7766335010528564\n",
      "Epoch 4, Iteration 68, Loss: 1.0221779346466064\n",
      "Epoch 4, Iteration 69, Loss: 1.1311570405960083\n",
      "Epoch 4, Iteration 70, Loss: 1.2370394468307495\n",
      "Epoch 4, Iteration 71, Loss: 1.0399134159088135\n",
      "Epoch 4, Iteration 72, Loss: 0.975234866142273\n",
      "Epoch 4, Iteration 73, Loss: 0.8397454619407654\n",
      "Epoch 4, Iteration 74, Loss: 1.2318857908248901\n",
      "Epoch 4, Iteration 75, Loss: 1.0675255060195923\n",
      "Epoch 4, Iteration 76, Loss: 0.9524127840995789\n",
      "Epoch 4, Iteration 77, Loss: 0.7767812013626099\n",
      "Epoch 4, Iteration 78, Loss: 1.2348991632461548\n",
      "Epoch 4, Iteration 79, Loss: 1.269155740737915\n",
      "Epoch 4, Iteration 80, Loss: 0.8462423086166382\n",
      "Epoch 4, Iteration 81, Loss: 1.262032151222229\n",
      "Epoch 4, Iteration 82, Loss: 1.0176899433135986\n",
      "Epoch 4, Iteration 83, Loss: 0.780750572681427\n",
      "Epoch 4, Iteration 84, Loss: 1.1648318767547607\n",
      "Epoch 4, Iteration 85, Loss: 1.0028105974197388\n",
      "Epoch 4, Iteration 86, Loss: 1.2528326511383057\n",
      "Epoch 4, Iteration 87, Loss: 0.9641894102096558\n",
      "Epoch 4, Iteration 88, Loss: 0.9550859928131104\n",
      "Epoch 4, Iteration 89, Loss: 1.1221951246261597\n",
      "Epoch 4, Iteration 90, Loss: 0.8643262982368469\n",
      "Epoch 4, Iteration 91, Loss: 0.9606046080589294\n",
      "Epoch 4, Iteration 92, Loss: 0.7536492943763733\n",
      "Epoch 4, Iteration 93, Loss: 0.9457417726516724\n",
      "Epoch 4, Iteration 94, Loss: 1.2672992944717407\n",
      "Epoch 4, Iteration 95, Loss: 0.8276141285896301\n",
      "Epoch 4, Iteration 96, Loss: 1.0036189556121826\n",
      "Epoch 4, Iteration 97, Loss: 0.8344535231590271\n",
      "Epoch 4, Iteration 98, Loss: 1.0206553936004639\n",
      "Epoch 4, Iteration 99, Loss: 1.0749164819717407\n",
      "Epoch 4, Iteration 100, Loss: 0.8530787229537964\n",
      "Epoch 4, Iteration 100, Valid Loss: 0.8358920812606812\n",
      "Epoch 4, Iteration 101, Loss: 1.1367682218551636\n",
      "Epoch 4, Iteration 102, Loss: 1.1306297779083252\n",
      "Epoch 4, Iteration 103, Loss: 0.9723981022834778\n",
      "Epoch 4, Iteration 104, Loss: 1.1775285005569458\n",
      "Epoch 4, Iteration 105, Loss: 1.0836060047149658\n",
      "Epoch 4, Iteration 106, Loss: 0.9983205199241638\n",
      "Epoch 4, Iteration 107, Loss: 1.1856428384780884\n",
      "Epoch 4, Iteration 108, Loss: 0.9502626061439514\n",
      "Epoch 4, Iteration 109, Loss: 1.2779102325439453\n",
      "Epoch 4, Iteration 110, Loss: 1.2964445352554321\n",
      "Epoch 4, Iteration 111, Loss: 1.025543212890625\n",
      "Epoch 4, Iteration 112, Loss: 1.0854824781417847\n",
      "Epoch 4, Iteration 113, Loss: 0.8665279746055603\n",
      "Epoch 4, Iteration 114, Loss: 1.2526308298110962\n",
      "Epoch 4, Iteration 115, Loss: 1.064929723739624\n",
      "Epoch 4, Iteration 116, Loss: 0.8378612399101257\n",
      "Epoch 4, Iteration 117, Loss: 0.7548313736915588\n",
      "Epoch 4, Iteration 118, Loss: 1.089975118637085\n",
      "Epoch 4, Iteration 119, Loss: 0.8983413577079773\n",
      "Epoch 4, Iteration 120, Loss: 1.0431067943572998\n",
      "Epoch 4, Iteration 121, Loss: 1.026232361793518\n",
      "Epoch 4, Iteration 122, Loss: 1.007375717163086\n",
      "Epoch 4, Iteration 123, Loss: 0.9735695719718933\n",
      "Epoch 4, Iteration 124, Loss: 0.8594191670417786\n",
      "Epoch 4, Iteration 125, Loss: 0.9335448741912842\n",
      "Epoch 4, Iteration 126, Loss: 0.8527103066444397\n",
      "Epoch 4, Iteration 127, Loss: 1.22467839717865\n",
      "Epoch 4, Iteration 128, Loss: 1.1758852005004883\n",
      "Epoch 4, Iteration 129, Loss: 1.093228816986084\n",
      "Epoch 4, Iteration 130, Loss: 1.3685531616210938\n",
      "Epoch 4, Iteration 131, Loss: 0.8632916212081909\n",
      "Epoch 4, Iteration 132, Loss: 0.8966405391693115\n",
      "Epoch 4, Iteration 133, Loss: 1.2935482263565063\n",
      "Epoch 4, Iteration 134, Loss: 0.9857550859451294\n",
      "Epoch 4, Iteration 135, Loss: 1.164029836654663\n",
      "Epoch 4, Iteration 136, Loss: 0.9759976863861084\n",
      "Epoch 4, Iteration 137, Loss: 0.9328454732894897\n",
      "Epoch 4, Iteration 138, Loss: 0.9089829325675964\n",
      "Epoch 4, Iteration 139, Loss: 0.9844706058502197\n",
      "Epoch 4, Iteration 140, Loss: 0.9412792325019836\n",
      "Epoch 4, Iteration 141, Loss: 1.5075807571411133\n",
      "Epoch 4, Iteration 142, Loss: 0.9746980667114258\n",
      "Epoch 4, Iteration 143, Loss: 0.9841091632843018\n",
      "Epoch 4, Iteration 144, Loss: 0.9087378978729248\n",
      "Epoch 4, Iteration 145, Loss: 1.0709941387176514\n",
      "Epoch 4, Iteration 146, Loss: 1.0315333604812622\n",
      "Epoch 4, Iteration 147, Loss: 0.9750149250030518\n",
      "Epoch 4, Iteration 148, Loss: 0.8013947606086731\n",
      "Epoch 4, Iteration 149, Loss: 0.9442793130874634\n",
      "Epoch 4, Iteration 150, Loss: 0.9885415434837341\n",
      "Epoch 4, Iteration 150, Valid Loss: 0.8005504012107849\n",
      "Epoch 4, Iteration 151, Loss: 1.048242449760437\n",
      "Epoch 4, Iteration 152, Loss: 0.9130152463912964\n",
      "Epoch 4, Iteration 153, Loss: 1.0007877349853516\n",
      "Epoch 4, Iteration 154, Loss: 0.785528302192688\n",
      "Epoch 4, Iteration 155, Loss: 1.0587940216064453\n",
      "Epoch 4, Iteration 156, Loss: 0.9069067239761353\n",
      "Epoch 4, Iteration 157, Loss: 1.395507574081421\n",
      "Epoch 4, Iteration 158, Loss: 1.0592247247695923\n",
      "Epoch 4, Iteration 159, Loss: 1.123335599899292\n",
      "Epoch 4, Iteration 160, Loss: 0.973874032497406\n",
      "Epoch 4, Iteration 161, Loss: 1.0100972652435303\n",
      "Epoch 4, Iteration 162, Loss: 1.163033127784729\n",
      "Epoch 4, Iteration 163, Loss: 0.7532022595405579\n",
      "Epoch 4, Iteration 164, Loss: 0.9793041348457336\n",
      "Epoch 4, Iteration 165, Loss: 0.944180965423584\n",
      "Epoch 4, Iteration 166, Loss: 0.9875420928001404\n",
      "Epoch 4, Iteration 167, Loss: 1.0977280139923096\n",
      "Epoch 4, Iteration 168, Loss: 1.0338459014892578\n",
      "Epoch 4, Iteration 169, Loss: 0.9565362930297852\n",
      "Epoch 4, Iteration 170, Loss: 1.2069339752197266\n",
      "Epoch 4, Iteration 171, Loss: 1.3006178140640259\n",
      "Epoch 4, Iteration 172, Loss: 0.9766036868095398\n",
      "Epoch 4, Iteration 173, Loss: 0.9348631501197815\n",
      "Epoch 4, Iteration 174, Loss: 1.1666356325149536\n",
      "Epoch 4, Iteration 175, Loss: 1.0312952995300293\n",
      "Epoch 4, Iteration 176, Loss: 0.7275395393371582\n",
      "Epoch 4, Iteration 177, Loss: 1.3922151327133179\n",
      "Epoch 4, Iteration 178, Loss: 1.255466341972351\n",
      "Epoch 4, Iteration 179, Loss: 1.0007988214492798\n",
      "Epoch 4, Iteration 180, Loss: 0.9192686080932617\n",
      "Epoch 4, Iteration 181, Loss: 1.0477421283721924\n",
      "Epoch 4, Iteration 182, Loss: 1.1131047010421753\n",
      "Epoch 4, Iteration 183, Loss: 0.9019898176193237\n",
      "Epoch 4, Iteration 184, Loss: 0.8490493893623352\n",
      "Epoch 4, Iteration 185, Loss: 0.8884671330451965\n",
      "Epoch 4, Iteration 186, Loss: 1.005288004875183\n",
      "Epoch 4, Iteration 187, Loss: 0.9145441651344299\n",
      "Epoch 4, Iteration 188, Loss: 0.9966986775398254\n",
      "Epoch 4, Iteration 189, Loss: 0.923087477684021\n",
      "Epoch 4, Iteration 190, Loss: 1.0873467922210693\n",
      "Epoch 4, Iteration 191, Loss: 1.0415529012680054\n",
      "Epoch 4, Iteration 192, Loss: 1.051780343055725\n",
      "Epoch 4, Iteration 193, Loss: 1.0371942520141602\n",
      "Epoch 4, Iteration 194, Loss: 0.9284853935241699\n",
      "Epoch 4, Iteration 195, Loss: 0.8590038418769836\n",
      "Epoch 4, Iteration 196, Loss: 1.0250333547592163\n",
      "Epoch 4, Iteration 197, Loss: 0.9701385498046875\n",
      "Epoch 4, Iteration 198, Loss: 1.002532958984375\n",
      "Epoch 4, Iteration 199, Loss: 1.0520479679107666\n",
      "Epoch 4, Iteration 200, Loss: 0.906775176525116\n",
      "Epoch 4, Iteration 200, Valid Loss: 0.7884825468063354\n",
      "Epoch 4, Iteration 201, Loss: 0.9057056307792664\n",
      "Epoch 4, Iteration 202, Loss: 1.0271027088165283\n",
      "Epoch 4, Iteration 203, Loss: 0.8727607131004333\n",
      "Epoch 4, Iteration 204, Loss: 0.9244159460067749\n",
      "Epoch 4, Iteration 205, Loss: 0.7402735948562622\n",
      "Epoch 4, Iteration 206, Loss: 0.8623390197753906\n",
      "Epoch 4, Iteration 207, Loss: 1.0889272689819336\n",
      "Epoch 4, Iteration 208, Loss: 1.375203251838684\n",
      "Epoch 4, Iteration 209, Loss: 1.3705648183822632\n",
      "Epoch 4, Iteration 210, Loss: 1.2871265411376953\n",
      "Epoch 4, Iteration 211, Loss: 1.4108376502990723\n",
      "Epoch 4, Iteration 212, Loss: 1.2058161497116089\n",
      "Epoch 4, Iteration 213, Loss: 1.4323828220367432\n",
      "Epoch 4, Iteration 214, Loss: 1.0427522659301758\n",
      "Epoch 4, Iteration 215, Loss: 0.842722475528717\n",
      "Epoch 4, Iteration 216, Loss: 0.8903362154960632\n",
      "Epoch 4, Iteration 217, Loss: 0.938605010509491\n",
      "Epoch 4, Iteration 218, Loss: 1.0334800481796265\n",
      "Epoch 4, Iteration 219, Loss: 1.1145973205566406\n",
      "Epoch 4, Iteration 220, Loss: 0.9439339637756348\n",
      "Epoch 4, Iteration 221, Loss: 1.028719425201416\n",
      "Epoch 4, Iteration 222, Loss: 1.0397146940231323\n",
      "Epoch 4, Iteration 223, Loss: 0.9531627297401428\n",
      "Epoch 4, Iteration 224, Loss: 0.8542208671569824\n",
      "Epoch 4, Iteration 225, Loss: 1.1103458404541016\n",
      "Epoch 4, Iteration 226, Loss: 1.1312689781188965\n",
      "Epoch 4, Iteration 227, Loss: 1.1284061670303345\n",
      "Epoch 4, Iteration 228, Loss: 1.06633722782135\n",
      "Epoch 4, Iteration 229, Loss: 0.9239729642868042\n",
      "Epoch 4, Iteration 230, Loss: 0.9833782315254211\n",
      "Epoch 4, Iteration 231, Loss: 1.0202014446258545\n",
      "Epoch 4, Iteration 232, Loss: 1.0726557970046997\n",
      "Epoch 4, Iteration 233, Loss: 1.2659790515899658\n",
      "Epoch 4, Iteration 234, Loss: 1.1072067022323608\n",
      "Epoch 4, Iteration 235, Loss: 0.7578668594360352\n",
      "Epoch 4, Iteration 236, Loss: 1.037058711051941\n",
      "Epoch 4, Iteration 237, Loss: 0.9714236855506897\n",
      "Epoch 4, Iteration 238, Loss: 0.8227131962776184\n",
      "Epoch 4, Iteration 239, Loss: 1.1707442998886108\n",
      "Epoch 4, Iteration 240, Loss: 0.90836501121521\n",
      "Epoch 4, Iteration 241, Loss: 0.8773682117462158\n",
      "Epoch 4, Iteration 242, Loss: 0.9065148830413818\n",
      "Epoch 4, Iteration 243, Loss: 0.9834981560707092\n",
      "Epoch 4, Iteration 244, Loss: 0.9067969918251038\n",
      "Epoch 4, Iteration 245, Loss: 0.7182429432868958\n",
      "Epoch 4, Iteration 246, Loss: 0.9426921010017395\n",
      "Epoch 4, Iteration 247, Loss: 0.8898072242736816\n",
      "Epoch 4, Iteration 248, Loss: 0.9476214647293091\n",
      "Epoch 4, Iteration 249, Loss: 0.8944735527038574\n",
      "Epoch 4, Iteration 250, Loss: 1.1961883306503296\n",
      "Epoch 4, Iteration 250, Valid Loss: 0.7561030387878418\n",
      "Epoch 4, Iteration 251, Loss: 0.9501894116401672\n",
      "Epoch 4, Iteration 252, Loss: 1.0761252641677856\n",
      "Epoch 4, Iteration 253, Loss: 1.1006118059158325\n",
      "Epoch 4, Iteration 254, Loss: 1.0629301071166992\n",
      "Epoch 4, Iteration 255, Loss: 0.9724357724189758\n",
      "Epoch 4, Iteration 256, Loss: 1.0841792821884155\n",
      "Epoch 4, Iteration 257, Loss: 0.7964072227478027\n",
      "Epoch 4, Iteration 258, Loss: 1.1883913278579712\n",
      "Epoch 4, Iteration 259, Loss: 0.8510756492614746\n",
      "Epoch 4, Iteration 260, Loss: 1.0325790643692017\n",
      "Epoch 4, Iteration 261, Loss: 0.9506208896636963\n",
      "Epoch 4, Iteration 262, Loss: 0.8163789510726929\n",
      "Epoch 4, Iteration 263, Loss: 1.102190375328064\n",
      "Epoch 4, Iteration 264, Loss: 0.9832547307014465\n",
      "Epoch 4, Iteration 265, Loss: 1.0239163637161255\n",
      "Epoch 4, Iteration 266, Loss: 1.09663724899292\n",
      "Epoch 4, Iteration 267, Loss: 0.9625368714332581\n",
      "Epoch 4, Iteration 268, Loss: 0.9333730340003967\n",
      "Epoch 4, Iteration 269, Loss: 0.8945204615592957\n",
      "Epoch 4, Iteration 270, Loss: 0.9753462076187134\n",
      "Epoch 4, Iteration 271, Loss: 1.0707552433013916\n",
      "Epoch 4, Iteration 272, Loss: 0.8913940191268921\n",
      "Epoch 4, Iteration 273, Loss: 0.9140327572822571\n",
      "Epoch 4, Iteration 274, Loss: 0.8474997282028198\n",
      "Epoch 4, Iteration 275, Loss: 0.810172975063324\n",
      "Epoch 4, Iteration 276, Loss: 0.8478854894638062\n",
      "Epoch 4, Iteration 277, Loss: 0.8520662188529968\n",
      "Epoch 4, Iteration 278, Loss: 1.1034595966339111\n",
      "Epoch 4, Iteration 279, Loss: 0.8682688474655151\n",
      "Epoch 4, Iteration 280, Loss: 0.7712135910987854\n",
      "Epoch 4, Iteration 281, Loss: 0.8801217675209045\n",
      "Epoch 4, Iteration 282, Loss: 0.8711457848548889\n",
      "Epoch 4, Iteration 283, Loss: 0.8997748494148254\n",
      "Epoch 4, Iteration 284, Loss: 0.9355165958404541\n",
      "Epoch 4, Iteration 285, Loss: 0.7878260612487793\n",
      "Epoch 4, Iteration 286, Loss: 0.8657180666923523\n",
      "Epoch 4, Iteration 287, Loss: 0.8548921942710876\n",
      "Epoch 4, Iteration 288, Loss: 0.8136802911758423\n",
      "Epoch 4, Iteration 289, Loss: 0.8319209218025208\n",
      "Epoch 4, Iteration 290, Loss: 0.8363760113716125\n",
      "Epoch 4, Iteration 291, Loss: 0.7681821584701538\n",
      "Epoch 4, Iteration 292, Loss: 1.032207727432251\n",
      "Epoch 4, Iteration 293, Loss: 1.0473662614822388\n",
      "Epoch 4, Iteration 294, Loss: 0.899074375629425\n",
      "Epoch 4, Iteration 295, Loss: 0.8355186581611633\n",
      "Epoch 4, Iteration 296, Loss: 1.136673927307129\n",
      "Epoch 4, Iteration 297, Loss: 1.0927625894546509\n",
      "Epoch 4, Iteration 298, Loss: 0.9679494500160217\n",
      "Epoch 4, Iteration 299, Loss: 0.885459303855896\n",
      "Epoch 4, Iteration 300, Loss: 0.7614717483520508\n",
      "Epoch 4, Iteration 300, Valid Loss: 0.751833438873291\n",
      "Epoch 4, Iteration 301, Loss: 0.8769534230232239\n",
      "Epoch 4, Iteration 302, Loss: 0.8666693568229675\n",
      "Epoch 4, Iteration 303, Loss: 1.0209133625030518\n",
      "Epoch 4, Iteration 304, Loss: 0.9227496981620789\n",
      "Epoch 4, Iteration 305, Loss: 0.785307765007019\n",
      "Epoch 4, Iteration 306, Loss: 0.8360674381256104\n",
      "Epoch 4, Iteration 307, Loss: 0.9192147850990295\n",
      "Epoch 4, Iteration 308, Loss: 0.8306171894073486\n",
      "Epoch 4, Iteration 309, Loss: 0.9216207265853882\n",
      "Epoch 4, Iteration 310, Loss: 1.0503677129745483\n",
      "Epoch 4, Iteration 311, Loss: 0.8514872789382935\n",
      "Epoch 4, Iteration 312, Loss: 0.8770484924316406\n",
      "Epoch 4, Iteration 313, Loss: 1.1406798362731934\n",
      "Epoch 4, Iteration 314, Loss: 0.84687739610672\n",
      "Epoch 4, Iteration 315, Loss: 0.836872398853302\n",
      "Epoch 4, Iteration 316, Loss: 0.9976776838302612\n",
      "Epoch 4, Iteration 317, Loss: 0.923591673374176\n",
      "Epoch 4, Iteration 318, Loss: 0.8875922560691833\n",
      "Epoch 4, Iteration 319, Loss: 1.1049572229385376\n",
      "Epoch 4, Iteration 320, Loss: 0.6908894181251526\n",
      "Epoch 4, Iteration 321, Loss: 0.9398485422134399\n",
      "Epoch 4, Iteration 322, Loss: 0.826592743396759\n",
      "Epoch 4, Iteration 323, Loss: 0.8781220316886902\n",
      "Epoch 4, Iteration 324, Loss: 0.976137638092041\n",
      "Epoch 4, Iteration 325, Loss: 0.8855038285255432\n",
      "Epoch 4, Iteration 326, Loss: 0.8487350940704346\n",
      "Epoch 4, Iteration 327, Loss: 0.8048567771911621\n",
      "Epoch 4, Iteration 328, Loss: 0.7943364977836609\n",
      "Epoch 4, Iteration 329, Loss: 1.0495710372924805\n",
      "Epoch 4, Iteration 330, Loss: 0.7530981302261353\n",
      "Epoch 4, Iteration 331, Loss: 0.8933329582214355\n",
      "Epoch 4, Iteration 332, Loss: 1.1955134868621826\n",
      "Epoch 4, Iteration 333, Loss: 1.1015403270721436\n",
      "Epoch 4, Iteration 334, Loss: 0.8689898252487183\n",
      "Epoch 4, Iteration 335, Loss: 0.9878103733062744\n",
      "Epoch 4, Iteration 336, Loss: 0.9170424938201904\n",
      "Epoch 4, Iteration 337, Loss: 0.7995640635490417\n",
      "Epoch 4, Iteration 338, Loss: 1.0757330656051636\n",
      "Epoch 4, Iteration 339, Loss: 0.9188842177391052\n",
      "Epoch 4, Iteration 340, Loss: 0.785496711730957\n",
      "Epoch 4, Iteration 341, Loss: 0.8162081241607666\n",
      "Epoch 4, Iteration 342, Loss: 0.8331969380378723\n",
      "Epoch 4, Iteration 343, Loss: 0.844719409942627\n",
      "Epoch 4, Iteration 344, Loss: 0.9911347031593323\n",
      "Epoch 4, Iteration 345, Loss: 0.9650741815567017\n",
      "Epoch 4, Iteration 346, Loss: 0.8199370503425598\n",
      "Epoch 4, Iteration 347, Loss: 0.8269384503364563\n",
      "Epoch 4, Iteration 348, Loss: 0.7300985455513\n",
      "Epoch 4, Iteration 349, Loss: 0.9278534650802612\n",
      "Epoch 4, Iteration 350, Loss: 0.7726345062255859\n",
      "Epoch 4, Iteration 350, Valid Loss: 0.7559427618980408\n",
      "Epoch 4, Iteration 351, Loss: 0.9087878465652466\n",
      "Epoch 4, Iteration 352, Loss: 0.8309539556503296\n",
      "Epoch 4, Iteration 353, Loss: 0.9211757183074951\n",
      "Epoch 4, Iteration 354, Loss: 0.7994195222854614\n",
      "Epoch 4, Iteration 355, Loss: 0.8558520078659058\n",
      "Epoch 4, Iteration 356, Loss: 0.9186088442802429\n",
      "Epoch 4, Iteration 357, Loss: 0.804610013961792\n",
      "Epoch 4, Iteration 358, Loss: 0.8464780449867249\n",
      "Epoch 4, Iteration 359, Loss: 0.8857812285423279\n",
      "Epoch 4, Iteration 360, Loss: 0.7915959358215332\n",
      "Epoch 4, Iteration 361, Loss: 0.8301997780799866\n",
      "Epoch 4, Iteration 362, Loss: 0.8871037364006042\n",
      "Epoch 4, Iteration 363, Loss: 0.8019837141036987\n",
      "Epoch 4, Iteration 364, Loss: 0.9528142809867859\n",
      "Epoch 4, Iteration 365, Loss: 1.0461305379867554\n",
      "Epoch 4, Iteration 366, Loss: 0.9767888188362122\n",
      "Epoch 4, Iteration 367, Loss: 0.9091461896896362\n",
      "Epoch 4, Iteration 368, Loss: 0.8691506385803223\n",
      "Epoch 4, Iteration 369, Loss: 0.9510273337364197\n",
      "Epoch 4, Iteration 370, Loss: 0.9364050030708313\n",
      "Epoch 4, Iteration 371, Loss: 0.9894388914108276\n",
      "Epoch 4, Iteration 372, Loss: 0.7686964869499207\n",
      "Epoch 4, Iteration 373, Loss: 0.8886090517044067\n",
      "Epoch 4, Iteration 374, Loss: 0.9911856651306152\n",
      "Epoch 4, Iteration 375, Loss: 0.9209069609642029\n",
      "Epoch 4, Iteration 376, Loss: 0.9870278239250183\n",
      "Epoch 4, Iteration 377, Loss: 0.8758935332298279\n",
      "Epoch 4, Iteration 378, Loss: 0.7851151823997498\n",
      "Epoch 4, Iteration 379, Loss: 0.8106439709663391\n",
      "Epoch 4, Iteration 380, Loss: 0.7495490908622742\n",
      "Epoch 4, Iteration 381, Loss: 0.8121059536933899\n",
      "Epoch 4, Iteration 382, Loss: 0.65568608045578\n",
      "Epoch 4, Iteration 383, Loss: 0.8477744460105896\n",
      "Epoch 4, Iteration 384, Loss: 0.8916410207748413\n",
      "Epoch 4, Iteration 385, Loss: 0.8133599162101746\n",
      "Epoch 4, Iteration 386, Loss: 0.8121547698974609\n",
      "Epoch 4, Iteration 387, Loss: 1.2138957977294922\n",
      "Epoch 4, Iteration 388, Loss: 0.911602795124054\n",
      "Epoch 4, Iteration 389, Loss: 0.8992431163787842\n",
      "Epoch 4, Iteration 390, Loss: 1.0743464231491089\n",
      "Epoch 4, Iteration 391, Loss: 0.7966566681861877\n",
      "Epoch 5/15, Loss: 0.9814584411832751\n",
      "Epoch 5, Iteration 0, Loss: 0.884394645690918\n",
      "Epoch 5, Iteration 1, Loss: 1.2579989433288574\n",
      "Epoch 5, Iteration 2, Loss: 1.0682510137557983\n",
      "Epoch 5, Iteration 3, Loss: 1.0359280109405518\n",
      "Epoch 5, Iteration 4, Loss: 0.9036135673522949\n",
      "Epoch 5, Iteration 5, Loss: 1.0303943157196045\n",
      "Epoch 5, Iteration 6, Loss: 0.7774478793144226\n",
      "Epoch 5, Iteration 7, Loss: 0.9535058736801147\n",
      "Epoch 5, Iteration 8, Loss: 0.9151707887649536\n",
      "Epoch 5, Iteration 9, Loss: 1.106384515762329\n",
      "Epoch 5, Iteration 10, Loss: 1.083556056022644\n",
      "Epoch 5, Iteration 11, Loss: 0.8934468030929565\n",
      "Epoch 5, Iteration 12, Loss: 0.868467390537262\n",
      "Epoch 5, Iteration 13, Loss: 1.0571789741516113\n",
      "Epoch 5, Iteration 14, Loss: 0.9470281004905701\n",
      "Epoch 5, Iteration 15, Loss: 1.0468180179595947\n",
      "Epoch 5, Iteration 16, Loss: 0.8617860674858093\n",
      "Epoch 5, Iteration 17, Loss: 1.0487252473831177\n",
      "Epoch 5, Iteration 18, Loss: 0.8896021246910095\n",
      "Epoch 5, Iteration 19, Loss: 0.9605768322944641\n",
      "Epoch 5, Iteration 20, Loss: 0.8591107726097107\n",
      "Epoch 5, Iteration 21, Loss: 0.8626886606216431\n",
      "Epoch 5, Iteration 22, Loss: 0.9261230230331421\n",
      "Epoch 5, Iteration 23, Loss: 0.9751741886138916\n",
      "Epoch 5, Iteration 24, Loss: 0.8572389483451843\n",
      "Epoch 5, Iteration 25, Loss: 0.796962559223175\n",
      "Epoch 5, Iteration 26, Loss: 1.2469053268432617\n",
      "Epoch 5, Iteration 27, Loss: 1.0358610153198242\n",
      "Epoch 5, Iteration 28, Loss: 0.9995282888412476\n",
      "Epoch 5, Iteration 29, Loss: 1.0603505373001099\n",
      "Epoch 5, Iteration 30, Loss: 1.001497507095337\n",
      "Epoch 5, Iteration 31, Loss: 0.944724440574646\n",
      "Epoch 5, Iteration 32, Loss: 0.9418246746063232\n",
      "Epoch 5, Iteration 33, Loss: 0.9851481318473816\n",
      "Epoch 5, Iteration 34, Loss: 0.9350507259368896\n",
      "Epoch 5, Iteration 35, Loss: 1.0324138402938843\n",
      "Epoch 5, Iteration 36, Loss: 0.7898975014686584\n",
      "Epoch 5, Iteration 37, Loss: 0.8038830757141113\n",
      "Epoch 5, Iteration 38, Loss: 0.9231508374214172\n",
      "Epoch 5, Iteration 39, Loss: 0.97707599401474\n",
      "Epoch 5, Iteration 40, Loss: 0.915244460105896\n",
      "Epoch 5, Iteration 41, Loss: 0.9892341494560242\n",
      "Epoch 5, Iteration 42, Loss: 0.9090835452079773\n",
      "Epoch 5, Iteration 43, Loss: 0.7637709379196167\n",
      "Epoch 5, Iteration 44, Loss: 0.8577999472618103\n",
      "Epoch 5, Iteration 45, Loss: 0.8639558553695679\n",
      "Epoch 5, Iteration 46, Loss: 0.984246551990509\n",
      "Epoch 5, Iteration 47, Loss: 0.9978799819946289\n",
      "Epoch 5, Iteration 48, Loss: 1.0946075916290283\n",
      "Epoch 5, Iteration 49, Loss: 0.899505615234375\n",
      "Epoch 5, Iteration 50, Loss: 0.8936815857887268\n",
      "Epoch 5, Iteration 50, Valid Loss: 0.7263271808624268\n",
      "Epoch 5, Iteration 51, Loss: 0.9744623899459839\n",
      "Epoch 5, Iteration 52, Loss: 0.9084485173225403\n",
      "Epoch 5, Iteration 53, Loss: 0.999485194683075\n",
      "Epoch 5, Iteration 54, Loss: 1.0679084062576294\n",
      "Epoch 5, Iteration 55, Loss: 0.9844765663146973\n",
      "Epoch 5, Iteration 56, Loss: 0.9324766993522644\n",
      "Epoch 5, Iteration 57, Loss: 1.0865638256072998\n",
      "Epoch 5, Iteration 58, Loss: 0.907564640045166\n",
      "Epoch 5, Iteration 59, Loss: 0.9828992486000061\n",
      "Epoch 5, Iteration 60, Loss: 0.9950827360153198\n",
      "Epoch 5, Iteration 61, Loss: 0.7588973641395569\n",
      "Epoch 5, Iteration 62, Loss: 0.836691677570343\n",
      "Epoch 5, Iteration 63, Loss: 0.7743106484413147\n",
      "Epoch 5, Iteration 64, Loss: 0.9402368068695068\n",
      "Epoch 5, Iteration 65, Loss: 1.085877537727356\n",
      "Epoch 5, Iteration 66, Loss: 0.9116991758346558\n",
      "Epoch 5, Iteration 67, Loss: 0.7354905605316162\n",
      "Epoch 5, Iteration 68, Loss: 1.0090091228485107\n",
      "Epoch 5, Iteration 69, Loss: 1.0770787000656128\n",
      "Epoch 5, Iteration 70, Loss: 0.982099711894989\n",
      "Epoch 5, Iteration 71, Loss: 0.9817728400230408\n",
      "Epoch 5, Iteration 72, Loss: 0.9383513331413269\n",
      "Epoch 5, Iteration 73, Loss: 0.8100853562355042\n",
      "Epoch 5, Iteration 74, Loss: 1.1846343278884888\n",
      "Epoch 5, Iteration 75, Loss: 1.007015347480774\n",
      "Epoch 5, Iteration 76, Loss: 0.8788227438926697\n",
      "Epoch 5, Iteration 77, Loss: 0.7252619862556458\n",
      "Epoch 5, Iteration 78, Loss: 1.1216944456100464\n",
      "Epoch 5, Iteration 79, Loss: 1.1840555667877197\n",
      "Epoch 5, Iteration 80, Loss: 0.795661211013794\n",
      "Epoch 5, Iteration 81, Loss: 1.2214897871017456\n",
      "Epoch 5, Iteration 82, Loss: 1.029752254486084\n",
      "Epoch 5, Iteration 83, Loss: 0.8207478523254395\n",
      "Epoch 5, Iteration 84, Loss: 1.041855812072754\n",
      "Epoch 5, Iteration 85, Loss: 0.968654990196228\n",
      "Epoch 5, Iteration 86, Loss: 1.0116924047470093\n",
      "Epoch 5, Iteration 87, Loss: 0.9409275650978088\n",
      "Epoch 5, Iteration 88, Loss: 0.9252507090568542\n",
      "Epoch 5, Iteration 89, Loss: 1.0267913341522217\n",
      "Epoch 5, Iteration 90, Loss: 0.7833248972892761\n",
      "Epoch 5, Iteration 91, Loss: 0.8055408596992493\n",
      "Epoch 5, Iteration 92, Loss: 0.7650659084320068\n",
      "Epoch 5, Iteration 93, Loss: 0.9255911111831665\n",
      "Epoch 5, Iteration 94, Loss: 1.0926527976989746\n",
      "Epoch 5, Iteration 95, Loss: 1.1771776676177979\n",
      "Epoch 5, Iteration 96, Loss: 0.9264001846313477\n",
      "Epoch 5, Iteration 97, Loss: 0.7479454874992371\n",
      "Epoch 5, Iteration 98, Loss: 0.9520297646522522\n",
      "Epoch 5, Iteration 99, Loss: 0.9193183779716492\n",
      "Epoch 5, Iteration 100, Loss: 0.8965380191802979\n",
      "Epoch 5, Iteration 100, Valid Loss: 0.7713515162467957\n",
      "Epoch 5, Iteration 101, Loss: 1.0484068393707275\n",
      "Epoch 5, Iteration 102, Loss: 0.9897743463516235\n",
      "Epoch 5, Iteration 103, Loss: 1.0063323974609375\n",
      "Epoch 5, Iteration 104, Loss: 1.2180206775665283\n",
      "Epoch 5, Iteration 105, Loss: 0.9058063626289368\n",
      "Epoch 5, Iteration 106, Loss: 0.9317479729652405\n",
      "Epoch 5, Iteration 107, Loss: 1.1895027160644531\n",
      "Epoch 5, Iteration 108, Loss: 0.889420747756958\n",
      "Epoch 5, Iteration 109, Loss: 1.2259386777877808\n",
      "Epoch 5, Iteration 110, Loss: 1.2136846780776978\n",
      "Epoch 5, Iteration 111, Loss: 1.086555004119873\n",
      "Epoch 5, Iteration 112, Loss: 0.9792348146438599\n",
      "Epoch 5, Iteration 113, Loss: 0.905613899230957\n",
      "Epoch 5, Iteration 114, Loss: 1.2203993797302246\n",
      "Epoch 5, Iteration 115, Loss: 1.074102520942688\n",
      "Epoch 5, Iteration 116, Loss: 0.8712133765220642\n",
      "Epoch 5, Iteration 117, Loss: 0.686720073223114\n",
      "Epoch 5, Iteration 118, Loss: 1.027168869972229\n",
      "Epoch 5, Iteration 119, Loss: 0.896329402923584\n",
      "Epoch 5, Iteration 120, Loss: 1.0841903686523438\n",
      "Epoch 5, Iteration 121, Loss: 0.8501887321472168\n",
      "Epoch 5, Iteration 122, Loss: 0.9680610299110413\n",
      "Epoch 5, Iteration 123, Loss: 0.8577520847320557\n",
      "Epoch 5, Iteration 124, Loss: 0.7532271146774292\n",
      "Epoch 5, Iteration 125, Loss: 0.7792224287986755\n",
      "Epoch 5, Iteration 126, Loss: 0.820408821105957\n",
      "Epoch 5, Iteration 127, Loss: 1.1120648384094238\n",
      "Epoch 5, Iteration 128, Loss: 1.0324070453643799\n",
      "Epoch 5, Iteration 129, Loss: 1.0030714273452759\n",
      "Epoch 5, Iteration 130, Loss: 1.1942297220230103\n",
      "Epoch 5, Iteration 131, Loss: 0.7158713936805725\n",
      "Epoch 5, Iteration 132, Loss: 0.8963804841041565\n",
      "Epoch 5, Iteration 133, Loss: 1.2581897974014282\n",
      "Epoch 5, Iteration 134, Loss: 0.967275083065033\n",
      "Epoch 5, Iteration 135, Loss: 0.9848108291625977\n",
      "Epoch 5, Iteration 136, Loss: 0.9150444865226746\n",
      "Epoch 5, Iteration 137, Loss: 0.9483888149261475\n",
      "Epoch 5, Iteration 138, Loss: 0.8299671411514282\n",
      "Epoch 5, Iteration 139, Loss: 0.8539013862609863\n",
      "Epoch 5, Iteration 140, Loss: 0.8154962062835693\n",
      "Epoch 5, Iteration 141, Loss: 1.2493064403533936\n",
      "Epoch 5, Iteration 142, Loss: 0.9076390862464905\n",
      "Epoch 5, Iteration 143, Loss: 0.8412817716598511\n",
      "Epoch 5, Iteration 144, Loss: 0.8421246409416199\n",
      "Epoch 5, Iteration 145, Loss: 1.0139886140823364\n",
      "Epoch 5, Iteration 146, Loss: 0.9888368844985962\n",
      "Epoch 5, Iteration 147, Loss: 0.8701480627059937\n",
      "Epoch 5, Iteration 148, Loss: 0.7839016318321228\n",
      "Epoch 5, Iteration 149, Loss: 0.7649833559989929\n",
      "Epoch 5, Iteration 150, Loss: 0.9790865778923035\n",
      "Epoch 5, Iteration 150, Valid Loss: 0.7327585220336914\n",
      "Epoch 5, Iteration 151, Loss: 1.013650894165039\n",
      "Epoch 5, Iteration 152, Loss: 0.824421226978302\n",
      "Epoch 5, Iteration 153, Loss: 0.8685926198959351\n",
      "Epoch 5, Iteration 154, Loss: 0.7251030206680298\n",
      "Epoch 5, Iteration 155, Loss: 0.8768936395645142\n",
      "Epoch 5, Iteration 156, Loss: 0.77884441614151\n",
      "Epoch 5, Iteration 157, Loss: 1.3412221670150757\n",
      "Epoch 5, Iteration 158, Loss: 1.007033109664917\n",
      "Epoch 5, Iteration 159, Loss: 1.0104783773422241\n",
      "Epoch 5, Iteration 160, Loss: 0.840215265750885\n",
      "Epoch 5, Iteration 161, Loss: 0.8758112192153931\n",
      "Epoch 5, Iteration 162, Loss: 1.200938105583191\n",
      "Epoch 5, Iteration 163, Loss: 0.7719393968582153\n",
      "Epoch 5, Iteration 164, Loss: 0.9270544648170471\n",
      "Epoch 5, Iteration 165, Loss: 0.8956316709518433\n",
      "Epoch 5, Iteration 166, Loss: 0.8040003776550293\n",
      "Epoch 5, Iteration 167, Loss: 1.1292665004730225\n",
      "Epoch 5, Iteration 168, Loss: 0.9512305855751038\n",
      "Epoch 5, Iteration 169, Loss: 0.9633181095123291\n",
      "Epoch 5, Iteration 170, Loss: 1.0990756750106812\n",
      "Epoch 5, Iteration 171, Loss: 1.1014375686645508\n",
      "Epoch 5, Iteration 172, Loss: 0.9069215655326843\n",
      "Epoch 5, Iteration 173, Loss: 0.7975600957870483\n",
      "Epoch 5, Iteration 174, Loss: 1.0024791955947876\n",
      "Epoch 5, Iteration 175, Loss: 1.1846356391906738\n",
      "Epoch 5, Iteration 176, Loss: 0.7934995293617249\n",
      "Epoch 5, Iteration 177, Loss: 1.351770043373108\n",
      "Epoch 5, Iteration 178, Loss: 1.0763659477233887\n",
      "Epoch 5, Iteration 179, Loss: 0.879447340965271\n",
      "Epoch 5, Iteration 180, Loss: 0.9023174047470093\n",
      "Epoch 5, Iteration 181, Loss: 1.1140152215957642\n",
      "Epoch 5, Iteration 182, Loss: 0.9993410706520081\n",
      "Epoch 5, Iteration 183, Loss: 0.8394052982330322\n",
      "Epoch 5, Iteration 184, Loss: 0.8442624807357788\n",
      "Epoch 5, Iteration 185, Loss: 0.8365813493728638\n",
      "Epoch 5, Iteration 186, Loss: 0.8980485200881958\n",
      "Epoch 5, Iteration 187, Loss: 0.8350398540496826\n",
      "Epoch 5, Iteration 188, Loss: 0.9306190609931946\n",
      "Epoch 5, Iteration 189, Loss: 0.839415967464447\n",
      "Epoch 5, Iteration 190, Loss: 1.0248287916183472\n",
      "Epoch 5, Iteration 191, Loss: 1.0215461254119873\n",
      "Epoch 5, Iteration 192, Loss: 0.9501733183860779\n",
      "Epoch 5, Iteration 193, Loss: 0.91331946849823\n",
      "Epoch 5, Iteration 194, Loss: 0.9884145855903625\n",
      "Epoch 5, Iteration 195, Loss: 0.8908544778823853\n",
      "Epoch 5, Iteration 196, Loss: 0.927757203578949\n",
      "Epoch 5, Iteration 197, Loss: 0.8201237320899963\n",
      "Epoch 5, Iteration 198, Loss: 0.8810646533966064\n",
      "Epoch 5, Iteration 199, Loss: 0.8962581753730774\n",
      "Epoch 5, Iteration 200, Loss: 0.8041595816612244\n",
      "Epoch 5, Iteration 200, Valid Loss: 0.715997040271759\n",
      "Epoch 5, Iteration 201, Loss: 0.8966109156608582\n",
      "Epoch 5, Iteration 202, Loss: 0.9744777679443359\n",
      "Epoch 5, Iteration 203, Loss: 0.855931282043457\n",
      "Epoch 5, Iteration 204, Loss: 0.8002690672874451\n",
      "Epoch 5, Iteration 205, Loss: 0.69626784324646\n",
      "Epoch 5, Iteration 206, Loss: 0.809296190738678\n",
      "Epoch 5, Iteration 207, Loss: 1.161781668663025\n",
      "Epoch 5, Iteration 208, Loss: 1.317414402961731\n",
      "Epoch 5, Iteration 209, Loss: 1.2656656503677368\n",
      "Epoch 5, Iteration 210, Loss: 1.1894060373306274\n",
      "Epoch 5, Iteration 211, Loss: 1.3405773639678955\n",
      "Epoch 5, Iteration 212, Loss: 1.154932975769043\n",
      "Epoch 5, Iteration 213, Loss: 1.259844422340393\n",
      "Epoch 5, Iteration 214, Loss: 0.9338314533233643\n",
      "Epoch 5, Iteration 215, Loss: 0.808469295501709\n",
      "Epoch 5, Iteration 216, Loss: 0.8174273371696472\n",
      "Epoch 5, Iteration 217, Loss: 0.9014086127281189\n",
      "Epoch 5, Iteration 218, Loss: 0.8996066451072693\n",
      "Epoch 5, Iteration 219, Loss: 0.9397246837615967\n",
      "Epoch 5, Iteration 220, Loss: 0.8986825942993164\n",
      "Epoch 5, Iteration 221, Loss: 0.9103706479072571\n",
      "Epoch 5, Iteration 222, Loss: 0.8917944431304932\n",
      "Epoch 5, Iteration 223, Loss: 0.8377556800842285\n",
      "Epoch 5, Iteration 224, Loss: 0.7738320827484131\n",
      "Epoch 5, Iteration 225, Loss: 1.014127492904663\n",
      "Epoch 5, Iteration 226, Loss: 0.9477823972702026\n",
      "Epoch 5, Iteration 227, Loss: 0.9253453612327576\n",
      "Epoch 5, Iteration 228, Loss: 1.0045671463012695\n",
      "Epoch 5, Iteration 229, Loss: 0.8474324941635132\n",
      "Epoch 5, Iteration 230, Loss: 0.9586063027381897\n",
      "Epoch 5, Iteration 231, Loss: 0.9113729596138\n",
      "Epoch 5, Iteration 232, Loss: 1.0241596698760986\n",
      "Epoch 5, Iteration 233, Loss: 1.2133082151412964\n",
      "Epoch 5, Iteration 234, Loss: 1.0862135887145996\n",
      "Epoch 5, Iteration 235, Loss: 0.7320005893707275\n",
      "Epoch 5, Iteration 236, Loss: 0.9795392751693726\n",
      "Epoch 5, Iteration 237, Loss: 1.0128926038742065\n",
      "Epoch 5, Iteration 238, Loss: 0.7699392437934875\n",
      "Epoch 5, Iteration 239, Loss: 1.0507103204727173\n",
      "Epoch 5, Iteration 240, Loss: 0.8903288245201111\n",
      "Epoch 5, Iteration 241, Loss: 0.8081431984901428\n",
      "Epoch 5, Iteration 242, Loss: 0.8634990453720093\n",
      "Epoch 5, Iteration 243, Loss: 1.0006242990493774\n",
      "Epoch 5, Iteration 244, Loss: 0.9304997324943542\n",
      "Epoch 5, Iteration 245, Loss: 0.7217282056808472\n",
      "Epoch 5, Iteration 246, Loss: 0.9478335976600647\n",
      "Epoch 5, Iteration 247, Loss: 0.842568039894104\n",
      "Epoch 5, Iteration 248, Loss: 0.9337475895881653\n",
      "Epoch 5, Iteration 249, Loss: 0.8714432716369629\n",
      "Epoch 5, Iteration 250, Loss: 1.1429134607315063\n",
      "Epoch 5, Iteration 250, Valid Loss: 0.6889339685440063\n",
      "Epoch 5, Iteration 251, Loss: 0.887907087802887\n",
      "Epoch 5, Iteration 252, Loss: 1.1008117198944092\n",
      "Epoch 5, Iteration 253, Loss: 0.9467989206314087\n",
      "Epoch 5, Iteration 254, Loss: 0.9279571175575256\n",
      "Epoch 5, Iteration 255, Loss: 0.9493245482444763\n",
      "Epoch 5, Iteration 256, Loss: 0.9809020161628723\n",
      "Epoch 5, Iteration 257, Loss: 0.9299946427345276\n",
      "Epoch 5, Iteration 258, Loss: 1.0286271572113037\n",
      "Epoch 5, Iteration 259, Loss: 0.7050159573554993\n",
      "Epoch 5, Iteration 260, Loss: 1.0526057481765747\n",
      "Epoch 5, Iteration 261, Loss: 0.9715385437011719\n",
      "Epoch 5, Iteration 262, Loss: 0.8461102843284607\n",
      "Epoch 5, Iteration 263, Loss: 0.9969690442085266\n",
      "Epoch 5, Iteration 264, Loss: 0.9042397141456604\n",
      "Epoch 5, Iteration 265, Loss: 0.8671963810920715\n",
      "Epoch 5, Iteration 266, Loss: 0.9439837336540222\n",
      "Epoch 5, Iteration 267, Loss: 0.8273338675498962\n",
      "Epoch 5, Iteration 268, Loss: 0.8154692053794861\n",
      "Epoch 5, Iteration 269, Loss: 0.7519480586051941\n",
      "Epoch 5, Iteration 270, Loss: 0.8198016881942749\n",
      "Epoch 5, Iteration 271, Loss: 0.9873077273368835\n",
      "Epoch 5, Iteration 272, Loss: 0.810758113861084\n",
      "Epoch 5, Iteration 273, Loss: 0.9276093244552612\n",
      "Epoch 5, Iteration 274, Loss: 0.7980026602745056\n",
      "Epoch 5, Iteration 275, Loss: 0.6889552474021912\n",
      "Epoch 5, Iteration 276, Loss: 0.8470510840415955\n",
      "Epoch 5, Iteration 277, Loss: 0.8548813462257385\n",
      "Epoch 5, Iteration 278, Loss: 1.0595389604568481\n",
      "Epoch 5, Iteration 279, Loss: 0.7674639821052551\n",
      "Epoch 5, Iteration 280, Loss: 0.755033016204834\n",
      "Epoch 5, Iteration 281, Loss: 0.7684227228164673\n",
      "Epoch 5, Iteration 282, Loss: 0.8803818821907043\n",
      "Epoch 5, Iteration 283, Loss: 0.8441841006278992\n",
      "Epoch 5, Iteration 284, Loss: 0.8623666167259216\n",
      "Epoch 5, Iteration 285, Loss: 0.9237198829650879\n",
      "Epoch 5, Iteration 286, Loss: 0.7794068455696106\n",
      "Epoch 5, Iteration 287, Loss: 0.8073089718818665\n",
      "Epoch 5, Iteration 288, Loss: 0.7571302056312561\n",
      "Epoch 5, Iteration 289, Loss: 0.9105303883552551\n",
      "Epoch 5, Iteration 290, Loss: 0.76063472032547\n",
      "Epoch 5, Iteration 291, Loss: 0.7636615633964539\n",
      "Epoch 5, Iteration 292, Loss: 1.0067775249481201\n",
      "Epoch 5, Iteration 293, Loss: 1.0767441987991333\n",
      "Epoch 5, Iteration 294, Loss: 0.8436024785041809\n",
      "Epoch 5, Iteration 295, Loss: 0.7997060418128967\n",
      "Epoch 5, Iteration 296, Loss: 1.0766769647598267\n",
      "Epoch 5, Iteration 297, Loss: 0.9747413992881775\n",
      "Epoch 5, Iteration 298, Loss: 0.862576961517334\n",
      "Epoch 5, Iteration 299, Loss: 0.8780789971351624\n",
      "Epoch 5, Iteration 300, Loss: 0.6310812830924988\n",
      "Epoch 5, Iteration 300, Valid Loss: 0.6788691878318787\n",
      "Epoch 5, Iteration 301, Loss: 0.7935691475868225\n",
      "Epoch 5, Iteration 302, Loss: 0.7139683365821838\n",
      "Epoch 5, Iteration 303, Loss: 0.9619147777557373\n",
      "Epoch 5, Iteration 304, Loss: 0.7929319739341736\n",
      "Epoch 5, Iteration 305, Loss: 0.6757140159606934\n",
      "Epoch 5, Iteration 306, Loss: 0.7397986054420471\n",
      "Epoch 5, Iteration 307, Loss: 0.8250908851623535\n",
      "Epoch 5, Iteration 308, Loss: 0.7750455141067505\n",
      "Epoch 5, Iteration 309, Loss: 0.9023435115814209\n",
      "Epoch 5, Iteration 310, Loss: 0.8966914415359497\n",
      "Epoch 5, Iteration 311, Loss: 0.8531238436698914\n",
      "Epoch 5, Iteration 312, Loss: 0.7686631083488464\n",
      "Epoch 5, Iteration 313, Loss: 0.9623885750770569\n",
      "Epoch 5, Iteration 314, Loss: 0.7524663209915161\n",
      "Epoch 5, Iteration 315, Loss: 0.7766845226287842\n",
      "Epoch 5, Iteration 316, Loss: 0.9719342589378357\n",
      "Epoch 5, Iteration 317, Loss: 0.9286282658576965\n",
      "Epoch 5, Iteration 318, Loss: 0.7626041173934937\n",
      "Epoch 5, Iteration 319, Loss: 0.9856462478637695\n",
      "Epoch 5, Iteration 320, Loss: 0.7051263451576233\n",
      "Epoch 5, Iteration 321, Loss: 0.8057668209075928\n",
      "Epoch 5, Iteration 322, Loss: 0.7782263159751892\n",
      "Epoch 5, Iteration 323, Loss: 0.9969339966773987\n",
      "Epoch 5, Iteration 324, Loss: 0.8862449526786804\n",
      "Epoch 5, Iteration 325, Loss: 0.9713782072067261\n",
      "Epoch 5, Iteration 326, Loss: 0.7887703776359558\n",
      "Epoch 5, Iteration 327, Loss: 0.7665274739265442\n",
      "Epoch 5, Iteration 328, Loss: 0.7282541394233704\n",
      "Epoch 5, Iteration 329, Loss: 0.8933536410331726\n",
      "Epoch 5, Iteration 330, Loss: 0.6923465132713318\n",
      "Epoch 5, Iteration 331, Loss: 0.8224169015884399\n",
      "Epoch 5, Iteration 332, Loss: 0.8870472311973572\n",
      "Epoch 5, Iteration 333, Loss: 1.0216788053512573\n",
      "Epoch 5, Iteration 334, Loss: 0.81321781873703\n",
      "Epoch 5, Iteration 335, Loss: 0.9781239032745361\n",
      "Epoch 5, Iteration 336, Loss: 0.91551274061203\n",
      "Epoch 5, Iteration 337, Loss: 0.6454699039459229\n",
      "Epoch 5, Iteration 338, Loss: 0.910150408744812\n",
      "Epoch 5, Iteration 339, Loss: 0.8162691593170166\n",
      "Epoch 5, Iteration 340, Loss: 0.6940956711769104\n",
      "Epoch 5, Iteration 341, Loss: 0.8579455614089966\n",
      "Epoch 5, Iteration 342, Loss: 0.7647557854652405\n",
      "Epoch 5, Iteration 343, Loss: 0.8266955614089966\n",
      "Epoch 5, Iteration 344, Loss: 0.8656100630760193\n",
      "Epoch 5, Iteration 345, Loss: 0.8155207633972168\n",
      "Epoch 5, Iteration 346, Loss: 0.8622501492500305\n",
      "Epoch 5, Iteration 347, Loss: 0.7275625467300415\n",
      "Epoch 5, Iteration 348, Loss: 0.6842432022094727\n",
      "Epoch 5, Iteration 349, Loss: 0.8444309234619141\n",
      "Epoch 5, Iteration 350, Loss: 0.7168900966644287\n",
      "Epoch 5, Iteration 350, Valid Loss: 0.7086721062660217\n",
      "Epoch 5, Iteration 351, Loss: 0.8990697860717773\n",
      "Epoch 5, Iteration 352, Loss: 0.7834992408752441\n",
      "Epoch 5, Iteration 353, Loss: 0.8197696208953857\n",
      "Epoch 5, Iteration 354, Loss: 0.8659552931785583\n",
      "Epoch 5, Iteration 355, Loss: 0.7832712531089783\n",
      "Epoch 5, Iteration 356, Loss: 0.9046263694763184\n",
      "Epoch 5, Iteration 357, Loss: 0.7476892471313477\n",
      "Epoch 5, Iteration 358, Loss: 0.7681100964546204\n",
      "Epoch 5, Iteration 359, Loss: 0.8769936561584473\n",
      "Epoch 5, Iteration 360, Loss: 0.7673957347869873\n",
      "Epoch 5, Iteration 361, Loss: 0.7609142661094666\n",
      "Epoch 5, Iteration 362, Loss: 0.8012883067131042\n",
      "Epoch 5, Iteration 363, Loss: 0.7415135502815247\n",
      "Epoch 5, Iteration 364, Loss: 0.8306207656860352\n",
      "Epoch 5, Iteration 365, Loss: 0.9381104707717896\n",
      "Epoch 5, Iteration 366, Loss: 0.8485137820243835\n",
      "Epoch 5, Iteration 367, Loss: 0.8111186623573303\n",
      "Epoch 5, Iteration 368, Loss: 0.7907158732414246\n",
      "Epoch 5, Iteration 369, Loss: 0.8870471715927124\n",
      "Epoch 5, Iteration 370, Loss: 0.8408163189888\n",
      "Epoch 5, Iteration 371, Loss: 0.8109263777732849\n",
      "Epoch 5, Iteration 372, Loss: 0.7910946011543274\n",
      "Epoch 5, Iteration 373, Loss: 0.8093287944793701\n",
      "Epoch 5, Iteration 374, Loss: 0.9760022759437561\n",
      "Epoch 5, Iteration 375, Loss: 0.7164379358291626\n",
      "Epoch 5, Iteration 376, Loss: 0.9291775226593018\n",
      "Epoch 5, Iteration 377, Loss: 0.8528191447257996\n",
      "Epoch 5, Iteration 378, Loss: 0.8250283598899841\n",
      "Epoch 5, Iteration 379, Loss: 0.7422199845314026\n",
      "Epoch 5, Iteration 380, Loss: 0.7351760268211365\n",
      "Epoch 5, Iteration 381, Loss: 0.7573671936988831\n",
      "Epoch 5, Iteration 382, Loss: 0.6338618993759155\n",
      "Epoch 5, Iteration 383, Loss: 0.7606631517410278\n",
      "Epoch 5, Iteration 384, Loss: 0.8873195052146912\n",
      "Epoch 5, Iteration 385, Loss: 0.792352020740509\n",
      "Epoch 5, Iteration 386, Loss: 0.8870450854301453\n",
      "Epoch 5, Iteration 387, Loss: 1.0583899021148682\n",
      "Epoch 5, Iteration 388, Loss: 0.7912602424621582\n",
      "Epoch 5, Iteration 389, Loss: 0.9016507267951965\n",
      "Epoch 5, Iteration 390, Loss: 1.0358854532241821\n",
      "Epoch 5, Iteration 391, Loss: 0.7164614796638489\n",
      "Epoch 6/15, Loss: 0.9147950722550859\n",
      "Epoch 6, Iteration 0, Loss: 0.8594175577163696\n",
      "Epoch 6, Iteration 1, Loss: 1.0759161710739136\n",
      "Epoch 6, Iteration 2, Loss: 0.9430351853370667\n",
      "Epoch 6, Iteration 3, Loss: 1.0035046339035034\n",
      "Epoch 6, Iteration 4, Loss: 0.8436338901519775\n",
      "Epoch 6, Iteration 5, Loss: 0.9538593888282776\n",
      "Epoch 6, Iteration 6, Loss: 0.7391550540924072\n",
      "Epoch 6, Iteration 7, Loss: 0.8385874629020691\n",
      "Epoch 6, Iteration 8, Loss: 0.8361474871635437\n",
      "Epoch 6, Iteration 9, Loss: 1.0280513763427734\n",
      "Epoch 6, Iteration 10, Loss: 0.9331762194633484\n",
      "Epoch 6, Iteration 11, Loss: 0.8299064636230469\n",
      "Epoch 6, Iteration 12, Loss: 0.822239339351654\n",
      "Epoch 6, Iteration 13, Loss: 0.9905847311019897\n",
      "Epoch 6, Iteration 14, Loss: 0.8632118105888367\n",
      "Epoch 6, Iteration 15, Loss: 0.967007040977478\n",
      "Epoch 6, Iteration 16, Loss: 0.8198705315589905\n",
      "Epoch 6, Iteration 17, Loss: 0.9969351291656494\n",
      "Epoch 6, Iteration 18, Loss: 0.857495903968811\n",
      "Epoch 6, Iteration 19, Loss: 0.9545444250106812\n",
      "Epoch 6, Iteration 20, Loss: 0.7880669832229614\n",
      "Epoch 6, Iteration 21, Loss: 0.7877264618873596\n",
      "Epoch 6, Iteration 22, Loss: 0.9097462296485901\n",
      "Epoch 6, Iteration 23, Loss: 0.8413934111595154\n",
      "Epoch 6, Iteration 24, Loss: 0.7834715843200684\n",
      "Epoch 6, Iteration 25, Loss: 0.7236260771751404\n",
      "Epoch 6, Iteration 26, Loss: 1.1891891956329346\n",
      "Epoch 6, Iteration 27, Loss: 0.9398706555366516\n",
      "Epoch 6, Iteration 28, Loss: 0.8925186991691589\n",
      "Epoch 6, Iteration 29, Loss: 1.1595838069915771\n",
      "Epoch 6, Iteration 30, Loss: 1.0795261859893799\n",
      "Epoch 6, Iteration 31, Loss: 0.9852692484855652\n",
      "Epoch 6, Iteration 32, Loss: 0.8619179725646973\n",
      "Epoch 6, Iteration 33, Loss: 0.8509320616722107\n",
      "Epoch 6, Iteration 34, Loss: 0.8306223154067993\n",
      "Epoch 6, Iteration 35, Loss: 0.9848834872245789\n",
      "Epoch 6, Iteration 36, Loss: 0.7861210107803345\n",
      "Epoch 6, Iteration 37, Loss: 0.7184361219406128\n",
      "Epoch 6, Iteration 38, Loss: 0.7965115904808044\n",
      "Epoch 6, Iteration 39, Loss: 0.884726881980896\n",
      "Epoch 6, Iteration 40, Loss: 0.8014770746231079\n",
      "Epoch 6, Iteration 41, Loss: 0.9835236668586731\n",
      "Epoch 6, Iteration 42, Loss: 0.8483246564865112\n",
      "Epoch 6, Iteration 43, Loss: 0.7528148889541626\n",
      "Epoch 6, Iteration 44, Loss: 0.889013409614563\n",
      "Epoch 6, Iteration 45, Loss: 0.933479368686676\n",
      "Epoch 6, Iteration 46, Loss: 0.8894347548484802\n",
      "Epoch 6, Iteration 47, Loss: 0.8640682697296143\n",
      "Epoch 6, Iteration 48, Loss: 0.9934970140457153\n",
      "Epoch 6, Iteration 49, Loss: 0.8580187559127808\n",
      "Epoch 6, Iteration 50, Loss: 0.7986711859703064\n",
      "Epoch 6, Iteration 50, Valid Loss: 0.6698076725006104\n",
      "Epoch 6, Iteration 51, Loss: 0.8216466307640076\n",
      "Epoch 6, Iteration 52, Loss: 0.7602869272232056\n",
      "Epoch 6, Iteration 53, Loss: 0.9279433488845825\n",
      "Epoch 6, Iteration 54, Loss: 1.1672775745391846\n",
      "Epoch 6, Iteration 55, Loss: 1.0171258449554443\n",
      "Epoch 6, Iteration 56, Loss: 0.9538783431053162\n",
      "Epoch 6, Iteration 57, Loss: 1.068547248840332\n",
      "Epoch 6, Iteration 58, Loss: 0.7690836191177368\n",
      "Epoch 6, Iteration 59, Loss: 0.769170880317688\n",
      "Epoch 6, Iteration 60, Loss: 0.8431824445724487\n",
      "Epoch 6, Iteration 61, Loss: 0.652539074420929\n",
      "Epoch 6, Iteration 62, Loss: 0.8628672361373901\n",
      "Epoch 6, Iteration 63, Loss: 0.8644426465034485\n",
      "Epoch 6, Iteration 64, Loss: 0.9093078374862671\n",
      "Epoch 6, Iteration 65, Loss: 0.9307922124862671\n",
      "Epoch 6, Iteration 66, Loss: 0.8702802658081055\n",
      "Epoch 6, Iteration 67, Loss: 0.7116170525550842\n",
      "Epoch 6, Iteration 68, Loss: 0.9372941851615906\n",
      "Epoch 6, Iteration 69, Loss: 1.0184491872787476\n",
      "Epoch 6, Iteration 70, Loss: 0.9362576007843018\n",
      "Epoch 6, Iteration 71, Loss: 0.971579372882843\n",
      "Epoch 6, Iteration 72, Loss: 0.8725391626358032\n",
      "Epoch 6, Iteration 73, Loss: 0.7473114132881165\n",
      "Epoch 6, Iteration 74, Loss: 1.119905948638916\n",
      "Epoch 6, Iteration 75, Loss: 0.8806660175323486\n",
      "Epoch 6, Iteration 76, Loss: 0.8578635454177856\n",
      "Epoch 6, Iteration 77, Loss: 0.6962897777557373\n",
      "Epoch 6, Iteration 78, Loss: 1.1401362419128418\n",
      "Epoch 6, Iteration 79, Loss: 1.1277107000350952\n",
      "Epoch 6, Iteration 80, Loss: 0.7647656202316284\n",
      "Epoch 6, Iteration 81, Loss: 1.0688159465789795\n",
      "Epoch 6, Iteration 82, Loss: 0.9314989447593689\n",
      "Epoch 6, Iteration 83, Loss: 0.7180019617080688\n",
      "Epoch 6, Iteration 84, Loss: 1.082261562347412\n",
      "Epoch 6, Iteration 85, Loss: 0.9355524778366089\n",
      "Epoch 6, Iteration 86, Loss: 0.988848090171814\n",
      "Epoch 6, Iteration 87, Loss: 0.8042548298835754\n",
      "Epoch 6, Iteration 88, Loss: 0.9078264832496643\n",
      "Epoch 6, Iteration 89, Loss: 1.0582998991012573\n",
      "Epoch 6, Iteration 90, Loss: 0.7811892628669739\n",
      "Epoch 6, Iteration 91, Loss: 0.8530024290084839\n",
      "Epoch 6, Iteration 92, Loss: 0.6403101086616516\n",
      "Epoch 6, Iteration 93, Loss: 0.853617250919342\n",
      "Epoch 6, Iteration 94, Loss: 1.0597187280654907\n",
      "Epoch 6, Iteration 95, Loss: 0.6890443563461304\n",
      "Epoch 6, Iteration 96, Loss: 0.8532246351242065\n",
      "Epoch 6, Iteration 97, Loss: 0.6852580308914185\n",
      "Epoch 6, Iteration 98, Loss: 0.8767299652099609\n",
      "Epoch 6, Iteration 99, Loss: 0.8512222170829773\n",
      "Epoch 6, Iteration 100, Loss: 0.7403267025947571\n",
      "Epoch 6, Iteration 100, Valid Loss: 0.7105984687805176\n",
      "Epoch 6, Iteration 101, Loss: 1.0109994411468506\n",
      "Epoch 6, Iteration 102, Loss: 0.875578761100769\n",
      "Epoch 6, Iteration 103, Loss: 0.9779369831085205\n",
      "Epoch 6, Iteration 104, Loss: 1.1340125799179077\n",
      "Epoch 6, Iteration 105, Loss: 0.9576484560966492\n",
      "Epoch 6, Iteration 106, Loss: 0.8547083735466003\n",
      "Epoch 6, Iteration 107, Loss: 1.2294483184814453\n",
      "Epoch 6, Iteration 108, Loss: 0.8852860927581787\n",
      "Epoch 6, Iteration 109, Loss: 1.1563729047775269\n",
      "Epoch 6, Iteration 110, Loss: 1.037299633026123\n",
      "Epoch 6, Iteration 111, Loss: 0.9818747043609619\n",
      "Epoch 6, Iteration 112, Loss: 0.9753317832946777\n",
      "Epoch 6, Iteration 113, Loss: 0.851366400718689\n",
      "Epoch 6, Iteration 114, Loss: 1.1085752248764038\n",
      "Epoch 6, Iteration 115, Loss: 0.972334623336792\n",
      "Epoch 6, Iteration 116, Loss: 0.7220052480697632\n",
      "Epoch 6, Iteration 117, Loss: 0.7063284516334534\n",
      "Epoch 6, Iteration 118, Loss: 0.9192146062850952\n",
      "Epoch 6, Iteration 119, Loss: 0.7702147960662842\n",
      "Epoch 6, Iteration 120, Loss: 1.0825676918029785\n",
      "Epoch 6, Iteration 121, Loss: 0.9004216194152832\n",
      "Epoch 6, Iteration 122, Loss: 0.8970498442649841\n",
      "Epoch 6, Iteration 123, Loss: 0.8422186374664307\n",
      "Epoch 6, Iteration 124, Loss: 0.7777725458145142\n",
      "Epoch 6, Iteration 125, Loss: 0.8352141976356506\n",
      "Epoch 6, Iteration 126, Loss: 0.7271013855934143\n",
      "Epoch 6, Iteration 127, Loss: 0.9582388997077942\n",
      "Epoch 6, Iteration 128, Loss: 0.9088283777236938\n",
      "Epoch 6, Iteration 129, Loss: 1.0948553085327148\n",
      "Epoch 6, Iteration 130, Loss: 1.1835355758666992\n",
      "Epoch 6, Iteration 131, Loss: 0.7275248765945435\n",
      "Epoch 6, Iteration 132, Loss: 0.7363311052322388\n",
      "Epoch 6, Iteration 133, Loss: 1.2110555171966553\n",
      "Epoch 6, Iteration 134, Loss: 0.9864180088043213\n",
      "Epoch 6, Iteration 135, Loss: 0.9770371317863464\n",
      "Epoch 6, Iteration 136, Loss: 0.8202813863754272\n",
      "Epoch 6, Iteration 137, Loss: 0.8311468958854675\n",
      "Epoch 6, Iteration 138, Loss: 0.7413490414619446\n",
      "Epoch 6, Iteration 139, Loss: 0.7856631278991699\n",
      "Epoch 6, Iteration 140, Loss: 0.7632600665092468\n",
      "Epoch 6, Iteration 141, Loss: 1.1401373147964478\n",
      "Epoch 6, Iteration 142, Loss: 0.950867235660553\n",
      "Epoch 6, Iteration 143, Loss: 0.8413683772087097\n",
      "Epoch 6, Iteration 144, Loss: 0.9080895781517029\n",
      "Epoch 6, Iteration 145, Loss: 0.9222009778022766\n",
      "Epoch 6, Iteration 146, Loss: 0.9166352152824402\n",
      "Epoch 6, Iteration 147, Loss: 0.8098812699317932\n",
      "Epoch 6, Iteration 148, Loss: 0.6633644104003906\n",
      "Epoch 6, Iteration 149, Loss: 0.8278647661209106\n",
      "Epoch 6, Iteration 150, Loss: 0.8787568211555481\n",
      "Epoch 6, Iteration 150, Valid Loss: 0.6959850192070007\n",
      "Epoch 6, Iteration 151, Loss: 0.9233571887016296\n",
      "Epoch 6, Iteration 152, Loss: 0.7299131751060486\n",
      "Epoch 6, Iteration 153, Loss: 0.9517775774002075\n",
      "Epoch 6, Iteration 154, Loss: 0.7029328346252441\n",
      "Epoch 6, Iteration 155, Loss: 0.8918576240539551\n",
      "Epoch 6, Iteration 156, Loss: 0.7118650078773499\n",
      "Epoch 6, Iteration 157, Loss: 1.1638963222503662\n",
      "Epoch 6, Iteration 158, Loss: 0.8969911336898804\n",
      "Epoch 6, Iteration 159, Loss: 0.9525386095046997\n",
      "Epoch 6, Iteration 160, Loss: 0.8004649877548218\n",
      "Epoch 6, Iteration 161, Loss: 0.7844141721725464\n",
      "Epoch 6, Iteration 162, Loss: 1.1929374933242798\n",
      "Epoch 6, Iteration 163, Loss: 0.6502330899238586\n",
      "Epoch 6, Iteration 164, Loss: 0.8407535552978516\n",
      "Epoch 6, Iteration 165, Loss: 0.8082656860351562\n",
      "Epoch 6, Iteration 166, Loss: 0.7633293867111206\n",
      "Epoch 6, Iteration 167, Loss: 1.024147391319275\n",
      "Epoch 6, Iteration 168, Loss: 0.9337095022201538\n",
      "Epoch 6, Iteration 169, Loss: 0.833037257194519\n",
      "Epoch 6, Iteration 170, Loss: 1.075677514076233\n",
      "Epoch 6, Iteration 171, Loss: 0.8775444626808167\n",
      "Epoch 6, Iteration 172, Loss: 0.9009642601013184\n",
      "Epoch 6, Iteration 173, Loss: 0.7857665419578552\n",
      "Epoch 6, Iteration 174, Loss: 0.9361878633499146\n",
      "Epoch 6, Iteration 175, Loss: 0.8959227800369263\n",
      "Epoch 6, Iteration 176, Loss: 0.6326186060905457\n",
      "Epoch 6, Iteration 177, Loss: 1.2717725038528442\n",
      "Epoch 6, Iteration 178, Loss: 1.0488193035125732\n",
      "Epoch 6, Iteration 179, Loss: 0.8356373906135559\n",
      "Epoch 6, Iteration 180, Loss: 0.8522340655326843\n",
      "Epoch 6, Iteration 181, Loss: 0.9670564532279968\n",
      "Epoch 6, Iteration 182, Loss: 0.9664798378944397\n",
      "Epoch 6, Iteration 183, Loss: 0.8280174732208252\n",
      "Epoch 6, Iteration 184, Loss: 0.9147965312004089\n",
      "Epoch 6, Iteration 185, Loss: 0.7495693564414978\n",
      "Epoch 6, Iteration 186, Loss: 0.8091386556625366\n",
      "Epoch 6, Iteration 187, Loss: 0.8829290866851807\n",
      "Epoch 6, Iteration 188, Loss: 0.75847989320755\n",
      "Epoch 6, Iteration 189, Loss: 0.7839475870132446\n",
      "Epoch 6, Iteration 190, Loss: 1.0049636363983154\n",
      "Epoch 6, Iteration 191, Loss: 0.8256292343139648\n",
      "Epoch 6, Iteration 192, Loss: 0.9336563944816589\n",
      "Epoch 6, Iteration 193, Loss: 0.8860770463943481\n",
      "Epoch 6, Iteration 194, Loss: 0.9615448713302612\n",
      "Epoch 6, Iteration 195, Loss: 0.7781893610954285\n",
      "Epoch 6, Iteration 196, Loss: 0.8948297500610352\n",
      "Epoch 6, Iteration 197, Loss: 0.80575031042099\n",
      "Epoch 6, Iteration 198, Loss: 0.8384503126144409\n",
      "Epoch 6, Iteration 199, Loss: 0.9516425728797913\n",
      "Epoch 6, Iteration 200, Loss: 0.7909442782402039\n",
      "Epoch 6, Iteration 200, Valid Loss: 0.6812244057655334\n",
      "Epoch 6, Iteration 201, Loss: 0.8874480128288269\n",
      "Epoch 6, Iteration 202, Loss: 0.8987159132957458\n",
      "Epoch 6, Iteration 203, Loss: 0.8180950880050659\n",
      "Epoch 6, Iteration 204, Loss: 0.8456130027770996\n",
      "Epoch 6, Iteration 205, Loss: 0.6218857169151306\n",
      "Epoch 6, Iteration 206, Loss: 0.7247331142425537\n",
      "Epoch 6, Iteration 207, Loss: 0.9171234965324402\n",
      "Epoch 6, Iteration 208, Loss: 1.2368780374526978\n",
      "Epoch 6, Iteration 209, Loss: 1.249558925628662\n",
      "Epoch 6, Iteration 210, Loss: 1.1673370599746704\n",
      "Epoch 6, Iteration 211, Loss: 1.2390395402908325\n",
      "Epoch 6, Iteration 212, Loss: 1.0033774375915527\n",
      "Epoch 6, Iteration 213, Loss: 1.103043794631958\n",
      "Epoch 6, Iteration 214, Loss: 0.8786277174949646\n",
      "Epoch 6, Iteration 215, Loss: 0.7632778882980347\n",
      "Epoch 6, Iteration 216, Loss: 0.7618739008903503\n",
      "Epoch 6, Iteration 217, Loss: 0.9002114534378052\n",
      "Epoch 6, Iteration 218, Loss: 0.8652570843696594\n",
      "Epoch 6, Iteration 219, Loss: 0.9015162587165833\n",
      "Epoch 6, Iteration 220, Loss: 0.8281254768371582\n",
      "Epoch 6, Iteration 221, Loss: 0.8962692618370056\n",
      "Epoch 6, Iteration 222, Loss: 0.9535874128341675\n",
      "Epoch 6, Iteration 223, Loss: 0.7960355877876282\n",
      "Epoch 6, Iteration 224, Loss: 0.7278186082839966\n",
      "Epoch 6, Iteration 225, Loss: 0.9836578965187073\n",
      "Epoch 6, Iteration 226, Loss: 1.0390868186950684\n",
      "Epoch 6, Iteration 227, Loss: 0.9871849417686462\n",
      "Epoch 6, Iteration 228, Loss: 0.8470430970191956\n",
      "Epoch 6, Iteration 229, Loss: 0.7417988181114197\n",
      "Epoch 6, Iteration 230, Loss: 0.861361026763916\n",
      "Epoch 6, Iteration 231, Loss: 0.8611049652099609\n",
      "Epoch 6, Iteration 232, Loss: 0.9214960336685181\n",
      "Epoch 6, Iteration 233, Loss: 1.0909143686294556\n",
      "Epoch 6, Iteration 234, Loss: 1.0404163599014282\n",
      "Epoch 6, Iteration 235, Loss: 0.6633018255233765\n",
      "Epoch 6, Iteration 236, Loss: 0.9294000267982483\n",
      "Epoch 6, Iteration 237, Loss: 0.8215016722679138\n",
      "Epoch 6, Iteration 238, Loss: 0.7634950876235962\n",
      "Epoch 6, Iteration 239, Loss: 1.0576125383377075\n",
      "Epoch 6, Iteration 240, Loss: 0.8406976461410522\n",
      "Epoch 6, Iteration 241, Loss: 0.7521454095840454\n",
      "Epoch 6, Iteration 242, Loss: 0.7204045653343201\n",
      "Epoch 6, Iteration 243, Loss: 0.7986714839935303\n",
      "Epoch 6, Iteration 244, Loss: 0.8359876871109009\n",
      "Epoch 6, Iteration 245, Loss: 0.6613052487373352\n",
      "Epoch 6, Iteration 246, Loss: 0.8303834199905396\n",
      "Epoch 6, Iteration 247, Loss: 0.8313811421394348\n",
      "Epoch 6, Iteration 248, Loss: 0.9128468036651611\n",
      "Epoch 6, Iteration 249, Loss: 0.8257373571395874\n",
      "Epoch 6, Iteration 250, Loss: 0.9813454151153564\n",
      "Epoch 6, Iteration 250, Valid Loss: 0.6654561758041382\n",
      "Epoch 6, Iteration 251, Loss: 0.7748180031776428\n",
      "Epoch 6, Iteration 252, Loss: 0.965237557888031\n",
      "Epoch 6, Iteration 253, Loss: 0.9755709767341614\n",
      "Epoch 6, Iteration 254, Loss: 0.9613968133926392\n",
      "Epoch 6, Iteration 255, Loss: 0.9102425575256348\n",
      "Epoch 6, Iteration 256, Loss: 0.9660977125167847\n",
      "Epoch 6, Iteration 257, Loss: 0.7240080833435059\n",
      "Epoch 6, Iteration 258, Loss: 1.108786940574646\n",
      "Epoch 6, Iteration 259, Loss: 0.7054703235626221\n",
      "Epoch 6, Iteration 260, Loss: 0.9374891519546509\n",
      "Epoch 6, Iteration 261, Loss: 0.8519404530525208\n",
      "Epoch 6, Iteration 262, Loss: 0.7075837850570679\n",
      "Epoch 6, Iteration 263, Loss: 0.9603397250175476\n",
      "Epoch 6, Iteration 264, Loss: 0.8382358551025391\n",
      "Epoch 6, Iteration 265, Loss: 0.8742111325263977\n",
      "Epoch 6, Iteration 266, Loss: 0.9610850811004639\n",
      "Epoch 6, Iteration 267, Loss: 0.9026502966880798\n",
      "Epoch 6, Iteration 268, Loss: 0.7826032638549805\n",
      "Epoch 6, Iteration 269, Loss: 0.7698972821235657\n",
      "Epoch 6, Iteration 270, Loss: 0.7667831778526306\n",
      "Epoch 6, Iteration 271, Loss: 0.9711408615112305\n",
      "Epoch 6, Iteration 272, Loss: 0.784895122051239\n",
      "Epoch 6, Iteration 273, Loss: 0.9007733464241028\n",
      "Epoch 6, Iteration 274, Loss: 0.7812345027923584\n",
      "Epoch 6, Iteration 275, Loss: 0.8091396689414978\n",
      "Epoch 6, Iteration 276, Loss: 0.753328800201416\n",
      "Epoch 6, Iteration 277, Loss: 0.760895311832428\n",
      "Epoch 6, Iteration 278, Loss: 1.0093129873275757\n",
      "Epoch 6, Iteration 279, Loss: 0.7493207454681396\n",
      "Epoch 6, Iteration 280, Loss: 0.7046375870704651\n",
      "Epoch 6, Iteration 281, Loss: 0.8463343977928162\n",
      "Epoch 6, Iteration 282, Loss: 0.814285159111023\n",
      "Epoch 6, Iteration 283, Loss: 0.8032712936401367\n",
      "Epoch 6, Iteration 284, Loss: 0.8146592974662781\n",
      "Epoch 6, Iteration 285, Loss: 0.77628093957901\n",
      "Epoch 6, Iteration 286, Loss: 0.7743847370147705\n",
      "Epoch 6, Iteration 287, Loss: 0.8297123908996582\n",
      "Epoch 6, Iteration 288, Loss: 0.7838547825813293\n",
      "Epoch 6, Iteration 289, Loss: 0.7465675473213196\n",
      "Epoch 6, Iteration 290, Loss: 0.7675719261169434\n",
      "Epoch 6, Iteration 291, Loss: 0.7113670110702515\n",
      "Epoch 6, Iteration 292, Loss: 0.8983471989631653\n",
      "Epoch 6, Iteration 293, Loss: 1.0260648727416992\n",
      "Epoch 6, Iteration 294, Loss: 0.8463627696037292\n",
      "Epoch 6, Iteration 295, Loss: 0.6781067252159119\n",
      "Epoch 6, Iteration 296, Loss: 0.9508926868438721\n",
      "Epoch 6, Iteration 297, Loss: 0.9280799031257629\n",
      "Epoch 6, Iteration 298, Loss: 0.8318788409233093\n",
      "Epoch 6, Iteration 299, Loss: 0.8049178123474121\n",
      "Epoch 6, Iteration 300, Loss: 0.656991183757782\n",
      "Epoch 6, Iteration 300, Valid Loss: 0.6723811626434326\n",
      "Epoch 6, Iteration 301, Loss: 0.7700635194778442\n",
      "Epoch 6, Iteration 302, Loss: 0.7209358811378479\n",
      "Epoch 6, Iteration 303, Loss: 0.8965691328048706\n",
      "Epoch 6, Iteration 304, Loss: 0.8522589802742004\n",
      "Epoch 6, Iteration 305, Loss: 0.872219979763031\n",
      "Epoch 6, Iteration 306, Loss: 0.7363453507423401\n",
      "Epoch 6, Iteration 307, Loss: 0.8276856541633606\n",
      "Epoch 6, Iteration 308, Loss: 0.7736508250236511\n",
      "Epoch 6, Iteration 309, Loss: 0.8277667760848999\n",
      "Epoch 6, Iteration 310, Loss: 0.8952963948249817\n",
      "Epoch 6, Iteration 311, Loss: 0.7993751168251038\n",
      "Epoch 6, Iteration 312, Loss: 0.6925045847892761\n",
      "Epoch 6, Iteration 313, Loss: 0.9354389309883118\n",
      "Epoch 6, Iteration 314, Loss: 0.7068763971328735\n",
      "Epoch 6, Iteration 315, Loss: 0.7324127554893494\n",
      "Epoch 6, Iteration 316, Loss: 0.975164532661438\n",
      "Epoch 6, Iteration 317, Loss: 0.8804457783699036\n",
      "Epoch 6, Iteration 318, Loss: 0.7506800889968872\n",
      "Epoch 6, Iteration 319, Loss: 0.8671658039093018\n",
      "Epoch 6, Iteration 320, Loss: 0.6032296419143677\n",
      "Epoch 6, Iteration 321, Loss: 0.8408318161964417\n",
      "Epoch 6, Iteration 322, Loss: 0.7719646096229553\n",
      "Epoch 6, Iteration 323, Loss: 0.8662304282188416\n",
      "Epoch 6, Iteration 324, Loss: 0.8822501301765442\n",
      "Epoch 6, Iteration 325, Loss: 0.8453167080879211\n",
      "Epoch 6, Iteration 326, Loss: 0.7752760648727417\n",
      "Epoch 6, Iteration 327, Loss: 0.6854181289672852\n",
      "Epoch 6, Iteration 328, Loss: 0.7351261973381042\n",
      "Epoch 6, Iteration 329, Loss: 0.9411176443099976\n",
      "Epoch 6, Iteration 330, Loss: 0.7343527674674988\n",
      "Epoch 6, Iteration 331, Loss: 0.7623389959335327\n",
      "Epoch 6, Iteration 332, Loss: 0.845463752746582\n",
      "Epoch 6, Iteration 333, Loss: 1.007458209991455\n",
      "Epoch 6, Iteration 334, Loss: 0.7348706126213074\n",
      "Epoch 6, Iteration 335, Loss: 0.928810179233551\n",
      "Epoch 6, Iteration 336, Loss: 0.750552237033844\n",
      "Epoch 6, Iteration 337, Loss: 0.6319979429244995\n",
      "Epoch 6, Iteration 338, Loss: 0.8097029328346252\n",
      "Epoch 6, Iteration 339, Loss: 0.8039312362670898\n",
      "Epoch 6, Iteration 340, Loss: 0.6279537081718445\n",
      "Epoch 6, Iteration 341, Loss: 0.7890238761901855\n",
      "Epoch 6, Iteration 342, Loss: 0.7407404184341431\n",
      "Epoch 6, Iteration 343, Loss: 0.7449665069580078\n",
      "Epoch 6, Iteration 344, Loss: 0.8400739431381226\n",
      "Epoch 6, Iteration 345, Loss: 0.8390224575996399\n",
      "Epoch 6, Iteration 346, Loss: 0.7813303470611572\n",
      "Epoch 6, Iteration 347, Loss: 0.7464632391929626\n",
      "Epoch 6, Iteration 348, Loss: 0.6240138411521912\n",
      "Epoch 6, Iteration 349, Loss: 0.8102728724479675\n",
      "Epoch 6, Iteration 350, Loss: 0.6840846538543701\n",
      "Epoch 6, Iteration 350, Valid Loss: 0.6733394265174866\n",
      "Epoch 6, Iteration 351, Loss: 0.8618155717849731\n",
      "Epoch 6, Iteration 352, Loss: 0.6849775314331055\n",
      "Epoch 6, Iteration 353, Loss: 0.7676188945770264\n",
      "Epoch 6, Iteration 354, Loss: 0.7115797400474548\n",
      "Epoch 6, Iteration 355, Loss: 0.7843700051307678\n",
      "Epoch 6, Iteration 356, Loss: 0.8108558058738708\n",
      "Epoch 6, Iteration 357, Loss: 0.7276828289031982\n",
      "Epoch 6, Iteration 358, Loss: 0.7395079135894775\n",
      "Epoch 6, Iteration 359, Loss: 0.6970458626747131\n",
      "Epoch 6, Iteration 360, Loss: 0.6652472019195557\n",
      "Epoch 6, Iteration 361, Loss: 0.7837364673614502\n",
      "Epoch 6, Iteration 362, Loss: 0.7856165170669556\n",
      "Epoch 6, Iteration 363, Loss: 0.8579668402671814\n",
      "Epoch 6, Iteration 364, Loss: 0.7963702082633972\n",
      "Epoch 6, Iteration 365, Loss: 0.9225480556488037\n",
      "Epoch 6, Iteration 366, Loss: 0.8435182571411133\n",
      "Epoch 6, Iteration 367, Loss: 0.8026506900787354\n",
      "Epoch 6, Iteration 368, Loss: 0.7779878377914429\n",
      "Epoch 6, Iteration 369, Loss: 0.774703323841095\n",
      "Epoch 6, Iteration 370, Loss: 0.8537799119949341\n",
      "Epoch 6, Iteration 371, Loss: 0.8410583138465881\n",
      "Epoch 6, Iteration 372, Loss: 0.699502170085907\n",
      "Epoch 6, Iteration 373, Loss: 0.7817139029502869\n",
      "Epoch 6, Iteration 374, Loss: 0.947114884853363\n",
      "Epoch 6, Iteration 375, Loss: 0.7886350750923157\n",
      "Epoch 6, Iteration 376, Loss: 0.8739297986030579\n",
      "Epoch 6, Iteration 377, Loss: 0.7294161319732666\n",
      "Epoch 6, Iteration 378, Loss: 0.7348195314407349\n",
      "Epoch 6, Iteration 379, Loss: 0.6775740385055542\n",
      "Epoch 6, Iteration 380, Loss: 0.7235389947891235\n",
      "Epoch 6, Iteration 381, Loss: 0.7107208371162415\n",
      "Epoch 6, Iteration 382, Loss: 0.5475091934204102\n",
      "Epoch 6, Iteration 383, Loss: 0.7564748525619507\n",
      "Epoch 6, Iteration 384, Loss: 0.7827076315879822\n",
      "Epoch 6, Iteration 385, Loss: 0.6911670565605164\n",
      "Epoch 6, Iteration 386, Loss: 0.6493139266967773\n",
      "Epoch 6, Iteration 387, Loss: 1.0093966722488403\n",
      "Epoch 6, Iteration 388, Loss: 0.725048303604126\n",
      "Epoch 6, Iteration 389, Loss: 0.7678507566452026\n",
      "Epoch 6, Iteration 390, Loss: 0.9057670831680298\n",
      "Epoch 6, Iteration 391, Loss: 0.7412339448928833\n",
      "Epoch 7/15, Loss: 0.8613868478913697\n",
      "Epoch 7, Iteration 0, Loss: 0.7807587385177612\n",
      "Epoch 7, Iteration 1, Loss: 1.0001089572906494\n",
      "Epoch 7, Iteration 2, Loss: 0.9142112731933594\n",
      "Epoch 7, Iteration 3, Loss: 0.8562283515930176\n",
      "Epoch 7, Iteration 4, Loss: 0.7417850494384766\n",
      "Epoch 7, Iteration 5, Loss: 0.9724060893058777\n",
      "Epoch 7, Iteration 6, Loss: 0.6957538723945618\n",
      "Epoch 7, Iteration 7, Loss: 0.832771897315979\n",
      "Epoch 7, Iteration 8, Loss: 0.7268743515014648\n",
      "Epoch 7, Iteration 9, Loss: 0.9779843688011169\n",
      "Epoch 7, Iteration 10, Loss: 0.8833597302436829\n",
      "Epoch 7, Iteration 11, Loss: 0.7476693987846375\n",
      "Epoch 7, Iteration 12, Loss: 0.6793031096458435\n",
      "Epoch 7, Iteration 13, Loss: 0.9141262173652649\n",
      "Epoch 7, Iteration 14, Loss: 0.7714180946350098\n",
      "Epoch 7, Iteration 15, Loss: 0.8723652362823486\n",
      "Epoch 7, Iteration 16, Loss: 0.821052074432373\n",
      "Epoch 7, Iteration 17, Loss: 0.8993593454360962\n",
      "Epoch 7, Iteration 18, Loss: 0.8166019916534424\n",
      "Epoch 7, Iteration 19, Loss: 0.899639904499054\n",
      "Epoch 7, Iteration 20, Loss: 0.7197369337081909\n",
      "Epoch 7, Iteration 21, Loss: 0.7466498613357544\n",
      "Epoch 7, Iteration 22, Loss: 0.751678466796875\n",
      "Epoch 7, Iteration 23, Loss: 0.8705089092254639\n",
      "Epoch 7, Iteration 24, Loss: 0.7678444385528564\n",
      "Epoch 7, Iteration 25, Loss: 0.7687444686889648\n",
      "Epoch 7, Iteration 26, Loss: 1.06522798538208\n",
      "Epoch 7, Iteration 27, Loss: 0.9706442356109619\n",
      "Epoch 7, Iteration 28, Loss: 0.7914252877235413\n",
      "Epoch 7, Iteration 29, Loss: 1.109683871269226\n",
      "Epoch 7, Iteration 30, Loss: 0.8063588738441467\n",
      "Epoch 7, Iteration 31, Loss: 0.8573222756385803\n",
      "Epoch 7, Iteration 32, Loss: 0.8311380743980408\n",
      "Epoch 7, Iteration 33, Loss: 0.815100908279419\n",
      "Epoch 7, Iteration 34, Loss: 0.7969427704811096\n",
      "Epoch 7, Iteration 35, Loss: 0.9582988619804382\n",
      "Epoch 7, Iteration 36, Loss: 0.6743867993354797\n",
      "Epoch 7, Iteration 37, Loss: 0.7531744837760925\n",
      "Epoch 7, Iteration 38, Loss: 0.7738423347473145\n",
      "Epoch 7, Iteration 39, Loss: 0.8731199502944946\n",
      "Epoch 7, Iteration 40, Loss: 0.8307611346244812\n",
      "Epoch 7, Iteration 41, Loss: 0.8794283270835876\n",
      "Epoch 7, Iteration 42, Loss: 0.771126389503479\n",
      "Epoch 7, Iteration 43, Loss: 0.6778325438499451\n",
      "Epoch 7, Iteration 44, Loss: 0.8004294633865356\n",
      "Epoch 7, Iteration 45, Loss: 0.7408488392829895\n",
      "Epoch 7, Iteration 46, Loss: 0.8696110844612122\n",
      "Epoch 7, Iteration 47, Loss: 0.8234677314758301\n",
      "Epoch 7, Iteration 48, Loss: 0.9729644656181335\n",
      "Epoch 7, Iteration 49, Loss: 0.7887186408042908\n",
      "Epoch 7, Iteration 50, Loss: 0.7869209051132202\n",
      "Epoch 7, Iteration 50, Valid Loss: 0.6288120746612549\n",
      "Epoch 7, Iteration 51, Loss: 0.792698323726654\n",
      "Epoch 7, Iteration 52, Loss: 0.9187628626823425\n",
      "Epoch 7, Iteration 53, Loss: 0.9170027375221252\n",
      "Epoch 7, Iteration 54, Loss: 1.1417737007141113\n",
      "Epoch 7, Iteration 55, Loss: 0.9357349276542664\n",
      "Epoch 7, Iteration 56, Loss: 0.8049165606498718\n",
      "Epoch 7, Iteration 57, Loss: 0.9790053963661194\n",
      "Epoch 7, Iteration 58, Loss: 0.8553678393363953\n",
      "Epoch 7, Iteration 59, Loss: 0.7022162675857544\n",
      "Epoch 7, Iteration 60, Loss: 0.8281328678131104\n",
      "Epoch 7, Iteration 61, Loss: 0.6451684832572937\n",
      "Epoch 7, Iteration 62, Loss: 0.7270545363426208\n",
      "Epoch 7, Iteration 63, Loss: 0.7217098474502563\n",
      "Epoch 7, Iteration 64, Loss: 0.9404202699661255\n",
      "Epoch 7, Iteration 65, Loss: 1.0401118993759155\n",
      "Epoch 7, Iteration 66, Loss: 0.8350660800933838\n",
      "Epoch 7, Iteration 67, Loss: 0.6777006387710571\n",
      "Epoch 7, Iteration 68, Loss: 0.9667060375213623\n",
      "Epoch 7, Iteration 69, Loss: 0.9481001496315002\n",
      "Epoch 7, Iteration 70, Loss: 0.872506320476532\n",
      "Epoch 7, Iteration 71, Loss: 0.8490266799926758\n",
      "Epoch 7, Iteration 72, Loss: 0.7356780171394348\n",
      "Epoch 7, Iteration 73, Loss: 0.8106260299682617\n",
      "Epoch 7, Iteration 74, Loss: 1.068251609802246\n",
      "Epoch 7, Iteration 75, Loss: 0.9205963015556335\n",
      "Epoch 7, Iteration 76, Loss: 0.8189281821250916\n",
      "Epoch 7, Iteration 77, Loss: 0.7224507927894592\n",
      "Epoch 7, Iteration 78, Loss: 1.0833154916763306\n",
      "Epoch 7, Iteration 79, Loss: 1.0487403869628906\n",
      "Epoch 7, Iteration 80, Loss: 0.7666139602661133\n",
      "Epoch 7, Iteration 81, Loss: 1.1327390670776367\n",
      "Epoch 7, Iteration 82, Loss: 0.9706342220306396\n",
      "Epoch 7, Iteration 83, Loss: 0.7399483323097229\n",
      "Epoch 7, Iteration 84, Loss: 1.0021376609802246\n",
      "Epoch 7, Iteration 85, Loss: 0.865048348903656\n",
      "Epoch 7, Iteration 86, Loss: 0.9029746055603027\n",
      "Epoch 7, Iteration 87, Loss: 0.8550764918327332\n",
      "Epoch 7, Iteration 88, Loss: 0.8765568733215332\n",
      "Epoch 7, Iteration 89, Loss: 0.8712485432624817\n",
      "Epoch 7, Iteration 90, Loss: 0.7038930654525757\n",
      "Epoch 7, Iteration 91, Loss: 0.6517089605331421\n",
      "Epoch 7, Iteration 92, Loss: 0.7897401452064514\n",
      "Epoch 7, Iteration 93, Loss: 0.8190420866012573\n",
      "Epoch 7, Iteration 94, Loss: 1.0281083583831787\n",
      "Epoch 7, Iteration 95, Loss: 0.5801501870155334\n",
      "Epoch 7, Iteration 96, Loss: 0.8187569379806519\n",
      "Epoch 7, Iteration 97, Loss: 0.637419581413269\n",
      "Epoch 7, Iteration 98, Loss: 0.8148011565208435\n",
      "Epoch 7, Iteration 99, Loss: 0.8036811351776123\n",
      "Epoch 7, Iteration 100, Loss: 0.7397749423980713\n",
      "Epoch 7, Iteration 100, Valid Loss: 0.7026565670967102\n",
      "Epoch 7, Iteration 101, Loss: 1.0668973922729492\n",
      "Epoch 7, Iteration 102, Loss: 0.9446930289268494\n",
      "Epoch 7, Iteration 103, Loss: 0.7974791526794434\n",
      "Epoch 7, Iteration 104, Loss: 1.0220744609832764\n",
      "Epoch 7, Iteration 105, Loss: 0.8688572645187378\n",
      "Epoch 7, Iteration 106, Loss: 0.7336182594299316\n",
      "Epoch 7, Iteration 107, Loss: 1.13853919506073\n",
      "Epoch 7, Iteration 108, Loss: 0.7906147241592407\n",
      "Epoch 7, Iteration 109, Loss: 1.0961719751358032\n",
      "Epoch 7, Iteration 110, Loss: 0.9893525838851929\n",
      "Epoch 7, Iteration 111, Loss: 0.7828582525253296\n",
      "Epoch 7, Iteration 112, Loss: 0.9076240062713623\n",
      "Epoch 7, Iteration 113, Loss: 0.6746313571929932\n",
      "Epoch 7, Iteration 114, Loss: 1.0412676334381104\n",
      "Epoch 7, Iteration 115, Loss: 0.9247589111328125\n",
      "Epoch 7, Iteration 116, Loss: 0.7271558046340942\n",
      "Epoch 7, Iteration 117, Loss: 0.5627004504203796\n",
      "Epoch 7, Iteration 118, Loss: 0.9107057452201843\n",
      "Epoch 7, Iteration 119, Loss: 0.7778766751289368\n",
      "Epoch 7, Iteration 120, Loss: 0.9817062020301819\n",
      "Epoch 7, Iteration 121, Loss: 0.7532413601875305\n",
      "Epoch 7, Iteration 122, Loss: 0.8131604194641113\n",
      "Epoch 7, Iteration 123, Loss: 0.8326100707054138\n",
      "Epoch 7, Iteration 124, Loss: 0.7116755247116089\n",
      "Epoch 7, Iteration 125, Loss: 0.8256444931030273\n",
      "Epoch 7, Iteration 126, Loss: 0.7144587635993958\n",
      "Epoch 7, Iteration 127, Loss: 0.8898270726203918\n",
      "Epoch 7, Iteration 128, Loss: 0.8527031540870667\n",
      "Epoch 7, Iteration 129, Loss: 0.9023072123527527\n",
      "Epoch 7, Iteration 130, Loss: 1.0497626066207886\n",
      "Epoch 7, Iteration 131, Loss: 0.7554911375045776\n",
      "Epoch 7, Iteration 132, Loss: 0.7451658248901367\n",
      "Epoch 7, Iteration 133, Loss: 1.0863544940948486\n",
      "Epoch 7, Iteration 134, Loss: 0.886617124080658\n",
      "Epoch 7, Iteration 135, Loss: 0.8720197677612305\n",
      "Epoch 7, Iteration 136, Loss: 0.8927602171897888\n",
      "Epoch 7, Iteration 137, Loss: 0.8644930720329285\n",
      "Epoch 7, Iteration 138, Loss: 0.6755325794219971\n",
      "Epoch 7, Iteration 139, Loss: 0.857010006904602\n",
      "Epoch 7, Iteration 140, Loss: 0.7116042971611023\n",
      "Epoch 7, Iteration 141, Loss: 1.158316969871521\n",
      "Epoch 7, Iteration 142, Loss: 0.8740724921226501\n",
      "Epoch 7, Iteration 143, Loss: 0.771639883518219\n",
      "Epoch 7, Iteration 144, Loss: 0.7964283227920532\n",
      "Epoch 7, Iteration 145, Loss: 0.795951247215271\n",
      "Epoch 7, Iteration 146, Loss: 0.8856279253959656\n",
      "Epoch 7, Iteration 147, Loss: 0.8656407594680786\n",
      "Epoch 7, Iteration 148, Loss: 0.7230308055877686\n",
      "Epoch 7, Iteration 149, Loss: 0.7785241007804871\n",
      "Epoch 7, Iteration 150, Loss: 0.9398717880249023\n",
      "Epoch 7, Iteration 150, Valid Loss: 0.6531211137771606\n",
      "Epoch 7, Iteration 151, Loss: 0.886091411113739\n",
      "Epoch 7, Iteration 152, Loss: 0.7791476845741272\n",
      "Epoch 7, Iteration 153, Loss: 1.007896900177002\n",
      "Epoch 7, Iteration 154, Loss: 0.7259949445724487\n",
      "Epoch 7, Iteration 155, Loss: 0.8568269610404968\n",
      "Epoch 7, Iteration 156, Loss: 0.7256686687469482\n",
      "Epoch 7, Iteration 157, Loss: 1.1538000106811523\n",
      "Epoch 7, Iteration 158, Loss: 0.8428224921226501\n",
      "Epoch 7, Iteration 159, Loss: 0.8633370995521545\n",
      "Epoch 7, Iteration 160, Loss: 0.742670476436615\n",
      "Epoch 7, Iteration 161, Loss: 0.7796298861503601\n",
      "Epoch 7, Iteration 162, Loss: 1.1156240701675415\n",
      "Epoch 7, Iteration 163, Loss: 0.6733419895172119\n",
      "Epoch 7, Iteration 164, Loss: 0.8307566046714783\n",
      "Epoch 7, Iteration 165, Loss: 0.7248823642730713\n",
      "Epoch 7, Iteration 166, Loss: 0.7284976840019226\n",
      "Epoch 7, Iteration 167, Loss: 0.974826991558075\n",
      "Epoch 7, Iteration 168, Loss: 0.8500487804412842\n",
      "Epoch 7, Iteration 169, Loss: 0.8158398866653442\n",
      "Epoch 7, Iteration 170, Loss: 0.9765042066574097\n",
      "Epoch 7, Iteration 171, Loss: 0.9286813735961914\n",
      "Epoch 7, Iteration 172, Loss: 0.8045305013656616\n",
      "Epoch 7, Iteration 173, Loss: 0.708111584186554\n",
      "Epoch 7, Iteration 174, Loss: 0.8857104778289795\n",
      "Epoch 7, Iteration 175, Loss: 0.8836666941642761\n",
      "Epoch 7, Iteration 176, Loss: 0.6345216035842896\n",
      "Epoch 7, Iteration 177, Loss: 1.1519745588302612\n",
      "Epoch 7, Iteration 178, Loss: 1.034260630607605\n",
      "Epoch 7, Iteration 179, Loss: 0.7985706329345703\n",
      "Epoch 7, Iteration 180, Loss: 0.7984645962715149\n",
      "Epoch 7, Iteration 181, Loss: 0.9611436724662781\n",
      "Epoch 7, Iteration 182, Loss: 0.934952974319458\n",
      "Epoch 7, Iteration 183, Loss: 0.7081317901611328\n",
      "Epoch 7, Iteration 184, Loss: 0.7929804921150208\n",
      "Epoch 7, Iteration 185, Loss: 0.7154640555381775\n",
      "Epoch 7, Iteration 186, Loss: 0.7990390658378601\n",
      "Epoch 7, Iteration 187, Loss: 0.7476531863212585\n",
      "Epoch 7, Iteration 188, Loss: 0.7328268885612488\n",
      "Epoch 7, Iteration 189, Loss: 0.7467116117477417\n",
      "Epoch 7, Iteration 190, Loss: 0.9646768569946289\n",
      "Epoch 7, Iteration 191, Loss: 0.8866062164306641\n",
      "Epoch 7, Iteration 192, Loss: 0.9186716675758362\n",
      "Epoch 7, Iteration 193, Loss: 0.7944021821022034\n",
      "Epoch 7, Iteration 194, Loss: 0.8060475587844849\n",
      "Epoch 7, Iteration 195, Loss: 0.8513054847717285\n",
      "Epoch 7, Iteration 196, Loss: 0.8343818783760071\n",
      "Epoch 7, Iteration 197, Loss: 0.823411762714386\n",
      "Epoch 7, Iteration 198, Loss: 0.8194068670272827\n",
      "Epoch 7, Iteration 199, Loss: 0.9225341081619263\n",
      "Epoch 7, Iteration 200, Loss: 0.7667677402496338\n",
      "Epoch 7, Iteration 200, Valid Loss: 0.6609580516815186\n",
      "Epoch 7, Iteration 201, Loss: 0.7626671195030212\n",
      "Epoch 7, Iteration 202, Loss: 0.8589255213737488\n",
      "Epoch 7, Iteration 203, Loss: 0.7599508762359619\n",
      "Epoch 7, Iteration 204, Loss: 0.7599952220916748\n",
      "Epoch 7, Iteration 205, Loss: 0.6746100187301636\n",
      "Epoch 7, Iteration 206, Loss: 0.7011725902557373\n",
      "Epoch 7, Iteration 207, Loss: 0.9271546006202698\n",
      "Epoch 7, Iteration 208, Loss: 1.2229461669921875\n",
      "Epoch 7, Iteration 209, Loss: 1.041476845741272\n",
      "Epoch 7, Iteration 210, Loss: 1.065731406211853\n",
      "Epoch 7, Iteration 211, Loss: 1.1518243551254272\n",
      "Epoch 7, Iteration 212, Loss: 0.9851424694061279\n",
      "Epoch 7, Iteration 213, Loss: 1.1388111114501953\n",
      "Epoch 7, Iteration 214, Loss: 0.7997285723686218\n",
      "Epoch 7, Iteration 215, Loss: 0.7460931539535522\n",
      "Epoch 7, Iteration 216, Loss: 0.7811265587806702\n",
      "Epoch 7, Iteration 217, Loss: 0.7082324624061584\n",
      "Epoch 7, Iteration 218, Loss: 0.8397226333618164\n",
      "Epoch 7, Iteration 219, Loss: 0.9248088002204895\n",
      "Epoch 7, Iteration 220, Loss: 0.7643944621086121\n",
      "Epoch 7, Iteration 221, Loss: 0.8548832535743713\n",
      "Epoch 7, Iteration 222, Loss: 0.8093224763870239\n",
      "Epoch 7, Iteration 223, Loss: 0.7124581933021545\n",
      "Epoch 7, Iteration 224, Loss: 0.7258070111274719\n",
      "Epoch 7, Iteration 225, Loss: 0.8755045533180237\n",
      "Epoch 7, Iteration 226, Loss: 0.893294632434845\n",
      "Epoch 7, Iteration 227, Loss: 0.8423646688461304\n",
      "Epoch 7, Iteration 228, Loss: 0.9031882882118225\n",
      "Epoch 7, Iteration 229, Loss: 0.7102035880088806\n",
      "Epoch 7, Iteration 230, Loss: 0.8160422444343567\n",
      "Epoch 7, Iteration 231, Loss: 0.7218504548072815\n",
      "Epoch 7, Iteration 232, Loss: 0.9350214004516602\n",
      "Epoch 7, Iteration 233, Loss: 1.0997238159179688\n",
      "Epoch 7, Iteration 234, Loss: 0.9350956082344055\n",
      "Epoch 7, Iteration 235, Loss: 0.631614625453949\n",
      "Epoch 7, Iteration 236, Loss: 0.8057868480682373\n",
      "Epoch 7, Iteration 237, Loss: 0.8099339604377747\n",
      "Epoch 7, Iteration 238, Loss: 0.6778084635734558\n",
      "Epoch 7, Iteration 239, Loss: 0.844700038433075\n",
      "Epoch 7, Iteration 240, Loss: 0.7368040084838867\n",
      "Epoch 7, Iteration 241, Loss: 0.7869619727134705\n",
      "Epoch 7, Iteration 242, Loss: 0.7878249287605286\n",
      "Epoch 7, Iteration 243, Loss: 0.7885993123054504\n",
      "Epoch 7, Iteration 244, Loss: 0.8073578476905823\n",
      "Epoch 7, Iteration 245, Loss: 0.5742329955101013\n",
      "Epoch 7, Iteration 246, Loss: 0.8221732974052429\n",
      "Epoch 7, Iteration 247, Loss: 0.7262783050537109\n",
      "Epoch 7, Iteration 248, Loss: 0.9164894223213196\n",
      "Epoch 7, Iteration 249, Loss: 0.8047215342521667\n",
      "Epoch 7, Iteration 250, Loss: 0.9085180163383484\n",
      "Epoch 7, Iteration 250, Valid Loss: 0.627094030380249\n",
      "Epoch 7, Iteration 251, Loss: 0.7317272424697876\n",
      "Epoch 7, Iteration 252, Loss: 0.9444628953933716\n",
      "Epoch 7, Iteration 253, Loss: 0.8962719440460205\n",
      "Epoch 7, Iteration 254, Loss: 0.9491063952445984\n",
      "Epoch 7, Iteration 255, Loss: 0.9014657139778137\n",
      "Epoch 7, Iteration 256, Loss: 0.8931984305381775\n",
      "Epoch 7, Iteration 257, Loss: 0.7183470726013184\n",
      "Epoch 7, Iteration 258, Loss: 1.0030369758605957\n",
      "Epoch 7, Iteration 259, Loss: 0.7068907022476196\n",
      "Epoch 7, Iteration 260, Loss: 0.9327701926231384\n",
      "Epoch 7, Iteration 261, Loss: 0.8705060482025146\n",
      "Epoch 7, Iteration 262, Loss: 0.7120449542999268\n",
      "Epoch 7, Iteration 263, Loss: 0.8602250814437866\n",
      "Epoch 7, Iteration 264, Loss: 0.7601279616355896\n",
      "Epoch 7, Iteration 265, Loss: 0.8962103724479675\n",
      "Epoch 7, Iteration 266, Loss: 0.8069462180137634\n",
      "Epoch 7, Iteration 267, Loss: 0.7904554605484009\n",
      "Epoch 7, Iteration 268, Loss: 0.7126280069351196\n",
      "Epoch 7, Iteration 269, Loss: 0.7954975962638855\n",
      "Epoch 7, Iteration 270, Loss: 0.80109703540802\n",
      "Epoch 7, Iteration 271, Loss: 0.9165465831756592\n",
      "Epoch 7, Iteration 272, Loss: 0.7227517366409302\n",
      "Epoch 7, Iteration 273, Loss: 0.9179272651672363\n",
      "Epoch 7, Iteration 274, Loss: 0.7401010394096375\n",
      "Epoch 7, Iteration 275, Loss: 0.6845203638076782\n",
      "Epoch 7, Iteration 276, Loss: 0.6621360778808594\n",
      "Epoch 7, Iteration 277, Loss: 0.7227115631103516\n",
      "Epoch 7, Iteration 278, Loss: 0.8167589902877808\n",
      "Epoch 7, Iteration 279, Loss: 0.6525457501411438\n",
      "Epoch 7, Iteration 280, Loss: 0.6281647682189941\n",
      "Epoch 7, Iteration 281, Loss: 0.7477750182151794\n",
      "Epoch 7, Iteration 282, Loss: 0.7669596076011658\n",
      "Epoch 7, Iteration 283, Loss: 0.7667103409767151\n",
      "Epoch 7, Iteration 284, Loss: 0.8369082808494568\n",
      "Epoch 7, Iteration 285, Loss: 0.6811191439628601\n",
      "Epoch 7, Iteration 286, Loss: 0.8009266257286072\n",
      "Epoch 7, Iteration 287, Loss: 0.7053644061088562\n",
      "Epoch 7, Iteration 288, Loss: 0.7682638764381409\n",
      "Epoch 7, Iteration 289, Loss: 0.6455366611480713\n",
      "Epoch 7, Iteration 290, Loss: 0.6104114651679993\n",
      "Epoch 7, Iteration 291, Loss: 0.5994965434074402\n",
      "Epoch 7, Iteration 292, Loss: 0.8337138295173645\n",
      "Epoch 7, Iteration 293, Loss: 0.948569655418396\n",
      "Epoch 7, Iteration 294, Loss: 0.7552927732467651\n",
      "Epoch 7, Iteration 295, Loss: 0.5968097448348999\n",
      "Epoch 7, Iteration 296, Loss: 0.9142616987228394\n",
      "Epoch 7, Iteration 297, Loss: 0.9165624976158142\n",
      "Epoch 7, Iteration 298, Loss: 0.7819798588752747\n",
      "Epoch 7, Iteration 299, Loss: 0.7430936694145203\n",
      "Epoch 7, Iteration 300, Loss: 0.6683322787284851\n",
      "Epoch 7, Iteration 300, Valid Loss: 0.6409880518913269\n",
      "Epoch 7, Iteration 301, Loss: 0.6985966563224792\n",
      "Epoch 7, Iteration 302, Loss: 0.6870680451393127\n",
      "Epoch 7, Iteration 303, Loss: 0.9397466778755188\n",
      "Epoch 7, Iteration 304, Loss: 0.7313122749328613\n",
      "Epoch 7, Iteration 305, Loss: 0.6557945013046265\n",
      "Epoch 7, Iteration 306, Loss: 0.6574673652648926\n",
      "Epoch 7, Iteration 307, Loss: 0.6491492390632629\n",
      "Epoch 7, Iteration 308, Loss: 0.7109147906303406\n",
      "Epoch 7, Iteration 309, Loss: 0.776026725769043\n",
      "Epoch 7, Iteration 310, Loss: 0.8346061706542969\n",
      "Epoch 7, Iteration 311, Loss: 0.8232980370521545\n",
      "Epoch 7, Iteration 312, Loss: 0.7135326266288757\n",
      "Epoch 7, Iteration 313, Loss: 0.9989252686500549\n",
      "Epoch 7, Iteration 314, Loss: 0.6837284564971924\n",
      "Epoch 7, Iteration 315, Loss: 0.6759939193725586\n",
      "Epoch 7, Iteration 316, Loss: 0.8613730072975159\n",
      "Epoch 7, Iteration 317, Loss: 0.79620361328125\n",
      "Epoch 7, Iteration 318, Loss: 0.6262668371200562\n",
      "Epoch 7, Iteration 319, Loss: 0.9005968570709229\n",
      "Epoch 7, Iteration 320, Loss: 0.5295506119728088\n",
      "Epoch 7, Iteration 321, Loss: 0.8226194977760315\n",
      "Epoch 7, Iteration 322, Loss: 0.7078061103820801\n",
      "Epoch 7, Iteration 323, Loss: 0.7586493492126465\n",
      "Epoch 7, Iteration 324, Loss: 0.8237130045890808\n",
      "Epoch 7, Iteration 325, Loss: 0.8329858183860779\n",
      "Epoch 7, Iteration 326, Loss: 0.7182661294937134\n",
      "Epoch 7, Iteration 327, Loss: 0.713573694229126\n",
      "Epoch 7, Iteration 328, Loss: 0.6911936402320862\n",
      "Epoch 7, Iteration 329, Loss: 0.8356702923774719\n",
      "Epoch 7, Iteration 330, Loss: 0.6107012629508972\n",
      "Epoch 7, Iteration 331, Loss: 0.7576987147331238\n",
      "Epoch 7, Iteration 332, Loss: 0.7525964975357056\n",
      "Epoch 7, Iteration 333, Loss: 0.8894855976104736\n",
      "Epoch 7, Iteration 334, Loss: 0.7548941373825073\n",
      "Epoch 7, Iteration 335, Loss: 0.8681884407997131\n",
      "Epoch 7, Iteration 336, Loss: 0.8685750961303711\n",
      "Epoch 7, Iteration 337, Loss: 0.6563715934753418\n",
      "Epoch 7, Iteration 338, Loss: 0.9659472107887268\n",
      "Epoch 7, Iteration 339, Loss: 0.7712991237640381\n",
      "Epoch 7, Iteration 340, Loss: 0.603645384311676\n",
      "Epoch 7, Iteration 341, Loss: 0.747826099395752\n",
      "Epoch 7, Iteration 342, Loss: 0.7541590929031372\n",
      "Epoch 7, Iteration 343, Loss: 0.7641313076019287\n",
      "Epoch 7, Iteration 344, Loss: 0.8549860119819641\n",
      "Epoch 7, Iteration 345, Loss: 0.7608649730682373\n",
      "Epoch 7, Iteration 346, Loss: 0.7051149606704712\n",
      "Epoch 7, Iteration 347, Loss: 0.6613991856575012\n",
      "Epoch 7, Iteration 348, Loss: 0.5783682465553284\n",
      "Epoch 7, Iteration 349, Loss: 0.7442683577537537\n",
      "Epoch 7, Iteration 350, Loss: 0.7483622431755066\n",
      "Epoch 7, Iteration 350, Valid Loss: 0.663077175617218\n",
      "Epoch 7, Iteration 351, Loss: 0.7348140478134155\n",
      "Epoch 7, Iteration 352, Loss: 0.6448012590408325\n",
      "Epoch 7, Iteration 353, Loss: 0.7284764647483826\n",
      "Epoch 7, Iteration 354, Loss: 0.6336501240730286\n",
      "Epoch 7, Iteration 355, Loss: 0.7166846394538879\n",
      "Epoch 7, Iteration 356, Loss: 0.7972567677497864\n",
      "Epoch 7, Iteration 357, Loss: 0.6800937652587891\n",
      "Epoch 7, Iteration 358, Loss: 0.6545470356941223\n",
      "Epoch 7, Iteration 359, Loss: 0.7312576174736023\n",
      "Epoch 7, Iteration 360, Loss: 0.5788196325302124\n",
      "Epoch 7, Iteration 361, Loss: 0.7070230841636658\n",
      "Epoch 7, Iteration 362, Loss: 0.724740207195282\n",
      "Epoch 7, Iteration 363, Loss: 0.7240682244300842\n",
      "Epoch 7, Iteration 364, Loss: 0.7444657683372498\n",
      "Epoch 7, Iteration 365, Loss: 0.8479467034339905\n",
      "Epoch 7, Iteration 366, Loss: 0.7864348888397217\n",
      "Epoch 7, Iteration 367, Loss: 0.743439257144928\n",
      "Epoch 7, Iteration 368, Loss: 0.7411530613899231\n",
      "Epoch 7, Iteration 369, Loss: 0.7810795307159424\n",
      "Epoch 7, Iteration 370, Loss: 0.7564297914505005\n",
      "Epoch 7, Iteration 371, Loss: 0.7863228917121887\n",
      "Epoch 7, Iteration 372, Loss: 0.7309903502464294\n",
      "Epoch 7, Iteration 373, Loss: 0.683765172958374\n",
      "Epoch 7, Iteration 374, Loss: 0.9511817693710327\n",
      "Epoch 7, Iteration 375, Loss: 0.7082496285438538\n",
      "Epoch 7, Iteration 376, Loss: 0.8383395671844482\n",
      "Epoch 7, Iteration 377, Loss: 0.8339261412620544\n",
      "Epoch 7, Iteration 378, Loss: 0.6917149424552917\n",
      "Epoch 7, Iteration 379, Loss: 0.6655032634735107\n",
      "Epoch 7, Iteration 380, Loss: 0.6975058317184448\n",
      "Epoch 7, Iteration 381, Loss: 0.7019885182380676\n",
      "Epoch 7, Iteration 382, Loss: 0.5766870379447937\n",
      "Epoch 7, Iteration 383, Loss: 0.8348678350448608\n",
      "Epoch 7, Iteration 384, Loss: 0.7688816785812378\n",
      "Epoch 7, Iteration 385, Loss: 0.6765870451927185\n",
      "Epoch 7, Iteration 386, Loss: 0.6258069276809692\n",
      "Epoch 7, Iteration 387, Loss: 0.9047914743423462\n",
      "Epoch 7, Iteration 388, Loss: 0.6695424914360046\n",
      "Epoch 7, Iteration 389, Loss: 0.7368347644805908\n",
      "Epoch 7, Iteration 390, Loss: 0.9141240119934082\n",
      "Epoch 7, Iteration 391, Loss: 0.6896296143531799\n",
      "Epoch 8/15, Loss: 0.8145121693307039\n",
      "Epoch 8, Iteration 0, Loss: 0.7723314166069031\n",
      "Epoch 8, Iteration 1, Loss: 0.9333529472351074\n",
      "Epoch 8, Iteration 2, Loss: 0.9342523217201233\n",
      "Epoch 8, Iteration 3, Loss: 0.8840513229370117\n",
      "Epoch 8, Iteration 4, Loss: 0.7673226594924927\n",
      "Epoch 8, Iteration 5, Loss: 0.869283139705658\n",
      "Epoch 8, Iteration 6, Loss: 0.6541190147399902\n",
      "Epoch 8, Iteration 7, Loss: 0.7809027433395386\n",
      "Epoch 8, Iteration 8, Loss: 0.6874029636383057\n",
      "Epoch 8, Iteration 9, Loss: 0.8833932876586914\n",
      "Epoch 8, Iteration 10, Loss: 0.8319272994995117\n",
      "Epoch 8, Iteration 11, Loss: 0.7463294267654419\n",
      "Epoch 8, Iteration 12, Loss: 0.7184756994247437\n",
      "Epoch 8, Iteration 13, Loss: 0.916750431060791\n",
      "Epoch 8, Iteration 14, Loss: 0.8316477537155151\n",
      "Epoch 8, Iteration 15, Loss: 0.9646238088607788\n",
      "Epoch 8, Iteration 16, Loss: 0.7435988783836365\n",
      "Epoch 8, Iteration 17, Loss: 0.8644112348556519\n",
      "Epoch 8, Iteration 18, Loss: 0.7481285333633423\n",
      "Epoch 8, Iteration 19, Loss: 0.8123845458030701\n",
      "Epoch 8, Iteration 20, Loss: 0.6940682530403137\n",
      "Epoch 8, Iteration 21, Loss: 0.7382153868675232\n",
      "Epoch 8, Iteration 22, Loss: 0.6932944655418396\n",
      "Epoch 8, Iteration 23, Loss: 0.7573882937431335\n",
      "Epoch 8, Iteration 24, Loss: 0.648131787776947\n",
      "Epoch 8, Iteration 25, Loss: 0.6381077170372009\n",
      "Epoch 8, Iteration 26, Loss: 1.0425432920455933\n",
      "Epoch 8, Iteration 27, Loss: 0.7811651825904846\n",
      "Epoch 8, Iteration 28, Loss: 0.7566598653793335\n",
      "Epoch 8, Iteration 29, Loss: 1.0129474401474\n",
      "Epoch 8, Iteration 30, Loss: 0.8601498603820801\n",
      "Epoch 8, Iteration 31, Loss: 0.7387085556983948\n",
      "Epoch 8, Iteration 32, Loss: 0.7336837649345398\n",
      "Epoch 8, Iteration 33, Loss: 0.8684208393096924\n",
      "Epoch 8, Iteration 34, Loss: 0.717205822467804\n",
      "Epoch 8, Iteration 35, Loss: 0.8121412992477417\n",
      "Epoch 8, Iteration 36, Loss: 0.637554943561554\n",
      "Epoch 8, Iteration 37, Loss: 0.6456922888755798\n",
      "Epoch 8, Iteration 38, Loss: 0.7362203001976013\n",
      "Epoch 8, Iteration 39, Loss: 0.7768717408180237\n",
      "Epoch 8, Iteration 40, Loss: 0.7174690961837769\n",
      "Epoch 8, Iteration 41, Loss: 0.8613415360450745\n",
      "Epoch 8, Iteration 42, Loss: 0.7906469702720642\n",
      "Epoch 8, Iteration 43, Loss: 0.7483316659927368\n",
      "Epoch 8, Iteration 44, Loss: 0.7168462872505188\n",
      "Epoch 8, Iteration 45, Loss: 0.6911691427230835\n",
      "Epoch 8, Iteration 46, Loss: 0.8288882374763489\n",
      "Epoch 8, Iteration 47, Loss: 0.74136883020401\n",
      "Epoch 8, Iteration 48, Loss: 0.8879780769348145\n",
      "Epoch 8, Iteration 49, Loss: 0.7466059923171997\n",
      "Epoch 8, Iteration 50, Loss: 0.7328747510910034\n",
      "Epoch 8, Iteration 50, Valid Loss: 0.6179069876670837\n",
      "Epoch 8, Iteration 51, Loss: 0.7690640091896057\n",
      "Epoch 8, Iteration 52, Loss: 0.6960077881813049\n",
      "Epoch 8, Iteration 53, Loss: 0.733831524848938\n",
      "Epoch 8, Iteration 54, Loss: 1.0023905038833618\n",
      "Epoch 8, Iteration 55, Loss: 0.8943944573402405\n",
      "Epoch 8, Iteration 56, Loss: 0.8267918825149536\n",
      "Epoch 8, Iteration 57, Loss: 1.002090334892273\n",
      "Epoch 8, Iteration 58, Loss: 0.7740632891654968\n",
      "Epoch 8, Iteration 59, Loss: 0.7052252292633057\n",
      "Epoch 8, Iteration 60, Loss: 0.7883196473121643\n",
      "Epoch 8, Iteration 61, Loss: 0.6196491122245789\n",
      "Epoch 8, Iteration 62, Loss: 0.7479863166809082\n",
      "Epoch 8, Iteration 63, Loss: 0.7060490846633911\n",
      "Epoch 8, Iteration 64, Loss: 0.7857531309127808\n",
      "Epoch 8, Iteration 65, Loss: 0.9729222059249878\n",
      "Epoch 8, Iteration 66, Loss: 0.7388310432434082\n",
      "Epoch 8, Iteration 67, Loss: 0.5539466142654419\n",
      "Epoch 8, Iteration 68, Loss: 0.9452335834503174\n",
      "Epoch 8, Iteration 69, Loss: 0.8556772470474243\n",
      "Epoch 8, Iteration 70, Loss: 0.7945333123207092\n",
      "Epoch 8, Iteration 71, Loss: 0.9160225987434387\n",
      "Epoch 8, Iteration 72, Loss: 0.7246264219284058\n",
      "Epoch 8, Iteration 73, Loss: 0.6762353777885437\n",
      "Epoch 8, Iteration 74, Loss: 0.9243794679641724\n",
      "Epoch 8, Iteration 75, Loss: 0.8630112409591675\n",
      "Epoch 8, Iteration 76, Loss: 0.7713794708251953\n",
      "Epoch 8, Iteration 77, Loss: 0.6407169103622437\n",
      "Epoch 8, Iteration 78, Loss: 0.9536401033401489\n",
      "Epoch 8, Iteration 79, Loss: 0.9975892901420593\n",
      "Epoch 8, Iteration 80, Loss: 0.7111271619796753\n",
      "Epoch 8, Iteration 81, Loss: 0.961274266242981\n",
      "Epoch 8, Iteration 82, Loss: 0.9065226912498474\n",
      "Epoch 8, Iteration 83, Loss: 0.6358131170272827\n",
      "Epoch 8, Iteration 84, Loss: 0.8619581460952759\n",
      "Epoch 8, Iteration 85, Loss: 0.8530079126358032\n",
      "Epoch 8, Iteration 86, Loss: 0.8726113438606262\n",
      "Epoch 8, Iteration 87, Loss: 0.9020432233810425\n",
      "Epoch 8, Iteration 88, Loss: 0.8701348304748535\n",
      "Epoch 8, Iteration 89, Loss: 1.1263315677642822\n",
      "Epoch 8, Iteration 90, Loss: 0.6347926259040833\n",
      "Epoch 8, Iteration 91, Loss: 0.7265244126319885\n",
      "Epoch 8, Iteration 92, Loss: 0.5613253116607666\n",
      "Epoch 8, Iteration 93, Loss: 0.7694932818412781\n",
      "Epoch 8, Iteration 94, Loss: 0.9448686838150024\n",
      "Epoch 8, Iteration 95, Loss: 0.688216507434845\n",
      "Epoch 8, Iteration 96, Loss: 0.8024515509605408\n",
      "Epoch 8, Iteration 97, Loss: 0.6573487520217896\n",
      "Epoch 8, Iteration 98, Loss: 0.761480450630188\n",
      "Epoch 8, Iteration 99, Loss: 0.8248940110206604\n",
      "Epoch 8, Iteration 100, Loss: 0.5826283693313599\n",
      "Epoch 8, Iteration 100, Valid Loss: 0.6737052798271179\n",
      "Epoch 8, Iteration 101, Loss: 0.904826283454895\n",
      "Epoch 8, Iteration 102, Loss: 0.9116339683532715\n",
      "Epoch 8, Iteration 103, Loss: 0.7551209926605225\n",
      "Epoch 8, Iteration 104, Loss: 0.943060576915741\n",
      "Epoch 8, Iteration 105, Loss: 0.7408137917518616\n",
      "Epoch 8, Iteration 106, Loss: 0.7344620227813721\n",
      "Epoch 8, Iteration 107, Loss: 0.9208418726921082\n",
      "Epoch 8, Iteration 108, Loss: 0.7266799211502075\n",
      "Epoch 8, Iteration 109, Loss: 0.9877540469169617\n",
      "Epoch 8, Iteration 110, Loss: 0.9691860675811768\n",
      "Epoch 8, Iteration 111, Loss: 0.9345133900642395\n",
      "Epoch 8, Iteration 112, Loss: 0.870850682258606\n",
      "Epoch 8, Iteration 113, Loss: 0.6397858262062073\n",
      "Epoch 8, Iteration 114, Loss: 0.8992208242416382\n",
      "Epoch 8, Iteration 115, Loss: 0.9015724062919617\n",
      "Epoch 8, Iteration 116, Loss: 0.6488872766494751\n",
      "Epoch 8, Iteration 117, Loss: 0.5476059913635254\n",
      "Epoch 8, Iteration 118, Loss: 0.7780063152313232\n",
      "Epoch 8, Iteration 119, Loss: 0.7030429244041443\n",
      "Epoch 8, Iteration 120, Loss: 0.8754627108573914\n",
      "Epoch 8, Iteration 121, Loss: 0.7071446180343628\n",
      "Epoch 8, Iteration 122, Loss: 0.8263083100318909\n",
      "Epoch 8, Iteration 123, Loss: 0.7530467510223389\n",
      "Epoch 8, Iteration 124, Loss: 0.6616172790527344\n",
      "Epoch 8, Iteration 125, Loss: 0.7474989295005798\n",
      "Epoch 8, Iteration 126, Loss: 0.7333531975746155\n",
      "Epoch 8, Iteration 127, Loss: 0.804903507232666\n",
      "Epoch 8, Iteration 128, Loss: 0.8953490257263184\n",
      "Epoch 8, Iteration 129, Loss: 0.9186651110649109\n",
      "Epoch 8, Iteration 130, Loss: 1.0772676467895508\n",
      "Epoch 8, Iteration 131, Loss: 0.689012348651886\n",
      "Epoch 8, Iteration 132, Loss: 0.7412035465240479\n",
      "Epoch 8, Iteration 133, Loss: 1.1038097143173218\n",
      "Epoch 8, Iteration 134, Loss: 0.932055652141571\n",
      "Epoch 8, Iteration 135, Loss: 1.0814642906188965\n",
      "Epoch 8, Iteration 136, Loss: 0.7482474446296692\n",
      "Epoch 8, Iteration 137, Loss: 0.7650377750396729\n",
      "Epoch 8, Iteration 138, Loss: 0.7316889762878418\n",
      "Epoch 8, Iteration 139, Loss: 0.8316344022750854\n",
      "Epoch 8, Iteration 140, Loss: 0.6388121247291565\n",
      "Epoch 8, Iteration 141, Loss: 1.235364317893982\n",
      "Epoch 8, Iteration 142, Loss: 0.8545442819595337\n",
      "Epoch 8, Iteration 143, Loss: 0.7343149185180664\n",
      "Epoch 8, Iteration 144, Loss: 0.7430980801582336\n",
      "Epoch 8, Iteration 145, Loss: 0.7858081459999084\n",
      "Epoch 8, Iteration 146, Loss: 0.8181036114692688\n",
      "Epoch 8, Iteration 147, Loss: 0.7796630263328552\n",
      "Epoch 8, Iteration 148, Loss: 0.6740622520446777\n",
      "Epoch 8, Iteration 149, Loss: 0.7987098693847656\n",
      "Epoch 8, Iteration 150, Loss: 0.8844696283340454\n",
      "Epoch 8, Iteration 150, Valid Loss: 0.6491300463676453\n",
      "Epoch 8, Iteration 151, Loss: 0.8823569416999817\n",
      "Epoch 8, Iteration 152, Loss: 0.6598419547080994\n",
      "Epoch 8, Iteration 153, Loss: 0.921294629573822\n",
      "Epoch 8, Iteration 154, Loss: 0.626501739025116\n",
      "Epoch 8, Iteration 155, Loss: 0.7858733534812927\n",
      "Epoch 8, Iteration 156, Loss: 0.6248051524162292\n",
      "Epoch 8, Iteration 157, Loss: 1.1559652090072632\n",
      "Epoch 8, Iteration 158, Loss: 0.8027684092521667\n",
      "Epoch 8, Iteration 159, Loss: 0.8869659900665283\n",
      "Epoch 8, Iteration 160, Loss: 0.7894882559776306\n",
      "Epoch 8, Iteration 161, Loss: 0.7609946131706238\n",
      "Epoch 8, Iteration 162, Loss: 1.0725276470184326\n",
      "Epoch 8, Iteration 163, Loss: 0.6728034019470215\n",
      "Epoch 8, Iteration 164, Loss: 0.721744179725647\n",
      "Epoch 8, Iteration 165, Loss: 0.7102029323577881\n",
      "Epoch 8, Iteration 166, Loss: 0.7352766990661621\n",
      "Epoch 8, Iteration 167, Loss: 0.9089962840080261\n",
      "Epoch 8, Iteration 168, Loss: 0.750244140625\n",
      "Epoch 8, Iteration 169, Loss: 0.760367214679718\n",
      "Epoch 8, Iteration 170, Loss: 0.9600703716278076\n",
      "Epoch 8, Iteration 171, Loss: 1.0451133251190186\n",
      "Epoch 8, Iteration 172, Loss: 0.7953265309333801\n",
      "Epoch 8, Iteration 173, Loss: 0.7146149277687073\n",
      "Epoch 8, Iteration 174, Loss: 0.9229426980018616\n",
      "Epoch 8, Iteration 175, Loss: 0.8634028434753418\n",
      "Epoch 8, Iteration 176, Loss: 0.5767017602920532\n",
      "Epoch 8, Iteration 177, Loss: 1.1635457277297974\n",
      "Epoch 8, Iteration 178, Loss: 0.9362952709197998\n",
      "Epoch 8, Iteration 179, Loss: 0.7171728610992432\n",
      "Epoch 8, Iteration 180, Loss: 0.7928593158721924\n",
      "Epoch 8, Iteration 181, Loss: 0.9365570545196533\n",
      "Epoch 8, Iteration 182, Loss: 0.7867081761360168\n",
      "Epoch 8, Iteration 183, Loss: 0.672962486743927\n",
      "Epoch 8, Iteration 184, Loss: 0.791535496711731\n",
      "Epoch 8, Iteration 185, Loss: 0.692977786064148\n",
      "Epoch 8, Iteration 186, Loss: 0.7312636375427246\n",
      "Epoch 8, Iteration 187, Loss: 0.7403221726417542\n",
      "Epoch 8, Iteration 188, Loss: 0.7097447514533997\n",
      "Epoch 8, Iteration 189, Loss: 0.7065077424049377\n",
      "Epoch 8, Iteration 190, Loss: 0.8884391188621521\n",
      "Epoch 8, Iteration 191, Loss: 0.7985609173774719\n",
      "Epoch 8, Iteration 192, Loss: 0.8718447089195251\n",
      "Epoch 8, Iteration 193, Loss: 0.803961992263794\n",
      "Epoch 8, Iteration 194, Loss: 0.8012021780014038\n",
      "Epoch 8, Iteration 195, Loss: 0.6494418978691101\n",
      "Epoch 8, Iteration 196, Loss: 0.8047611117362976\n",
      "Epoch 8, Iteration 197, Loss: 0.7607775330543518\n",
      "Epoch 8, Iteration 198, Loss: 0.769475519657135\n",
      "Epoch 8, Iteration 199, Loss: 0.9120612144470215\n",
      "Epoch 8, Iteration 200, Loss: 0.6760099530220032\n",
      "Epoch 8, Iteration 200, Valid Loss: 0.6328034400939941\n",
      "Epoch 8, Iteration 201, Loss: 0.7792357206344604\n",
      "Epoch 8, Iteration 202, Loss: 0.8484064936637878\n",
      "Epoch 8, Iteration 203, Loss: 0.7280508875846863\n",
      "Epoch 8, Iteration 204, Loss: 0.6681937575340271\n",
      "Epoch 8, Iteration 205, Loss: 0.579771101474762\n",
      "Epoch 8, Iteration 206, Loss: 0.785650908946991\n",
      "Epoch 8, Iteration 207, Loss: 0.9252059459686279\n",
      "Epoch 8, Iteration 208, Loss: 1.1565996408462524\n",
      "Epoch 8, Iteration 209, Loss: 1.0047434568405151\n",
      "Epoch 8, Iteration 210, Loss: 1.0386778116226196\n",
      "Epoch 8, Iteration 211, Loss: 1.0903306007385254\n",
      "Epoch 8, Iteration 212, Loss: 0.9303727746009827\n",
      "Epoch 8, Iteration 213, Loss: 1.0466989278793335\n",
      "Epoch 8, Iteration 214, Loss: 0.8871487379074097\n",
      "Epoch 8, Iteration 215, Loss: 0.7567092180252075\n",
      "Epoch 8, Iteration 216, Loss: 0.6968377828598022\n",
      "Epoch 8, Iteration 217, Loss: 0.6954073309898376\n",
      "Epoch 8, Iteration 218, Loss: 0.7415695786476135\n",
      "Epoch 8, Iteration 219, Loss: 0.8556378483772278\n",
      "Epoch 8, Iteration 220, Loss: 0.7421422600746155\n",
      "Epoch 8, Iteration 221, Loss: 0.7626097798347473\n",
      "Epoch 8, Iteration 222, Loss: 0.7737033367156982\n",
      "Epoch 8, Iteration 223, Loss: 0.6479175686836243\n",
      "Epoch 8, Iteration 224, Loss: 0.7104569673538208\n",
      "Epoch 8, Iteration 225, Loss: 0.9054924249649048\n",
      "Epoch 8, Iteration 226, Loss: 0.8818700313568115\n",
      "Epoch 8, Iteration 227, Loss: 0.8464369773864746\n",
      "Epoch 8, Iteration 228, Loss: 1.3620857000350952\n",
      "Epoch 8, Iteration 229, Loss: 0.7524613738059998\n",
      "Epoch 8, Iteration 230, Loss: 0.7878682017326355\n",
      "Epoch 8, Iteration 231, Loss: 0.8529690504074097\n",
      "Epoch 8, Iteration 232, Loss: 0.8215423226356506\n",
      "Epoch 8, Iteration 233, Loss: 1.0554969310760498\n",
      "Epoch 8, Iteration 234, Loss: 0.9237075448036194\n",
      "Epoch 8, Iteration 235, Loss: 0.6037575602531433\n",
      "Epoch 8, Iteration 236, Loss: 0.7795801162719727\n",
      "Epoch 8, Iteration 237, Loss: 0.7507767081260681\n",
      "Epoch 8, Iteration 238, Loss: 0.6513497829437256\n",
      "Epoch 8, Iteration 239, Loss: 0.9009432792663574\n",
      "Epoch 8, Iteration 240, Loss: 0.7980290651321411\n",
      "Epoch 8, Iteration 241, Loss: 0.6793574690818787\n",
      "Epoch 8, Iteration 242, Loss: 0.7437796592712402\n",
      "Epoch 8, Iteration 243, Loss: 0.7811084389686584\n",
      "Epoch 8, Iteration 244, Loss: 0.7688733339309692\n",
      "Epoch 8, Iteration 245, Loss: 0.653424084186554\n",
      "Epoch 8, Iteration 246, Loss: 0.7568688988685608\n",
      "Epoch 8, Iteration 247, Loss: 0.6615779399871826\n",
      "Epoch 8, Iteration 248, Loss: 0.7636298537254333\n",
      "Epoch 8, Iteration 249, Loss: 0.8164746761322021\n",
      "Epoch 8, Iteration 250, Loss: 0.9223189353942871\n",
      "Epoch 8, Iteration 250, Valid Loss: 0.5939038395881653\n",
      "Epoch 8, Iteration 251, Loss: 0.6304099559783936\n",
      "Epoch 8, Iteration 252, Loss: 0.9045558571815491\n",
      "Epoch 8, Iteration 253, Loss: 0.8142085075378418\n",
      "Epoch 8, Iteration 254, Loss: 0.8590786457061768\n",
      "Epoch 8, Iteration 255, Loss: 0.7709758877754211\n",
      "Epoch 8, Iteration 256, Loss: 0.8363463878631592\n",
      "Epoch 8, Iteration 257, Loss: 0.714305579662323\n",
      "Epoch 8, Iteration 258, Loss: 0.9932155013084412\n",
      "Epoch 8, Iteration 259, Loss: 0.7078449726104736\n",
      "Epoch 8, Iteration 260, Loss: 0.9480971693992615\n",
      "Epoch 8, Iteration 261, Loss: 0.7829503417015076\n",
      "Epoch 8, Iteration 262, Loss: 0.7747987508773804\n",
      "Epoch 8, Iteration 263, Loss: 0.8574857711791992\n",
      "Epoch 8, Iteration 264, Loss: 0.6991180777549744\n",
      "Epoch 8, Iteration 265, Loss: 0.8100812435150146\n",
      "Epoch 8, Iteration 266, Loss: 0.8409783840179443\n",
      "Epoch 8, Iteration 267, Loss: 0.7668648958206177\n",
      "Epoch 8, Iteration 268, Loss: 0.6646692156791687\n",
      "Epoch 8, Iteration 269, Loss: 0.683812141418457\n",
      "Epoch 8, Iteration 270, Loss: 0.7261198163032532\n",
      "Epoch 8, Iteration 271, Loss: 0.8448901176452637\n",
      "Epoch 8, Iteration 272, Loss: 0.7102393507957458\n",
      "Epoch 8, Iteration 273, Loss: 0.7826150059700012\n",
      "Epoch 8, Iteration 274, Loss: 0.6290282607078552\n",
      "Epoch 8, Iteration 275, Loss: 0.7079584002494812\n",
      "Epoch 8, Iteration 276, Loss: 0.72487473487854\n",
      "Epoch 8, Iteration 277, Loss: 0.6930526494979858\n",
      "Epoch 8, Iteration 278, Loss: 0.9064182639122009\n",
      "Epoch 8, Iteration 279, Loss: 0.6494485139846802\n",
      "Epoch 8, Iteration 280, Loss: 0.6803093552589417\n",
      "Epoch 8, Iteration 281, Loss: 0.6965422034263611\n",
      "Epoch 8, Iteration 282, Loss: 0.7065133452415466\n",
      "Epoch 8, Iteration 283, Loss: 0.7496386170387268\n",
      "Epoch 8, Iteration 284, Loss: 0.7932451367378235\n",
      "Epoch 8, Iteration 285, Loss: 0.7486773133277893\n",
      "Epoch 8, Iteration 286, Loss: 0.7502000331878662\n",
      "Epoch 8, Iteration 287, Loss: 0.842180073261261\n",
      "Epoch 8, Iteration 288, Loss: 0.6883050203323364\n",
      "Epoch 8, Iteration 289, Loss: 0.7455174326896667\n",
      "Epoch 8, Iteration 290, Loss: 0.6704219579696655\n",
      "Epoch 8, Iteration 291, Loss: 0.5835399031639099\n",
      "Epoch 8, Iteration 292, Loss: 0.8237259387969971\n",
      "Epoch 8, Iteration 293, Loss: 0.8928553462028503\n",
      "Epoch 8, Iteration 294, Loss: 0.7327695488929749\n",
      "Epoch 8, Iteration 295, Loss: 0.6476801037788391\n",
      "Epoch 8, Iteration 296, Loss: 0.8370634317398071\n",
      "Epoch 8, Iteration 297, Loss: 0.850090503692627\n",
      "Epoch 8, Iteration 298, Loss: 0.7561913132667542\n",
      "Epoch 8, Iteration 299, Loss: 0.730344831943512\n",
      "Epoch 8, Iteration 300, Loss: 0.6372542977333069\n",
      "Epoch 8, Iteration 300, Valid Loss: 0.6033097505569458\n",
      "Epoch 8, Iteration 301, Loss: 0.6617618799209595\n",
      "Epoch 8, Iteration 302, Loss: 0.6903174519538879\n",
      "Epoch 8, Iteration 303, Loss: 0.8731063604354858\n",
      "Epoch 8, Iteration 304, Loss: 0.6515899896621704\n",
      "Epoch 8, Iteration 305, Loss: 0.6236833930015564\n",
      "Epoch 8, Iteration 306, Loss: 0.6717817783355713\n",
      "Epoch 8, Iteration 307, Loss: 0.7009583711624146\n",
      "Epoch 8, Iteration 308, Loss: 0.735345184803009\n",
      "Epoch 8, Iteration 309, Loss: 0.7400347590446472\n",
      "Epoch 8, Iteration 310, Loss: 0.766219973564148\n",
      "Epoch 8, Iteration 311, Loss: 0.6998669505119324\n",
      "Epoch 8, Iteration 312, Loss: 0.5787780284881592\n",
      "Epoch 8, Iteration 313, Loss: 0.9783509373664856\n",
      "Epoch 8, Iteration 314, Loss: 0.7547295689582825\n",
      "Epoch 8, Iteration 315, Loss: 0.6450908184051514\n",
      "Epoch 8, Iteration 316, Loss: 1.0493972301483154\n",
      "Epoch 8, Iteration 317, Loss: 0.8147729635238647\n",
      "Epoch 8, Iteration 318, Loss: 0.6884039640426636\n",
      "Epoch 8, Iteration 319, Loss: 0.8708907961845398\n",
      "Epoch 8, Iteration 320, Loss: 0.5926024913787842\n",
      "Epoch 8, Iteration 321, Loss: 0.8083166480064392\n",
      "Epoch 8, Iteration 322, Loss: 0.6785273551940918\n",
      "Epoch 8, Iteration 323, Loss: 0.7413535118103027\n",
      "Epoch 8, Iteration 324, Loss: 0.8233560919761658\n",
      "Epoch 8, Iteration 325, Loss: 0.815676748752594\n",
      "Epoch 8, Iteration 326, Loss: 0.6826938390731812\n",
      "Epoch 8, Iteration 327, Loss: 0.6460734605789185\n",
      "Epoch 8, Iteration 328, Loss: 0.6977885961532593\n",
      "Epoch 8, Iteration 329, Loss: 0.8502275347709656\n",
      "Epoch 8, Iteration 330, Loss: 0.6362216472625732\n",
      "Epoch 8, Iteration 331, Loss: 0.7267106771469116\n",
      "Epoch 8, Iteration 332, Loss: 0.7606513500213623\n",
      "Epoch 8, Iteration 333, Loss: 0.8923608660697937\n",
      "Epoch 8, Iteration 334, Loss: 0.6787383556365967\n",
      "Epoch 8, Iteration 335, Loss: 0.9468817114830017\n",
      "Epoch 8, Iteration 336, Loss: 0.7534368634223938\n",
      "Epoch 8, Iteration 337, Loss: 0.5926101207733154\n",
      "Epoch 8, Iteration 338, Loss: 0.8432535529136658\n",
      "Epoch 8, Iteration 339, Loss: 0.7032656669616699\n",
      "Epoch 8, Iteration 340, Loss: 0.5530920624732971\n",
      "Epoch 8, Iteration 341, Loss: 0.7305872440338135\n",
      "Epoch 8, Iteration 342, Loss: 0.6303037405014038\n",
      "Epoch 8, Iteration 343, Loss: 0.6865366101264954\n",
      "Epoch 8, Iteration 344, Loss: 0.7347243428230286\n",
      "Epoch 8, Iteration 345, Loss: 0.7859869599342346\n",
      "Epoch 8, Iteration 346, Loss: 0.6723437309265137\n",
      "Epoch 8, Iteration 347, Loss: 0.6415800452232361\n",
      "Epoch 8, Iteration 348, Loss: 0.5984153747558594\n",
      "Epoch 8, Iteration 349, Loss: 0.7107439637184143\n",
      "Epoch 8, Iteration 350, Loss: 0.6521517634391785\n",
      "Epoch 8, Iteration 350, Valid Loss: 0.6143916845321655\n",
      "Epoch 8, Iteration 351, Loss: 0.7411661744117737\n",
      "Epoch 8, Iteration 352, Loss: 0.6050631999969482\n",
      "Epoch 8, Iteration 353, Loss: 0.7594234943389893\n",
      "Epoch 8, Iteration 354, Loss: 0.6397687196731567\n",
      "Epoch 8, Iteration 355, Loss: 0.7249277234077454\n",
      "Epoch 8, Iteration 356, Loss: 0.7719408273696899\n",
      "Epoch 8, Iteration 357, Loss: 0.5798927545547485\n",
      "Epoch 8, Iteration 358, Loss: 0.6239722371101379\n",
      "Epoch 8, Iteration 359, Loss: 0.6329523324966431\n",
      "Epoch 8, Iteration 360, Loss: 0.6414828896522522\n",
      "Epoch 8, Iteration 361, Loss: 0.630365788936615\n",
      "Epoch 8, Iteration 362, Loss: 0.7138701677322388\n",
      "Epoch 8, Iteration 363, Loss: 0.6001951694488525\n",
      "Epoch 8, Iteration 364, Loss: 0.6211658716201782\n",
      "Epoch 8, Iteration 365, Loss: 0.7817566394805908\n",
      "Epoch 8, Iteration 366, Loss: 0.7351313829421997\n",
      "Epoch 8, Iteration 367, Loss: 0.8156201839447021\n",
      "Epoch 8, Iteration 368, Loss: 0.7189382910728455\n",
      "Epoch 8, Iteration 369, Loss: 0.7429583072662354\n",
      "Epoch 8, Iteration 370, Loss: 0.6975385546684265\n",
      "Epoch 8, Iteration 371, Loss: 0.6611320972442627\n",
      "Epoch 8, Iteration 372, Loss: 0.6636021733283997\n",
      "Epoch 8, Iteration 373, Loss: 0.6655458807945251\n",
      "Epoch 8, Iteration 374, Loss: 0.8084112405776978\n",
      "Epoch 8, Iteration 375, Loss: 0.7208682298660278\n",
      "Epoch 8, Iteration 376, Loss: 0.7731591463088989\n",
      "Epoch 8, Iteration 377, Loss: 0.7027878761291504\n",
      "Epoch 8, Iteration 378, Loss: 0.6478819251060486\n",
      "Epoch 8, Iteration 379, Loss: 0.6351879239082336\n",
      "Epoch 8, Iteration 380, Loss: 0.6914784908294678\n",
      "Epoch 8, Iteration 381, Loss: 0.6582031846046448\n",
      "Epoch 8, Iteration 382, Loss: 0.5600651502609253\n",
      "Epoch 8, Iteration 383, Loss: 0.7266520857810974\n",
      "Epoch 8, Iteration 384, Loss: 0.6990786790847778\n",
      "Epoch 8, Iteration 385, Loss: 0.6761002540588379\n",
      "Epoch 8, Iteration 386, Loss: 0.5896896719932556\n",
      "Epoch 8, Iteration 387, Loss: 0.9530796408653259\n",
      "Epoch 8, Iteration 388, Loss: 0.6738588809967041\n",
      "Epoch 8, Iteration 389, Loss: 0.7628483772277832\n",
      "Epoch 8, Iteration 390, Loss: 0.9321476817131042\n",
      "Epoch 8, Iteration 391, Loss: 0.6669455766677856\n",
      "Epoch 9/15, Loss: 0.779655286091931\n",
      "Epoch 9, Iteration 0, Loss: 0.7308241724967957\n",
      "Epoch 9, Iteration 1, Loss: 0.9465048313140869\n",
      "Epoch 9, Iteration 2, Loss: 0.8350211381912231\n",
      "Epoch 9, Iteration 3, Loss: 0.8826941251754761\n",
      "Epoch 9, Iteration 4, Loss: 0.6940438747406006\n",
      "Epoch 9, Iteration 5, Loss: 0.7862438559532166\n",
      "Epoch 9, Iteration 6, Loss: 0.6901811957359314\n",
      "Epoch 9, Iteration 7, Loss: 0.7413474917411804\n",
      "Epoch 9, Iteration 8, Loss: 0.6922501921653748\n",
      "Epoch 9, Iteration 9, Loss: 0.8858712911605835\n",
      "Epoch 9, Iteration 10, Loss: 0.7279002070426941\n",
      "Epoch 9, Iteration 11, Loss: 0.7368876338005066\n",
      "Epoch 9, Iteration 12, Loss: 0.6380183696746826\n",
      "Epoch 9, Iteration 13, Loss: 0.8103405237197876\n",
      "Epoch 9, Iteration 14, Loss: 0.7480902671813965\n",
      "Epoch 9, Iteration 15, Loss: 0.8882638216018677\n",
      "Epoch 9, Iteration 16, Loss: 0.7155240774154663\n",
      "Epoch 9, Iteration 17, Loss: 0.7721894383430481\n",
      "Epoch 9, Iteration 18, Loss: 0.7131714224815369\n",
      "Epoch 9, Iteration 19, Loss: 0.8018889427185059\n",
      "Epoch 9, Iteration 20, Loss: 0.630958080291748\n",
      "Epoch 9, Iteration 21, Loss: 0.7048038244247437\n",
      "Epoch 9, Iteration 22, Loss: 0.6879834532737732\n",
      "Epoch 9, Iteration 23, Loss: 0.7734124660491943\n",
      "Epoch 9, Iteration 24, Loss: 0.6751303672790527\n",
      "Epoch 9, Iteration 25, Loss: 0.6605287790298462\n",
      "Epoch 9, Iteration 26, Loss: 1.0718750953674316\n",
      "Epoch 9, Iteration 27, Loss: 0.8207003474235535\n",
      "Epoch 9, Iteration 28, Loss: 0.8042131066322327\n",
      "Epoch 9, Iteration 29, Loss: 0.928263247013092\n",
      "Epoch 9, Iteration 30, Loss: 0.7363741397857666\n",
      "Epoch 9, Iteration 31, Loss: 0.7310901284217834\n",
      "Epoch 9, Iteration 32, Loss: 0.7527525424957275\n",
      "Epoch 9, Iteration 33, Loss: 0.7155691385269165\n",
      "Epoch 9, Iteration 34, Loss: 0.6964547634124756\n",
      "Epoch 9, Iteration 35, Loss: 0.7616868019104004\n",
      "Epoch 9, Iteration 36, Loss: 0.6030333638191223\n",
      "Epoch 9, Iteration 37, Loss: 0.6083875894546509\n",
      "Epoch 9, Iteration 38, Loss: 0.6841759085655212\n",
      "Epoch 9, Iteration 39, Loss: 0.7393600344657898\n",
      "Epoch 9, Iteration 40, Loss: 0.7011383771896362\n",
      "Epoch 9, Iteration 41, Loss: 0.8121935725212097\n",
      "Epoch 9, Iteration 42, Loss: 0.7478061318397522\n",
      "Epoch 9, Iteration 43, Loss: 0.6560901999473572\n",
      "Epoch 9, Iteration 44, Loss: 0.6721383333206177\n",
      "Epoch 9, Iteration 45, Loss: 0.7498594522476196\n",
      "Epoch 9, Iteration 46, Loss: 0.7984776496887207\n",
      "Epoch 9, Iteration 47, Loss: 0.7617945075035095\n",
      "Epoch 9, Iteration 48, Loss: 0.9457508325576782\n",
      "Epoch 9, Iteration 49, Loss: 0.7176660895347595\n",
      "Epoch 9, Iteration 50, Loss: 0.739052414894104\n",
      "Epoch 9, Iteration 50, Valid Loss: 0.5829022526741028\n",
      "Epoch 9, Iteration 51, Loss: 0.7087310552597046\n",
      "Epoch 9, Iteration 52, Loss: 0.6508805155754089\n",
      "Epoch 9, Iteration 53, Loss: 0.7918763160705566\n",
      "Epoch 9, Iteration 54, Loss: 0.9624983072280884\n",
      "Epoch 9, Iteration 55, Loss: 0.7900527715682983\n",
      "Epoch 9, Iteration 56, Loss: 0.7544360160827637\n",
      "Epoch 9, Iteration 57, Loss: 0.9780333042144775\n",
      "Epoch 9, Iteration 58, Loss: 0.7307156920433044\n",
      "Epoch 9, Iteration 59, Loss: 0.6337934732437134\n",
      "Epoch 9, Iteration 60, Loss: 0.7773463129997253\n",
      "Epoch 9, Iteration 61, Loss: 0.6249812245368958\n",
      "Epoch 9, Iteration 62, Loss: 0.7629194855690002\n",
      "Epoch 9, Iteration 63, Loss: 0.6283774971961975\n",
      "Epoch 9, Iteration 64, Loss: 0.712641179561615\n",
      "Epoch 9, Iteration 65, Loss: 0.8886286020278931\n",
      "Epoch 9, Iteration 66, Loss: 0.7949448227882385\n",
      "Epoch 9, Iteration 67, Loss: 0.5640498995780945\n",
      "Epoch 9, Iteration 68, Loss: 0.9276461005210876\n",
      "Epoch 9, Iteration 69, Loss: 0.9058628082275391\n",
      "Epoch 9, Iteration 70, Loss: 0.7991460561752319\n",
      "Epoch 9, Iteration 71, Loss: 0.7800227403640747\n",
      "Epoch 9, Iteration 72, Loss: 0.7259728908538818\n",
      "Epoch 9, Iteration 73, Loss: 0.667104959487915\n",
      "Epoch 9, Iteration 74, Loss: 0.9484119415283203\n",
      "Epoch 9, Iteration 75, Loss: 0.9026415348052979\n",
      "Epoch 9, Iteration 76, Loss: 0.7131703495979309\n",
      "Epoch 9, Iteration 77, Loss: 0.6673716902732849\n",
      "Epoch 9, Iteration 78, Loss: 0.8565850257873535\n",
      "Epoch 9, Iteration 79, Loss: 0.9942042231559753\n",
      "Epoch 9, Iteration 80, Loss: 0.6868330240249634\n",
      "Epoch 9, Iteration 81, Loss: 0.9082403182983398\n",
      "Epoch 9, Iteration 82, Loss: 0.9861974716186523\n",
      "Epoch 9, Iteration 83, Loss: 0.6732343435287476\n",
      "Epoch 9, Iteration 84, Loss: 0.9294126629829407\n",
      "Epoch 9, Iteration 85, Loss: 0.7786957621574402\n",
      "Epoch 9, Iteration 86, Loss: 0.8111003041267395\n",
      "Epoch 9, Iteration 87, Loss: 0.901748776435852\n",
      "Epoch 9, Iteration 88, Loss: 0.9353042840957642\n",
      "Epoch 9, Iteration 89, Loss: 0.7882871031761169\n",
      "Epoch 9, Iteration 90, Loss: 0.6331316232681274\n",
      "Epoch 9, Iteration 91, Loss: 0.7667720913887024\n",
      "Epoch 9, Iteration 92, Loss: 0.5190502405166626\n",
      "Epoch 9, Iteration 93, Loss: 0.8116421699523926\n",
      "Epoch 9, Iteration 94, Loss: 0.8437437415122986\n",
      "Epoch 9, Iteration 95, Loss: 0.5981365442276001\n",
      "Epoch 9, Iteration 96, Loss: 0.7559809684753418\n",
      "Epoch 9, Iteration 97, Loss: 0.6261113286018372\n",
      "Epoch 9, Iteration 98, Loss: 0.7746842503547668\n",
      "Epoch 9, Iteration 99, Loss: 0.7415294051170349\n",
      "Epoch 9, Iteration 100, Loss: 0.7066133618354797\n",
      "Epoch 9, Iteration 100, Valid Loss: 0.6670176982879639\n",
      "Epoch 9, Iteration 101, Loss: 0.9464544057846069\n",
      "Epoch 9, Iteration 102, Loss: 0.9147846698760986\n",
      "Epoch 9, Iteration 103, Loss: 0.7796867489814758\n",
      "Epoch 9, Iteration 104, Loss: 0.8210607171058655\n",
      "Epoch 9, Iteration 105, Loss: 0.8024045825004578\n",
      "Epoch 9, Iteration 106, Loss: 0.5901901125907898\n",
      "Epoch 9, Iteration 107, Loss: 1.0022063255310059\n",
      "Epoch 9, Iteration 108, Loss: 0.7651452422142029\n",
      "Epoch 9, Iteration 109, Loss: 0.9115220904350281\n",
      "Epoch 9, Iteration 110, Loss: 1.0262733697891235\n",
      "Epoch 9, Iteration 111, Loss: 0.8397724032402039\n",
      "Epoch 9, Iteration 112, Loss: 0.7979373335838318\n",
      "Epoch 9, Iteration 113, Loss: 0.7071547508239746\n",
      "Epoch 9, Iteration 114, Loss: 0.888207733631134\n",
      "Epoch 9, Iteration 115, Loss: 0.9418433904647827\n",
      "Epoch 9, Iteration 116, Loss: 0.699211597442627\n",
      "Epoch 9, Iteration 117, Loss: 0.5538270473480225\n",
      "Epoch 9, Iteration 118, Loss: 0.8070988655090332\n",
      "Epoch 9, Iteration 119, Loss: 0.6610010862350464\n",
      "Epoch 9, Iteration 120, Loss: 0.8447210788726807\n",
      "Epoch 9, Iteration 121, Loss: 0.7047539353370667\n",
      "Epoch 9, Iteration 122, Loss: 0.7228104472160339\n",
      "Epoch 9, Iteration 123, Loss: 0.7019258141517639\n",
      "Epoch 9, Iteration 124, Loss: 0.6570252180099487\n",
      "Epoch 9, Iteration 125, Loss: 0.7151142954826355\n",
      "Epoch 9, Iteration 126, Loss: 0.5851654410362244\n",
      "Epoch 9, Iteration 127, Loss: 0.8285530805587769\n",
      "Epoch 9, Iteration 128, Loss: 0.7521159648895264\n",
      "Epoch 9, Iteration 129, Loss: 0.8374610543251038\n",
      "Epoch 9, Iteration 130, Loss: 1.025610089302063\n",
      "Epoch 9, Iteration 131, Loss: 0.6196690201759338\n",
      "Epoch 9, Iteration 132, Loss: 0.7580296993255615\n",
      "Epoch 9, Iteration 133, Loss: 1.1228702068328857\n",
      "Epoch 9, Iteration 134, Loss: 0.8963144421577454\n",
      "Epoch 9, Iteration 135, Loss: 0.9527294635772705\n",
      "Epoch 9, Iteration 136, Loss: 0.8718239068984985\n",
      "Epoch 9, Iteration 137, Loss: 0.7491002082824707\n",
      "Epoch 9, Iteration 138, Loss: 0.6815611720085144\n",
      "Epoch 9, Iteration 139, Loss: 0.7123928070068359\n",
      "Epoch 9, Iteration 140, Loss: 0.7916346788406372\n",
      "Epoch 9, Iteration 141, Loss: 0.9130162596702576\n",
      "Epoch 9, Iteration 142, Loss: 0.8228277564048767\n",
      "Epoch 9, Iteration 143, Loss: 0.658242404460907\n",
      "Epoch 9, Iteration 144, Loss: 0.7395537495613098\n",
      "Epoch 9, Iteration 145, Loss: 0.8068856000900269\n",
      "Epoch 9, Iteration 146, Loss: 1.0959910154342651\n",
      "Epoch 9, Iteration 147, Loss: 0.8362852334976196\n",
      "Epoch 9, Iteration 148, Loss: 0.63627028465271\n",
      "Epoch 9, Iteration 149, Loss: 0.7026801705360413\n",
      "Epoch 9, Iteration 150, Loss: 0.8587268590927124\n",
      "Epoch 9, Iteration 150, Valid Loss: 0.6298393607139587\n",
      "Epoch 9, Iteration 151, Loss: 0.7882240414619446\n",
      "Epoch 9, Iteration 152, Loss: 0.6431488990783691\n",
      "Epoch 9, Iteration 153, Loss: 0.7746491432189941\n",
      "Epoch 9, Iteration 154, Loss: 0.5729898810386658\n",
      "Epoch 9, Iteration 155, Loss: 0.9210677742958069\n",
      "Epoch 9, Iteration 156, Loss: 0.6389308571815491\n",
      "Epoch 9, Iteration 157, Loss: 1.0677552223205566\n",
      "Epoch 9, Iteration 158, Loss: 0.7799802422523499\n",
      "Epoch 9, Iteration 159, Loss: 0.8569083213806152\n",
      "Epoch 9, Iteration 160, Loss: 0.7181677222251892\n",
      "Epoch 9, Iteration 161, Loss: 0.7701174020767212\n",
      "Epoch 9, Iteration 162, Loss: 1.0319719314575195\n",
      "Epoch 9, Iteration 163, Loss: 0.588152289390564\n",
      "Epoch 9, Iteration 164, Loss: 0.7557175755500793\n",
      "Epoch 9, Iteration 165, Loss: 0.6992985010147095\n",
      "Epoch 9, Iteration 166, Loss: 0.7738352417945862\n",
      "Epoch 9, Iteration 167, Loss: 0.8576986789703369\n",
      "Epoch 9, Iteration 168, Loss: 0.7841176986694336\n",
      "Epoch 9, Iteration 169, Loss: 0.7145552635192871\n",
      "Epoch 9, Iteration 170, Loss: 0.8393662571907043\n",
      "Epoch 9, Iteration 171, Loss: 0.8896420001983643\n",
      "Epoch 9, Iteration 172, Loss: 0.7893933057785034\n",
      "Epoch 9, Iteration 173, Loss: 0.6683463454246521\n",
      "Epoch 9, Iteration 174, Loss: 0.8080376982688904\n",
      "Epoch 9, Iteration 175, Loss: 0.9036136865615845\n",
      "Epoch 9, Iteration 176, Loss: 0.5323201417922974\n",
      "Epoch 9, Iteration 177, Loss: 1.2218198776245117\n",
      "Epoch 9, Iteration 178, Loss: 0.8702268004417419\n",
      "Epoch 9, Iteration 179, Loss: 0.7944566011428833\n",
      "Epoch 9, Iteration 180, Loss: 0.7439631223678589\n",
      "Epoch 9, Iteration 181, Loss: 0.9097834229469299\n",
      "Epoch 9, Iteration 182, Loss: 0.8733106851577759\n",
      "Epoch 9, Iteration 183, Loss: 0.6372396349906921\n",
      "Epoch 9, Iteration 184, Loss: 0.7922108173370361\n",
      "Epoch 9, Iteration 185, Loss: 0.6060683727264404\n",
      "Epoch 9, Iteration 186, Loss: 0.734189510345459\n",
      "Epoch 9, Iteration 187, Loss: 0.7575653791427612\n",
      "Epoch 9, Iteration 188, Loss: 0.6478419899940491\n",
      "Epoch 9, Iteration 189, Loss: 0.7230785489082336\n",
      "Epoch 9, Iteration 190, Loss: 0.8440952301025391\n",
      "Epoch 9, Iteration 191, Loss: 0.7497754693031311\n",
      "Epoch 9, Iteration 192, Loss: 0.8223125338554382\n",
      "Epoch 9, Iteration 193, Loss: 0.7719319462776184\n",
      "Epoch 9, Iteration 194, Loss: 0.7919462323188782\n",
      "Epoch 9, Iteration 195, Loss: 0.689611554145813\n",
      "Epoch 9, Iteration 196, Loss: 0.7912310361862183\n",
      "Epoch 9, Iteration 197, Loss: 0.7527295351028442\n",
      "Epoch 9, Iteration 198, Loss: 0.7306608557701111\n",
      "Epoch 9, Iteration 199, Loss: 0.7667921185493469\n",
      "Epoch 9, Iteration 200, Loss: 0.6846836805343628\n",
      "Epoch 9, Iteration 200, Valid Loss: 0.5953073501586914\n",
      "Epoch 9, Iteration 201, Loss: 0.7450470328330994\n",
      "Epoch 9, Iteration 202, Loss: 0.7820550799369812\n",
      "Epoch 9, Iteration 203, Loss: 0.764255940914154\n",
      "Epoch 9, Iteration 204, Loss: 0.7193447947502136\n",
      "Epoch 9, Iteration 205, Loss: 0.5334543585777283\n",
      "Epoch 9, Iteration 206, Loss: 0.6467786431312561\n",
      "Epoch 9, Iteration 207, Loss: 0.8735195398330688\n",
      "Epoch 9, Iteration 208, Loss: 1.1428240537643433\n",
      "Epoch 9, Iteration 209, Loss: 1.049645185470581\n",
      "Epoch 9, Iteration 210, Loss: 0.999504566192627\n",
      "Epoch 9, Iteration 211, Loss: 1.0677597522735596\n",
      "Epoch 9, Iteration 212, Loss: 0.8813190460205078\n",
      "Epoch 9, Iteration 213, Loss: 0.9970723390579224\n",
      "Epoch 9, Iteration 214, Loss: 0.7564807534217834\n",
      "Epoch 9, Iteration 215, Loss: 0.6702298521995544\n",
      "Epoch 9, Iteration 216, Loss: 0.7057702541351318\n",
      "Epoch 9, Iteration 217, Loss: 0.6815439462661743\n",
      "Epoch 9, Iteration 218, Loss: 0.7581273913383484\n",
      "Epoch 9, Iteration 219, Loss: 0.7732579112052917\n",
      "Epoch 9, Iteration 220, Loss: 0.6858800053596497\n",
      "Epoch 9, Iteration 221, Loss: 0.7110912799835205\n",
      "Epoch 9, Iteration 222, Loss: 0.8320658206939697\n",
      "Epoch 9, Iteration 223, Loss: 0.6944090127944946\n",
      "Epoch 9, Iteration 224, Loss: 0.6148818731307983\n",
      "Epoch 9, Iteration 225, Loss: 0.8730373382568359\n",
      "Epoch 9, Iteration 226, Loss: 0.8541958928108215\n",
      "Epoch 9, Iteration 227, Loss: 0.7688654065132141\n",
      "Epoch 9, Iteration 228, Loss: 0.7504370212554932\n",
      "Epoch 9, Iteration 229, Loss: 0.7154635787010193\n",
      "Epoch 9, Iteration 230, Loss: 0.8089712262153625\n",
      "Epoch 9, Iteration 231, Loss: 0.6641757488250732\n",
      "Epoch 9, Iteration 232, Loss: 0.8076749444007874\n",
      "Epoch 9, Iteration 233, Loss: 0.9533230066299438\n",
      "Epoch 9, Iteration 234, Loss: 0.9400076270103455\n",
      "Epoch 9, Iteration 235, Loss: 0.6595286726951599\n",
      "Epoch 9, Iteration 236, Loss: 0.7435531616210938\n",
      "Epoch 9, Iteration 237, Loss: 0.6790565252304077\n",
      "Epoch 9, Iteration 238, Loss: 0.5297216773033142\n",
      "Epoch 9, Iteration 239, Loss: 0.9798758625984192\n",
      "Epoch 9, Iteration 240, Loss: 0.6965345144271851\n",
      "Epoch 9, Iteration 241, Loss: 0.6893171668052673\n",
      "Epoch 9, Iteration 242, Loss: 0.7391933798789978\n",
      "Epoch 9, Iteration 243, Loss: 0.7433725595474243\n",
      "Epoch 9, Iteration 244, Loss: 0.6921083331108093\n",
      "Epoch 9, Iteration 245, Loss: 0.5473451614379883\n",
      "Epoch 9, Iteration 246, Loss: 0.8021020889282227\n",
      "Epoch 9, Iteration 247, Loss: 0.708761990070343\n",
      "Epoch 9, Iteration 248, Loss: 0.7335415482521057\n",
      "Epoch 9, Iteration 249, Loss: 0.7621409893035889\n",
      "Epoch 9, Iteration 250, Loss: 0.8938029408454895\n",
      "Epoch 9, Iteration 250, Valid Loss: 0.5728676915168762\n",
      "Epoch 9, Iteration 251, Loss: 0.7511805295944214\n",
      "Epoch 9, Iteration 252, Loss: 0.9164286851882935\n",
      "Epoch 9, Iteration 253, Loss: 0.7527827024459839\n",
      "Epoch 9, Iteration 254, Loss: 0.8836206197738647\n",
      "Epoch 9, Iteration 255, Loss: 0.8197833299636841\n",
      "Epoch 9, Iteration 256, Loss: 0.9068291783332825\n",
      "Epoch 9, Iteration 257, Loss: 0.8051047325134277\n",
      "Epoch 9, Iteration 258, Loss: 0.9544641971588135\n",
      "Epoch 9, Iteration 259, Loss: 0.6037098169326782\n",
      "Epoch 9, Iteration 260, Loss: 0.8705965280532837\n",
      "Epoch 9, Iteration 261, Loss: 0.7494175434112549\n",
      "Epoch 9, Iteration 262, Loss: 0.6916728019714355\n",
      "Epoch 9, Iteration 263, Loss: 0.9497328400611877\n",
      "Epoch 9, Iteration 264, Loss: 0.7760531902313232\n",
      "Epoch 9, Iteration 265, Loss: 0.8184323906898499\n",
      "Epoch 9, Iteration 266, Loss: 0.8153864741325378\n",
      "Epoch 9, Iteration 267, Loss: 0.7028369903564453\n",
      "Epoch 9, Iteration 268, Loss: 0.6515352129936218\n",
      "Epoch 9, Iteration 269, Loss: 0.6851238012313843\n",
      "Epoch 9, Iteration 270, Loss: 0.6824880242347717\n",
      "Epoch 9, Iteration 271, Loss: 0.7732277512550354\n",
      "Epoch 9, Iteration 272, Loss: 0.789249837398529\n",
      "Epoch 9, Iteration 273, Loss: 0.7601547837257385\n",
      "Epoch 9, Iteration 274, Loss: 0.7538531422615051\n",
      "Epoch 9, Iteration 275, Loss: 0.6882658004760742\n",
      "Epoch 9, Iteration 276, Loss: 0.6120421886444092\n",
      "Epoch 9, Iteration 277, Loss: 0.6577579975128174\n",
      "Epoch 9, Iteration 278, Loss: 0.9504434466362\n",
      "Epoch 9, Iteration 279, Loss: 0.6039125323295593\n",
      "Epoch 9, Iteration 280, Loss: 0.5849049091339111\n",
      "Epoch 9, Iteration 281, Loss: 0.7317599654197693\n",
      "Epoch 9, Iteration 282, Loss: 0.6749445199966431\n",
      "Epoch 9, Iteration 283, Loss: 0.7124390006065369\n",
      "Epoch 9, Iteration 284, Loss: 0.7541038990020752\n",
      "Epoch 9, Iteration 285, Loss: 0.636192798614502\n",
      "Epoch 9, Iteration 286, Loss: 0.6868395209312439\n",
      "Epoch 9, Iteration 287, Loss: 0.6887539625167847\n",
      "Epoch 9, Iteration 288, Loss: 0.6970751881599426\n",
      "Epoch 9, Iteration 289, Loss: 0.6849132776260376\n",
      "Epoch 9, Iteration 290, Loss: 0.5872591733932495\n",
      "Epoch 9, Iteration 291, Loss: 0.5689773559570312\n",
      "Epoch 9, Iteration 292, Loss: 0.8312098383903503\n",
      "Epoch 9, Iteration 293, Loss: 0.9277780055999756\n",
      "Epoch 9, Iteration 294, Loss: 0.6969774961471558\n",
      "Epoch 9, Iteration 295, Loss: 0.6408982276916504\n",
      "Epoch 9, Iteration 296, Loss: 0.9372750520706177\n",
      "Epoch 9, Iteration 297, Loss: 0.8353238105773926\n",
      "Epoch 9, Iteration 298, Loss: 0.7484338879585266\n",
      "Epoch 9, Iteration 299, Loss: 0.7396026849746704\n",
      "Epoch 9, Iteration 300, Loss: 0.5660464763641357\n",
      "Epoch 9, Iteration 300, Valid Loss: 0.5725482106208801\n",
      "Epoch 9, Iteration 301, Loss: 0.6376891136169434\n",
      "Epoch 9, Iteration 302, Loss: 0.6009703874588013\n",
      "Epoch 9, Iteration 303, Loss: 0.83037269115448\n",
      "Epoch 9, Iteration 304, Loss: 0.6454769968986511\n",
      "Epoch 9, Iteration 305, Loss: 0.6138301491737366\n",
      "Epoch 9, Iteration 306, Loss: 0.5859816074371338\n",
      "Epoch 9, Iteration 307, Loss: 0.654991865158081\n",
      "Epoch 9, Iteration 308, Loss: 0.6098645329475403\n",
      "Epoch 9, Iteration 309, Loss: 0.7446835041046143\n",
      "Epoch 9, Iteration 310, Loss: 0.8695237040519714\n",
      "Epoch 9, Iteration 311, Loss: 0.7682286500930786\n",
      "Epoch 9, Iteration 312, Loss: 0.6362056732177734\n",
      "Epoch 9, Iteration 313, Loss: 0.9041121602058411\n",
      "Epoch 9, Iteration 314, Loss: 0.6587628722190857\n",
      "Epoch 9, Iteration 315, Loss: 0.6505283713340759\n",
      "Epoch 9, Iteration 316, Loss: 0.974670946598053\n",
      "Epoch 9, Iteration 317, Loss: 0.6791342496871948\n",
      "Epoch 9, Iteration 318, Loss: 0.6791183948516846\n",
      "Epoch 9, Iteration 319, Loss: 0.8520302176475525\n",
      "Epoch 9, Iteration 320, Loss: 0.5372095108032227\n",
      "Epoch 9, Iteration 321, Loss: 0.7100684642791748\n",
      "Epoch 9, Iteration 322, Loss: 0.6891412138938904\n",
      "Epoch 9, Iteration 323, Loss: 0.7517508268356323\n",
      "Epoch 9, Iteration 324, Loss: 0.6871730089187622\n",
      "Epoch 9, Iteration 325, Loss: 0.7374390959739685\n",
      "Epoch 9, Iteration 326, Loss: 0.7335934042930603\n",
      "Epoch 9, Iteration 327, Loss: 0.5759890079498291\n",
      "Epoch 9, Iteration 328, Loss: 0.5664149522781372\n",
      "Epoch 9, Iteration 329, Loss: 0.8054542541503906\n",
      "Epoch 9, Iteration 330, Loss: 0.5474905967712402\n",
      "Epoch 9, Iteration 331, Loss: 0.6857388019561768\n",
      "Epoch 9, Iteration 332, Loss: 0.7410913109779358\n",
      "Epoch 9, Iteration 333, Loss: 0.8653730154037476\n",
      "Epoch 9, Iteration 334, Loss: 0.691712498664856\n",
      "Epoch 9, Iteration 335, Loss: 0.8222153186798096\n",
      "Epoch 9, Iteration 336, Loss: 0.6993371844291687\n",
      "Epoch 9, Iteration 337, Loss: 0.5066707730293274\n",
      "Epoch 9, Iteration 338, Loss: 0.7924480438232422\n",
      "Epoch 9, Iteration 339, Loss: 0.6859145760536194\n",
      "Epoch 9, Iteration 340, Loss: 0.6151975393295288\n",
      "Epoch 9, Iteration 341, Loss: 0.6986725926399231\n",
      "Epoch 9, Iteration 342, Loss: 0.6431083083152771\n",
      "Epoch 9, Iteration 343, Loss: 0.6375458240509033\n",
      "Epoch 9, Iteration 344, Loss: 0.782133162021637\n",
      "Epoch 9, Iteration 345, Loss: 0.7242532968521118\n",
      "Epoch 9, Iteration 346, Loss: 0.6952593922615051\n",
      "Epoch 9, Iteration 347, Loss: 0.6812937259674072\n",
      "Epoch 9, Iteration 348, Loss: 0.6292823553085327\n",
      "Epoch 9, Iteration 349, Loss: 0.812605082988739\n",
      "Epoch 9, Iteration 350, Loss: 0.6835975646972656\n",
      "Epoch 9, Iteration 350, Valid Loss: 0.6027361750602722\n",
      "Epoch 9, Iteration 351, Loss: 0.727184534072876\n",
      "Epoch 9, Iteration 352, Loss: 0.6032825708389282\n",
      "Epoch 9, Iteration 353, Loss: 0.7752993702888489\n",
      "Epoch 9, Iteration 354, Loss: 0.6014406681060791\n",
      "Epoch 9, Iteration 355, Loss: 0.5959606766700745\n",
      "Epoch 9, Iteration 356, Loss: 0.704997181892395\n",
      "Epoch 9, Iteration 357, Loss: 0.6224934458732605\n",
      "Epoch 9, Iteration 358, Loss: 0.6084299087524414\n",
      "Epoch 9, Iteration 359, Loss: 0.6161983013153076\n",
      "Epoch 9, Iteration 360, Loss: 0.6085823774337769\n",
      "Epoch 9, Iteration 361, Loss: 0.7766433954238892\n",
      "Epoch 9, Iteration 362, Loss: 0.7282424569129944\n",
      "Epoch 9, Iteration 363, Loss: 0.6602478623390198\n",
      "Epoch 9, Iteration 364, Loss: 0.7501680850982666\n",
      "Epoch 9, Iteration 365, Loss: 0.818701982498169\n",
      "Epoch 9, Iteration 366, Loss: 0.7815721035003662\n",
      "Epoch 9, Iteration 367, Loss: 0.6480079889297485\n",
      "Epoch 9, Iteration 368, Loss: 0.6935354471206665\n",
      "Epoch 9, Iteration 369, Loss: 0.7383248209953308\n",
      "Epoch 9, Iteration 370, Loss: 0.6945074796676636\n",
      "Epoch 9, Iteration 371, Loss: 0.7064132690429688\n",
      "Epoch 9, Iteration 372, Loss: 0.6064289808273315\n",
      "Epoch 9, Iteration 373, Loss: 0.7421363592147827\n",
      "Epoch 9, Iteration 374, Loss: 0.8901442885398865\n",
      "Epoch 9, Iteration 375, Loss: 0.614199161529541\n",
      "Epoch 9, Iteration 376, Loss: 0.7550197243690491\n",
      "Epoch 9, Iteration 377, Loss: 0.6971667408943176\n",
      "Epoch 9, Iteration 378, Loss: 0.7137177586555481\n",
      "Epoch 9, Iteration 379, Loss: 0.6186983585357666\n",
      "Epoch 9, Iteration 380, Loss: 0.617209255695343\n",
      "Epoch 9, Iteration 381, Loss: 0.622280478477478\n",
      "Epoch 9, Iteration 382, Loss: 0.5838761329650879\n",
      "Epoch 9, Iteration 383, Loss: 0.689920961856842\n",
      "Epoch 9, Iteration 384, Loss: 0.7811902761459351\n",
      "Epoch 9, Iteration 385, Loss: 0.6319943070411682\n",
      "Epoch 9, Iteration 386, Loss: 0.554760754108429\n",
      "Epoch 9, Iteration 387, Loss: 0.8771708011627197\n",
      "Epoch 9, Iteration 388, Loss: 0.7034414410591125\n",
      "Epoch 9, Iteration 389, Loss: 0.6532014012336731\n",
      "Epoch 9, Iteration 390, Loss: 0.8668028712272644\n",
      "Epoch 9, Iteration 391, Loss: 0.6327071189880371\n",
      "Epoch 10/15, Loss: 0.7544707242323427\n",
      "Epoch 10, Iteration 0, Loss: 0.6941603422164917\n",
      "Epoch 10, Iteration 1, Loss: 0.9121127724647522\n",
      "Epoch 10, Iteration 2, Loss: 0.801249623298645\n",
      "Epoch 10, Iteration 3, Loss: 0.8549344539642334\n",
      "Epoch 10, Iteration 4, Loss: 0.7360625267028809\n",
      "Epoch 10, Iteration 5, Loss: 0.8426286578178406\n",
      "Epoch 10, Iteration 6, Loss: 0.6116130948066711\n",
      "Epoch 10, Iteration 7, Loss: 0.7815054059028625\n",
      "Epoch 10, Iteration 8, Loss: 0.6729539632797241\n",
      "Epoch 10, Iteration 9, Loss: 0.795387327671051\n",
      "Epoch 10, Iteration 10, Loss: 0.773945152759552\n",
      "Epoch 10, Iteration 11, Loss: 0.7456849813461304\n",
      "Epoch 10, Iteration 12, Loss: 0.6912989616394043\n",
      "Epoch 10, Iteration 13, Loss: 0.8152952194213867\n",
      "Epoch 10, Iteration 14, Loss: 0.746321976184845\n",
      "Epoch 10, Iteration 15, Loss: 0.8382605314254761\n",
      "Epoch 10, Iteration 16, Loss: 0.7041322588920593\n",
      "Epoch 10, Iteration 17, Loss: 0.7595746517181396\n",
      "Epoch 10, Iteration 18, Loss: 0.6280320882797241\n",
      "Epoch 10, Iteration 19, Loss: 0.7818087935447693\n",
      "Epoch 10, Iteration 20, Loss: 0.6089149713516235\n",
      "Epoch 10, Iteration 21, Loss: 0.6628143787384033\n",
      "Epoch 10, Iteration 22, Loss: 0.6843055486679077\n",
      "Epoch 10, Iteration 23, Loss: 0.7910325527191162\n",
      "Epoch 10, Iteration 24, Loss: 0.6703596711158752\n",
      "Epoch 10, Iteration 25, Loss: 0.6111669540405273\n",
      "Epoch 10, Iteration 26, Loss: 1.0091544389724731\n",
      "Epoch 10, Iteration 27, Loss: 0.8511229157447815\n",
      "Epoch 10, Iteration 28, Loss: 0.8185375332832336\n",
      "Epoch 10, Iteration 29, Loss: 0.9332282543182373\n",
      "Epoch 10, Iteration 30, Loss: 0.7859931588172913\n",
      "Epoch 10, Iteration 31, Loss: 0.7047330737113953\n",
      "Epoch 10, Iteration 32, Loss: 0.7782139778137207\n",
      "Epoch 10, Iteration 33, Loss: 0.7272104620933533\n",
      "Epoch 10, Iteration 34, Loss: 0.7002266049385071\n",
      "Epoch 10, Iteration 35, Loss: 0.7363100647926331\n",
      "Epoch 10, Iteration 36, Loss: 0.5131064653396606\n",
      "Epoch 10, Iteration 37, Loss: 0.5824116468429565\n",
      "Epoch 10, Iteration 38, Loss: 0.6742957830429077\n",
      "Epoch 10, Iteration 39, Loss: 0.6837071180343628\n",
      "Epoch 10, Iteration 40, Loss: 0.6474604606628418\n",
      "Epoch 10, Iteration 41, Loss: 0.8162046670913696\n",
      "Epoch 10, Iteration 42, Loss: 0.7184785604476929\n",
      "Epoch 10, Iteration 43, Loss: 0.6038572192192078\n",
      "Epoch 10, Iteration 44, Loss: 0.6682573556900024\n",
      "Epoch 10, Iteration 45, Loss: 0.738457977771759\n",
      "Epoch 10, Iteration 46, Loss: 0.7508317828178406\n",
      "Epoch 10, Iteration 47, Loss: 0.7143579721450806\n",
      "Epoch 10, Iteration 48, Loss: 0.9684680104255676\n",
      "Epoch 10, Iteration 49, Loss: 0.7504740357398987\n",
      "Epoch 10, Iteration 50, Loss: 0.6774473786354065\n",
      "Epoch 10, Iteration 50, Valid Loss: 0.5646939277648926\n",
      "Epoch 10, Iteration 51, Loss: 0.7655192017555237\n",
      "Epoch 10, Iteration 52, Loss: 0.6305049061775208\n",
      "Epoch 10, Iteration 53, Loss: 0.7779906988143921\n",
      "Epoch 10, Iteration 54, Loss: 0.9764517545700073\n",
      "Epoch 10, Iteration 55, Loss: 0.841484785079956\n",
      "Epoch 10, Iteration 56, Loss: 0.7210801839828491\n",
      "Epoch 10, Iteration 57, Loss: 0.8888958692550659\n",
      "Epoch 10, Iteration 58, Loss: 0.6804090142250061\n",
      "Epoch 10, Iteration 59, Loss: 0.919581413269043\n",
      "Epoch 10, Iteration 60, Loss: 0.6897583603858948\n",
      "Epoch 10, Iteration 61, Loss: 0.6125369071960449\n",
      "Epoch 10, Iteration 62, Loss: 0.7075123190879822\n",
      "Epoch 10, Iteration 63, Loss: 0.7648829221725464\n",
      "Epoch 10, Iteration 64, Loss: 0.7166243195533752\n",
      "Epoch 10, Iteration 65, Loss: 1.0398520231246948\n",
      "Epoch 10, Iteration 66, Loss: 0.7432412505149841\n",
      "Epoch 10, Iteration 67, Loss: 0.5931038856506348\n",
      "Epoch 10, Iteration 68, Loss: 0.8035410046577454\n",
      "Epoch 10, Iteration 69, Loss: 0.8078197240829468\n",
      "Epoch 10, Iteration 70, Loss: 0.7530755996704102\n",
      "Epoch 10, Iteration 71, Loss: 0.8132965564727783\n",
      "Epoch 10, Iteration 72, Loss: 0.8127638697624207\n",
      "Epoch 10, Iteration 73, Loss: 0.7058660984039307\n",
      "Epoch 10, Iteration 74, Loss: 0.9644125699996948\n",
      "Epoch 10, Iteration 75, Loss: 1.046295166015625\n",
      "Epoch 10, Iteration 76, Loss: 0.6951137185096741\n",
      "Epoch 10, Iteration 77, Loss: 0.613939642906189\n",
      "Epoch 10, Iteration 78, Loss: 0.9001856446266174\n",
      "Epoch 10, Iteration 79, Loss: 1.0647848844528198\n",
      "Epoch 10, Iteration 80, Loss: 0.5911253690719604\n",
      "Epoch 10, Iteration 81, Loss: 0.965757429599762\n",
      "Epoch 10, Iteration 82, Loss: 0.7568934559822083\n",
      "Epoch 10, Iteration 83, Loss: 0.5859569311141968\n",
      "Epoch 10, Iteration 84, Loss: 0.9343733191490173\n",
      "Epoch 10, Iteration 85, Loss: 0.7163803577423096\n",
      "Epoch 10, Iteration 86, Loss: 0.8680612444877625\n",
      "Epoch 10, Iteration 87, Loss: 0.6720395684242249\n",
      "Epoch 10, Iteration 88, Loss: 0.7886990904808044\n",
      "Epoch 10, Iteration 89, Loss: 0.8365699052810669\n",
      "Epoch 10, Iteration 90, Loss: 0.5694050192832947\n",
      "Epoch 10, Iteration 91, Loss: 0.7421297430992126\n",
      "Epoch 10, Iteration 92, Loss: 0.6328316330909729\n",
      "Epoch 10, Iteration 93, Loss: 0.76069176197052\n",
      "Epoch 10, Iteration 94, Loss: 0.8262234926223755\n",
      "Epoch 10, Iteration 95, Loss: 0.51578688621521\n",
      "Epoch 10, Iteration 96, Loss: 0.7220892906188965\n",
      "Epoch 10, Iteration 97, Loss: 0.6005618572235107\n",
      "Epoch 10, Iteration 98, Loss: 0.7297185659408569\n",
      "Epoch 10, Iteration 99, Loss: 0.8142919540405273\n",
      "Epoch 10, Iteration 100, Loss: 0.6186021566390991\n",
      "Epoch 10, Iteration 100, Valid Loss: 0.5935762524604797\n",
      "Epoch 10, Iteration 101, Loss: 0.9378654956817627\n",
      "Epoch 10, Iteration 102, Loss: 0.7896130681037903\n",
      "Epoch 10, Iteration 103, Loss: 0.8839844465255737\n",
      "Epoch 10, Iteration 104, Loss: 0.9371281862258911\n",
      "Epoch 10, Iteration 105, Loss: 0.7133784890174866\n",
      "Epoch 10, Iteration 106, Loss: 0.5743808746337891\n",
      "Epoch 10, Iteration 107, Loss: 0.8873748779296875\n",
      "Epoch 10, Iteration 108, Loss: 0.7285658121109009\n",
      "Epoch 10, Iteration 109, Loss: 0.9432621598243713\n",
      "Epoch 10, Iteration 110, Loss: 0.8967631459236145\n",
      "Epoch 10, Iteration 111, Loss: 0.7806292772293091\n",
      "Epoch 10, Iteration 112, Loss: 0.7485266327857971\n",
      "Epoch 10, Iteration 113, Loss: 0.6144186854362488\n",
      "Epoch 10, Iteration 114, Loss: 1.01872718334198\n",
      "Epoch 10, Iteration 115, Loss: 0.9313822388648987\n",
      "Epoch 10, Iteration 116, Loss: 0.685436487197876\n",
      "Epoch 10, Iteration 117, Loss: 0.505425214767456\n",
      "Epoch 10, Iteration 118, Loss: 0.7342525720596313\n",
      "Epoch 10, Iteration 119, Loss: 0.6460834741592407\n",
      "Epoch 10, Iteration 120, Loss: 0.9409077167510986\n",
      "Epoch 10, Iteration 121, Loss: 0.7103379964828491\n",
      "Epoch 10, Iteration 122, Loss: 0.7750341296195984\n",
      "Epoch 10, Iteration 123, Loss: 0.7703580856323242\n",
      "Epoch 10, Iteration 124, Loss: 0.5846555829048157\n",
      "Epoch 10, Iteration 125, Loss: 0.7210730314254761\n",
      "Epoch 10, Iteration 126, Loss: 0.5922013521194458\n",
      "Epoch 10, Iteration 127, Loss: 0.8443806767463684\n",
      "Epoch 10, Iteration 128, Loss: 0.9767475724220276\n",
      "Epoch 10, Iteration 129, Loss: 0.8052440285682678\n",
      "Epoch 10, Iteration 130, Loss: 0.9829390048980713\n",
      "Epoch 10, Iteration 131, Loss: 0.7366750836372375\n",
      "Epoch 10, Iteration 132, Loss: 0.6292023062705994\n",
      "Epoch 10, Iteration 133, Loss: 1.1387101411819458\n",
      "Epoch 10, Iteration 134, Loss: 0.8499776124954224\n",
      "Epoch 10, Iteration 135, Loss: 0.8184048533439636\n",
      "Epoch 10, Iteration 136, Loss: 0.7745359539985657\n",
      "Epoch 10, Iteration 137, Loss: 0.7071245312690735\n",
      "Epoch 10, Iteration 138, Loss: 0.6762761473655701\n",
      "Epoch 10, Iteration 139, Loss: 0.6998870968818665\n",
      "Epoch 10, Iteration 140, Loss: 0.6813833117485046\n",
      "Epoch 10, Iteration 141, Loss: 0.9120232462882996\n",
      "Epoch 10, Iteration 142, Loss: 0.7615529894828796\n",
      "Epoch 10, Iteration 143, Loss: 0.6769667267799377\n",
      "Epoch 10, Iteration 144, Loss: 0.6910528540611267\n",
      "Epoch 10, Iteration 145, Loss: 0.7742070555686951\n",
      "Epoch 10, Iteration 146, Loss: 0.8384502530097961\n",
      "Epoch 10, Iteration 147, Loss: 0.7873073816299438\n",
      "Epoch 10, Iteration 148, Loss: 0.5566807985305786\n",
      "Epoch 10, Iteration 149, Loss: 0.7326437830924988\n",
      "Epoch 10, Iteration 150, Loss: 0.7690501809120178\n",
      "Epoch 10, Iteration 150, Valid Loss: 0.6186087131500244\n",
      "Epoch 10, Iteration 151, Loss: 0.766526460647583\n",
      "Epoch 10, Iteration 152, Loss: 0.6648402810096741\n",
      "Epoch 10, Iteration 153, Loss: 0.6817734241485596\n",
      "Epoch 10, Iteration 154, Loss: 0.5006027817726135\n",
      "Epoch 10, Iteration 155, Loss: 0.8818252086639404\n",
      "Epoch 10, Iteration 156, Loss: 0.7000746726989746\n",
      "Epoch 10, Iteration 157, Loss: 1.0607458353042603\n",
      "Epoch 10, Iteration 158, Loss: 0.7856533527374268\n",
      "Epoch 10, Iteration 159, Loss: 0.7563639283180237\n",
      "Epoch 10, Iteration 160, Loss: 0.6540985107421875\n",
      "Epoch 10, Iteration 161, Loss: 0.6927825808525085\n",
      "Epoch 10, Iteration 162, Loss: 0.9577944874763489\n",
      "Epoch 10, Iteration 163, Loss: 0.5840796828269958\n",
      "Epoch 10, Iteration 164, Loss: 0.6740800738334656\n",
      "Epoch 10, Iteration 165, Loss: 0.767454981803894\n",
      "Epoch 10, Iteration 166, Loss: 0.6642184853553772\n",
      "Epoch 10, Iteration 167, Loss: 0.9544862508773804\n",
      "Epoch 10, Iteration 168, Loss: 0.7741713523864746\n",
      "Epoch 10, Iteration 169, Loss: 0.7487072348594666\n",
      "Epoch 10, Iteration 170, Loss: 0.9114552140235901\n",
      "Epoch 10, Iteration 171, Loss: 0.8929017782211304\n",
      "Epoch 10, Iteration 172, Loss: 0.6960786581039429\n",
      "Epoch 10, Iteration 173, Loss: 0.7404016256332397\n",
      "Epoch 10, Iteration 174, Loss: 0.8023961782455444\n",
      "Epoch 10, Iteration 175, Loss: 0.7905619740486145\n",
      "Epoch 10, Iteration 176, Loss: 0.581392228603363\n",
      "Epoch 10, Iteration 177, Loss: 1.029721736907959\n",
      "Epoch 10, Iteration 178, Loss: 0.82248455286026\n",
      "Epoch 10, Iteration 179, Loss: 0.6775387525558472\n",
      "Epoch 10, Iteration 180, Loss: 0.702146589756012\n",
      "Epoch 10, Iteration 181, Loss: 0.75860995054245\n",
      "Epoch 10, Iteration 182, Loss: 0.8195267915725708\n",
      "Epoch 10, Iteration 183, Loss: 0.6623228192329407\n",
      "Epoch 10, Iteration 184, Loss: 0.7173915505409241\n",
      "Epoch 10, Iteration 185, Loss: 0.6233001947402954\n",
      "Epoch 10, Iteration 186, Loss: 0.6487166881561279\n",
      "Epoch 10, Iteration 187, Loss: 0.7055772542953491\n",
      "Epoch 10, Iteration 188, Loss: 0.6488667130470276\n",
      "Epoch 10, Iteration 189, Loss: 0.6604911684989929\n",
      "Epoch 10, Iteration 190, Loss: 0.8460264801979065\n",
      "Epoch 10, Iteration 191, Loss: 0.7295841574668884\n",
      "Epoch 10, Iteration 192, Loss: 0.7800165414810181\n",
      "Epoch 10, Iteration 193, Loss: 0.7407735586166382\n",
      "Epoch 10, Iteration 194, Loss: 0.7662461400032043\n",
      "Epoch 10, Iteration 195, Loss: 0.6357730031013489\n",
      "Epoch 10, Iteration 196, Loss: 0.742868959903717\n",
      "Epoch 10, Iteration 197, Loss: 0.684909999370575\n",
      "Epoch 10, Iteration 198, Loss: 0.6679373979568481\n",
      "Epoch 10, Iteration 199, Loss: 0.8563295602798462\n",
      "Epoch 10, Iteration 200, Loss: 0.6409726738929749\n",
      "Epoch 10, Iteration 200, Valid Loss: 0.5743182301521301\n",
      "Epoch 10, Iteration 201, Loss: 0.6706652045249939\n",
      "Epoch 10, Iteration 202, Loss: 0.7648370862007141\n",
      "Epoch 10, Iteration 203, Loss: 0.6519437432289124\n",
      "Epoch 10, Iteration 204, Loss: 0.5481966733932495\n",
      "Epoch 10, Iteration 205, Loss: 0.5176790952682495\n",
      "Epoch 10, Iteration 206, Loss: 0.6611182689666748\n",
      "Epoch 10, Iteration 207, Loss: 0.9427447319030762\n",
      "Epoch 10, Iteration 208, Loss: 1.1028672456741333\n",
      "Epoch 10, Iteration 209, Loss: 1.07598078250885\n",
      "Epoch 10, Iteration 210, Loss: 1.02100670337677\n",
      "Epoch 10, Iteration 211, Loss: 1.098413109779358\n",
      "Epoch 10, Iteration 212, Loss: 0.8806502223014832\n",
      "Epoch 10, Iteration 213, Loss: 0.9492775797843933\n",
      "Epoch 10, Iteration 214, Loss: 0.7214271426200867\n",
      "Epoch 10, Iteration 215, Loss: 0.6506844758987427\n",
      "Epoch 10, Iteration 216, Loss: 0.6766752004623413\n",
      "Epoch 10, Iteration 217, Loss: 0.6437801122665405\n",
      "Epoch 10, Iteration 218, Loss: 0.7815642952919006\n",
      "Epoch 10, Iteration 219, Loss: 0.8523040413856506\n",
      "Epoch 10, Iteration 220, Loss: 0.672357976436615\n",
      "Epoch 10, Iteration 221, Loss: 0.7504586577415466\n",
      "Epoch 10, Iteration 222, Loss: 0.6701706051826477\n",
      "Epoch 10, Iteration 223, Loss: 0.6027670502662659\n",
      "Epoch 10, Iteration 224, Loss: 0.6325260996818542\n",
      "Epoch 10, Iteration 225, Loss: 0.8266700506210327\n",
      "Epoch 10, Iteration 226, Loss: 0.770339846611023\n",
      "Epoch 10, Iteration 227, Loss: 0.8777580857276917\n",
      "Epoch 10, Iteration 228, Loss: 0.7112290263175964\n",
      "Epoch 10, Iteration 229, Loss: 0.6070895791053772\n",
      "Epoch 10, Iteration 230, Loss: 0.7547205090522766\n",
      "Epoch 10, Iteration 231, Loss: 0.7307193279266357\n",
      "Epoch 10, Iteration 232, Loss: 0.7967984080314636\n",
      "Epoch 10, Iteration 233, Loss: 1.0054471492767334\n",
      "Epoch 10, Iteration 234, Loss: 0.8256056904792786\n",
      "Epoch 10, Iteration 235, Loss: 1.0016772747039795\n",
      "Epoch 10, Iteration 236, Loss: 0.7089691758155823\n",
      "Epoch 10, Iteration 237, Loss: 0.6708499193191528\n",
      "Epoch 10, Iteration 238, Loss: 0.6065192818641663\n",
      "Epoch 10, Iteration 239, Loss: 0.7347651720046997\n",
      "Epoch 10, Iteration 240, Loss: 0.6613755822181702\n",
      "Epoch 10, Iteration 241, Loss: 0.7381796836853027\n",
      "Epoch 10, Iteration 242, Loss: 0.6906258463859558\n",
      "Epoch 10, Iteration 243, Loss: 0.7288327217102051\n",
      "Epoch 10, Iteration 244, Loss: 0.7345824837684631\n",
      "Epoch 10, Iteration 245, Loss: 0.4920479953289032\n",
      "Epoch 10, Iteration 246, Loss: 0.715110719203949\n",
      "Epoch 10, Iteration 247, Loss: 0.6111136078834534\n",
      "Epoch 10, Iteration 248, Loss: 0.7207657098770142\n",
      "Epoch 10, Iteration 249, Loss: 0.7677707672119141\n",
      "Epoch 10, Iteration 250, Loss: 0.8631688952445984\n",
      "Epoch 10, Iteration 250, Valid Loss: 0.5504011511802673\n",
      "Epoch 10, Iteration 251, Loss: 0.6561771035194397\n",
      "Epoch 10, Iteration 252, Loss: 0.9588308930397034\n",
      "Epoch 10, Iteration 253, Loss: 0.7828091382980347\n",
      "Epoch 10, Iteration 254, Loss: 0.8474927544593811\n",
      "Epoch 10, Iteration 255, Loss: 0.8627131581306458\n",
      "Epoch 10, Iteration 256, Loss: 0.761641263961792\n",
      "Epoch 10, Iteration 257, Loss: 0.5783657431602478\n",
      "Epoch 10, Iteration 258, Loss: 0.9014609456062317\n",
      "Epoch 10, Iteration 259, Loss: 0.6264707446098328\n",
      "Epoch 10, Iteration 260, Loss: 0.7745126485824585\n",
      "Epoch 10, Iteration 261, Loss: 0.7050879001617432\n",
      "Epoch 10, Iteration 262, Loss: 0.5822306871414185\n",
      "Epoch 10, Iteration 263, Loss: 0.8365021347999573\n",
      "Epoch 10, Iteration 264, Loss: 0.7574198842048645\n",
      "Epoch 10, Iteration 265, Loss: 0.7455630302429199\n",
      "Epoch 10, Iteration 266, Loss: 0.700753390789032\n",
      "Epoch 10, Iteration 267, Loss: 0.718884289264679\n",
      "Epoch 10, Iteration 268, Loss: 0.6650766134262085\n",
      "Epoch 10, Iteration 269, Loss: 0.7242072820663452\n",
      "Epoch 10, Iteration 270, Loss: 0.6870084404945374\n",
      "Epoch 10, Iteration 271, Loss: 0.7116851806640625\n",
      "Epoch 10, Iteration 272, Loss: 0.657995343208313\n",
      "Epoch 10, Iteration 273, Loss: 0.6438336372375488\n",
      "Epoch 10, Iteration 274, Loss: 0.7360741496086121\n",
      "Epoch 10, Iteration 275, Loss: 0.701709508895874\n",
      "Epoch 10, Iteration 276, Loss: 0.6678832173347473\n",
      "Epoch 10, Iteration 277, Loss: 0.7032811641693115\n",
      "Epoch 10, Iteration 278, Loss: 0.8900871872901917\n",
      "Epoch 10, Iteration 279, Loss: 0.6256862282752991\n",
      "Epoch 10, Iteration 280, Loss: 0.51121586561203\n",
      "Epoch 10, Iteration 281, Loss: 0.6841310858726501\n",
      "Epoch 10, Iteration 282, Loss: 0.688979983329773\n",
      "Epoch 10, Iteration 283, Loss: 0.674055278301239\n",
      "Epoch 10, Iteration 284, Loss: 0.6844362616539001\n",
      "Epoch 10, Iteration 285, Loss: 0.6384543776512146\n",
      "Epoch 10, Iteration 286, Loss: 0.7142648100852966\n",
      "Epoch 10, Iteration 287, Loss: 0.667657196521759\n",
      "Epoch 10, Iteration 288, Loss: 0.5384620428085327\n",
      "Epoch 10, Iteration 289, Loss: 0.6726642847061157\n",
      "Epoch 10, Iteration 290, Loss: 0.6010797619819641\n",
      "Epoch 10, Iteration 291, Loss: 0.6296411752700806\n",
      "Epoch 10, Iteration 292, Loss: 0.8138992190361023\n",
      "Epoch 10, Iteration 293, Loss: 0.8846218585968018\n",
      "Epoch 10, Iteration 294, Loss: 0.6795259714126587\n",
      "Epoch 10, Iteration 295, Loss: 0.5945744514465332\n",
      "Epoch 10, Iteration 296, Loss: 0.8290864825248718\n",
      "Epoch 10, Iteration 297, Loss: 0.7845622301101685\n",
      "Epoch 10, Iteration 298, Loss: 0.7777056097984314\n",
      "Epoch 10, Iteration 299, Loss: 0.7214019298553467\n",
      "Epoch 10, Iteration 300, Loss: 0.6035046577453613\n",
      "Epoch 10, Iteration 300, Valid Loss: 0.5557551980018616\n",
      "Epoch 10, Iteration 301, Loss: 0.6539008021354675\n",
      "Epoch 10, Iteration 302, Loss: 0.6069904565811157\n",
      "Epoch 10, Iteration 303, Loss: 0.7707574367523193\n",
      "Epoch 10, Iteration 304, Loss: 0.7117477655410767\n",
      "Epoch 10, Iteration 305, Loss: 0.5745503902435303\n",
      "Epoch 10, Iteration 306, Loss: 0.6007079482078552\n",
      "Epoch 10, Iteration 307, Loss: 0.6313223838806152\n",
      "Epoch 10, Iteration 308, Loss: 0.6527943015098572\n",
      "Epoch 10, Iteration 309, Loss: 0.6324784755706787\n",
      "Epoch 10, Iteration 310, Loss: 0.8787057995796204\n",
      "Epoch 10, Iteration 311, Loss: 0.6591940522193909\n",
      "Epoch 10, Iteration 312, Loss: 0.589541494846344\n",
      "Epoch 10, Iteration 313, Loss: 0.7831821441650391\n",
      "Epoch 10, Iteration 314, Loss: 0.6550608277320862\n",
      "Epoch 10, Iteration 315, Loss: 0.6151639223098755\n",
      "Epoch 10, Iteration 316, Loss: 0.8148202896118164\n",
      "Epoch 10, Iteration 317, Loss: 0.7439497709274292\n",
      "Epoch 10, Iteration 318, Loss: 0.7473531365394592\n",
      "Epoch 10, Iteration 319, Loss: 0.8006598353385925\n",
      "Epoch 10, Iteration 320, Loss: 0.5651202201843262\n",
      "Epoch 10, Iteration 321, Loss: 0.6667767763137817\n",
      "Epoch 10, Iteration 322, Loss: 0.6580484509468079\n",
      "Epoch 10, Iteration 323, Loss: 0.666596531867981\n",
      "Epoch 10, Iteration 324, Loss: 0.7767106294631958\n",
      "Epoch 10, Iteration 325, Loss: 0.7353595495223999\n",
      "Epoch 10, Iteration 326, Loss: 0.715372622013092\n",
      "Epoch 10, Iteration 327, Loss: 0.6271296143531799\n",
      "Epoch 10, Iteration 328, Loss: 0.6826103329658508\n",
      "Epoch 10, Iteration 329, Loss: 0.8405982255935669\n",
      "Epoch 10, Iteration 330, Loss: 0.5478883385658264\n",
      "Epoch 10, Iteration 331, Loss: 0.6306619048118591\n",
      "Epoch 10, Iteration 332, Loss: 0.7813678979873657\n",
      "Epoch 10, Iteration 333, Loss: 0.814903199672699\n",
      "Epoch 10, Iteration 334, Loss: 0.7711772918701172\n",
      "Epoch 10, Iteration 335, Loss: 0.9561102986335754\n",
      "Epoch 10, Iteration 336, Loss: 0.7349669337272644\n",
      "Epoch 10, Iteration 337, Loss: 0.6829584240913391\n",
      "Epoch 10, Iteration 338, Loss: 0.7673381567001343\n",
      "Epoch 10, Iteration 339, Loss: 0.7515866756439209\n",
      "Epoch 10, Iteration 340, Loss: 0.568173885345459\n",
      "Epoch 10, Iteration 341, Loss: 0.6508288979530334\n",
      "Epoch 10, Iteration 342, Loss: 0.7587412595748901\n",
      "Epoch 10, Iteration 343, Loss: 0.6001796126365662\n",
      "Epoch 10, Iteration 344, Loss: 0.7477498054504395\n",
      "Epoch 10, Iteration 345, Loss: 0.7083740234375\n",
      "Epoch 10, Iteration 346, Loss: 0.6925488114356995\n",
      "Epoch 10, Iteration 347, Loss: 0.5945508480072021\n",
      "Epoch 10, Iteration 348, Loss: 0.6250824928283691\n",
      "Epoch 10, Iteration 349, Loss: 0.6370269060134888\n",
      "Epoch 10, Iteration 350, Loss: 0.553481936454773\n",
      "Epoch 10, Iteration 350, Valid Loss: 0.5864779949188232\n",
      "Epoch 10, Iteration 351, Loss: 0.7451006770133972\n",
      "Epoch 10, Iteration 352, Loss: 0.5752909183502197\n",
      "Epoch 10, Iteration 353, Loss: 0.7061761617660522\n",
      "Epoch 10, Iteration 354, Loss: 0.5584087371826172\n",
      "Epoch 10, Iteration 355, Loss: 0.6344838738441467\n",
      "Epoch 10, Iteration 356, Loss: 0.705750584602356\n",
      "Epoch 10, Iteration 357, Loss: 0.6736392974853516\n",
      "Epoch 10, Iteration 358, Loss: 0.6159852147102356\n",
      "Epoch 10, Iteration 359, Loss: 0.6367098093032837\n",
      "Epoch 10, Iteration 360, Loss: 0.5615674257278442\n",
      "Epoch 10, Iteration 361, Loss: 0.7077704668045044\n",
      "Epoch 10, Iteration 362, Loss: 0.7054967880249023\n",
      "Epoch 10, Iteration 363, Loss: 0.6565071940422058\n",
      "Epoch 10, Iteration 364, Loss: 0.6527601480484009\n",
      "Epoch 10, Iteration 365, Loss: 0.7506481409072876\n",
      "Epoch 10, Iteration 366, Loss: 0.7201902270317078\n",
      "Epoch 10, Iteration 367, Loss: 0.6431786417961121\n",
      "Epoch 10, Iteration 368, Loss: 0.6408833861351013\n",
      "Epoch 10, Iteration 369, Loss: 0.6227806210517883\n",
      "Epoch 10, Iteration 370, Loss: 0.6809090375900269\n",
      "Epoch 10, Iteration 371, Loss: 0.6802048087120056\n",
      "Epoch 10, Iteration 372, Loss: 0.5819459557533264\n",
      "Epoch 10, Iteration 373, Loss: 0.6230805516242981\n",
      "Epoch 10, Iteration 374, Loss: 0.9183531999588013\n",
      "Epoch 10, Iteration 375, Loss: 0.6080155968666077\n",
      "Epoch 10, Iteration 376, Loss: 0.7125503420829773\n",
      "Epoch 10, Iteration 377, Loss: 0.6342138051986694\n",
      "Epoch 10, Iteration 378, Loss: 0.5610761642456055\n",
      "Epoch 10, Iteration 379, Loss: 0.5570324063301086\n",
      "Epoch 10, Iteration 380, Loss: 0.6039465665817261\n",
      "Epoch 10, Iteration 381, Loss: 0.5890289545059204\n",
      "Epoch 10, Iteration 382, Loss: 0.4715224504470825\n",
      "Epoch 10, Iteration 383, Loss: 0.6662879586219788\n",
      "Epoch 10, Iteration 384, Loss: 0.6853749752044678\n",
      "Epoch 10, Iteration 385, Loss: 0.6175462007522583\n",
      "Epoch 10, Iteration 386, Loss: 0.5697802901268005\n",
      "Epoch 10, Iteration 387, Loss: 0.9229133129119873\n",
      "Epoch 10, Iteration 388, Loss: 0.5914174914360046\n",
      "Epoch 10, Iteration 389, Loss: 0.7061011791229248\n",
      "Epoch 10, Iteration 390, Loss: 0.8191821575164795\n",
      "Epoch 10, Iteration 391, Loss: 0.6114296317100525\n",
      "Epoch 11/15, Loss: 0.7327804779063682\n",
      "Epoch 11, Iteration 0, Loss: 0.7016355395317078\n",
      "Epoch 11, Iteration 1, Loss: 0.7958127856254578\n",
      "Epoch 11, Iteration 2, Loss: 0.7641939520835876\n",
      "Epoch 11, Iteration 3, Loss: 0.8292186856269836\n",
      "Epoch 11, Iteration 4, Loss: 0.6860394477844238\n",
      "Epoch 11, Iteration 5, Loss: 0.7741234302520752\n",
      "Epoch 11, Iteration 6, Loss: 0.5703004598617554\n",
      "Epoch 11, Iteration 7, Loss: 0.6935155987739563\n",
      "Epoch 11, Iteration 8, Loss: 0.5974258184432983\n",
      "Epoch 11, Iteration 9, Loss: 0.7895264625549316\n",
      "Epoch 11, Iteration 10, Loss: 0.7498070597648621\n",
      "Epoch 11, Iteration 11, Loss: 0.6577965617179871\n",
      "Epoch 11, Iteration 12, Loss: 0.6262067556381226\n",
      "Epoch 11, Iteration 13, Loss: 0.7220155000686646\n",
      "Epoch 11, Iteration 14, Loss: 0.6378489136695862\n",
      "Epoch 11, Iteration 15, Loss: 0.8371140360832214\n",
      "Epoch 11, Iteration 16, Loss: 0.6864043474197388\n",
      "Epoch 11, Iteration 17, Loss: 0.7997525930404663\n",
      "Epoch 11, Iteration 18, Loss: 0.6409203410148621\n",
      "Epoch 11, Iteration 19, Loss: 0.7335864901542664\n",
      "Epoch 11, Iteration 20, Loss: 0.5980731248855591\n",
      "Epoch 11, Iteration 21, Loss: 0.6108127236366272\n",
      "Epoch 11, Iteration 22, Loss: 0.6074884533882141\n",
      "Epoch 11, Iteration 23, Loss: 0.7290917634963989\n",
      "Epoch 11, Iteration 24, Loss: 0.680160403251648\n",
      "Epoch 11, Iteration 25, Loss: 0.5797147154808044\n",
      "Epoch 11, Iteration 26, Loss: 0.9656983017921448\n",
      "Epoch 11, Iteration 27, Loss: 0.6812348365783691\n",
      "Epoch 11, Iteration 28, Loss: 0.7334570288658142\n",
      "Epoch 11, Iteration 29, Loss: 0.8356876373291016\n",
      "Epoch 11, Iteration 30, Loss: 0.675916314125061\n",
      "Epoch 11, Iteration 31, Loss: 0.7303800582885742\n",
      "Epoch 11, Iteration 32, Loss: 0.7287982106208801\n",
      "Epoch 11, Iteration 33, Loss: 0.7361166477203369\n",
      "Epoch 11, Iteration 34, Loss: 0.6768385767936707\n",
      "Epoch 11, Iteration 35, Loss: 0.7603305578231812\n",
      "Epoch 11, Iteration 36, Loss: 0.6040888428688049\n",
      "Epoch 11, Iteration 37, Loss: 0.5855453014373779\n",
      "Epoch 11, Iteration 38, Loss: 0.631179928779602\n",
      "Epoch 11, Iteration 39, Loss: 0.7580052614212036\n",
      "Epoch 11, Iteration 40, Loss: 0.6634421944618225\n",
      "Epoch 11, Iteration 41, Loss: 0.7394941449165344\n",
      "Epoch 11, Iteration 42, Loss: 0.6453126668930054\n",
      "Epoch 11, Iteration 43, Loss: 0.5635202527046204\n",
      "Epoch 11, Iteration 44, Loss: 0.6407620906829834\n",
      "Epoch 11, Iteration 45, Loss: 0.7344962954521179\n",
      "Epoch 11, Iteration 46, Loss: 0.7255427241325378\n",
      "Epoch 11, Iteration 47, Loss: 0.6627433896064758\n",
      "Epoch 11, Iteration 48, Loss: 0.8032912015914917\n",
      "Epoch 11, Iteration 49, Loss: 0.660552442073822\n",
      "Epoch 11, Iteration 50, Loss: 0.6443440318107605\n",
      "Epoch 11, Iteration 50, Valid Loss: 0.5848653316497803\n",
      "Epoch 11, Iteration 51, Loss: 0.8275666236877441\n",
      "Epoch 11, Iteration 52, Loss: 0.5462636947631836\n",
      "Epoch 11, Iteration 53, Loss: 0.6839600205421448\n",
      "Epoch 11, Iteration 54, Loss: 0.980158269405365\n",
      "Epoch 11, Iteration 55, Loss: 0.7693561911582947\n",
      "Epoch 11, Iteration 56, Loss: 0.6987866759300232\n",
      "Epoch 11, Iteration 57, Loss: 0.7813132405281067\n",
      "Epoch 11, Iteration 58, Loss: 0.6944959163665771\n",
      "Epoch 11, Iteration 59, Loss: 0.6436046957969666\n",
      "Epoch 11, Iteration 60, Loss: 0.6738095283508301\n",
      "Epoch 11, Iteration 61, Loss: 0.5575127005577087\n",
      "Epoch 11, Iteration 62, Loss: 0.6775184273719788\n",
      "Epoch 11, Iteration 63, Loss: 0.5660097599029541\n",
      "Epoch 11, Iteration 64, Loss: 0.8429458737373352\n",
      "Epoch 11, Iteration 65, Loss: 0.9692674279212952\n",
      "Epoch 11, Iteration 66, Loss: 0.6977846026420593\n",
      "Epoch 11, Iteration 67, Loss: 0.5850030779838562\n",
      "Epoch 11, Iteration 68, Loss: 0.9167719483375549\n",
      "Epoch 11, Iteration 69, Loss: 0.7904755473136902\n",
      "Epoch 11, Iteration 70, Loss: 0.5794886350631714\n",
      "Epoch 11, Iteration 71, Loss: 0.8602553606033325\n",
      "Epoch 11, Iteration 72, Loss: 0.7160502076148987\n",
      "Epoch 11, Iteration 73, Loss: 0.7043562531471252\n",
      "Epoch 11, Iteration 74, Loss: 0.9686292409896851\n",
      "Epoch 11, Iteration 75, Loss: 0.7644162178039551\n",
      "Epoch 11, Iteration 76, Loss: 0.7246971726417542\n",
      "Epoch 11, Iteration 77, Loss: 0.6934483647346497\n",
      "Epoch 11, Iteration 78, Loss: 0.8779032826423645\n",
      "Epoch 11, Iteration 79, Loss: 0.899221658706665\n",
      "Epoch 11, Iteration 80, Loss: 0.5616312623023987\n",
      "Epoch 11, Iteration 81, Loss: 0.8780055046081543\n",
      "Epoch 11, Iteration 82, Loss: 0.829157829284668\n",
      "Epoch 11, Iteration 83, Loss: 0.6211012601852417\n",
      "Epoch 11, Iteration 84, Loss: 0.8381749987602234\n",
      "Epoch 11, Iteration 85, Loss: 0.6625975370407104\n",
      "Epoch 11, Iteration 86, Loss: 0.8480207324028015\n",
      "Epoch 11, Iteration 87, Loss: 0.7154961824417114\n",
      "Epoch 11, Iteration 88, Loss: 0.8197358846664429\n",
      "Epoch 11, Iteration 89, Loss: 0.7898690700531006\n",
      "Epoch 11, Iteration 90, Loss: 0.5682766437530518\n",
      "Epoch 11, Iteration 91, Loss: 0.6831996440887451\n",
      "Epoch 11, Iteration 92, Loss: 0.4894576668739319\n",
      "Epoch 11, Iteration 93, Loss: 0.7669128775596619\n",
      "Epoch 11, Iteration 94, Loss: 0.8113728165626526\n",
      "Epoch 11, Iteration 95, Loss: 0.5814016461372375\n",
      "Epoch 11, Iteration 96, Loss: 0.7451983690261841\n",
      "Epoch 11, Iteration 97, Loss: 0.5599753856658936\n",
      "Epoch 11, Iteration 98, Loss: 0.6928027272224426\n",
      "Epoch 11, Iteration 99, Loss: 0.6592565178871155\n",
      "Epoch 11, Iteration 100, Loss: 0.6010146737098694\n",
      "Epoch 11, Iteration 100, Valid Loss: 0.6339892745018005\n",
      "Epoch 11, Iteration 101, Loss: 0.9322881102561951\n",
      "Epoch 11, Iteration 102, Loss: 0.7911035418510437\n",
      "Epoch 11, Iteration 103, Loss: 0.7354932427406311\n",
      "Epoch 11, Iteration 104, Loss: 0.8568803668022156\n",
      "Epoch 11, Iteration 105, Loss: 0.750784695148468\n",
      "Epoch 11, Iteration 106, Loss: 0.6459167003631592\n",
      "Epoch 11, Iteration 107, Loss: 0.8916794657707214\n",
      "Epoch 11, Iteration 108, Loss: 0.6391187310218811\n",
      "Epoch 11, Iteration 109, Loss: 0.852087140083313\n",
      "Epoch 11, Iteration 110, Loss: 0.8874152898788452\n",
      "Epoch 11, Iteration 111, Loss: 0.6859068274497986\n",
      "Epoch 11, Iteration 112, Loss: 0.7303762435913086\n",
      "Epoch 11, Iteration 113, Loss: 0.6080019474029541\n",
      "Epoch 11, Iteration 114, Loss: 0.8980609774589539\n",
      "Epoch 11, Iteration 115, Loss: 0.8728168606758118\n",
      "Epoch 11, Iteration 116, Loss: 0.6574658751487732\n",
      "Epoch 11, Iteration 117, Loss: 0.5024585723876953\n",
      "Epoch 11, Iteration 118, Loss: 0.6969947814941406\n",
      "Epoch 11, Iteration 119, Loss: 0.5606737732887268\n",
      "Epoch 11, Iteration 120, Loss: 1.1385856866836548\n",
      "Epoch 11, Iteration 121, Loss: 0.6212441921234131\n",
      "Epoch 11, Iteration 122, Loss: 0.6351438164710999\n",
      "Epoch 11, Iteration 123, Loss: 0.6661232709884644\n",
      "Epoch 11, Iteration 124, Loss: 0.5899012088775635\n",
      "Epoch 11, Iteration 125, Loss: 0.683624804019928\n",
      "Epoch 11, Iteration 126, Loss: 0.5934281349182129\n",
      "Epoch 11, Iteration 127, Loss: 0.7890117168426514\n",
      "Epoch 11, Iteration 128, Loss: 0.7473239898681641\n",
      "Epoch 11, Iteration 129, Loss: 0.8331882953643799\n",
      "Epoch 11, Iteration 130, Loss: 0.9669003486633301\n",
      "Epoch 11, Iteration 131, Loss: 0.665132999420166\n",
      "Epoch 11, Iteration 132, Loss: 0.6973826885223389\n",
      "Epoch 11, Iteration 133, Loss: 0.9957250356674194\n",
      "Epoch 11, Iteration 134, Loss: 0.7772979140281677\n",
      "Epoch 11, Iteration 135, Loss: 0.7940321564674377\n",
      "Epoch 11, Iteration 136, Loss: 0.5943321585655212\n",
      "Epoch 11, Iteration 137, Loss: 0.6846792101860046\n",
      "Epoch 11, Iteration 138, Loss: 0.6196141839027405\n",
      "Epoch 11, Iteration 139, Loss: 0.6141042709350586\n",
      "Epoch 11, Iteration 140, Loss: 0.5955337882041931\n",
      "Epoch 11, Iteration 141, Loss: 1.021721601486206\n",
      "Epoch 11, Iteration 142, Loss: 0.8055428862571716\n",
      "Epoch 11, Iteration 143, Loss: 0.6650749444961548\n",
      "Epoch 11, Iteration 144, Loss: 0.6552848219871521\n",
      "Epoch 11, Iteration 145, Loss: 0.7500748038291931\n",
      "Epoch 11, Iteration 146, Loss: 0.755699872970581\n",
      "Epoch 11, Iteration 147, Loss: 0.7468070983886719\n",
      "Epoch 11, Iteration 148, Loss: 0.5697548985481262\n",
      "Epoch 11, Iteration 149, Loss: 0.6593852043151855\n",
      "Epoch 11, Iteration 150, Loss: 0.7460546493530273\n",
      "Epoch 11, Iteration 150, Valid Loss: 0.589823305606842\n",
      "Epoch 11, Iteration 151, Loss: 0.7059467434883118\n",
      "Epoch 11, Iteration 152, Loss: 0.6114403605461121\n",
      "Epoch 11, Iteration 153, Loss: 0.7800742387771606\n",
      "Epoch 11, Iteration 154, Loss: 0.5697222948074341\n",
      "Epoch 11, Iteration 155, Loss: 0.7189901471138\n",
      "Epoch 11, Iteration 156, Loss: 0.5989800691604614\n",
      "Epoch 11, Iteration 157, Loss: 1.1997401714324951\n",
      "Epoch 11, Iteration 158, Loss: 0.7328051924705505\n",
      "Epoch 11, Iteration 159, Loss: 0.7885064482688904\n",
      "Epoch 11, Iteration 160, Loss: 0.6559047102928162\n",
      "Epoch 11, Iteration 161, Loss: 0.6434035301208496\n",
      "Epoch 11, Iteration 162, Loss: 0.884608268737793\n",
      "Epoch 11, Iteration 163, Loss: 0.6101012825965881\n",
      "Epoch 11, Iteration 164, Loss: 0.7243504524230957\n",
      "Epoch 11, Iteration 165, Loss: 0.7038038372993469\n",
      "Epoch 11, Iteration 166, Loss: 0.6093520522117615\n",
      "Epoch 11, Iteration 167, Loss: 0.8356261849403381\n",
      "Epoch 11, Iteration 168, Loss: 0.7441189885139465\n",
      "Epoch 11, Iteration 169, Loss: 0.6703001260757446\n",
      "Epoch 11, Iteration 170, Loss: 0.8500682711601257\n",
      "Epoch 11, Iteration 171, Loss: 0.8364060521125793\n",
      "Epoch 11, Iteration 172, Loss: 0.6594241261482239\n",
      "Epoch 11, Iteration 173, Loss: 0.5889284610748291\n",
      "Epoch 11, Iteration 174, Loss: 0.806122899055481\n",
      "Epoch 11, Iteration 175, Loss: 0.7159833312034607\n",
      "Epoch 11, Iteration 176, Loss: 0.49993664026260376\n",
      "Epoch 11, Iteration 177, Loss: 0.9869811534881592\n",
      "Epoch 11, Iteration 178, Loss: 0.8179988265037537\n",
      "Epoch 11, Iteration 179, Loss: 0.6375972628593445\n",
      "Epoch 11, Iteration 180, Loss: 0.6958033442497253\n",
      "Epoch 11, Iteration 181, Loss: 0.8222627639770508\n",
      "Epoch 11, Iteration 182, Loss: 0.765865683555603\n",
      "Epoch 11, Iteration 183, Loss: 0.6059404015541077\n",
      "Epoch 11, Iteration 184, Loss: 0.6953216791152954\n",
      "Epoch 11, Iteration 185, Loss: 0.5777532458305359\n",
      "Epoch 11, Iteration 186, Loss: 0.5829187035560608\n",
      "Epoch 11, Iteration 187, Loss: 0.7430180907249451\n",
      "Epoch 11, Iteration 188, Loss: 0.5966593623161316\n",
      "Epoch 11, Iteration 189, Loss: 0.6424821615219116\n",
      "Epoch 11, Iteration 190, Loss: 0.818540096282959\n",
      "Epoch 11, Iteration 191, Loss: 0.7179632186889648\n",
      "Epoch 11, Iteration 192, Loss: 0.8015732765197754\n",
      "Epoch 11, Iteration 193, Loss: 0.6872007250785828\n",
      "Epoch 11, Iteration 194, Loss: 0.929430365562439\n",
      "Epoch 11, Iteration 195, Loss: 0.6512874364852905\n",
      "Epoch 11, Iteration 196, Loss: 0.7549152374267578\n",
      "Epoch 11, Iteration 197, Loss: 0.7201743125915527\n",
      "Epoch 11, Iteration 198, Loss: 0.7050696611404419\n",
      "Epoch 11, Iteration 199, Loss: 0.7407805323600769\n",
      "Epoch 11, Iteration 200, Loss: 0.6100530028343201\n",
      "Epoch 11, Iteration 200, Valid Loss: 0.5724600553512573\n",
      "Epoch 11, Iteration 201, Loss: 0.7007068395614624\n",
      "Epoch 11, Iteration 202, Loss: 0.8313390016555786\n",
      "Epoch 11, Iteration 203, Loss: 0.6462913751602173\n",
      "Epoch 11, Iteration 204, Loss: 0.6313725113868713\n",
      "Epoch 11, Iteration 205, Loss: 0.5766469836235046\n",
      "Epoch 11, Iteration 206, Loss: 0.5910604596138\n",
      "Epoch 11, Iteration 207, Loss: 0.8092253804206848\n",
      "Epoch 11, Iteration 208, Loss: 1.0709021091461182\n",
      "Epoch 11, Iteration 209, Loss: 1.0633827447891235\n",
      "Epoch 11, Iteration 210, Loss: 0.9673187136650085\n",
      "Epoch 11, Iteration 211, Loss: 0.9665571451187134\n",
      "Epoch 11, Iteration 212, Loss: 0.8132576942443848\n",
      "Epoch 11, Iteration 213, Loss: 0.8810999989509583\n",
      "Epoch 11, Iteration 214, Loss: 0.6756473779678345\n",
      "Epoch 11, Iteration 215, Loss: 0.6649514436721802\n",
      "Epoch 11, Iteration 216, Loss: 0.577170193195343\n",
      "Epoch 11, Iteration 217, Loss: 0.6059964299201965\n",
      "Epoch 11, Iteration 218, Loss: 0.70539790391922\n",
      "Epoch 11, Iteration 219, Loss: 0.7151912450790405\n",
      "Epoch 11, Iteration 220, Loss: 0.5927819609642029\n",
      "Epoch 11, Iteration 221, Loss: 0.7252864837646484\n",
      "Epoch 11, Iteration 222, Loss: 0.7283315062522888\n",
      "Epoch 11, Iteration 223, Loss: 0.6460860371589661\n",
      "Epoch 11, Iteration 224, Loss: 0.59513920545578\n",
      "Epoch 11, Iteration 225, Loss: 0.8099744915962219\n",
      "Epoch 11, Iteration 226, Loss: 0.7911078929901123\n",
      "Epoch 11, Iteration 227, Loss: 0.7437506914138794\n",
      "Epoch 11, Iteration 228, Loss: 0.7431491017341614\n",
      "Epoch 11, Iteration 229, Loss: 0.6552041172981262\n",
      "Epoch 11, Iteration 230, Loss: 0.7400884628295898\n",
      "Epoch 11, Iteration 231, Loss: 0.701139509677887\n",
      "Epoch 11, Iteration 232, Loss: 0.8653304576873779\n",
      "Epoch 11, Iteration 233, Loss: 0.8134966492652893\n",
      "Epoch 11, Iteration 234, Loss: 0.8714884519577026\n",
      "Epoch 11, Iteration 235, Loss: 0.5782630443572998\n",
      "Epoch 11, Iteration 236, Loss: 0.7403836846351624\n",
      "Epoch 11, Iteration 237, Loss: 0.5734928250312805\n",
      "Epoch 11, Iteration 238, Loss: 0.5748691558837891\n",
      "Epoch 11, Iteration 239, Loss: 0.8499137759208679\n",
      "Epoch 11, Iteration 240, Loss: 0.7124727368354797\n",
      "Epoch 11, Iteration 241, Loss: 0.6292638182640076\n",
      "Epoch 11, Iteration 242, Loss: 0.6428384780883789\n",
      "Epoch 11, Iteration 243, Loss: 0.6717172861099243\n",
      "Epoch 11, Iteration 244, Loss: 0.6868097186088562\n",
      "Epoch 11, Iteration 245, Loss: 0.5418257117271423\n",
      "Epoch 11, Iteration 246, Loss: 0.6941010355949402\n",
      "Epoch 11, Iteration 247, Loss: 0.6320768594741821\n",
      "Epoch 11, Iteration 248, Loss: 0.7983066439628601\n",
      "Epoch 11, Iteration 249, Loss: 0.691463828086853\n",
      "Epoch 11, Iteration 250, Loss: 0.8682649731636047\n",
      "Epoch 11, Iteration 250, Valid Loss: 0.5585439205169678\n",
      "Epoch 11, Iteration 251, Loss: 0.6655974388122559\n",
      "Epoch 11, Iteration 252, Loss: 0.787370502948761\n",
      "Epoch 11, Iteration 253, Loss: 0.7895509004592896\n",
      "Epoch 11, Iteration 254, Loss: 0.7818082571029663\n",
      "Epoch 11, Iteration 255, Loss: 0.7468234300613403\n",
      "Epoch 11, Iteration 256, Loss: 0.7632429003715515\n",
      "Epoch 11, Iteration 257, Loss: 0.7654913067817688\n",
      "Epoch 11, Iteration 258, Loss: 0.8914918899536133\n",
      "Epoch 11, Iteration 259, Loss: 0.6005858778953552\n",
      "Epoch 11, Iteration 260, Loss: 0.6866844892501831\n",
      "Epoch 11, Iteration 261, Loss: 0.670460045337677\n",
      "Epoch 11, Iteration 262, Loss: 0.6071642637252808\n",
      "Epoch 11, Iteration 263, Loss: 0.8285476565361023\n",
      "Epoch 11, Iteration 264, Loss: 0.6786581873893738\n",
      "Epoch 11, Iteration 265, Loss: 0.7620296478271484\n",
      "Epoch 11, Iteration 266, Loss: 0.8228822350502014\n",
      "Epoch 11, Iteration 267, Loss: 0.5967289805412292\n",
      "Epoch 11, Iteration 268, Loss: 0.6166386008262634\n",
      "Epoch 11, Iteration 269, Loss: 0.6990569829940796\n",
      "Epoch 11, Iteration 270, Loss: 0.7128511667251587\n",
      "Epoch 11, Iteration 271, Loss: 0.8369568586349487\n",
      "Epoch 11, Iteration 272, Loss: 0.5953711271286011\n",
      "Epoch 11, Iteration 273, Loss: 0.7517962455749512\n",
      "Epoch 11, Iteration 274, Loss: 0.6434080600738525\n",
      "Epoch 11, Iteration 275, Loss: 0.6169404983520508\n",
      "Epoch 11, Iteration 276, Loss: 0.6136918663978577\n",
      "Epoch 11, Iteration 277, Loss: 0.6511863470077515\n",
      "Epoch 11, Iteration 278, Loss: 0.847588062286377\n",
      "Epoch 11, Iteration 279, Loss: 0.6365419030189514\n",
      "Epoch 11, Iteration 280, Loss: 0.49029719829559326\n",
      "Epoch 11, Iteration 281, Loss: 0.6643944382667542\n",
      "Epoch 11, Iteration 282, Loss: 0.6970327496528625\n",
      "Epoch 11, Iteration 283, Loss: 0.6669929623603821\n",
      "Epoch 11, Iteration 284, Loss: 0.62897127866745\n",
      "Epoch 11, Iteration 285, Loss: 0.5734574794769287\n",
      "Epoch 11, Iteration 286, Loss: 0.7050370573997498\n",
      "Epoch 11, Iteration 287, Loss: 0.6435580253601074\n",
      "Epoch 11, Iteration 288, Loss: 0.6610857248306274\n",
      "Epoch 11, Iteration 289, Loss: 0.6897035241127014\n",
      "Epoch 11, Iteration 290, Loss: 0.5676261782646179\n",
      "Epoch 11, Iteration 291, Loss: 0.5837550163269043\n",
      "Epoch 11, Iteration 292, Loss: 0.7596551179885864\n",
      "Epoch 11, Iteration 293, Loss: 0.8433253169059753\n",
      "Epoch 11, Iteration 294, Loss: 0.7853558659553528\n",
      "Epoch 11, Iteration 295, Loss: 0.6030640006065369\n",
      "Epoch 11, Iteration 296, Loss: 0.8234965801239014\n",
      "Epoch 11, Iteration 297, Loss: 0.8190540671348572\n",
      "Epoch 11, Iteration 298, Loss: 0.7436918020248413\n",
      "Epoch 11, Iteration 299, Loss: 0.6077051162719727\n",
      "Epoch 11, Iteration 300, Loss: 0.5548031330108643\n",
      "Epoch 11, Iteration 300, Valid Loss: 0.5653539896011353\n",
      "Epoch 11, Iteration 301, Loss: 0.6484084725379944\n",
      "Epoch 11, Iteration 302, Loss: 0.7153722643852234\n",
      "Epoch 11, Iteration 303, Loss: 0.8034527897834778\n",
      "Epoch 11, Iteration 304, Loss: 0.5746414661407471\n",
      "Epoch 11, Iteration 305, Loss: 0.6737744212150574\n",
      "Epoch 11, Iteration 306, Loss: 0.5071656107902527\n",
      "Epoch 11, Iteration 307, Loss: 0.6133145689964294\n",
      "Epoch 11, Iteration 308, Loss: 0.6595855355262756\n",
      "Epoch 11, Iteration 309, Loss: 0.6158607006072998\n",
      "Epoch 11, Iteration 310, Loss: 0.7340025305747986\n",
      "Epoch 11, Iteration 311, Loss: 0.8770658373832703\n",
      "Epoch 11, Iteration 312, Loss: 0.5982293486595154\n",
      "Epoch 11, Iteration 313, Loss: 0.7901511192321777\n",
      "Epoch 11, Iteration 314, Loss: 0.6006894707679749\n",
      "Epoch 11, Iteration 315, Loss: 0.5855558514595032\n",
      "Epoch 11, Iteration 316, Loss: 0.9190396070480347\n",
      "Epoch 11, Iteration 317, Loss: 0.7441155314445496\n",
      "Epoch 11, Iteration 318, Loss: 0.7304530143737793\n",
      "Epoch 11, Iteration 319, Loss: 0.7392343878746033\n",
      "Epoch 11, Iteration 320, Loss: 0.5354325175285339\n",
      "Epoch 11, Iteration 321, Loss: 0.6731188893318176\n",
      "Epoch 11, Iteration 322, Loss: 0.6749829649925232\n",
      "Epoch 11, Iteration 323, Loss: 0.6267775893211365\n",
      "Epoch 11, Iteration 324, Loss: 0.6869545578956604\n",
      "Epoch 11, Iteration 325, Loss: 0.6481311917304993\n",
      "Epoch 11, Iteration 326, Loss: 0.5557000637054443\n",
      "Epoch 11, Iteration 327, Loss: 0.6065683364868164\n",
      "Epoch 11, Iteration 328, Loss: 0.6545423865318298\n",
      "Epoch 11, Iteration 329, Loss: 0.7377175092697144\n",
      "Epoch 11, Iteration 330, Loss: 0.5383874177932739\n",
      "Epoch 11, Iteration 331, Loss: 0.6167901158332825\n",
      "Epoch 11, Iteration 332, Loss: 0.6958348155021667\n",
      "Epoch 11, Iteration 333, Loss: 0.796514093875885\n",
      "Epoch 11, Iteration 334, Loss: 0.6370205879211426\n",
      "Epoch 11, Iteration 335, Loss: 0.8901472091674805\n",
      "Epoch 11, Iteration 336, Loss: 0.6548120975494385\n",
      "Epoch 11, Iteration 337, Loss: 0.701209545135498\n",
      "Epoch 11, Iteration 338, Loss: 0.7411563992500305\n",
      "Epoch 11, Iteration 339, Loss: 0.571604311466217\n",
      "Epoch 11, Iteration 340, Loss: 0.5827687382698059\n",
      "Epoch 11, Iteration 341, Loss: 0.6270954608917236\n",
      "Epoch 11, Iteration 342, Loss: 0.6360418796539307\n",
      "Epoch 11, Iteration 343, Loss: 0.5646310448646545\n",
      "Epoch 11, Iteration 344, Loss: 0.6880703568458557\n",
      "Epoch 11, Iteration 345, Loss: 0.7217388153076172\n",
      "Epoch 11, Iteration 346, Loss: 0.5983189940452576\n",
      "Epoch 11, Iteration 347, Loss: 0.5801386833190918\n",
      "Epoch 11, Iteration 348, Loss: 0.5331215262413025\n",
      "Epoch 11, Iteration 349, Loss: 0.6848185658454895\n",
      "Epoch 11, Iteration 350, Loss: 0.5643404722213745\n",
      "Epoch 11, Iteration 350, Valid Loss: 0.5931947231292725\n",
      "Epoch 11, Iteration 351, Loss: 0.7489498257637024\n",
      "Epoch 11, Iteration 352, Loss: 0.6710003018379211\n",
      "Epoch 11, Iteration 353, Loss: 0.7125159502029419\n",
      "Epoch 11, Iteration 354, Loss: 0.6138120293617249\n",
      "Epoch 11, Iteration 355, Loss: 0.6622584462165833\n",
      "Epoch 11, Iteration 356, Loss: 0.6305034756660461\n",
      "Epoch 11, Iteration 357, Loss: 0.6317611336708069\n",
      "Epoch 11, Iteration 358, Loss: 0.6196842789649963\n",
      "Epoch 11, Iteration 359, Loss: 0.5871303677558899\n",
      "Epoch 11, Iteration 360, Loss: 0.5740067958831787\n",
      "Epoch 11, Iteration 361, Loss: 0.7250677347183228\n",
      "Epoch 11, Iteration 362, Loss: 0.6824941039085388\n",
      "Epoch 11, Iteration 363, Loss: 0.54549241065979\n",
      "Epoch 11, Iteration 364, Loss: 0.695803165435791\n",
      "Epoch 11, Iteration 365, Loss: 0.7150589227676392\n",
      "Epoch 11, Iteration 366, Loss: 0.7291958928108215\n",
      "Epoch 11, Iteration 367, Loss: 0.6595176458358765\n",
      "Epoch 11, Iteration 368, Loss: 0.653155505657196\n",
      "Epoch 11, Iteration 369, Loss: 0.6384285092353821\n",
      "Epoch 11, Iteration 370, Loss: 0.6439762711524963\n",
      "Epoch 11, Iteration 371, Loss: 0.6569918990135193\n",
      "Epoch 11, Iteration 372, Loss: 0.6582575440406799\n",
      "Epoch 11, Iteration 373, Loss: 0.6460606455802917\n",
      "Epoch 11, Iteration 374, Loss: 0.7778922915458679\n",
      "Epoch 11, Iteration 375, Loss: 0.6334316730499268\n",
      "Epoch 11, Iteration 376, Loss: 0.7363283634185791\n",
      "Epoch 11, Iteration 377, Loss: 0.6645421981811523\n",
      "Epoch 11, Iteration 378, Loss: 0.583436131477356\n",
      "Epoch 11, Iteration 379, Loss: 0.6230367422103882\n",
      "Epoch 11, Iteration 380, Loss: 0.5789002180099487\n",
      "Epoch 11, Iteration 381, Loss: 0.5322118401527405\n",
      "Epoch 11, Iteration 382, Loss: 0.42642709612846375\n",
      "Epoch 11, Iteration 383, Loss: 0.5685510039329529\n",
      "Epoch 11, Iteration 384, Loss: 0.6603624820709229\n",
      "Epoch 11, Iteration 385, Loss: 0.6078435778617859\n",
      "Epoch 11, Iteration 386, Loss: 0.5949740409851074\n",
      "Epoch 11, Iteration 387, Loss: 0.8615894913673401\n",
      "Epoch 11, Iteration 388, Loss: 0.5877623558044434\n",
      "Epoch 11, Iteration 389, Loss: 0.6272212862968445\n",
      "Epoch 11, Iteration 390, Loss: 0.787345826625824\n",
      "Epoch 11, Iteration 391, Loss: 0.556693971157074\n",
      "Epoch 12/15, Loss: 0.7037499479341264\n",
      "Epoch 12, Iteration 0, Loss: 0.6568449139595032\n",
      "Epoch 12, Iteration 1, Loss: 0.8363197445869446\n",
      "Epoch 12, Iteration 2, Loss: 0.6939600706100464\n",
      "Epoch 12, Iteration 3, Loss: 0.7406249046325684\n",
      "Epoch 12, Iteration 4, Loss: 0.6112293601036072\n",
      "Epoch 12, Iteration 5, Loss: 0.7106138467788696\n",
      "Epoch 12, Iteration 6, Loss: 0.5636447668075562\n",
      "Epoch 12, Iteration 7, Loss: 0.6249756217002869\n",
      "Epoch 12, Iteration 8, Loss: 0.6454930305480957\n",
      "Epoch 12, Iteration 9, Loss: 0.7738726139068604\n",
      "Epoch 12, Iteration 10, Loss: 0.8000297546386719\n",
      "Epoch 12, Iteration 11, Loss: 0.6427926421165466\n",
      "Epoch 12, Iteration 12, Loss: 0.6537761092185974\n",
      "Epoch 12, Iteration 13, Loss: 0.7890626192092896\n",
      "Epoch 12, Iteration 14, Loss: 0.629528820514679\n",
      "Epoch 12, Iteration 15, Loss: 0.8550499081611633\n",
      "Epoch 12, Iteration 16, Loss: 0.7084514498710632\n",
      "Epoch 12, Iteration 17, Loss: 0.7026957869529724\n",
      "Epoch 12, Iteration 18, Loss: 0.6070947051048279\n",
      "Epoch 12, Iteration 19, Loss: 0.6933379173278809\n",
      "Epoch 12, Iteration 20, Loss: 0.5957990884780884\n",
      "Epoch 12, Iteration 21, Loss: 0.5882430672645569\n",
      "Epoch 12, Iteration 22, Loss: 0.5802764296531677\n",
      "Epoch 12, Iteration 23, Loss: 0.7259061932563782\n",
      "Epoch 12, Iteration 24, Loss: 0.6018282771110535\n",
      "Epoch 12, Iteration 25, Loss: 0.6287326216697693\n",
      "Epoch 12, Iteration 26, Loss: 0.8585256934165955\n",
      "Epoch 12, Iteration 27, Loss: 0.6512879729270935\n",
      "Epoch 12, Iteration 28, Loss: 0.7434072494506836\n",
      "Epoch 12, Iteration 29, Loss: 0.9457018971443176\n",
      "Epoch 12, Iteration 30, Loss: 0.6782987117767334\n",
      "Epoch 12, Iteration 31, Loss: 0.6574193239212036\n",
      "Epoch 12, Iteration 32, Loss: 0.6569512486457825\n",
      "Epoch 12, Iteration 33, Loss: 0.7287991642951965\n",
      "Epoch 12, Iteration 34, Loss: 0.6434160470962524\n",
      "Epoch 12, Iteration 35, Loss: 0.7559475302696228\n",
      "Epoch 12, Iteration 36, Loss: 0.520266592502594\n",
      "Epoch 12, Iteration 37, Loss: 0.5748946666717529\n",
      "Epoch 12, Iteration 38, Loss: 0.5849885940551758\n",
      "Epoch 12, Iteration 39, Loss: 0.68671053647995\n",
      "Epoch 12, Iteration 40, Loss: 0.6592679023742676\n",
      "Epoch 12, Iteration 41, Loss: 0.8109171986579895\n",
      "Epoch 12, Iteration 42, Loss: 0.6601985692977905\n",
      "Epoch 12, Iteration 43, Loss: 0.6227901577949524\n",
      "Epoch 12, Iteration 44, Loss: 0.6272566914558411\n",
      "Epoch 12, Iteration 45, Loss: 0.6960394978523254\n",
      "Epoch 12, Iteration 46, Loss: 0.6591695547103882\n",
      "Epoch 12, Iteration 47, Loss: 0.6766395568847656\n",
      "Epoch 12, Iteration 48, Loss: 0.816263735294342\n",
      "Epoch 12, Iteration 49, Loss: 0.6474166512489319\n",
      "Epoch 12, Iteration 50, Loss: 0.6094987392425537\n",
      "Epoch 12, Iteration 50, Valid Loss: 0.563954770565033\n",
      "Epoch 12, Iteration 51, Loss: 0.6565244197845459\n",
      "Epoch 12, Iteration 52, Loss: 0.5281943082809448\n",
      "Epoch 12, Iteration 53, Loss: 0.7587250471115112\n",
      "Epoch 12, Iteration 54, Loss: 0.780896782875061\n",
      "Epoch 12, Iteration 55, Loss: 0.7491088509559631\n",
      "Epoch 12, Iteration 56, Loss: 0.7986566424369812\n",
      "Epoch 12, Iteration 57, Loss: 0.9435210824012756\n",
      "Epoch 12, Iteration 58, Loss: 0.6520014405250549\n",
      "Epoch 12, Iteration 59, Loss: 0.7157620787620544\n",
      "Epoch 12, Iteration 60, Loss: 0.6735484004020691\n",
      "Epoch 12, Iteration 61, Loss: 0.5055965185165405\n",
      "Epoch 12, Iteration 62, Loss: 0.7518361806869507\n",
      "Epoch 12, Iteration 63, Loss: 0.5689620971679688\n",
      "Epoch 12, Iteration 64, Loss: 0.8421869874000549\n",
      "Epoch 12, Iteration 65, Loss: 0.8406164646148682\n",
      "Epoch 12, Iteration 66, Loss: 0.6962373852729797\n",
      "Epoch 12, Iteration 67, Loss: 0.571485698223114\n",
      "Epoch 12, Iteration 68, Loss: 0.899570643901825\n",
      "Epoch 12, Iteration 69, Loss: 0.6802021265029907\n",
      "Epoch 12, Iteration 70, Loss: 0.6841991543769836\n",
      "Epoch 12, Iteration 71, Loss: 0.6993293762207031\n",
      "Epoch 12, Iteration 72, Loss: 0.6266782283782959\n",
      "Epoch 12, Iteration 73, Loss: 0.6834997534751892\n",
      "Epoch 12, Iteration 74, Loss: 0.8830878734588623\n",
      "Epoch 12, Iteration 75, Loss: 0.6842490434646606\n",
      "Epoch 12, Iteration 76, Loss: 0.6975783705711365\n",
      "Epoch 12, Iteration 77, Loss: 0.6040670871734619\n",
      "Epoch 12, Iteration 78, Loss: 0.8781250715255737\n",
      "Epoch 12, Iteration 79, Loss: 0.9807980060577393\n",
      "Epoch 12, Iteration 80, Loss: 0.5059412717819214\n",
      "Epoch 12, Iteration 81, Loss: 0.8432604670524597\n",
      "Epoch 12, Iteration 82, Loss: 0.7260284423828125\n",
      "Epoch 12, Iteration 83, Loss: 0.6457304954528809\n",
      "Epoch 12, Iteration 84, Loss: 0.7079273462295532\n",
      "Epoch 12, Iteration 85, Loss: 0.7358682751655579\n",
      "Epoch 12, Iteration 86, Loss: 0.728297770023346\n",
      "Epoch 12, Iteration 87, Loss: 0.6911118030548096\n",
      "Epoch 12, Iteration 88, Loss: 0.8331302404403687\n",
      "Epoch 12, Iteration 89, Loss: 0.7691755294799805\n",
      "Epoch 12, Iteration 90, Loss: 0.571700930595398\n",
      "Epoch 12, Iteration 91, Loss: 0.6095790863037109\n",
      "Epoch 12, Iteration 92, Loss: 0.5775955319404602\n",
      "Epoch 12, Iteration 93, Loss: 0.6707546710968018\n",
      "Epoch 12, Iteration 94, Loss: 0.796422004699707\n",
      "Epoch 12, Iteration 95, Loss: 0.5224534869194031\n",
      "Epoch 12, Iteration 96, Loss: 0.708753228187561\n",
      "Epoch 12, Iteration 97, Loss: 0.5089064240455627\n",
      "Epoch 12, Iteration 98, Loss: 0.6705639958381653\n",
      "Epoch 12, Iteration 99, Loss: 0.6046554446220398\n",
      "Epoch 12, Iteration 100, Loss: 0.6060697436332703\n",
      "Epoch 12, Iteration 100, Valid Loss: 0.6063714027404785\n",
      "Epoch 12, Iteration 101, Loss: 0.8856608867645264\n",
      "Epoch 12, Iteration 102, Loss: 0.748146116733551\n",
      "Epoch 12, Iteration 103, Loss: 0.6142606735229492\n",
      "Epoch 12, Iteration 104, Loss: 0.8580472469329834\n",
      "Epoch 12, Iteration 105, Loss: 0.678429365158081\n",
      "Epoch 12, Iteration 106, Loss: 0.6253702044487\n",
      "Epoch 12, Iteration 107, Loss: 0.7625164985656738\n",
      "Epoch 12, Iteration 108, Loss: 0.6941782236099243\n",
      "Epoch 12, Iteration 109, Loss: 0.8000922799110413\n",
      "Epoch 12, Iteration 110, Loss: 0.997535228729248\n",
      "Epoch 12, Iteration 111, Loss: 0.7261545062065125\n",
      "Epoch 12, Iteration 112, Loss: 0.6955594420433044\n",
      "Epoch 12, Iteration 113, Loss: 0.6432717442512512\n",
      "Epoch 12, Iteration 114, Loss: 0.8935322165489197\n",
      "Epoch 12, Iteration 115, Loss: 0.8152012825012207\n",
      "Epoch 12, Iteration 116, Loss: 0.565895676612854\n",
      "Epoch 12, Iteration 117, Loss: 0.4386434555053711\n",
      "Epoch 12, Iteration 118, Loss: 0.7434420585632324\n",
      "Epoch 12, Iteration 119, Loss: 0.6413832902908325\n",
      "Epoch 12, Iteration 120, Loss: 0.969093382358551\n",
      "Epoch 12, Iteration 121, Loss: 0.6549428105354309\n",
      "Epoch 12, Iteration 122, Loss: 0.7277224659919739\n",
      "Epoch 12, Iteration 123, Loss: 0.6684926152229309\n",
      "Epoch 12, Iteration 124, Loss: 0.5910991430282593\n",
      "Epoch 12, Iteration 125, Loss: 0.705163300037384\n",
      "Epoch 12, Iteration 126, Loss: 0.5926144123077393\n",
      "Epoch 12, Iteration 127, Loss: 0.7386837005615234\n",
      "Epoch 12, Iteration 128, Loss: 0.7567336559295654\n",
      "Epoch 12, Iteration 129, Loss: 0.7389841675758362\n",
      "Epoch 12, Iteration 130, Loss: 0.8871852159500122\n",
      "Epoch 12, Iteration 131, Loss: 0.6655809879302979\n",
      "Epoch 12, Iteration 132, Loss: 0.5964688658714294\n",
      "Epoch 12, Iteration 133, Loss: 0.9687548279762268\n",
      "Epoch 12, Iteration 134, Loss: 0.9597373604774475\n",
      "Epoch 12, Iteration 135, Loss: 0.8521321415901184\n",
      "Epoch 12, Iteration 136, Loss: 0.6660590767860413\n",
      "Epoch 12, Iteration 137, Loss: 0.6042613983154297\n",
      "Epoch 12, Iteration 138, Loss: 0.5834081172943115\n",
      "Epoch 12, Iteration 139, Loss: 0.6738053560256958\n",
      "Epoch 12, Iteration 140, Loss: 0.5924054384231567\n",
      "Epoch 12, Iteration 141, Loss: 1.0954848527908325\n",
      "Epoch 12, Iteration 142, Loss: 0.6713139414787292\n",
      "Epoch 12, Iteration 143, Loss: 0.7075262665748596\n",
      "Epoch 12, Iteration 144, Loss: 0.6683007478713989\n",
      "Epoch 12, Iteration 145, Loss: 0.7556949853897095\n",
      "Epoch 12, Iteration 146, Loss: 0.7553350329399109\n",
      "Epoch 12, Iteration 147, Loss: 0.6976659893989563\n",
      "Epoch 12, Iteration 148, Loss: 0.5887616872787476\n",
      "Epoch 12, Iteration 149, Loss: 0.7087492942810059\n",
      "Epoch 12, Iteration 150, Loss: 0.7918977737426758\n",
      "Epoch 12, Iteration 150, Valid Loss: 0.5703864097595215\n",
      "Epoch 12, Iteration 151, Loss: 0.7726421356201172\n",
      "Epoch 12, Iteration 152, Loss: 0.5964515805244446\n",
      "Epoch 12, Iteration 153, Loss: 0.7313245534896851\n",
      "Epoch 12, Iteration 154, Loss: 0.48841890692710876\n",
      "Epoch 12, Iteration 155, Loss: 0.7229663133621216\n",
      "Epoch 12, Iteration 156, Loss: 0.6014457941055298\n",
      "Epoch 12, Iteration 157, Loss: 1.0660055875778198\n",
      "Epoch 12, Iteration 158, Loss: 0.6798323392868042\n",
      "Epoch 12, Iteration 159, Loss: 0.7526645064353943\n",
      "Epoch 12, Iteration 160, Loss: 0.6885420083999634\n",
      "Epoch 12, Iteration 161, Loss: 0.686445951461792\n",
      "Epoch 12, Iteration 162, Loss: 0.8842900395393372\n",
      "Epoch 12, Iteration 163, Loss: 0.58762526512146\n",
      "Epoch 12, Iteration 164, Loss: 0.6145221590995789\n",
      "Epoch 12, Iteration 165, Loss: 0.7226593494415283\n",
      "Epoch 12, Iteration 166, Loss: 0.8252316117286682\n",
      "Epoch 12, Iteration 167, Loss: 0.8111972808837891\n",
      "Epoch 12, Iteration 168, Loss: 0.706730306148529\n",
      "Epoch 12, Iteration 169, Loss: 0.650056004524231\n",
      "Epoch 12, Iteration 170, Loss: 0.7392907738685608\n",
      "Epoch 12, Iteration 171, Loss: 0.796772301197052\n",
      "Epoch 12, Iteration 172, Loss: 0.7585059404373169\n",
      "Epoch 12, Iteration 173, Loss: 0.6386265158653259\n",
      "Epoch 12, Iteration 174, Loss: 0.7307277321815491\n",
      "Epoch 12, Iteration 175, Loss: 0.7716832160949707\n",
      "Epoch 12, Iteration 176, Loss: 0.4602678418159485\n",
      "Epoch 12, Iteration 177, Loss: 0.9706681370735168\n",
      "Epoch 12, Iteration 178, Loss: 0.8278900384902954\n",
      "Epoch 12, Iteration 179, Loss: 0.676292896270752\n",
      "Epoch 12, Iteration 180, Loss: 0.7914326786994934\n",
      "Epoch 12, Iteration 181, Loss: 0.7538164854049683\n",
      "Epoch 12, Iteration 182, Loss: 0.7207825183868408\n",
      "Epoch 12, Iteration 183, Loss: 0.6469002962112427\n",
      "Epoch 12, Iteration 184, Loss: 0.6337469220161438\n",
      "Epoch 12, Iteration 185, Loss: 0.5959925651550293\n",
      "Epoch 12, Iteration 186, Loss: 0.7088250517845154\n",
      "Epoch 12, Iteration 187, Loss: 0.6467636227607727\n",
      "Epoch 12, Iteration 188, Loss: 0.5925607681274414\n",
      "Epoch 12, Iteration 189, Loss: 0.5739235877990723\n",
      "Epoch 12, Iteration 190, Loss: 0.7374098300933838\n",
      "Epoch 12, Iteration 191, Loss: 0.6971942782402039\n",
      "Epoch 12, Iteration 192, Loss: 0.7413206696510315\n",
      "Epoch 12, Iteration 193, Loss: 0.6942657232284546\n",
      "Epoch 12, Iteration 194, Loss: 0.6878374814987183\n",
      "Epoch 12, Iteration 195, Loss: 0.7424432039260864\n",
      "Epoch 12, Iteration 196, Loss: 0.6616921424865723\n",
      "Epoch 12, Iteration 197, Loss: 0.6250096559524536\n",
      "Epoch 12, Iteration 198, Loss: 0.6376633644104004\n",
      "Epoch 12, Iteration 199, Loss: 0.6936150193214417\n",
      "Epoch 12, Iteration 200, Loss: 0.6370465755462646\n",
      "Epoch 12, Iteration 200, Valid Loss: 0.5690305829048157\n",
      "Epoch 12, Iteration 201, Loss: 0.6837143301963806\n",
      "Epoch 12, Iteration 202, Loss: 0.7606225609779358\n",
      "Epoch 12, Iteration 203, Loss: 0.5787481069564819\n",
      "Epoch 12, Iteration 204, Loss: 0.6492983102798462\n",
      "Epoch 12, Iteration 205, Loss: 0.4708752930164337\n",
      "Epoch 12, Iteration 206, Loss: 0.5557031035423279\n",
      "Epoch 12, Iteration 207, Loss: 0.8978127241134644\n",
      "Epoch 12, Iteration 208, Loss: 0.999801754951477\n",
      "Epoch 12, Iteration 209, Loss: 0.8413430452346802\n",
      "Epoch 12, Iteration 210, Loss: 0.9724841117858887\n",
      "Epoch 12, Iteration 211, Loss: 1.0207953453063965\n",
      "Epoch 12, Iteration 212, Loss: 0.789883017539978\n",
      "Epoch 12, Iteration 213, Loss: 0.9093247652053833\n",
      "Epoch 12, Iteration 214, Loss: 0.7432177662849426\n",
      "Epoch 12, Iteration 215, Loss: 0.5946078300476074\n",
      "Epoch 12, Iteration 216, Loss: 0.5530593991279602\n",
      "Epoch 12, Iteration 217, Loss: 0.577680230140686\n",
      "Epoch 12, Iteration 218, Loss: 0.6331996321678162\n",
      "Epoch 12, Iteration 219, Loss: 0.7227334976196289\n",
      "Epoch 12, Iteration 220, Loss: 0.606448233127594\n",
      "Epoch 12, Iteration 221, Loss: 0.628558337688446\n",
      "Epoch 12, Iteration 222, Loss: 0.6845555901527405\n",
      "Epoch 12, Iteration 223, Loss: 0.6538602709770203\n",
      "Epoch 12, Iteration 224, Loss: 0.5873462557792664\n",
      "Epoch 12, Iteration 225, Loss: 0.7254934310913086\n",
      "Epoch 12, Iteration 226, Loss: 0.7594764828681946\n",
      "Epoch 12, Iteration 227, Loss: 0.7750918865203857\n",
      "Epoch 12, Iteration 228, Loss: 0.715153694152832\n",
      "Epoch 12, Iteration 229, Loss: 0.667525589466095\n",
      "Epoch 12, Iteration 230, Loss: 0.7703633308410645\n",
      "Epoch 12, Iteration 231, Loss: 0.6601042747497559\n",
      "Epoch 12, Iteration 232, Loss: 0.7759727239608765\n",
      "Epoch 12, Iteration 233, Loss: 0.9126551151275635\n",
      "Epoch 12, Iteration 234, Loss: 0.7199811339378357\n",
      "Epoch 12, Iteration 235, Loss: 0.5557383298873901\n",
      "Epoch 12, Iteration 236, Loss: 0.734442949295044\n",
      "Epoch 12, Iteration 237, Loss: 0.6448183059692383\n",
      "Epoch 12, Iteration 238, Loss: 0.5656598806381226\n",
      "Epoch 12, Iteration 239, Loss: 0.8552877902984619\n",
      "Epoch 12, Iteration 240, Loss: 0.6980603337287903\n",
      "Epoch 12, Iteration 241, Loss: 0.6050447225570679\n",
      "Epoch 12, Iteration 242, Loss: 0.5921297073364258\n",
      "Epoch 12, Iteration 243, Loss: 0.6841475963592529\n",
      "Epoch 12, Iteration 244, Loss: 0.6367369890213013\n",
      "Epoch 12, Iteration 245, Loss: 0.5076414346694946\n",
      "Epoch 12, Iteration 246, Loss: 0.6375519037246704\n",
      "Epoch 12, Iteration 247, Loss: 0.6253756880760193\n",
      "Epoch 12, Iteration 248, Loss: 0.7796540856361389\n",
      "Epoch 12, Iteration 249, Loss: 0.5495964884757996\n",
      "Epoch 12, Iteration 250, Loss: 0.8081930875778198\n",
      "Epoch 12, Iteration 250, Valid Loss: 0.5570864677429199\n",
      "Epoch 12, Iteration 251, Loss: 0.6231285929679871\n",
      "Epoch 12, Iteration 252, Loss: 0.8807200193405151\n",
      "Epoch 12, Iteration 253, Loss: 0.7529283165931702\n",
      "Epoch 12, Iteration 254, Loss: 0.7224769592285156\n",
      "Epoch 12, Iteration 255, Loss: 0.7146294713020325\n",
      "Epoch 12, Iteration 256, Loss: 0.8091220259666443\n",
      "Epoch 12, Iteration 257, Loss: 0.5838637351989746\n",
      "Epoch 12, Iteration 258, Loss: 0.8697358965873718\n",
      "Epoch 12, Iteration 259, Loss: 0.5803751945495605\n",
      "Epoch 12, Iteration 260, Loss: 0.8070374727249146\n",
      "Epoch 12, Iteration 261, Loss: 0.6652547121047974\n",
      "Epoch 12, Iteration 262, Loss: 0.6954692602157593\n",
      "Epoch 12, Iteration 263, Loss: 0.8529732823371887\n",
      "Epoch 12, Iteration 264, Loss: 0.6528290510177612\n",
      "Epoch 12, Iteration 265, Loss: 0.7455966472625732\n",
      "Epoch 12, Iteration 266, Loss: 0.7429371476173401\n",
      "Epoch 12, Iteration 267, Loss: 0.6317426562309265\n",
      "Epoch 12, Iteration 268, Loss: 0.612075686454773\n",
      "Epoch 12, Iteration 269, Loss: 0.6068467497825623\n",
      "Epoch 12, Iteration 270, Loss: 0.6601804494857788\n",
      "Epoch 12, Iteration 271, Loss: 0.6480994820594788\n",
      "Epoch 12, Iteration 272, Loss: 0.6568427681922913\n",
      "Epoch 12, Iteration 273, Loss: 0.7169361710548401\n",
      "Epoch 12, Iteration 274, Loss: 0.6717016100883484\n",
      "Epoch 12, Iteration 275, Loss: 0.685418426990509\n",
      "Epoch 12, Iteration 276, Loss: 0.6204932332038879\n",
      "Epoch 12, Iteration 277, Loss: 0.6174273490905762\n",
      "Epoch 12, Iteration 278, Loss: 0.8457092046737671\n",
      "Epoch 12, Iteration 279, Loss: 0.6185529232025146\n",
      "Epoch 12, Iteration 280, Loss: 0.5341119170188904\n",
      "Epoch 12, Iteration 281, Loss: 0.6238279342651367\n",
      "Epoch 12, Iteration 282, Loss: 0.5895655155181885\n",
      "Epoch 12, Iteration 283, Loss: 0.6020956635475159\n",
      "Epoch 12, Iteration 284, Loss: 0.6760523319244385\n",
      "Epoch 12, Iteration 285, Loss: 0.5873041152954102\n",
      "Epoch 12, Iteration 286, Loss: 0.650956928730011\n",
      "Epoch 12, Iteration 287, Loss: 0.6200615763664246\n",
      "Epoch 12, Iteration 288, Loss: 0.5651206970214844\n",
      "Epoch 12, Iteration 289, Loss: 0.5838401913642883\n",
      "Epoch 12, Iteration 290, Loss: 0.6087412238121033\n",
      "Epoch 12, Iteration 291, Loss: 0.4839743673801422\n",
      "Epoch 12, Iteration 292, Loss: 0.7084386348724365\n",
      "Epoch 12, Iteration 293, Loss: 0.8436455726623535\n",
      "Epoch 12, Iteration 294, Loss: 0.5915015339851379\n",
      "Epoch 12, Iteration 295, Loss: 0.5725932717323303\n",
      "Epoch 12, Iteration 296, Loss: 0.7548174262046814\n",
      "Epoch 12, Iteration 297, Loss: 0.7328206896781921\n",
      "Epoch 12, Iteration 298, Loss: 0.7239324450492859\n",
      "Epoch 12, Iteration 299, Loss: 0.6120402216911316\n",
      "Epoch 12, Iteration 300, Loss: 0.5409374833106995\n",
      "Epoch 12, Iteration 300, Valid Loss: 0.5632458329200745\n",
      "Epoch 12, Iteration 301, Loss: 0.621661901473999\n",
      "Epoch 12, Iteration 302, Loss: 0.71905118227005\n",
      "Epoch 12, Iteration 303, Loss: 0.7211012840270996\n",
      "Epoch 12, Iteration 304, Loss: 0.688876748085022\n",
      "Epoch 12, Iteration 305, Loss: 0.6217105388641357\n",
      "Epoch 12, Iteration 306, Loss: 0.6494845151901245\n",
      "Epoch 12, Iteration 307, Loss: 0.5677961707115173\n",
      "Epoch 12, Iteration 308, Loss: 0.6185854077339172\n",
      "Epoch 12, Iteration 309, Loss: 0.6474722623825073\n",
      "Epoch 12, Iteration 310, Loss: 0.7559255957603455\n",
      "Epoch 12, Iteration 311, Loss: 0.6249098777770996\n",
      "Epoch 12, Iteration 312, Loss: 0.5213160514831543\n",
      "Epoch 12, Iteration 313, Loss: 0.7171804308891296\n",
      "Epoch 12, Iteration 314, Loss: 0.5642819404602051\n",
      "Epoch 12, Iteration 315, Loss: 0.5938356518745422\n",
      "Epoch 12, Iteration 316, Loss: 0.8674930334091187\n",
      "Epoch 12, Iteration 317, Loss: 0.6228688359260559\n",
      "Epoch 12, Iteration 318, Loss: 0.6613593101501465\n",
      "Epoch 12, Iteration 319, Loss: 0.7291938662528992\n",
      "Epoch 12, Iteration 320, Loss: 0.48885679244995117\n",
      "Epoch 12, Iteration 321, Loss: 0.6275345683097839\n",
      "Epoch 12, Iteration 322, Loss: 0.6997523903846741\n",
      "Epoch 12, Iteration 323, Loss: 0.7474322319030762\n",
      "Epoch 12, Iteration 324, Loss: 0.7203731536865234\n",
      "Epoch 12, Iteration 325, Loss: 0.6890019774436951\n",
      "Epoch 12, Iteration 326, Loss: 0.5753498077392578\n",
      "Epoch 12, Iteration 327, Loss: 0.5635858178138733\n",
      "Epoch 12, Iteration 328, Loss: 0.6164000630378723\n",
      "Epoch 12, Iteration 329, Loss: 0.7439813613891602\n",
      "Epoch 12, Iteration 330, Loss: 0.660763144493103\n",
      "Epoch 12, Iteration 331, Loss: 0.6519826054573059\n",
      "Epoch 12, Iteration 332, Loss: 0.6265028715133667\n",
      "Epoch 12, Iteration 333, Loss: 0.8475507497787476\n",
      "Epoch 12, Iteration 334, Loss: 0.6307401657104492\n",
      "Epoch 12, Iteration 335, Loss: 0.6913691163063049\n",
      "Epoch 12, Iteration 336, Loss: 0.696003258228302\n",
      "Epoch 12, Iteration 337, Loss: 0.5770631432533264\n",
      "Epoch 12, Iteration 338, Loss: 0.7392356991767883\n",
      "Epoch 12, Iteration 339, Loss: 0.649958610534668\n",
      "Epoch 12, Iteration 340, Loss: 0.5495017766952515\n",
      "Epoch 12, Iteration 341, Loss: 0.6343424916267395\n",
      "Epoch 12, Iteration 342, Loss: 0.5970922708511353\n",
      "Epoch 12, Iteration 343, Loss: 0.5644674897193909\n",
      "Epoch 12, Iteration 344, Loss: 0.6338986754417419\n",
      "Epoch 12, Iteration 345, Loss: 0.6702957153320312\n",
      "Epoch 12, Iteration 346, Loss: 0.6458607912063599\n",
      "Epoch 12, Iteration 347, Loss: 0.47897201776504517\n",
      "Epoch 12, Iteration 348, Loss: 0.606807291507721\n",
      "Epoch 12, Iteration 349, Loss: 0.6542986035346985\n",
      "Epoch 12, Iteration 350, Loss: 0.592146635055542\n",
      "Epoch 12, Iteration 350, Valid Loss: 0.5827816724777222\n",
      "Epoch 12, Iteration 351, Loss: 0.6612714529037476\n",
      "Epoch 12, Iteration 352, Loss: 0.5448667407035828\n",
      "Epoch 12, Iteration 353, Loss: 0.670574426651001\n",
      "Epoch 12, Iteration 354, Loss: 0.5367546677589417\n",
      "Epoch 12, Iteration 355, Loss: 0.5937746167182922\n",
      "Epoch 12, Iteration 356, Loss: 0.7581662535667419\n",
      "Epoch 12, Iteration 357, Loss: 0.5133867859840393\n",
      "Epoch 12, Iteration 358, Loss: 0.5833415389060974\n",
      "Epoch 12, Iteration 359, Loss: 0.6494256258010864\n",
      "Epoch 12, Iteration 360, Loss: 0.5523288249969482\n",
      "Epoch 12, Iteration 361, Loss: 0.5583425164222717\n",
      "Epoch 12, Iteration 362, Loss: 0.6068685054779053\n",
      "Epoch 12, Iteration 363, Loss: 0.5048115849494934\n",
      "Epoch 12, Iteration 364, Loss: 0.6159924864768982\n",
      "Epoch 12, Iteration 365, Loss: 0.7303284406661987\n",
      "Epoch 12, Iteration 366, Loss: 0.6977646350860596\n",
      "Epoch 12, Iteration 367, Loss: 0.6605126857757568\n",
      "Epoch 12, Iteration 368, Loss: 0.6227686405181885\n",
      "Epoch 12, Iteration 369, Loss: 0.673995852470398\n",
      "Epoch 12, Iteration 370, Loss: 0.6440722942352295\n",
      "Epoch 12, Iteration 371, Loss: 0.6991247534751892\n",
      "Epoch 12, Iteration 372, Loss: 0.5909242630004883\n",
      "Epoch 12, Iteration 373, Loss: 0.6207037568092346\n",
      "Epoch 12, Iteration 374, Loss: 0.8465309143066406\n",
      "Epoch 12, Iteration 375, Loss: 0.5937764048576355\n",
      "Epoch 12, Iteration 376, Loss: 0.7147645950317383\n",
      "Epoch 12, Iteration 377, Loss: 0.5935096740722656\n",
      "Epoch 12, Iteration 378, Loss: 0.598800003528595\n",
      "Epoch 12, Iteration 379, Loss: 0.529411256313324\n",
      "Epoch 12, Iteration 380, Loss: 0.5322306156158447\n",
      "Epoch 12, Iteration 381, Loss: 0.5625734329223633\n",
      "Epoch 12, Iteration 382, Loss: 0.4410038888454437\n",
      "Epoch 12, Iteration 383, Loss: 0.592435359954834\n",
      "Epoch 12, Iteration 384, Loss: 0.6555529236793518\n",
      "Epoch 12, Iteration 385, Loss: 0.5720872282981873\n",
      "Epoch 12, Iteration 386, Loss: 0.5232383608818054\n",
      "Epoch 12, Iteration 387, Loss: 0.9207006692886353\n",
      "Epoch 12, Iteration 388, Loss: 0.6442392468452454\n",
      "Epoch 12, Iteration 389, Loss: 0.6525067090988159\n",
      "Epoch 12, Iteration 390, Loss: 0.8506009578704834\n",
      "Epoch 12, Iteration 391, Loss: 0.5534098148345947\n",
      "Epoch 13/15, Loss: 0.684108627541941\n",
      "Epoch 13, Iteration 0, Loss: 0.7541003823280334\n",
      "Epoch 13, Iteration 1, Loss: 0.783478856086731\n",
      "Epoch 13, Iteration 2, Loss: 0.7238490581512451\n",
      "Epoch 13, Iteration 3, Loss: 0.7502067685127258\n",
      "Epoch 13, Iteration 4, Loss: 0.6422494649887085\n",
      "Epoch 13, Iteration 5, Loss: 0.67867511510849\n",
      "Epoch 13, Iteration 6, Loss: 0.5702010989189148\n",
      "Epoch 13, Iteration 7, Loss: 0.6090347766876221\n",
      "Epoch 13, Iteration 8, Loss: 0.6087172031402588\n",
      "Epoch 13, Iteration 9, Loss: 0.7656405568122864\n",
      "Epoch 13, Iteration 10, Loss: 0.6494601964950562\n",
      "Epoch 13, Iteration 11, Loss: 0.6146577000617981\n",
      "Epoch 13, Iteration 12, Loss: 0.5765826106071472\n",
      "Epoch 13, Iteration 13, Loss: 0.6959236860275269\n",
      "Epoch 13, Iteration 14, Loss: 0.6492409706115723\n",
      "Epoch 13, Iteration 15, Loss: 0.8547345995903015\n",
      "Epoch 13, Iteration 16, Loss: 0.6393290162086487\n",
      "Epoch 13, Iteration 17, Loss: 0.6324170231819153\n",
      "Epoch 13, Iteration 18, Loss: 0.5769569277763367\n",
      "Epoch 13, Iteration 19, Loss: 0.7111883759498596\n",
      "Epoch 13, Iteration 20, Loss: 0.5435076355934143\n",
      "Epoch 13, Iteration 21, Loss: 0.5327888131141663\n",
      "Epoch 13, Iteration 22, Loss: 0.6851834654808044\n",
      "Epoch 13, Iteration 23, Loss: 0.7486628890037537\n",
      "Epoch 13, Iteration 24, Loss: 0.5313237309455872\n",
      "Epoch 13, Iteration 25, Loss: 0.5879163146018982\n",
      "Epoch 13, Iteration 26, Loss: 0.8286535739898682\n",
      "Epoch 13, Iteration 27, Loss: 0.721372663974762\n",
      "Epoch 13, Iteration 28, Loss: 0.6661770939826965\n",
      "Epoch 13, Iteration 29, Loss: 0.9194177985191345\n",
      "Epoch 13, Iteration 30, Loss: 0.6865109801292419\n",
      "Epoch 13, Iteration 31, Loss: 0.6843438744544983\n",
      "Epoch 13, Iteration 32, Loss: 0.7301740646362305\n",
      "Epoch 13, Iteration 33, Loss: 0.7438278794288635\n",
      "Epoch 13, Iteration 34, Loss: 0.6451728940010071\n",
      "Epoch 13, Iteration 35, Loss: 0.7180076837539673\n",
      "Epoch 13, Iteration 36, Loss: 0.6137770414352417\n",
      "Epoch 13, Iteration 37, Loss: 0.5490659475326538\n",
      "Epoch 13, Iteration 38, Loss: 0.599900484085083\n",
      "Epoch 13, Iteration 39, Loss: 0.6683241724967957\n",
      "Epoch 13, Iteration 40, Loss: 0.6499337553977966\n",
      "Epoch 13, Iteration 41, Loss: 0.6423831582069397\n",
      "Epoch 13, Iteration 42, Loss: 0.6735366582870483\n",
      "Epoch 13, Iteration 43, Loss: 0.6329287886619568\n",
      "Epoch 13, Iteration 44, Loss: 0.6158536672592163\n",
      "Epoch 13, Iteration 45, Loss: 0.6671175956726074\n",
      "Epoch 13, Iteration 46, Loss: 0.7408674955368042\n",
      "Epoch 13, Iteration 47, Loss: 0.6664164066314697\n",
      "Epoch 13, Iteration 48, Loss: 0.7449133992195129\n",
      "Epoch 13, Iteration 49, Loss: 0.6372542381286621\n",
      "Epoch 13, Iteration 50, Loss: 0.5818886756896973\n",
      "Epoch 13, Iteration 50, Valid Loss: 0.5560964345932007\n",
      "Epoch 13, Iteration 51, Loss: 0.6562507152557373\n",
      "Epoch 13, Iteration 52, Loss: 0.526974618434906\n",
      "Epoch 13, Iteration 53, Loss: 0.7882638573646545\n",
      "Epoch 13, Iteration 54, Loss: 0.7874255180358887\n",
      "Epoch 13, Iteration 55, Loss: 0.7323215007781982\n",
      "Epoch 13, Iteration 56, Loss: 0.6586768627166748\n",
      "Epoch 13, Iteration 57, Loss: 0.868309736251831\n",
      "Epoch 13, Iteration 58, Loss: 0.6994621157646179\n",
      "Epoch 13, Iteration 59, Loss: 0.5581678152084351\n",
      "Epoch 13, Iteration 60, Loss: 0.7153843641281128\n",
      "Epoch 13, Iteration 61, Loss: 0.4522961974143982\n",
      "Epoch 13, Iteration 62, Loss: 0.5858079195022583\n",
      "Epoch 13, Iteration 63, Loss: 0.5750867128372192\n",
      "Epoch 13, Iteration 64, Loss: 0.7118102312088013\n",
      "Epoch 13, Iteration 65, Loss: 0.9629397988319397\n",
      "Epoch 13, Iteration 66, Loss: 0.6497518420219421\n",
      "Epoch 13, Iteration 67, Loss: 0.5441535115242004\n",
      "Epoch 13, Iteration 68, Loss: 0.7706834673881531\n",
      "Epoch 13, Iteration 69, Loss: 0.7151171565055847\n",
      "Epoch 13, Iteration 70, Loss: 0.59974205493927\n",
      "Epoch 13, Iteration 71, Loss: 0.7497006058692932\n",
      "Epoch 13, Iteration 72, Loss: 0.6522829532623291\n",
      "Epoch 13, Iteration 73, Loss: 0.6292251348495483\n",
      "Epoch 13, Iteration 74, Loss: 0.9844247102737427\n",
      "Epoch 13, Iteration 75, Loss: 0.7954590916633606\n",
      "Epoch 13, Iteration 76, Loss: 0.6947998404502869\n",
      "Epoch 13, Iteration 77, Loss: 0.5397679805755615\n",
      "Epoch 13, Iteration 78, Loss: 0.7122768759727478\n",
      "Epoch 13, Iteration 79, Loss: 0.8440039753913879\n",
      "Epoch 13, Iteration 80, Loss: 0.6008031964302063\n",
      "Epoch 13, Iteration 81, Loss: 0.787492573261261\n",
      "Epoch 13, Iteration 82, Loss: 0.7382606267929077\n",
      "Epoch 13, Iteration 83, Loss: 0.565271258354187\n",
      "Epoch 13, Iteration 84, Loss: 0.8115630745887756\n",
      "Epoch 13, Iteration 85, Loss: 0.6607409715652466\n",
      "Epoch 13, Iteration 86, Loss: 0.7663052678108215\n",
      "Epoch 13, Iteration 87, Loss: 0.7205507755279541\n",
      "Epoch 13, Iteration 88, Loss: 0.6648988723754883\n",
      "Epoch 13, Iteration 89, Loss: 0.7992281317710876\n",
      "Epoch 13, Iteration 90, Loss: 0.7071175575256348\n",
      "Epoch 13, Iteration 91, Loss: 0.6645991206169128\n",
      "Epoch 13, Iteration 92, Loss: 0.5037364363670349\n",
      "Epoch 13, Iteration 93, Loss: 0.6623037457466125\n",
      "Epoch 13, Iteration 94, Loss: 0.7610778212547302\n",
      "Epoch 13, Iteration 95, Loss: 0.5692229270935059\n",
      "Epoch 13, Iteration 96, Loss: 0.6044310927391052\n",
      "Epoch 13, Iteration 97, Loss: 0.5181778073310852\n",
      "Epoch 13, Iteration 98, Loss: 0.6062214374542236\n",
      "Epoch 13, Iteration 99, Loss: 0.6324540376663208\n",
      "Epoch 13, Iteration 100, Loss: 0.6238957047462463\n",
      "Epoch 13, Iteration 100, Valid Loss: 0.6154080033302307\n",
      "Epoch 13, Iteration 101, Loss: 0.8368492722511292\n",
      "Epoch 13, Iteration 102, Loss: 0.829004168510437\n",
      "Epoch 13, Iteration 103, Loss: 0.6824560165405273\n",
      "Epoch 13, Iteration 104, Loss: 0.8424863219261169\n",
      "Epoch 13, Iteration 105, Loss: 0.5624220371246338\n",
      "Epoch 13, Iteration 106, Loss: 0.5815985202789307\n",
      "Epoch 13, Iteration 107, Loss: 0.8960512280464172\n",
      "Epoch 13, Iteration 108, Loss: 0.6542640328407288\n",
      "Epoch 13, Iteration 109, Loss: 0.7891736030578613\n",
      "Epoch 13, Iteration 110, Loss: 0.8442267775535583\n",
      "Epoch 13, Iteration 111, Loss: 0.8150606751441956\n",
      "Epoch 13, Iteration 112, Loss: 0.7490018606185913\n",
      "Epoch 13, Iteration 113, Loss: 0.6131221652030945\n",
      "Epoch 13, Iteration 114, Loss: 0.7513152956962585\n",
      "Epoch 13, Iteration 115, Loss: 0.8177233934402466\n",
      "Epoch 13, Iteration 116, Loss: 0.5306409597396851\n",
      "Epoch 13, Iteration 117, Loss: 0.44816723465919495\n",
      "Epoch 13, Iteration 118, Loss: 0.7066409587860107\n",
      "Epoch 13, Iteration 119, Loss: 0.6137055158615112\n",
      "Epoch 13, Iteration 120, Loss: 0.9079365730285645\n",
      "Epoch 13, Iteration 121, Loss: 0.5389054417610168\n",
      "Epoch 13, Iteration 122, Loss: 0.723271369934082\n",
      "Epoch 13, Iteration 123, Loss: 0.7076818346977234\n",
      "Epoch 13, Iteration 124, Loss: 0.5592305660247803\n",
      "Epoch 13, Iteration 125, Loss: 0.7922192215919495\n",
      "Epoch 13, Iteration 126, Loss: 0.5883603096008301\n",
      "Epoch 13, Iteration 127, Loss: 0.7508406043052673\n",
      "Epoch 13, Iteration 128, Loss: 0.6401393413543701\n",
      "Epoch 13, Iteration 129, Loss: 0.7533931732177734\n",
      "Epoch 13, Iteration 130, Loss: 0.8719950914382935\n",
      "Epoch 13, Iteration 131, Loss: 0.570587694644928\n",
      "Epoch 13, Iteration 132, Loss: 0.6623926758766174\n",
      "Epoch 13, Iteration 133, Loss: 1.0167325735092163\n",
      "Epoch 13, Iteration 134, Loss: 0.7704488039016724\n",
      "Epoch 13, Iteration 135, Loss: 0.7834851741790771\n",
      "Epoch 13, Iteration 136, Loss: 0.6425802111625671\n",
      "Epoch 13, Iteration 137, Loss: 0.6069326996803284\n",
      "Epoch 13, Iteration 138, Loss: 0.5687665343284607\n",
      "Epoch 13, Iteration 139, Loss: 0.5773062109947205\n",
      "Epoch 13, Iteration 140, Loss: 0.5783829092979431\n",
      "Epoch 13, Iteration 141, Loss: 0.8599048256874084\n",
      "Epoch 13, Iteration 142, Loss: 0.7549682855606079\n",
      "Epoch 13, Iteration 143, Loss: 0.6474561095237732\n",
      "Epoch 13, Iteration 144, Loss: 0.5916203260421753\n",
      "Epoch 13, Iteration 145, Loss: 0.7156282663345337\n",
      "Epoch 13, Iteration 146, Loss: 0.7230775952339172\n",
      "Epoch 13, Iteration 147, Loss: 0.7079474925994873\n",
      "Epoch 13, Iteration 148, Loss: 0.5656014680862427\n",
      "Epoch 13, Iteration 149, Loss: 0.6895380020141602\n",
      "Epoch 13, Iteration 150, Loss: 0.6728443503379822\n",
      "Epoch 13, Iteration 150, Valid Loss: 0.5873237252235413\n",
      "Epoch 13, Iteration 151, Loss: 0.6351728439331055\n",
      "Epoch 13, Iteration 152, Loss: 0.5728060603141785\n",
      "Epoch 13, Iteration 153, Loss: 0.775867223739624\n",
      "Epoch 13, Iteration 154, Loss: 0.4732632339000702\n",
      "Epoch 13, Iteration 155, Loss: 0.6163564324378967\n",
      "Epoch 13, Iteration 156, Loss: 0.4872061014175415\n",
      "Epoch 13, Iteration 157, Loss: 0.9976734519004822\n",
      "Epoch 13, Iteration 158, Loss: 0.6617039442062378\n",
      "Epoch 13, Iteration 159, Loss: 0.787868320941925\n",
      "Epoch 13, Iteration 160, Loss: 0.6608572006225586\n",
      "Epoch 13, Iteration 161, Loss: 0.6384433507919312\n",
      "Epoch 13, Iteration 162, Loss: 0.8547366261482239\n",
      "Epoch 13, Iteration 163, Loss: 0.5074565410614014\n",
      "Epoch 13, Iteration 164, Loss: 0.5984549522399902\n",
      "Epoch 13, Iteration 165, Loss: 0.6070508360862732\n",
      "Epoch 13, Iteration 166, Loss: 0.6343818306922913\n",
      "Epoch 13, Iteration 167, Loss: 0.732688307762146\n",
      "Epoch 13, Iteration 168, Loss: 0.6967794299125671\n",
      "Epoch 13, Iteration 169, Loss: 0.61832195520401\n",
      "Epoch 13, Iteration 170, Loss: 0.8238261342048645\n",
      "Epoch 13, Iteration 171, Loss: 0.8007268309593201\n",
      "Epoch 13, Iteration 172, Loss: 0.6817476153373718\n",
      "Epoch 13, Iteration 173, Loss: 0.6136518716812134\n",
      "Epoch 13, Iteration 174, Loss: 0.7078288197517395\n",
      "Epoch 13, Iteration 175, Loss: 0.7275047898292542\n",
      "Epoch 13, Iteration 176, Loss: 0.4744356572628021\n",
      "Epoch 13, Iteration 177, Loss: 1.0671250820159912\n",
      "Epoch 13, Iteration 178, Loss: 0.8809389472007751\n",
      "Epoch 13, Iteration 179, Loss: 0.6243717074394226\n",
      "Epoch 13, Iteration 180, Loss: 0.6775081157684326\n",
      "Epoch 13, Iteration 181, Loss: 0.7407517433166504\n",
      "Epoch 13, Iteration 182, Loss: 0.7046541571617126\n",
      "Epoch 13, Iteration 183, Loss: 0.5937149524688721\n",
      "Epoch 13, Iteration 184, Loss: 0.6121909618377686\n",
      "Epoch 13, Iteration 185, Loss: 0.5971145629882812\n",
      "Epoch 13, Iteration 186, Loss: 0.6460646390914917\n",
      "Epoch 13, Iteration 187, Loss: 0.5642869472503662\n",
      "Epoch 13, Iteration 188, Loss: 0.619149386882782\n",
      "Epoch 13, Iteration 189, Loss: 0.5810993313789368\n",
      "Epoch 13, Iteration 190, Loss: 0.73895263671875\n",
      "Epoch 13, Iteration 191, Loss: 0.6988013386726379\n",
      "Epoch 13, Iteration 192, Loss: 0.6811591386795044\n",
      "Epoch 13, Iteration 193, Loss: 0.7397804856300354\n",
      "Epoch 13, Iteration 194, Loss: 0.6332772970199585\n",
      "Epoch 13, Iteration 195, Loss: 0.5570693612098694\n",
      "Epoch 13, Iteration 196, Loss: 0.7056151628494263\n",
      "Epoch 13, Iteration 197, Loss: 0.6118237376213074\n",
      "Epoch 13, Iteration 198, Loss: 0.6452517509460449\n",
      "Epoch 13, Iteration 199, Loss: 0.7044764161109924\n",
      "Epoch 13, Iteration 200, Loss: 0.5488788485527039\n",
      "Epoch 13, Iteration 200, Valid Loss: 0.5695847868919373\n",
      "Epoch 13, Iteration 201, Loss: 0.6758789420127869\n",
      "Epoch 13, Iteration 202, Loss: 0.6968638300895691\n",
      "Epoch 13, Iteration 203, Loss: 0.5795868039131165\n",
      "Epoch 13, Iteration 204, Loss: 0.6678858399391174\n",
      "Epoch 13, Iteration 205, Loss: 0.4858320653438568\n",
      "Epoch 13, Iteration 206, Loss: 0.6744530200958252\n",
      "Epoch 13, Iteration 207, Loss: 0.7605844736099243\n",
      "Epoch 13, Iteration 208, Loss: 0.9807529449462891\n",
      "Epoch 13, Iteration 209, Loss: 0.9328815340995789\n",
      "Epoch 13, Iteration 210, Loss: 0.8611658215522766\n",
      "Epoch 13, Iteration 211, Loss: 0.9245225787162781\n",
      "Epoch 13, Iteration 212, Loss: 0.7676400542259216\n",
      "Epoch 13, Iteration 213, Loss: 0.9141910076141357\n",
      "Epoch 13, Iteration 214, Loss: 0.6467384099960327\n",
      "Epoch 13, Iteration 215, Loss: 0.6123660206794739\n",
      "Epoch 13, Iteration 216, Loss: 0.5324030518531799\n",
      "Epoch 13, Iteration 217, Loss: 0.5807316899299622\n",
      "Epoch 13, Iteration 218, Loss: 0.6334500312805176\n",
      "Epoch 13, Iteration 219, Loss: 0.780245304107666\n",
      "Epoch 13, Iteration 220, Loss: 0.6437515020370483\n",
      "Epoch 13, Iteration 221, Loss: 0.6986150145530701\n",
      "Epoch 13, Iteration 222, Loss: 0.7466448545455933\n",
      "Epoch 13, Iteration 223, Loss: 0.5040305852890015\n",
      "Epoch 13, Iteration 224, Loss: 0.5401215553283691\n",
      "Epoch 13, Iteration 225, Loss: 0.7418670058250427\n",
      "Epoch 13, Iteration 226, Loss: 0.760698676109314\n",
      "Epoch 13, Iteration 227, Loss: 0.6345388293266296\n",
      "Epoch 13, Iteration 228, Loss: 0.7022950053215027\n",
      "Epoch 13, Iteration 229, Loss: 0.6047330498695374\n",
      "Epoch 13, Iteration 230, Loss: 0.8091482520103455\n",
      "Epoch 13, Iteration 231, Loss: 0.6328886151313782\n",
      "Epoch 13, Iteration 232, Loss: 0.8165364861488342\n",
      "Epoch 13, Iteration 233, Loss: 1.0576192140579224\n",
      "Epoch 13, Iteration 234, Loss: 0.6914346814155579\n",
      "Epoch 13, Iteration 235, Loss: 0.5203074812889099\n",
      "Epoch 13, Iteration 236, Loss: 0.6695610880851746\n",
      "Epoch 13, Iteration 237, Loss: 0.5994977355003357\n",
      "Epoch 13, Iteration 238, Loss: 0.5840198397636414\n",
      "Epoch 13, Iteration 239, Loss: 0.6806063652038574\n",
      "Epoch 13, Iteration 240, Loss: 0.7116633653640747\n",
      "Epoch 13, Iteration 241, Loss: 0.6646847128868103\n",
      "Epoch 13, Iteration 242, Loss: 0.6214671730995178\n",
      "Epoch 13, Iteration 243, Loss: 0.6909418702125549\n",
      "Epoch 13, Iteration 244, Loss: 0.6153965592384338\n",
      "Epoch 13, Iteration 245, Loss: 0.5088751912117004\n",
      "Epoch 13, Iteration 246, Loss: 0.7483754754066467\n",
      "Epoch 13, Iteration 247, Loss: 0.6015863418579102\n",
      "Epoch 13, Iteration 248, Loss: 0.6290282607078552\n",
      "Epoch 13, Iteration 249, Loss: 0.6048339605331421\n",
      "Epoch 13, Iteration 250, Loss: 0.7910892367362976\n",
      "Epoch 13, Iteration 250, Valid Loss: 0.5347652435302734\n",
      "Epoch 13, Iteration 251, Loss: 0.5908398628234863\n",
      "Epoch 13, Iteration 252, Loss: 0.7410852909088135\n",
      "Epoch 13, Iteration 253, Loss: 0.7645257711410522\n",
      "Epoch 13, Iteration 254, Loss: 0.7330166697502136\n",
      "Epoch 13, Iteration 255, Loss: 0.7289804816246033\n",
      "Epoch 13, Iteration 256, Loss: 0.7072290182113647\n",
      "Epoch 13, Iteration 257, Loss: 0.5610490441322327\n",
      "Epoch 13, Iteration 258, Loss: 0.8671119809150696\n",
      "Epoch 13, Iteration 259, Loss: 0.595016598701477\n",
      "Epoch 13, Iteration 260, Loss: 0.749261200428009\n",
      "Epoch 13, Iteration 261, Loss: 0.707294762134552\n",
      "Epoch 13, Iteration 262, Loss: 0.5703744888305664\n",
      "Epoch 13, Iteration 263, Loss: 0.770214319229126\n",
      "Epoch 13, Iteration 264, Loss: 0.6916406750679016\n",
      "Epoch 13, Iteration 265, Loss: 0.6724513173103333\n",
      "Epoch 13, Iteration 266, Loss: 0.7457991242408752\n",
      "Epoch 13, Iteration 267, Loss: 0.6293697357177734\n",
      "Epoch 13, Iteration 268, Loss: 0.6934030652046204\n",
      "Epoch 13, Iteration 269, Loss: 0.6129976511001587\n",
      "Epoch 13, Iteration 270, Loss: 0.6333630681037903\n",
      "Epoch 13, Iteration 271, Loss: 0.6826475858688354\n",
      "Epoch 13, Iteration 272, Loss: 0.6189637184143066\n",
      "Epoch 13, Iteration 273, Loss: 0.7521273493766785\n",
      "Epoch 13, Iteration 274, Loss: 0.653328537940979\n",
      "Epoch 13, Iteration 275, Loss: 0.5675582885742188\n",
      "Epoch 13, Iteration 276, Loss: 0.687293529510498\n",
      "Epoch 13, Iteration 277, Loss: 0.6252930760383606\n",
      "Epoch 13, Iteration 278, Loss: 0.833075225353241\n",
      "Epoch 13, Iteration 279, Loss: 0.5777323842048645\n",
      "Epoch 13, Iteration 280, Loss: 0.5078616738319397\n",
      "Epoch 13, Iteration 281, Loss: 0.6338142156600952\n",
      "Epoch 13, Iteration 282, Loss: 0.6405162811279297\n",
      "Epoch 13, Iteration 283, Loss: 0.6930682063102722\n",
      "Epoch 13, Iteration 284, Loss: 0.612786591053009\n",
      "Epoch 13, Iteration 285, Loss: 0.6405318975448608\n",
      "Epoch 13, Iteration 286, Loss: 0.765401303768158\n",
      "Epoch 13, Iteration 287, Loss: 0.6150934100151062\n",
      "Epoch 13, Iteration 288, Loss: 0.6038191318511963\n",
      "Epoch 13, Iteration 289, Loss: 0.5971079468727112\n",
      "Epoch 13, Iteration 290, Loss: 0.520405650138855\n",
      "Epoch 13, Iteration 291, Loss: 0.5022106766700745\n",
      "Epoch 13, Iteration 292, Loss: 0.6974437832832336\n",
      "Epoch 13, Iteration 293, Loss: 0.7724741101264954\n",
      "Epoch 13, Iteration 294, Loss: 0.5957705974578857\n",
      "Epoch 13, Iteration 295, Loss: 0.5561946630477905\n",
      "Epoch 13, Iteration 296, Loss: 0.7553843259811401\n",
      "Epoch 13, Iteration 297, Loss: 0.7711932063102722\n",
      "Epoch 13, Iteration 298, Loss: 0.645752489566803\n",
      "Epoch 13, Iteration 299, Loss: 0.5873874425888062\n",
      "Epoch 13, Iteration 300, Loss: 0.4908904731273651\n",
      "Epoch 13, Iteration 300, Valid Loss: 0.5704235434532166\n",
      "Epoch 13, Iteration 301, Loss: 0.5666582584381104\n",
      "Epoch 13, Iteration 302, Loss: 0.5472562313079834\n",
      "Epoch 13, Iteration 303, Loss: 0.754734218120575\n",
      "Epoch 13, Iteration 304, Loss: 0.58228999376297\n",
      "Epoch 13, Iteration 305, Loss: 0.6257962584495544\n",
      "Epoch 13, Iteration 306, Loss: 0.5940038561820984\n",
      "Epoch 13, Iteration 307, Loss: 0.5864546298980713\n",
      "Epoch 13, Iteration 308, Loss: 0.5858905911445618\n",
      "Epoch 13, Iteration 309, Loss: 0.653976321220398\n",
      "Epoch 13, Iteration 310, Loss: 0.7096396088600159\n",
      "Epoch 13, Iteration 311, Loss: 0.6059466600418091\n",
      "Epoch 13, Iteration 312, Loss: 0.49178212881088257\n",
      "Epoch 13, Iteration 313, Loss: 0.7358594536781311\n",
      "Epoch 13, Iteration 314, Loss: 0.5361140370368958\n",
      "Epoch 13, Iteration 315, Loss: 0.581423819065094\n",
      "Epoch 13, Iteration 316, Loss: 0.8073376417160034\n",
      "Epoch 13, Iteration 317, Loss: 0.6147488951683044\n",
      "Epoch 13, Iteration 318, Loss: 0.6333742141723633\n",
      "Epoch 13, Iteration 319, Loss: 0.8631830215454102\n",
      "Epoch 13, Iteration 320, Loss: 0.4510369300842285\n",
      "Epoch 13, Iteration 321, Loss: 0.6638768911361694\n",
      "Epoch 13, Iteration 322, Loss: 0.6662980914115906\n",
      "Epoch 13, Iteration 323, Loss: 0.6045491099357605\n",
      "Epoch 13, Iteration 324, Loss: 0.6574144959449768\n",
      "Epoch 13, Iteration 325, Loss: 0.649509072303772\n",
      "Epoch 13, Iteration 326, Loss: 0.6110826134681702\n",
      "Epoch 13, Iteration 327, Loss: 0.5298998355865479\n",
      "Epoch 13, Iteration 328, Loss: 0.6194984316825867\n",
      "Epoch 13, Iteration 329, Loss: 0.6904320120811462\n",
      "Epoch 13, Iteration 330, Loss: 0.5360917448997498\n",
      "Epoch 13, Iteration 331, Loss: 0.7175260186195374\n",
      "Epoch 13, Iteration 332, Loss: 0.5732147693634033\n",
      "Epoch 13, Iteration 333, Loss: 0.8104386925697327\n",
      "Epoch 13, Iteration 334, Loss: 0.6015546917915344\n",
      "Epoch 13, Iteration 335, Loss: 0.8036767840385437\n",
      "Epoch 13, Iteration 336, Loss: 0.60140061378479\n",
      "Epoch 13, Iteration 337, Loss: 0.5180644989013672\n",
      "Epoch 13, Iteration 338, Loss: 0.6857075095176697\n",
      "Epoch 13, Iteration 339, Loss: 0.6543766856193542\n",
      "Epoch 13, Iteration 340, Loss: 0.5158476829528809\n",
      "Epoch 13, Iteration 341, Loss: 0.6035600304603577\n",
      "Epoch 13, Iteration 342, Loss: 0.5383918881416321\n",
      "Epoch 13, Iteration 343, Loss: 0.6697701215744019\n",
      "Epoch 13, Iteration 344, Loss: 0.7228982448577881\n",
      "Epoch 13, Iteration 345, Loss: 0.7348682284355164\n",
      "Epoch 13, Iteration 346, Loss: 0.6109570264816284\n",
      "Epoch 13, Iteration 347, Loss: 0.541968822479248\n",
      "Epoch 13, Iteration 348, Loss: 0.49446702003479004\n",
      "Epoch 13, Iteration 349, Loss: 0.6562536358833313\n",
      "Epoch 13, Iteration 350, Loss: 0.5655328631401062\n",
      "Epoch 13, Iteration 350, Valid Loss: 0.5587517023086548\n",
      "Epoch 13, Iteration 351, Loss: 0.6605768799781799\n",
      "Epoch 13, Iteration 352, Loss: 0.5775132775306702\n",
      "Epoch 13, Iteration 353, Loss: 0.6496226787567139\n",
      "Epoch 13, Iteration 354, Loss: 0.6064326167106628\n",
      "Epoch 13, Iteration 355, Loss: 0.6445798277854919\n",
      "Epoch 13, Iteration 356, Loss: 0.6118136048316956\n",
      "Epoch 13, Iteration 357, Loss: 0.49839720129966736\n",
      "Epoch 13, Iteration 358, Loss: 0.5637742280960083\n",
      "Epoch 13, Iteration 359, Loss: 0.6159358024597168\n",
      "Epoch 13, Iteration 360, Loss: 0.5916247367858887\n",
      "Epoch 13, Iteration 361, Loss: 0.5800893902778625\n",
      "Epoch 13, Iteration 362, Loss: 0.6295937895774841\n",
      "Epoch 13, Iteration 363, Loss: 0.5432410836219788\n",
      "Epoch 13, Iteration 364, Loss: 0.5756300091743469\n",
      "Epoch 13, Iteration 365, Loss: 0.6275191903114319\n",
      "Epoch 13, Iteration 366, Loss: 0.6630136370658875\n",
      "Epoch 13, Iteration 367, Loss: 0.6169953942298889\n",
      "Epoch 13, Iteration 368, Loss: 0.5924999713897705\n",
      "Epoch 13, Iteration 369, Loss: 0.6686941385269165\n",
      "Epoch 13, Iteration 370, Loss: 0.6524847745895386\n",
      "Epoch 13, Iteration 371, Loss: 0.5996202826499939\n",
      "Epoch 13, Iteration 372, Loss: 0.4916153848171234\n",
      "Epoch 13, Iteration 373, Loss: 0.6191608905792236\n",
      "Epoch 13, Iteration 374, Loss: 0.695557713508606\n",
      "Epoch 13, Iteration 375, Loss: 0.5635856986045837\n",
      "Epoch 13, Iteration 376, Loss: 0.664953887462616\n",
      "Epoch 13, Iteration 377, Loss: 0.581725001335144\n",
      "Epoch 13, Iteration 378, Loss: 0.5087494254112244\n",
      "Epoch 13, Iteration 379, Loss: 0.49017998576164246\n",
      "Epoch 13, Iteration 380, Loss: 0.5243696570396423\n",
      "Epoch 13, Iteration 381, Loss: 0.5907266736030579\n",
      "Epoch 13, Iteration 382, Loss: 0.48291856050491333\n",
      "Epoch 13, Iteration 383, Loss: 0.6286081075668335\n",
      "Epoch 13, Iteration 384, Loss: 0.665001630783081\n",
      "Epoch 13, Iteration 385, Loss: 0.5468718409538269\n",
      "Epoch 13, Iteration 386, Loss: 0.525941789150238\n",
      "Epoch 13, Iteration 387, Loss: 0.8434012532234192\n",
      "Epoch 13, Iteration 388, Loss: 0.6217186450958252\n",
      "Epoch 13, Iteration 389, Loss: 0.6176244616508484\n",
      "Epoch 13, Iteration 390, Loss: 0.7099931240081787\n",
      "Epoch 13, Iteration 391, Loss: 0.485775887966156\n",
      "Epoch 14/15, Loss: 0.6644022194098453\n",
      "Epoch 14, Iteration 0, Loss: 0.6107072234153748\n",
      "Epoch 14, Iteration 1, Loss: 0.7651224136352539\n",
      "Epoch 14, Iteration 2, Loss: 0.7542082667350769\n",
      "Epoch 14, Iteration 3, Loss: 0.7045360207557678\n",
      "Epoch 14, Iteration 4, Loss: 0.6236481070518494\n",
      "Epoch 14, Iteration 5, Loss: 0.7284846305847168\n",
      "Epoch 14, Iteration 6, Loss: 0.5345916152000427\n",
      "Epoch 14, Iteration 7, Loss: 0.6490391492843628\n",
      "Epoch 14, Iteration 8, Loss: 0.5848985314369202\n",
      "Epoch 14, Iteration 9, Loss: 0.6946085691452026\n",
      "Epoch 14, Iteration 10, Loss: 0.6922003030776978\n",
      "Epoch 14, Iteration 11, Loss: 0.6338554620742798\n",
      "Epoch 14, Iteration 12, Loss: 0.5765876173973083\n",
      "Epoch 14, Iteration 13, Loss: 0.7579389810562134\n",
      "Epoch 14, Iteration 14, Loss: 0.6010501980781555\n",
      "Epoch 14, Iteration 15, Loss: 0.7896642684936523\n",
      "Epoch 14, Iteration 16, Loss: 0.6226028800010681\n",
      "Epoch 14, Iteration 17, Loss: 0.6683012247085571\n",
      "Epoch 14, Iteration 18, Loss: 0.611186683177948\n",
      "Epoch 14, Iteration 19, Loss: 0.6884034872055054\n",
      "Epoch 14, Iteration 20, Loss: 0.5485475659370422\n",
      "Epoch 14, Iteration 21, Loss: 0.5578159689903259\n",
      "Epoch 14, Iteration 22, Loss: 0.6009384393692017\n",
      "Epoch 14, Iteration 23, Loss: 0.6593762636184692\n",
      "Epoch 14, Iteration 24, Loss: 0.533119797706604\n",
      "Epoch 14, Iteration 25, Loss: 0.5607812404632568\n",
      "Epoch 14, Iteration 26, Loss: 0.8831156492233276\n",
      "Epoch 14, Iteration 27, Loss: 0.6404747366905212\n",
      "Epoch 14, Iteration 28, Loss: 0.6023459434509277\n",
      "Epoch 14, Iteration 29, Loss: 0.7307798862457275\n",
      "Epoch 14, Iteration 30, Loss: 0.6242746114730835\n",
      "Epoch 14, Iteration 31, Loss: 0.6713030338287354\n",
      "Epoch 14, Iteration 32, Loss: 0.7076610326766968\n",
      "Epoch 14, Iteration 33, Loss: 0.6733909845352173\n",
      "Epoch 14, Iteration 34, Loss: 0.5910251140594482\n",
      "Epoch 14, Iteration 35, Loss: 0.7176856398582458\n",
      "Epoch 14, Iteration 36, Loss: 0.5264396667480469\n",
      "Epoch 14, Iteration 37, Loss: 0.5404697060585022\n",
      "Epoch 14, Iteration 38, Loss: 0.5552389025688171\n",
      "Epoch 14, Iteration 39, Loss: 0.6175008416175842\n",
      "Epoch 14, Iteration 40, Loss: 0.6083337068557739\n",
      "Epoch 14, Iteration 41, Loss: 0.7262285947799683\n",
      "Epoch 14, Iteration 42, Loss: 0.6179590821266174\n",
      "Epoch 14, Iteration 43, Loss: 0.5438846945762634\n",
      "Epoch 14, Iteration 44, Loss: 0.6162163615226746\n",
      "Epoch 14, Iteration 45, Loss: 0.6688645482063293\n",
      "Epoch 14, Iteration 46, Loss: 0.6636140942573547\n",
      "Epoch 14, Iteration 47, Loss: 0.5997145771980286\n",
      "Epoch 14, Iteration 48, Loss: 0.731526792049408\n",
      "Epoch 14, Iteration 49, Loss: 0.5901693105697632\n",
      "Epoch 14, Iteration 50, Loss: 0.6146365404129028\n",
      "Epoch 14, Iteration 50, Valid Loss: 0.5303436517715454\n",
      "Epoch 14, Iteration 51, Loss: 0.6098006367683411\n",
      "Epoch 14, Iteration 52, Loss: 0.5015780329704285\n",
      "Epoch 14, Iteration 53, Loss: 0.6410586833953857\n",
      "Epoch 14, Iteration 54, Loss: 0.7204937934875488\n",
      "Epoch 14, Iteration 55, Loss: 0.7217893600463867\n",
      "Epoch 14, Iteration 56, Loss: 0.6654707789421082\n",
      "Epoch 14, Iteration 57, Loss: 0.7897247672080994\n",
      "Epoch 14, Iteration 58, Loss: 0.6385961174964905\n",
      "Epoch 14, Iteration 59, Loss: 0.5876182317733765\n",
      "Epoch 14, Iteration 60, Loss: 0.576600193977356\n",
      "Epoch 14, Iteration 61, Loss: 0.5191096067428589\n",
      "Epoch 14, Iteration 62, Loss: 0.5531730651855469\n",
      "Epoch 14, Iteration 63, Loss: 0.571182131767273\n",
      "Epoch 14, Iteration 64, Loss: 0.7183638215065002\n",
      "Epoch 14, Iteration 65, Loss: 0.8945601582527161\n",
      "Epoch 14, Iteration 66, Loss: 0.6167870163917542\n",
      "Epoch 14, Iteration 67, Loss: 0.5402092933654785\n",
      "Epoch 14, Iteration 68, Loss: 0.7772874236106873\n",
      "Epoch 14, Iteration 69, Loss: 0.6737895607948303\n",
      "Epoch 14, Iteration 70, Loss: 0.5463982820510864\n",
      "Epoch 14, Iteration 71, Loss: 0.8060462474822998\n",
      "Epoch 14, Iteration 72, Loss: 0.6323464512825012\n",
      "Epoch 14, Iteration 73, Loss: 0.5288059711456299\n",
      "Epoch 14, Iteration 74, Loss: 0.7712869048118591\n",
      "Epoch 14, Iteration 75, Loss: 0.6805619597434998\n",
      "Epoch 14, Iteration 76, Loss: 0.5741592049598694\n",
      "Epoch 14, Iteration 77, Loss: 0.5228039622306824\n",
      "Epoch 14, Iteration 78, Loss: 0.7863436341285706\n",
      "Epoch 14, Iteration 79, Loss: 0.8232721090316772\n",
      "Epoch 14, Iteration 80, Loss: 0.6366491913795471\n",
      "Epoch 14, Iteration 81, Loss: 0.9476251602172852\n",
      "Epoch 14, Iteration 82, Loss: 0.7868000268936157\n",
      "Epoch 14, Iteration 83, Loss: 0.5359039902687073\n",
      "Epoch 14, Iteration 84, Loss: 0.6653822064399719\n",
      "Epoch 14, Iteration 85, Loss: 0.6701843738555908\n",
      "Epoch 14, Iteration 86, Loss: 0.7432978749275208\n",
      "Epoch 14, Iteration 87, Loss: 0.6623572111129761\n",
      "Epoch 14, Iteration 88, Loss: 0.807832658290863\n",
      "Epoch 14, Iteration 89, Loss: 0.8560396432876587\n",
      "Epoch 14, Iteration 90, Loss: 0.5428947806358337\n",
      "Epoch 14, Iteration 91, Loss: 0.5731862783432007\n",
      "Epoch 14, Iteration 92, Loss: 0.5128257274627686\n",
      "Epoch 14, Iteration 93, Loss: 0.6731340885162354\n",
      "Epoch 14, Iteration 94, Loss: 0.7486782073974609\n",
      "Epoch 14, Iteration 95, Loss: 0.5300270318984985\n",
      "Epoch 14, Iteration 96, Loss: 0.6259327530860901\n",
      "Epoch 14, Iteration 97, Loss: 0.5357972979545593\n",
      "Epoch 14, Iteration 98, Loss: 0.5948169231414795\n",
      "Epoch 14, Iteration 99, Loss: 0.6051891446113586\n",
      "Epoch 14, Iteration 100, Loss: 0.5017278790473938\n",
      "Epoch 14, Iteration 100, Valid Loss: 0.5842671394348145\n",
      "Epoch 14, Iteration 101, Loss: 0.8009840846061707\n",
      "Epoch 14, Iteration 102, Loss: 0.695949375629425\n",
      "Epoch 14, Iteration 103, Loss: 0.6009212136268616\n",
      "Epoch 14, Iteration 104, Loss: 0.7615051865577698\n",
      "Epoch 14, Iteration 105, Loss: 0.6280305981636047\n",
      "Epoch 14, Iteration 106, Loss: 0.5717982053756714\n",
      "Epoch 14, Iteration 107, Loss: 0.714202880859375\n",
      "Epoch 14, Iteration 108, Loss: 0.6158840656280518\n",
      "Epoch 14, Iteration 109, Loss: 0.8252966403961182\n",
      "Epoch 14, Iteration 110, Loss: 0.7647902369499207\n",
      "Epoch 14, Iteration 111, Loss: 0.6622416973114014\n",
      "Epoch 14, Iteration 112, Loss: 0.6175422072410583\n",
      "Epoch 14, Iteration 113, Loss: 0.5685501098632812\n",
      "Epoch 14, Iteration 114, Loss: 0.7397603392601013\n",
      "Epoch 14, Iteration 115, Loss: 0.7571986317634583\n",
      "Epoch 14, Iteration 116, Loss: 0.5805146098136902\n",
      "Epoch 14, Iteration 117, Loss: 0.4187462627887726\n",
      "Epoch 14, Iteration 118, Loss: 0.6895751953125\n",
      "Epoch 14, Iteration 119, Loss: 0.627479076385498\n",
      "Epoch 14, Iteration 120, Loss: 0.7280772924423218\n",
      "Epoch 14, Iteration 121, Loss: 0.5843213200569153\n",
      "Epoch 14, Iteration 122, Loss: 0.8341134786605835\n",
      "Epoch 14, Iteration 123, Loss: 0.6324335336685181\n",
      "Epoch 14, Iteration 124, Loss: 0.5473833084106445\n",
      "Epoch 14, Iteration 125, Loss: 0.5971825122833252\n",
      "Epoch 14, Iteration 126, Loss: 0.5486607551574707\n",
      "Epoch 14, Iteration 127, Loss: 0.6734400391578674\n",
      "Epoch 14, Iteration 128, Loss: 0.8162709474563599\n",
      "Epoch 14, Iteration 129, Loss: 0.8056705594062805\n",
      "Epoch 14, Iteration 130, Loss: 0.8165773153305054\n",
      "Epoch 14, Iteration 131, Loss: 0.5977594256401062\n",
      "Epoch 14, Iteration 132, Loss: 0.5945350527763367\n",
      "Epoch 14, Iteration 133, Loss: 0.9014208912849426\n",
      "Epoch 14, Iteration 134, Loss: 0.7289546132087708\n",
      "Epoch 14, Iteration 135, Loss: 0.6860867738723755\n",
      "Epoch 14, Iteration 136, Loss: 0.6348538398742676\n",
      "Epoch 14, Iteration 137, Loss: 0.6420860886573792\n",
      "Epoch 14, Iteration 138, Loss: 0.5706411600112915\n",
      "Epoch 14, Iteration 139, Loss: 0.5723981857299805\n",
      "Epoch 14, Iteration 140, Loss: 0.5500221252441406\n",
      "Epoch 14, Iteration 141, Loss: 0.9982789754867554\n",
      "Epoch 14, Iteration 142, Loss: 0.7401317954063416\n",
      "Epoch 14, Iteration 143, Loss: 0.6136390566825867\n",
      "Epoch 14, Iteration 144, Loss: 0.5692453384399414\n",
      "Epoch 14, Iteration 145, Loss: 0.6214507818222046\n",
      "Epoch 14, Iteration 146, Loss: 0.6983564496040344\n",
      "Epoch 14, Iteration 147, Loss: 0.6879479885101318\n",
      "Epoch 14, Iteration 148, Loss: 0.5575490593910217\n",
      "Epoch 14, Iteration 149, Loss: 0.6807994842529297\n",
      "Epoch 14, Iteration 150, Loss: 0.6685407161712646\n",
      "Epoch 14, Iteration 150, Valid Loss: 0.5642458200454712\n",
      "Epoch 14, Iteration 151, Loss: 0.579749345779419\n",
      "Epoch 14, Iteration 152, Loss: 0.5240301489830017\n",
      "Epoch 14, Iteration 153, Loss: 1.1266608238220215\n",
      "Epoch 14, Iteration 154, Loss: 0.49107685685157776\n",
      "Epoch 14, Iteration 155, Loss: 0.6667110919952393\n",
      "Epoch 14, Iteration 156, Loss: 0.6271978616714478\n",
      "Epoch 14, Iteration 157, Loss: 0.8935822248458862\n",
      "Epoch 14, Iteration 158, Loss: 0.6608150005340576\n",
      "Epoch 14, Iteration 159, Loss: 0.7523893117904663\n",
      "Epoch 14, Iteration 160, Loss: 0.6851851344108582\n",
      "Epoch 14, Iteration 161, Loss: 0.6141988039016724\n",
      "Epoch 14, Iteration 162, Loss: 0.8240697979927063\n",
      "Epoch 14, Iteration 163, Loss: 0.5382514595985413\n",
      "Epoch 14, Iteration 164, Loss: 0.6509600877761841\n",
      "Epoch 14, Iteration 165, Loss: 0.6785654425621033\n",
      "Epoch 14, Iteration 166, Loss: 0.5972216725349426\n",
      "Epoch 14, Iteration 167, Loss: 0.8067429661750793\n",
      "Epoch 14, Iteration 168, Loss: 0.6710888147354126\n",
      "Epoch 14, Iteration 169, Loss: 0.6239954829216003\n",
      "Epoch 14, Iteration 170, Loss: 0.780057430267334\n",
      "Epoch 14, Iteration 171, Loss: 0.780393660068512\n",
      "Epoch 14, Iteration 172, Loss: 0.6408101916313171\n",
      "Epoch 14, Iteration 173, Loss: 0.5783522129058838\n",
      "Epoch 14, Iteration 174, Loss: 0.75457364320755\n",
      "Epoch 14, Iteration 175, Loss: 0.7148157358169556\n",
      "Epoch 14, Iteration 176, Loss: 0.48266106843948364\n",
      "Epoch 14, Iteration 177, Loss: 1.0304592847824097\n",
      "Epoch 14, Iteration 178, Loss: 0.8249485492706299\n",
      "Epoch 14, Iteration 179, Loss: 0.6282718181610107\n",
      "Epoch 14, Iteration 180, Loss: 0.5859596729278564\n",
      "Epoch 14, Iteration 181, Loss: 0.7526180148124695\n",
      "Epoch 14, Iteration 182, Loss: 0.627107560634613\n",
      "Epoch 14, Iteration 183, Loss: 0.6058009266853333\n",
      "Epoch 14, Iteration 184, Loss: 0.6545619964599609\n",
      "Epoch 14, Iteration 185, Loss: 0.5878801941871643\n",
      "Epoch 14, Iteration 186, Loss: 0.6426315307617188\n",
      "Epoch 14, Iteration 187, Loss: 0.6183637380599976\n",
      "Epoch 14, Iteration 188, Loss: 0.5870542526245117\n",
      "Epoch 14, Iteration 189, Loss: 0.5854374766349792\n",
      "Epoch 14, Iteration 190, Loss: 0.7203412055969238\n",
      "Epoch 14, Iteration 191, Loss: 0.6264134049415588\n",
      "Epoch 14, Iteration 192, Loss: 0.6295006275177002\n",
      "Epoch 14, Iteration 193, Loss: 0.6148236393928528\n",
      "Epoch 14, Iteration 194, Loss: 0.685322642326355\n",
      "Epoch 14, Iteration 195, Loss: 0.6773267388343811\n",
      "Epoch 14, Iteration 196, Loss: 0.7126807570457458\n",
      "Epoch 14, Iteration 197, Loss: 0.6542168259620667\n",
      "Epoch 14, Iteration 198, Loss: 0.6497117280960083\n",
      "Epoch 14, Iteration 199, Loss: 0.7460067272186279\n",
      "Epoch 14, Iteration 200, Loss: 0.5301912426948547\n",
      "Epoch 14, Iteration 200, Valid Loss: 0.5511583089828491\n",
      "Epoch 14, Iteration 201, Loss: 0.6087249517440796\n",
      "Epoch 14, Iteration 202, Loss: 0.6487732529640198\n",
      "Epoch 14, Iteration 203, Loss: 0.6043097376823425\n",
      "Epoch 14, Iteration 204, Loss: 0.624016284942627\n",
      "Epoch 14, Iteration 205, Loss: 0.45410171151161194\n",
      "Epoch 14, Iteration 206, Loss: 0.567425012588501\n",
      "Epoch 14, Iteration 207, Loss: 0.7623317241668701\n",
      "Epoch 14, Iteration 208, Loss: 1.0109169483184814\n",
      "Epoch 14, Iteration 209, Loss: 0.9114813804626465\n",
      "Epoch 14, Iteration 210, Loss: 0.8696741461753845\n",
      "Epoch 14, Iteration 211, Loss: 0.9141328930854797\n",
      "Epoch 14, Iteration 212, Loss: 0.7774336934089661\n",
      "Epoch 14, Iteration 213, Loss: 0.8093644380569458\n",
      "Epoch 14, Iteration 214, Loss: 0.617143452167511\n",
      "Epoch 14, Iteration 215, Loss: 0.5544579029083252\n",
      "Epoch 14, Iteration 216, Loss: 0.4904244840145111\n",
      "Epoch 14, Iteration 217, Loss: 0.5986142158508301\n",
      "Epoch 14, Iteration 218, Loss: 0.5790656208992004\n",
      "Epoch 14, Iteration 219, Loss: 0.6976546049118042\n",
      "Epoch 14, Iteration 220, Loss: 0.5925308465957642\n",
      "Epoch 14, Iteration 221, Loss: 0.6425665616989136\n",
      "Epoch 14, Iteration 222, Loss: 0.7172425985336304\n",
      "Epoch 14, Iteration 223, Loss: 0.5189924240112305\n",
      "Epoch 14, Iteration 224, Loss: 0.5511044859886169\n",
      "Epoch 14, Iteration 225, Loss: 0.6958479285240173\n",
      "Epoch 14, Iteration 226, Loss: 0.7633757591247559\n",
      "Epoch 14, Iteration 227, Loss: 0.623642086982727\n",
      "Epoch 14, Iteration 228, Loss: 0.6867386698722839\n",
      "Epoch 14, Iteration 229, Loss: 0.8627510070800781\n",
      "Epoch 14, Iteration 230, Loss: 0.6126768589019775\n",
      "Epoch 14, Iteration 231, Loss: 0.6582214832305908\n",
      "Epoch 14, Iteration 232, Loss: 0.689460039138794\n",
      "Epoch 14, Iteration 233, Loss: 0.8365787267684937\n",
      "Epoch 14, Iteration 234, Loss: 0.8240333795547485\n",
      "Epoch 14, Iteration 235, Loss: 0.5648580193519592\n",
      "Epoch 14, Iteration 236, Loss: 0.5750499367713928\n",
      "Epoch 14, Iteration 237, Loss: 0.6929522752761841\n",
      "Epoch 14, Iteration 238, Loss: 0.533454954624176\n",
      "Epoch 14, Iteration 239, Loss: 0.7671695947647095\n",
      "Epoch 14, Iteration 240, Loss: 0.7719932198524475\n",
      "Epoch 14, Iteration 241, Loss: 0.5722347497940063\n",
      "Epoch 14, Iteration 242, Loss: 0.6432735919952393\n",
      "Epoch 14, Iteration 243, Loss: 0.615272045135498\n",
      "Epoch 14, Iteration 244, Loss: 0.6483468413352966\n",
      "Epoch 14, Iteration 245, Loss: 0.5017063617706299\n",
      "Epoch 14, Iteration 246, Loss: 0.5431618094444275\n",
      "Epoch 14, Iteration 247, Loss: 0.5860621929168701\n",
      "Epoch 14, Iteration 248, Loss: 0.6446748971939087\n",
      "Epoch 14, Iteration 249, Loss: 0.5970962643623352\n",
      "Epoch 14, Iteration 250, Loss: 0.746163547039032\n",
      "Epoch 14, Iteration 250, Valid Loss: 0.5581231117248535\n",
      "Epoch 14, Iteration 251, Loss: 0.5273298025131226\n",
      "Epoch 14, Iteration 252, Loss: 0.8119019269943237\n",
      "Epoch 14, Iteration 253, Loss: 0.7783250212669373\n",
      "Epoch 14, Iteration 254, Loss: 0.6832113265991211\n",
      "Epoch 14, Iteration 255, Loss: 0.6472228765487671\n",
      "Epoch 14, Iteration 256, Loss: 0.6837942004203796\n",
      "Epoch 14, Iteration 257, Loss: 0.5877158641815186\n",
      "Epoch 14, Iteration 258, Loss: 0.8337875604629517\n",
      "Epoch 14, Iteration 259, Loss: 0.5414684414863586\n",
      "Epoch 14, Iteration 260, Loss: 0.6666642427444458\n",
      "Epoch 14, Iteration 261, Loss: 0.6128965020179749\n",
      "Epoch 14, Iteration 262, Loss: 0.5700690150260925\n",
      "Epoch 14, Iteration 263, Loss: 0.7359674572944641\n",
      "Epoch 14, Iteration 264, Loss: 0.6223708391189575\n",
      "Epoch 14, Iteration 265, Loss: 0.6473710536956787\n",
      "Epoch 14, Iteration 266, Loss: 0.7085281610488892\n",
      "Epoch 14, Iteration 267, Loss: 0.5997295379638672\n",
      "Epoch 14, Iteration 268, Loss: 0.5991355180740356\n",
      "Epoch 14, Iteration 269, Loss: 0.641214907169342\n",
      "Epoch 14, Iteration 270, Loss: 0.6304333806037903\n",
      "Epoch 14, Iteration 271, Loss: 0.653393566608429\n",
      "Epoch 14, Iteration 272, Loss: 0.5860128998756409\n",
      "Epoch 14, Iteration 273, Loss: 0.7527735829353333\n",
      "Epoch 14, Iteration 274, Loss: 0.6573354005813599\n",
      "Epoch 14, Iteration 275, Loss: 0.5229412317276001\n",
      "Epoch 14, Iteration 276, Loss: 0.5610924363136292\n",
      "Epoch 14, Iteration 277, Loss: 0.5509462356567383\n",
      "Epoch 14, Iteration 278, Loss: 0.8243198990821838\n",
      "Epoch 14, Iteration 279, Loss: 0.5546190142631531\n",
      "Epoch 14, Iteration 280, Loss: 0.470173180103302\n",
      "Epoch 14, Iteration 281, Loss: 0.6214513182640076\n",
      "Epoch 14, Iteration 282, Loss: 0.5577471852302551\n",
      "Epoch 14, Iteration 283, Loss: 0.6121353507041931\n",
      "Epoch 14, Iteration 284, Loss: 0.5456141829490662\n",
      "Epoch 14, Iteration 285, Loss: 0.5175022482872009\n",
      "Epoch 14, Iteration 286, Loss: 0.7202721238136292\n",
      "Epoch 14, Iteration 287, Loss: 0.8068439364433289\n",
      "Epoch 14, Iteration 288, Loss: 0.49012646079063416\n",
      "Epoch 14, Iteration 289, Loss: 0.5144026279449463\n",
      "Epoch 14, Iteration 290, Loss: 0.6996760964393616\n",
      "Epoch 14, Iteration 291, Loss: 0.42148104310035706\n",
      "Epoch 14, Iteration 292, Loss: 0.6977176070213318\n",
      "Epoch 14, Iteration 293, Loss: 0.7223997712135315\n",
      "Epoch 14, Iteration 294, Loss: 0.588953971862793\n",
      "Epoch 14, Iteration 295, Loss: 0.4689693748950958\n",
      "Epoch 14, Iteration 296, Loss: 0.7474380135536194\n",
      "Epoch 14, Iteration 297, Loss: 0.6966290473937988\n",
      "Epoch 14, Iteration 298, Loss: 0.6713414192199707\n",
      "Epoch 14, Iteration 299, Loss: 0.5390381813049316\n",
      "Epoch 14, Iteration 300, Loss: 0.4777832627296448\n",
      "Epoch 14, Iteration 300, Valid Loss: 0.556969940662384\n",
      "Epoch 14, Iteration 301, Loss: 0.5622046589851379\n",
      "Epoch 14, Iteration 302, Loss: 0.5832691788673401\n",
      "Epoch 14, Iteration 303, Loss: 0.7290912866592407\n",
      "Epoch 14, Iteration 304, Loss: 0.5907501578330994\n",
      "Epoch 14, Iteration 305, Loss: 0.5676746368408203\n",
      "Epoch 14, Iteration 306, Loss: 0.5643795132637024\n",
      "Epoch 14, Iteration 307, Loss: 0.571269154548645\n",
      "Epoch 14, Iteration 308, Loss: 0.5755190253257751\n",
      "Epoch 14, Iteration 309, Loss: 0.6739599108695984\n",
      "Epoch 14, Iteration 310, Loss: 0.6442617177963257\n",
      "Epoch 14, Iteration 311, Loss: 0.6272590756416321\n",
      "Epoch 14, Iteration 312, Loss: 0.4842514991760254\n",
      "Epoch 14, Iteration 313, Loss: 0.6864299178123474\n",
      "Epoch 14, Iteration 314, Loss: 0.6153287291526794\n",
      "Epoch 14, Iteration 315, Loss: 0.7465932965278625\n",
      "Epoch 14, Iteration 316, Loss: 0.8259593844413757\n",
      "Epoch 14, Iteration 317, Loss: 0.6302006840705872\n",
      "Epoch 14, Iteration 318, Loss: 0.5446757078170776\n",
      "Epoch 14, Iteration 319, Loss: 0.7844739556312561\n",
      "Epoch 14, Iteration 320, Loss: 0.44046440720558167\n",
      "Epoch 14, Iteration 321, Loss: 0.6651378870010376\n",
      "Epoch 14, Iteration 322, Loss: 0.5873116254806519\n",
      "Epoch 14, Iteration 323, Loss: 0.6334177255630493\n",
      "Epoch 14, Iteration 324, Loss: 0.6252709627151489\n",
      "Epoch 14, Iteration 325, Loss: 0.6175390481948853\n",
      "Epoch 14, Iteration 326, Loss: 0.5635004043579102\n",
      "Epoch 14, Iteration 327, Loss: 0.4923475384712219\n",
      "Epoch 14, Iteration 328, Loss: 0.5868098735809326\n",
      "Epoch 14, Iteration 329, Loss: 0.7179847955703735\n",
      "Epoch 14, Iteration 330, Loss: 0.5451033711433411\n",
      "Epoch 14, Iteration 331, Loss: 0.6221243739128113\n",
      "Epoch 14, Iteration 332, Loss: 0.5824337005615234\n",
      "Epoch 14, Iteration 333, Loss: 0.7882437109947205\n",
      "Epoch 14, Iteration 334, Loss: 0.5671252012252808\n",
      "Epoch 14, Iteration 335, Loss: 0.8014878630638123\n",
      "Epoch 14, Iteration 336, Loss: 0.6549918055534363\n",
      "Epoch 14, Iteration 337, Loss: 0.5691003799438477\n",
      "Epoch 14, Iteration 338, Loss: 0.7254765629768372\n",
      "Epoch 14, Iteration 339, Loss: 0.5917448401451111\n",
      "Epoch 14, Iteration 340, Loss: 0.5065962076187134\n",
      "Epoch 14, Iteration 341, Loss: 0.6374509930610657\n",
      "Epoch 14, Iteration 342, Loss: 0.6174458861351013\n",
      "Epoch 14, Iteration 343, Loss: 0.569108784198761\n",
      "Epoch 14, Iteration 344, Loss: 0.7145522236824036\n",
      "Epoch 14, Iteration 345, Loss: 0.5942813158035278\n",
      "Epoch 14, Iteration 346, Loss: 0.5270751118659973\n",
      "Epoch 14, Iteration 347, Loss: 0.46178004145622253\n",
      "Epoch 14, Iteration 348, Loss: 0.49353665113449097\n",
      "Epoch 14, Iteration 349, Loss: 0.6756992340087891\n",
      "Epoch 14, Iteration 350, Loss: 0.5342204570770264\n",
      "Epoch 14, Iteration 350, Valid Loss: 0.5740734338760376\n",
      "Epoch 14, Iteration 351, Loss: 0.6610851287841797\n",
      "Epoch 14, Iteration 352, Loss: 0.5364763736724854\n",
      "Epoch 14, Iteration 353, Loss: 0.604708731174469\n",
      "Epoch 14, Iteration 354, Loss: 0.48500052094459534\n",
      "Epoch 14, Iteration 355, Loss: 0.5672560930252075\n",
      "Epoch 14, Iteration 356, Loss: 0.6837770938873291\n",
      "Epoch 14, Iteration 357, Loss: 0.4965064525604248\n",
      "Epoch 14, Iteration 358, Loss: 0.5248560309410095\n",
      "Epoch 14, Iteration 359, Loss: 0.6506376266479492\n",
      "Epoch 14, Iteration 360, Loss: 0.5922913551330566\n",
      "Epoch 14, Iteration 361, Loss: 0.6887060403823853\n",
      "Epoch 14, Iteration 362, Loss: 0.5828639268875122\n",
      "Epoch 14, Iteration 363, Loss: 0.5541618466377258\n",
      "Epoch 14, Iteration 364, Loss: 0.6418083310127258\n",
      "Epoch 14, Iteration 365, Loss: 0.7320284247398376\n",
      "Epoch 14, Iteration 366, Loss: 0.7109942436218262\n",
      "Epoch 14, Iteration 367, Loss: 0.5572710037231445\n",
      "Epoch 14, Iteration 368, Loss: 0.5640605092048645\n",
      "Epoch 14, Iteration 369, Loss: 0.6047599911689758\n",
      "Epoch 14, Iteration 370, Loss: 0.6522803902626038\n",
      "Epoch 14, Iteration 371, Loss: 0.6099329590797424\n",
      "Epoch 14, Iteration 372, Loss: 0.5074207782745361\n",
      "Epoch 14, Iteration 373, Loss: 0.5977193117141724\n",
      "Epoch 14, Iteration 374, Loss: 0.8016809821128845\n",
      "Epoch 14, Iteration 375, Loss: 0.579961359500885\n",
      "Epoch 14, Iteration 376, Loss: 0.6793494820594788\n",
      "Epoch 14, Iteration 377, Loss: 0.5821374654769897\n",
      "Epoch 14, Iteration 378, Loss: 0.5027801394462585\n",
      "Epoch 14, Iteration 379, Loss: 0.5250089764595032\n",
      "Epoch 14, Iteration 380, Loss: 0.5753220915794373\n",
      "Epoch 14, Iteration 381, Loss: 0.5200352668762207\n",
      "Epoch 14, Iteration 382, Loss: 0.4058254659175873\n",
      "Epoch 14, Iteration 383, Loss: 0.5597006678581238\n",
      "Epoch 14, Iteration 384, Loss: 0.5849471092224121\n",
      "Epoch 14, Iteration 385, Loss: 0.5235757827758789\n",
      "Epoch 14, Iteration 386, Loss: 0.531743049621582\n",
      "Epoch 14, Iteration 387, Loss: 0.9005934000015259\n",
      "Epoch 14, Iteration 388, Loss: 0.6104074716567993\n",
      "Epoch 14, Iteration 389, Loss: 0.6115405559539795\n",
      "Epoch 14, Iteration 390, Loss: 0.7072548866271973\n",
      "Epoch 14, Iteration 391, Loss: 0.5183640122413635\n",
      "Epoch 15/15, Loss: 0.6449628796960626\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>head_tagging_accuracy</td><td>▁▄▅▆▆▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▅▄▄▄▄▃▃▃▃▄▃▃▂▃▂▃▄▂▃▂▂▁▂▂▃▂▃▃▂▁▁▁▂▁▂▄▁▁▁</td></tr><tr><td>unlabeled_attachment_score</td><td>▁▄▅▆▇▇▇▇█▇█████</td></tr><tr><td>valid_loss</td><td>█▆▆▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>head_tagging_accuracy</td><td>0.8514</td></tr><tr><td>train_loss</td><td>0.51836</td></tr><tr><td>unlabeled_attachment_score</td><td>0.86436</td></tr><tr><td>valid_loss</td><td>0.57407</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dependency-parsing</strong> at: <a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/iidfj4nz' target=\"_blank\">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/iidfj4nz</a><br> View project at: <a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing' target=\"_blank\">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250113_214744-iidfj4nz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = DependencyParserModel()\n",
    "\n",
    "train(model, train_dataloader, valid_dataloader, device, lr=6e-4, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45155b67b1b56e51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T21:18:49.576602Z",
     "start_time": "2025-01-13T21:18:18.518909Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\WangEntang\\AppData\\Local\\Temp\\ipykernel_41496\\1613024923.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"dependency_parser_edgedim_1024.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batch: 65\n",
      "Batch 0\n",
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n",
      "Batch 11\n",
      "Batch 12\n",
      "Batch 13\n",
      "Batch 14\n",
      "Batch 15\n",
      "Batch 16\n",
      "Batch 17\n",
      "Batch 18\n",
      "Batch 19\n",
      "Batch 20\n",
      "Batch 21\n",
      "Batch 22\n",
      "Batch 23\n",
      "Batch 24\n",
      "Batch 25\n",
      "Batch 26\n",
      "Batch 27\n",
      "Batch 28\n",
      "Batch 29\n",
      "Batch 30\n",
      "Batch 31\n",
      "Batch 32\n",
      "Batch 33\n",
      "Batch 34\n",
      "Batch 35\n",
      "Batch 36\n",
      "Batch 37\n",
      "Batch 38\n",
      "Batch 39\n",
      "Batch 40\n",
      "Batch 41\n",
      "Batch 42\n",
      "Batch 43\n",
      "Batch 44\n",
      "Batch 45\n",
      "Batch 46\n",
      "Batch 47\n",
      "Batch 48\n",
      "Batch 49\n",
      "Batch 50\n",
      "Batch 51\n",
      "Batch 52\n",
      "Batch 53\n",
      "Batch 54\n",
      "Batch 55\n",
      "Batch 56\n",
      "Batch 57\n",
      "Batch 58\n",
      "Batch 59\n",
      "Batch 60\n",
      "Batch 61\n",
      "Batch 62\n",
      "Batch 63\n",
      "Batch 64\n",
      "UAS Accuracy: 0.8635639145680587\n"
     ]
    }
   ],
   "source": [
    "from ufal.chu_liu_edmonds import chu_liu_edmonds\n",
    "\n",
    "# Evaluation on the test set\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = DependencyParserModel().to(device)\n",
    "model.load_state_dict(torch.load(\"dependency_parser.pth\"))\n",
    "\n",
    "# # Extra: Pro model UAS Accuracy: 0.8642811603442779\n",
    "# model = DependencyParserProModel().to(device)\n",
    "# model.load_state_dict(torch.load(\"dependency_parser_edgedim_1024.pth\"))\n",
    "\n",
    "# Evaluate Head prediction\n",
    "model.eval()\n",
    "total_count = 0\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    print(f\"total batch: {len(test_dataloader)}\")\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        x = data[\"input_ids\"].to(device)\n",
    "        mask = data[\"attention_mask\"].to(device)\n",
    "        head = data[\"head\"].to(device)\n",
    "\n",
    "        H_head, H_dep, L_head, L_dep = model(x, mask)\n",
    "        edge_scores = model.score_edges(H_head, H_dep)  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # MST parsing UAS evaluation\n",
    "        log_probs = torch.log_softmax(edge_scores, dim=-1)\n",
    "        # apply chu_liu_edmons\n",
    "        predictions = []\n",
    "        for sent_scores in log_probs:\n",
    "            # convert to numpy\n",
    "            scores_np = sent_scores.cpu().double().numpy()\n",
    "            mst_heads = chu_liu_edmonds(scores_np)\n",
    "            predictions.append(mst_heads[0])\n",
    "\n",
    "        print(f\"Batch {i}\")\n",
    "\n",
    "        # convert predictions and gold heads into tensors\n",
    "        uas_predictions = torch.tensor(predictions, device=device)  # Shape: (batch_size, seq_len)\n",
    "        valid_positions = (head != -100) & (mask == 1)  # Mask for valid positions # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # calculate correct predictions and total count\n",
    "        total_correct += ((uas_predictions == head) & valid_positions).sum().item()\n",
    "        total_count += valid_positions.sum().item()\n",
    "\n",
    "print(f\"UAS Accuracy: {total_correct / total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26f806e4e936831d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T22:10:25.569727Z",
     "start_time": "2025-01-13T21:52:15.109654Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WangEntang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WangEntang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "wandb: Currently logged in as: maxewang10 (maxewang10-saarland-informatics-campus). Use `wandb login --relogin` to force relogin\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\WangEntang\\Desktop\\code\\CL\\a5\\wandb\\run-20250113_225223-er1khkx3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/er1khkx3' target=\"_blank\">dependency-parsing</a></strong> to <a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing' target=\"_blank\">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/er1khkx3' target=\"_blank\">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/er1khkx3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch num 392\n",
      "Epoch 0, Iteration 0, Loss: 9.847716331481934\n",
      "Epoch 0, Iteration 1, Loss: 9.546501159667969\n",
      "Epoch 0, Iteration 2, Loss: 9.301563262939453\n",
      "Epoch 0, Iteration 3, Loss: 9.073440551757812\n",
      "Epoch 0, Iteration 4, Loss: 8.827247619628906\n",
      "Epoch 0, Iteration 5, Loss: 8.644109725952148\n",
      "Epoch 0, Iteration 6, Loss: 8.412551879882812\n",
      "Epoch 0, Iteration 7, Loss: 8.29134464263916\n",
      "Epoch 0, Iteration 8, Loss: 8.082523345947266\n",
      "Epoch 0, Iteration 9, Loss: 7.994370460510254\n",
      "Epoch 0, Iteration 10, Loss: 7.656954765319824\n",
      "Epoch 0, Iteration 11, Loss: 7.419976234436035\n",
      "Epoch 0, Iteration 12, Loss: 7.286379814147949\n",
      "Epoch 0, Iteration 13, Loss: 7.291024684906006\n",
      "Epoch 0, Iteration 14, Loss: 7.00642728805542\n",
      "Epoch 0, Iteration 15, Loss: 6.925200462341309\n",
      "Epoch 0, Iteration 16, Loss: 6.721271514892578\n",
      "Epoch 0, Iteration 17, Loss: 6.732643127441406\n",
      "Epoch 0, Iteration 18, Loss: 6.394735813140869\n",
      "Epoch 0, Iteration 19, Loss: 6.45346736907959\n",
      "Epoch 0, Iteration 20, Loss: 6.432611465454102\n",
      "Epoch 0, Iteration 21, Loss: 6.348763465881348\n",
      "Epoch 0, Iteration 22, Loss: 6.085209846496582\n",
      "Epoch 0, Iteration 23, Loss: 6.124578952789307\n",
      "Epoch 0, Iteration 24, Loss: 6.039186477661133\n",
      "Epoch 0, Iteration 25, Loss: 5.9354400634765625\n",
      "Epoch 0, Iteration 26, Loss: 6.050265312194824\n",
      "Epoch 0, Iteration 27, Loss: 5.909564971923828\n",
      "Epoch 0, Iteration 28, Loss: 5.789445877075195\n",
      "Epoch 0, Iteration 29, Loss: 5.6460065841674805\n",
      "Epoch 0, Iteration 30, Loss: 5.536341667175293\n",
      "Epoch 0, Iteration 31, Loss: 5.399195194244385\n",
      "Epoch 0, Iteration 32, Loss: 5.497375011444092\n",
      "Epoch 0, Iteration 33, Loss: 5.18115234375\n",
      "Epoch 0, Iteration 34, Loss: 5.42244815826416\n",
      "Epoch 0, Iteration 35, Loss: 5.413187026977539\n",
      "Epoch 0, Iteration 36, Loss: 5.208505630493164\n",
      "Epoch 0, Iteration 37, Loss: 5.149444103240967\n",
      "Epoch 0, Iteration 38, Loss: 5.211514472961426\n",
      "Epoch 0, Iteration 39, Loss: 5.101093292236328\n",
      "Epoch 0, Iteration 40, Loss: 5.235589027404785\n",
      "Epoch 0, Iteration 41, Loss: 5.145758628845215\n",
      "Epoch 0, Iteration 42, Loss: 5.469198226928711\n",
      "Epoch 0, Iteration 43, Loss: 5.334802627563477\n",
      "Epoch 0, Iteration 44, Loss: 5.375179290771484\n",
      "Epoch 0, Iteration 45, Loss: 5.194720268249512\n",
      "Epoch 0, Iteration 46, Loss: 5.145501136779785\n",
      "Epoch 0, Iteration 47, Loss: 5.133576393127441\n",
      "Epoch 0, Iteration 48, Loss: 5.137571334838867\n",
      "Epoch 0, Iteration 49, Loss: 5.060364246368408\n",
      "Epoch 0, Iteration 50, Loss: 5.216657638549805\n",
      "Epoch 0, Iteration 50, Valid Loss: 2.7567293643951416\n",
      "Epoch 0, Iteration 51, Loss: 4.903803825378418\n",
      "Epoch 0, Iteration 52, Loss: 5.343389511108398\n",
      "Epoch 0, Iteration 53, Loss: 4.872424125671387\n",
      "Epoch 0, Iteration 54, Loss: 4.987217426300049\n",
      "Epoch 0, Iteration 55, Loss: 5.031269073486328\n",
      "Epoch 0, Iteration 56, Loss: 4.49389123916626\n",
      "Epoch 0, Iteration 57, Loss: 4.964474678039551\n",
      "Epoch 0, Iteration 58, Loss: 4.588923454284668\n",
      "Epoch 0, Iteration 59, Loss: 4.949513912200928\n",
      "Epoch 0, Iteration 60, Loss: 4.995227336883545\n",
      "Epoch 0, Iteration 61, Loss: 4.3268022537231445\n",
      "Epoch 0, Iteration 62, Loss: 4.652259826660156\n",
      "Epoch 0, Iteration 63, Loss: 4.610742092132568\n",
      "Epoch 0, Iteration 64, Loss: 4.3084306716918945\n",
      "Epoch 0, Iteration 65, Loss: 4.593693256378174\n",
      "Epoch 0, Iteration 66, Loss: 4.762104511260986\n",
      "Epoch 0, Iteration 67, Loss: 4.510526180267334\n",
      "Epoch 0, Iteration 68, Loss: 5.016168594360352\n",
      "Epoch 0, Iteration 69, Loss: 4.627368927001953\n",
      "Epoch 0, Iteration 70, Loss: 4.898713111877441\n",
      "Epoch 0, Iteration 71, Loss: 4.538634777069092\n",
      "Epoch 0, Iteration 72, Loss: 4.858819007873535\n",
      "Epoch 0, Iteration 73, Loss: 4.464206218719482\n",
      "Epoch 0, Iteration 74, Loss: 4.695379734039307\n",
      "Epoch 0, Iteration 75, Loss: 4.382760524749756\n",
      "Epoch 0, Iteration 76, Loss: 4.283947944641113\n",
      "Epoch 0, Iteration 77, Loss: 4.222935199737549\n",
      "Epoch 0, Iteration 78, Loss: 4.63950777053833\n",
      "Epoch 0, Iteration 79, Loss: 4.522343158721924\n",
      "Epoch 0, Iteration 80, Loss: 4.199439525604248\n",
      "Epoch 0, Iteration 81, Loss: 4.3742899894714355\n",
      "Epoch 0, Iteration 82, Loss: 4.285918712615967\n",
      "Epoch 0, Iteration 83, Loss: 4.284214973449707\n",
      "Epoch 0, Iteration 84, Loss: 4.512027263641357\n",
      "Epoch 0, Iteration 85, Loss: 4.361198425292969\n",
      "Epoch 0, Iteration 86, Loss: 4.247093200683594\n",
      "Epoch 0, Iteration 87, Loss: 4.1361260414123535\n",
      "Epoch 0, Iteration 88, Loss: 4.14699649810791\n",
      "Epoch 0, Iteration 89, Loss: 4.265470504760742\n",
      "Epoch 0, Iteration 90, Loss: 4.054080963134766\n",
      "Epoch 0, Iteration 91, Loss: 4.103646278381348\n",
      "Epoch 0, Iteration 92, Loss: 4.038068771362305\n",
      "Epoch 0, Iteration 93, Loss: 4.072612762451172\n",
      "Epoch 0, Iteration 94, Loss: 4.564094066619873\n",
      "Epoch 0, Iteration 95, Loss: 4.252722263336182\n",
      "Epoch 0, Iteration 96, Loss: 4.327775001525879\n",
      "Epoch 0, Iteration 97, Loss: 4.093064785003662\n",
      "Epoch 0, Iteration 98, Loss: 4.328907012939453\n",
      "Epoch 0, Iteration 99, Loss: 4.072683334350586\n",
      "Epoch 0, Iteration 100, Loss: 4.058050632476807\n",
      "Epoch 0, Iteration 100, Valid Loss: 2.6010122299194336\n",
      "Epoch 0, Iteration 101, Loss: 4.281209468841553\n",
      "Epoch 0, Iteration 102, Loss: 4.248579025268555\n",
      "Epoch 0, Iteration 103, Loss: 4.088962554931641\n",
      "Epoch 0, Iteration 104, Loss: 4.568479537963867\n",
      "Epoch 0, Iteration 105, Loss: 4.20873498916626\n",
      "Epoch 0, Iteration 106, Loss: 4.153979778289795\n",
      "Epoch 0, Iteration 107, Loss: 5.06034517288208\n",
      "Epoch 0, Iteration 108, Loss: 3.897047996520996\n",
      "Epoch 0, Iteration 109, Loss: 4.135614395141602\n",
      "Epoch 0, Iteration 110, Loss: 4.407773017883301\n",
      "Epoch 0, Iteration 111, Loss: 4.092238903045654\n",
      "Epoch 0, Iteration 112, Loss: 4.064296722412109\n",
      "Epoch 0, Iteration 113, Loss: 3.8016483783721924\n",
      "Epoch 0, Iteration 114, Loss: 4.406543731689453\n",
      "Epoch 0, Iteration 115, Loss: 4.303811550140381\n",
      "Epoch 0, Iteration 116, Loss: 4.160530090332031\n",
      "Epoch 0, Iteration 117, Loss: 3.8493471145629883\n",
      "Epoch 0, Iteration 118, Loss: 4.26616907119751\n",
      "Epoch 0, Iteration 119, Loss: 4.278938293457031\n",
      "Epoch 0, Iteration 120, Loss: 3.8935303688049316\n",
      "Epoch 0, Iteration 121, Loss: 3.909630298614502\n",
      "Epoch 0, Iteration 122, Loss: 3.924140214920044\n",
      "Epoch 0, Iteration 123, Loss: 3.8484206199645996\n",
      "Epoch 0, Iteration 124, Loss: 3.930314540863037\n",
      "Epoch 0, Iteration 125, Loss: 3.6281046867370605\n",
      "Epoch 0, Iteration 126, Loss: 3.780555248260498\n",
      "Epoch 0, Iteration 127, Loss: 4.071514129638672\n",
      "Epoch 0, Iteration 128, Loss: 4.015175819396973\n",
      "Epoch 0, Iteration 129, Loss: 3.880694627761841\n",
      "Epoch 0, Iteration 130, Loss: 3.952240467071533\n",
      "Epoch 0, Iteration 131, Loss: 3.933094024658203\n",
      "Epoch 0, Iteration 132, Loss: 3.8009700775146484\n",
      "Epoch 0, Iteration 133, Loss: 3.8316550254821777\n",
      "Epoch 0, Iteration 134, Loss: 3.471909523010254\n",
      "Epoch 0, Iteration 135, Loss: 3.6614160537719727\n",
      "Epoch 0, Iteration 136, Loss: 3.6766915321350098\n",
      "Epoch 0, Iteration 137, Loss: 3.674656867980957\n",
      "Epoch 0, Iteration 138, Loss: 3.532376766204834\n",
      "Epoch 0, Iteration 139, Loss: 3.503699779510498\n",
      "Epoch 0, Iteration 140, Loss: 3.7813942432403564\n",
      "Epoch 0, Iteration 141, Loss: 3.968003273010254\n",
      "Epoch 0, Iteration 142, Loss: 3.4633002281188965\n",
      "Epoch 0, Iteration 143, Loss: 3.7714452743530273\n",
      "Epoch 0, Iteration 144, Loss: 3.647156000137329\n",
      "Epoch 0, Iteration 145, Loss: 3.695132255554199\n",
      "Epoch 0, Iteration 146, Loss: 3.955199718475342\n",
      "Epoch 0, Iteration 147, Loss: 3.7446413040161133\n",
      "Epoch 0, Iteration 148, Loss: 3.3614439964294434\n",
      "Epoch 0, Iteration 149, Loss: 3.620366334915161\n",
      "Epoch 0, Iteration 150, Loss: 3.578016519546509\n",
      "Epoch 0, Iteration 150, Valid Loss: 2.5227532386779785\n",
      "Epoch 0, Iteration 151, Loss: 3.8323464393615723\n",
      "Epoch 0, Iteration 152, Loss: 3.76686954498291\n",
      "Epoch 0, Iteration 153, Loss: 3.427994728088379\n",
      "Epoch 0, Iteration 154, Loss: 3.547128200531006\n",
      "Epoch 0, Iteration 155, Loss: 3.598629951477051\n",
      "Epoch 0, Iteration 156, Loss: 3.7767210006713867\n",
      "Epoch 0, Iteration 157, Loss: 4.4557318687438965\n",
      "Epoch 0, Iteration 158, Loss: 3.748856544494629\n",
      "Epoch 0, Iteration 159, Loss: 3.887603759765625\n",
      "Epoch 0, Iteration 160, Loss: 3.736206531524658\n",
      "Epoch 0, Iteration 161, Loss: 3.681645631790161\n",
      "Epoch 0, Iteration 162, Loss: 3.929394245147705\n",
      "Epoch 0, Iteration 163, Loss: 3.531088352203369\n",
      "Epoch 0, Iteration 164, Loss: 3.248706817626953\n",
      "Epoch 0, Iteration 165, Loss: 3.447147846221924\n",
      "Epoch 0, Iteration 166, Loss: 3.4634389877319336\n",
      "Epoch 0, Iteration 167, Loss: 3.954288959503174\n",
      "Epoch 0, Iteration 168, Loss: 4.055214881896973\n",
      "Epoch 0, Iteration 169, Loss: 3.7507617473602295\n",
      "Epoch 0, Iteration 170, Loss: 3.736267566680908\n",
      "Epoch 0, Iteration 171, Loss: 3.3912806510925293\n",
      "Epoch 0, Iteration 172, Loss: 3.353628396987915\n",
      "Epoch 0, Iteration 173, Loss: 3.7273426055908203\n",
      "Epoch 0, Iteration 174, Loss: 3.734616994857788\n",
      "Epoch 0, Iteration 175, Loss: 3.7554707527160645\n",
      "Epoch 0, Iteration 176, Loss: 3.447787046432495\n",
      "Epoch 0, Iteration 177, Loss: 3.7519593238830566\n",
      "Epoch 0, Iteration 178, Loss: 3.683361530303955\n",
      "Epoch 0, Iteration 179, Loss: 3.6507482528686523\n",
      "Epoch 0, Iteration 180, Loss: 3.91841983795166\n",
      "Epoch 0, Iteration 181, Loss: 3.7679176330566406\n",
      "Epoch 0, Iteration 182, Loss: 3.7534079551696777\n",
      "Epoch 0, Iteration 183, Loss: 3.4155571460723877\n",
      "Epoch 0, Iteration 184, Loss: 3.320107936859131\n",
      "Epoch 0, Iteration 185, Loss: 3.692577838897705\n",
      "Epoch 0, Iteration 186, Loss: 3.4144287109375\n",
      "Epoch 0, Iteration 187, Loss: 3.5345802307128906\n",
      "Epoch 0, Iteration 188, Loss: 3.512542963027954\n",
      "Epoch 0, Iteration 189, Loss: 3.569976329803467\n",
      "Epoch 0, Iteration 190, Loss: 3.941693067550659\n",
      "Epoch 0, Iteration 191, Loss: 3.6970701217651367\n",
      "Epoch 0, Iteration 192, Loss: 3.631720542907715\n",
      "Epoch 0, Iteration 193, Loss: 3.4810075759887695\n",
      "Epoch 0, Iteration 194, Loss: 3.2687642574310303\n",
      "Epoch 0, Iteration 195, Loss: 3.3984241485595703\n",
      "Epoch 0, Iteration 196, Loss: 3.5122625827789307\n",
      "Epoch 0, Iteration 197, Loss: 3.4501469135284424\n",
      "Epoch 0, Iteration 198, Loss: 3.4164490699768066\n",
      "Epoch 0, Iteration 199, Loss: 3.6957316398620605\n",
      "Epoch 0, Iteration 200, Loss: 3.494436502456665\n",
      "Epoch 0, Iteration 200, Valid Loss: 2.3782942295074463\n",
      "Epoch 0, Iteration 201, Loss: 3.384122610092163\n",
      "Epoch 0, Iteration 202, Loss: 3.506242275238037\n",
      "Epoch 0, Iteration 203, Loss: 3.498509407043457\n",
      "Epoch 0, Iteration 204, Loss: 3.4828453063964844\n",
      "Epoch 0, Iteration 205, Loss: 3.1275980472564697\n",
      "Epoch 0, Iteration 206, Loss: 3.135586738586426\n",
      "Epoch 0, Iteration 207, Loss: 3.4551923274993896\n",
      "Epoch 0, Iteration 208, Loss: 3.9012317657470703\n",
      "Epoch 0, Iteration 209, Loss: 4.278946876525879\n",
      "Epoch 0, Iteration 210, Loss: 3.953396797180176\n",
      "Epoch 0, Iteration 211, Loss: 4.16502571105957\n",
      "Epoch 0, Iteration 212, Loss: 3.976043224334717\n",
      "Epoch 0, Iteration 213, Loss: 3.9038751125335693\n",
      "Epoch 0, Iteration 214, Loss: 3.531233310699463\n",
      "Epoch 0, Iteration 215, Loss: 3.3809006214141846\n",
      "Epoch 0, Iteration 216, Loss: 3.3256490230560303\n",
      "Epoch 0, Iteration 217, Loss: 3.6661274433135986\n",
      "Epoch 0, Iteration 218, Loss: 3.778454542160034\n",
      "Epoch 0, Iteration 219, Loss: 3.7397139072418213\n",
      "Epoch 0, Iteration 220, Loss: 3.642124652862549\n",
      "Epoch 0, Iteration 221, Loss: 3.543823719024658\n",
      "Epoch 0, Iteration 222, Loss: 3.522270679473877\n",
      "Epoch 0, Iteration 223, Loss: 3.603985071182251\n",
      "Epoch 0, Iteration 224, Loss: 3.348815441131592\n",
      "Epoch 0, Iteration 225, Loss: 3.6569294929504395\n",
      "Epoch 0, Iteration 226, Loss: 3.5514349937438965\n",
      "Epoch 0, Iteration 227, Loss: 3.4053359031677246\n",
      "Epoch 0, Iteration 228, Loss: 3.3436203002929688\n",
      "Epoch 0, Iteration 229, Loss: 3.1811766624450684\n",
      "Epoch 0, Iteration 230, Loss: 3.6392405033111572\n",
      "Epoch 0, Iteration 231, Loss: 3.389244556427002\n",
      "Epoch 0, Iteration 232, Loss: 3.488523006439209\n",
      "Epoch 0, Iteration 233, Loss: 3.9203543663024902\n",
      "Epoch 0, Iteration 234, Loss: 3.6974544525146484\n",
      "Epoch 0, Iteration 235, Loss: 3.1045353412628174\n",
      "Epoch 0, Iteration 236, Loss: 3.388026237487793\n",
      "Epoch 0, Iteration 237, Loss: 3.437232494354248\n",
      "Epoch 0, Iteration 238, Loss: 3.101548194885254\n",
      "Epoch 0, Iteration 239, Loss: 3.2691245079040527\n",
      "Epoch 0, Iteration 240, Loss: 3.019469738006592\n",
      "Epoch 0, Iteration 241, Loss: 3.112483501434326\n",
      "Epoch 0, Iteration 242, Loss: 2.957745313644409\n",
      "Epoch 0, Iteration 243, Loss: 3.120914936065674\n",
      "Epoch 0, Iteration 244, Loss: 3.1154966354370117\n",
      "Epoch 0, Iteration 245, Loss: 3.011948585510254\n",
      "Epoch 0, Iteration 246, Loss: 3.2363176345825195\n",
      "Epoch 0, Iteration 247, Loss: 3.0341367721557617\n",
      "Epoch 0, Iteration 248, Loss: 3.391435146331787\n",
      "Epoch 0, Iteration 249, Loss: 2.992872714996338\n",
      "Epoch 0, Iteration 250, Loss: 3.291639804840088\n",
      "Epoch 0, Iteration 250, Valid Loss: 2.294922113418579\n",
      "Epoch 0, Iteration 251, Loss: 3.2376084327697754\n",
      "Epoch 0, Iteration 252, Loss: 3.163817882537842\n",
      "Epoch 0, Iteration 253, Loss: 3.3494248390197754\n",
      "Epoch 0, Iteration 254, Loss: 3.229332685470581\n",
      "Epoch 0, Iteration 255, Loss: 3.3718204498291016\n",
      "Epoch 0, Iteration 256, Loss: 3.460712432861328\n",
      "Epoch 0, Iteration 257, Loss: 3.256852865219116\n",
      "Epoch 0, Iteration 258, Loss: 3.563138008117676\n",
      "Epoch 0, Iteration 259, Loss: 3.0314793586730957\n",
      "Epoch 0, Iteration 260, Loss: 3.227077007293701\n",
      "Epoch 0, Iteration 261, Loss: 3.3119606971740723\n",
      "Epoch 0, Iteration 262, Loss: 3.0377368927001953\n",
      "Epoch 0, Iteration 263, Loss: 3.298053741455078\n",
      "Epoch 0, Iteration 264, Loss: 3.3239855766296387\n",
      "Epoch 0, Iteration 265, Loss: 3.2758970260620117\n",
      "Epoch 0, Iteration 266, Loss: 3.3802247047424316\n",
      "Epoch 0, Iteration 267, Loss: 3.305941581726074\n",
      "Epoch 0, Iteration 268, Loss: 3.2057762145996094\n",
      "Epoch 0, Iteration 269, Loss: 3.099949836730957\n",
      "Epoch 0, Iteration 270, Loss: 3.019279956817627\n",
      "Epoch 0, Iteration 271, Loss: 3.190762996673584\n",
      "Epoch 0, Iteration 272, Loss: 3.08166766166687\n",
      "Epoch 0, Iteration 273, Loss: 3.159611225128174\n",
      "Epoch 0, Iteration 274, Loss: 3.0366032123565674\n",
      "Epoch 0, Iteration 275, Loss: 2.9583640098571777\n",
      "Epoch 0, Iteration 276, Loss: 2.9852094650268555\n",
      "Epoch 0, Iteration 277, Loss: 2.914320468902588\n",
      "Epoch 0, Iteration 278, Loss: 3.0117030143737793\n",
      "Epoch 0, Iteration 279, Loss: 2.9118754863739014\n",
      "Epoch 0, Iteration 280, Loss: 2.766672134399414\n",
      "Epoch 0, Iteration 281, Loss: 2.67610502243042\n",
      "Epoch 0, Iteration 282, Loss: 3.1962027549743652\n",
      "Epoch 0, Iteration 283, Loss: 3.154083251953125\n",
      "Epoch 0, Iteration 284, Loss: 3.0029783248901367\n",
      "Epoch 0, Iteration 285, Loss: 3.0706372261047363\n",
      "Epoch 0, Iteration 286, Loss: 2.9380156993865967\n",
      "Epoch 0, Iteration 287, Loss: 2.94240140914917\n",
      "Epoch 0, Iteration 288, Loss: 2.7780942916870117\n",
      "Epoch 0, Iteration 289, Loss: 3.0559120178222656\n",
      "Epoch 0, Iteration 290, Loss: 2.838951587677002\n",
      "Epoch 0, Iteration 291, Loss: 3.0171122550964355\n",
      "Epoch 0, Iteration 292, Loss: 3.4577763080596924\n",
      "Epoch 0, Iteration 293, Loss: 3.143867015838623\n",
      "Epoch 0, Iteration 294, Loss: 2.835533618927002\n",
      "Epoch 0, Iteration 295, Loss: 2.9257826805114746\n",
      "Epoch 0, Iteration 296, Loss: 3.3973464965820312\n",
      "Epoch 0, Iteration 297, Loss: 3.3526244163513184\n",
      "Epoch 0, Iteration 298, Loss: 3.0655784606933594\n",
      "Epoch 0, Iteration 299, Loss: 2.70121431350708\n",
      "Epoch 0, Iteration 300, Loss: 2.85284423828125\n",
      "Epoch 0, Iteration 300, Valid Loss: 2.1937718391418457\n",
      "Epoch 0, Iteration 301, Loss: 2.9772768020629883\n",
      "Epoch 0, Iteration 302, Loss: 2.8815865516662598\n",
      "Epoch 0, Iteration 303, Loss: 3.4632625579833984\n",
      "Epoch 0, Iteration 304, Loss: 2.8057823181152344\n",
      "Epoch 0, Iteration 305, Loss: 2.908209800720215\n",
      "Epoch 0, Iteration 306, Loss: 2.878495216369629\n",
      "Epoch 0, Iteration 307, Loss: 2.779040813446045\n",
      "Epoch 0, Iteration 308, Loss: 2.893857955932617\n",
      "Epoch 0, Iteration 309, Loss: 2.774521827697754\n",
      "Epoch 0, Iteration 310, Loss: 3.184093475341797\n",
      "Epoch 0, Iteration 311, Loss: 2.7823829650878906\n",
      "Epoch 0, Iteration 312, Loss: 2.703989028930664\n",
      "Epoch 0, Iteration 313, Loss: 3.0016660690307617\n",
      "Epoch 0, Iteration 314, Loss: 2.988527297973633\n",
      "Epoch 0, Iteration 315, Loss: 2.7406654357910156\n",
      "Epoch 0, Iteration 316, Loss: 2.767595052719116\n",
      "Epoch 0, Iteration 317, Loss: 2.7468929290771484\n",
      "Epoch 0, Iteration 318, Loss: 2.8384597301483154\n",
      "Epoch 0, Iteration 319, Loss: 2.9592981338500977\n",
      "Epoch 0, Iteration 320, Loss: 2.7644994258880615\n",
      "Epoch 0, Iteration 321, Loss: 2.868340492248535\n",
      "Epoch 0, Iteration 322, Loss: 2.669663906097412\n",
      "Epoch 0, Iteration 323, Loss: 2.7342095375061035\n",
      "Epoch 0, Iteration 324, Loss: 2.950629472732544\n",
      "Epoch 0, Iteration 325, Loss: 2.7911365032196045\n",
      "Epoch 0, Iteration 326, Loss: 2.9352779388427734\n",
      "Epoch 0, Iteration 327, Loss: 2.8059194087982178\n",
      "Epoch 0, Iteration 328, Loss: 2.682185649871826\n",
      "Epoch 0, Iteration 329, Loss: 3.044206142425537\n",
      "Epoch 0, Iteration 330, Loss: 2.5739288330078125\n",
      "Epoch 0, Iteration 331, Loss: 2.7095510959625244\n",
      "Epoch 0, Iteration 332, Loss: 2.9699816703796387\n",
      "Epoch 0, Iteration 333, Loss: 3.0616369247436523\n",
      "Epoch 0, Iteration 334, Loss: 2.701838254928589\n",
      "Epoch 0, Iteration 335, Loss: 2.9443092346191406\n",
      "Epoch 0, Iteration 336, Loss: 2.9674010276794434\n",
      "Epoch 0, Iteration 337, Loss: 2.6864845752716064\n",
      "Epoch 0, Iteration 338, Loss: 2.9434125423431396\n",
      "Epoch 0, Iteration 339, Loss: 2.823716163635254\n",
      "Epoch 0, Iteration 340, Loss: 2.6120057106018066\n",
      "Epoch 0, Iteration 341, Loss: 2.872532367706299\n",
      "Epoch 0, Iteration 342, Loss: 2.677905559539795\n",
      "Epoch 0, Iteration 343, Loss: 2.601562976837158\n",
      "Epoch 0, Iteration 344, Loss: 2.9277474880218506\n",
      "Epoch 0, Iteration 345, Loss: 2.986400604248047\n",
      "Epoch 0, Iteration 346, Loss: 2.7535345554351807\n",
      "Epoch 0, Iteration 347, Loss: 2.6225433349609375\n",
      "Epoch 0, Iteration 348, Loss: 2.637385129928589\n",
      "Epoch 0, Iteration 349, Loss: 2.9535601139068604\n",
      "Epoch 0, Iteration 350, Loss: 2.5958423614501953\n",
      "Epoch 0, Iteration 350, Valid Loss: 2.1282708644866943\n",
      "Epoch 0, Iteration 351, Loss: 2.805795192718506\n",
      "Epoch 0, Iteration 352, Loss: 2.6815176010131836\n",
      "Epoch 0, Iteration 353, Loss: 3.0265634059906006\n",
      "Epoch 0, Iteration 354, Loss: 2.74658465385437\n",
      "Epoch 0, Iteration 355, Loss: 2.797865152359009\n",
      "Epoch 0, Iteration 356, Loss: 2.836045742034912\n",
      "Epoch 0, Iteration 357, Loss: 2.819453716278076\n",
      "Epoch 0, Iteration 358, Loss: 2.8677399158477783\n",
      "Epoch 0, Iteration 359, Loss: 2.7354698181152344\n",
      "Epoch 0, Iteration 360, Loss: 2.4977173805236816\n",
      "Epoch 0, Iteration 361, Loss: 2.634648323059082\n",
      "Epoch 0, Iteration 362, Loss: 2.6301333904266357\n",
      "Epoch 0, Iteration 363, Loss: 2.8233461380004883\n",
      "Epoch 0, Iteration 364, Loss: 2.977174758911133\n",
      "Epoch 0, Iteration 365, Loss: 2.96736478805542\n",
      "Epoch 0, Iteration 366, Loss: 3.0962162017822266\n",
      "Epoch 0, Iteration 367, Loss: 2.914863109588623\n",
      "Epoch 0, Iteration 368, Loss: 2.739309072494507\n",
      "Epoch 0, Iteration 369, Loss: 3.0763838291168213\n",
      "Epoch 0, Iteration 370, Loss: 2.961979866027832\n",
      "Epoch 0, Iteration 371, Loss: 2.9392738342285156\n",
      "Epoch 0, Iteration 372, Loss: 2.434478521347046\n",
      "Epoch 0, Iteration 373, Loss: 2.6810829639434814\n",
      "Epoch 0, Iteration 374, Loss: 2.992007255554199\n",
      "Epoch 0, Iteration 375, Loss: 2.6845245361328125\n",
      "Epoch 0, Iteration 376, Loss: 3.063183069229126\n",
      "Epoch 0, Iteration 377, Loss: 2.886951208114624\n",
      "Epoch 0, Iteration 378, Loss: 2.6641807556152344\n",
      "Epoch 0, Iteration 379, Loss: 2.7994678020477295\n",
      "Epoch 0, Iteration 380, Loss: 2.748642921447754\n",
      "Epoch 0, Iteration 381, Loss: 2.5439846515655518\n",
      "Epoch 0, Iteration 382, Loss: 2.5802547931671143\n",
      "Epoch 0, Iteration 383, Loss: 2.7239553928375244\n",
      "Epoch 0, Iteration 384, Loss: 2.7361702919006348\n",
      "Epoch 0, Iteration 385, Loss: 2.6632509231567383\n",
      "Epoch 0, Iteration 386, Loss: 2.6112096309661865\n",
      "Epoch 0, Iteration 387, Loss: 2.9717981815338135\n",
      "Epoch 0, Iteration 388, Loss: 2.7765095233917236\n",
      "Epoch 0, Iteration 389, Loss: 2.8183541297912598\n",
      "Epoch 0, Iteration 390, Loss: 2.9897451400756836\n",
      "Epoch 0, Iteration 391, Loss: 2.7932586669921875\n",
      "Epoch 1/10, Loss: 3.870835578563262\n",
      "Epoch 1, Iteration 0, Loss: 2.929718494415283\n",
      "Epoch 1, Iteration 1, Loss: 3.2561216354370117\n",
      "Epoch 1, Iteration 2, Loss: 3.1509432792663574\n",
      "Epoch 1, Iteration 3, Loss: 3.0441513061523438\n",
      "Epoch 1, Iteration 4, Loss: 2.966804027557373\n",
      "Epoch 1, Iteration 5, Loss: 3.2198596000671387\n",
      "Epoch 1, Iteration 6, Loss: 2.822037935256958\n",
      "Epoch 1, Iteration 7, Loss: 2.9864470958709717\n",
      "Epoch 1, Iteration 8, Loss: 3.2148003578186035\n",
      "Epoch 1, Iteration 9, Loss: 3.3268485069274902\n",
      "Epoch 1, Iteration 10, Loss: 3.104628562927246\n",
      "Epoch 1, Iteration 11, Loss: 2.6856932640075684\n",
      "Epoch 1, Iteration 12, Loss: 2.802534580230713\n",
      "Epoch 1, Iteration 13, Loss: 3.0433216094970703\n",
      "Epoch 1, Iteration 14, Loss: 2.7401890754699707\n",
      "Epoch 1, Iteration 15, Loss: 2.9608383178710938\n",
      "Epoch 1, Iteration 16, Loss: 2.7410316467285156\n",
      "Epoch 1, Iteration 17, Loss: 3.0958080291748047\n",
      "Epoch 1, Iteration 18, Loss: 3.0232625007629395\n",
      "Epoch 1, Iteration 19, Loss: 3.059680223464966\n",
      "Epoch 1, Iteration 20, Loss: 2.9939215183258057\n",
      "Epoch 1, Iteration 21, Loss: 3.1133174896240234\n",
      "Epoch 1, Iteration 22, Loss: 2.767305374145508\n",
      "Epoch 1, Iteration 23, Loss: 2.8227672576904297\n",
      "Epoch 1, Iteration 24, Loss: 2.662497043609619\n",
      "Epoch 1, Iteration 25, Loss: 2.6886305809020996\n",
      "Epoch 1, Iteration 26, Loss: 3.2243032455444336\n",
      "Epoch 1, Iteration 27, Loss: 2.7773919105529785\n",
      "Epoch 1, Iteration 28, Loss: 2.893660545349121\n",
      "Epoch 1, Iteration 29, Loss: 2.9350647926330566\n",
      "Epoch 1, Iteration 30, Loss: 2.872114419937134\n",
      "Epoch 1, Iteration 31, Loss: 2.7804126739501953\n",
      "Epoch 1, Iteration 32, Loss: 2.624100685119629\n",
      "Epoch 1, Iteration 33, Loss: 2.712965488433838\n",
      "Epoch 1, Iteration 34, Loss: 2.78085994720459\n",
      "Epoch 1, Iteration 35, Loss: 2.911715030670166\n",
      "Epoch 1, Iteration 36, Loss: 2.6039206981658936\n",
      "Epoch 1, Iteration 37, Loss: 2.636409044265747\n",
      "Epoch 1, Iteration 38, Loss: 2.7570693492889404\n",
      "Epoch 1, Iteration 39, Loss: 2.644141435623169\n",
      "Epoch 1, Iteration 40, Loss: 2.795302391052246\n",
      "Epoch 1, Iteration 41, Loss: 2.882899522781372\n",
      "Epoch 1, Iteration 42, Loss: 3.060317277908325\n",
      "Epoch 1, Iteration 43, Loss: 2.7741219997406006\n",
      "Epoch 1, Iteration 44, Loss: 2.94028902053833\n",
      "Epoch 1, Iteration 45, Loss: 2.796900510787964\n",
      "Epoch 1, Iteration 46, Loss: 2.878718852996826\n",
      "Epoch 1, Iteration 47, Loss: 2.8957977294921875\n",
      "Epoch 1, Iteration 48, Loss: 3.0578763484954834\n",
      "Epoch 1, Iteration 49, Loss: 2.9502921104431152\n",
      "Epoch 1, Iteration 50, Loss: 2.858139753341675\n",
      "Epoch 1, Iteration 50, Valid Loss: 1.9666019678115845\n",
      "Epoch 1, Iteration 51, Loss: 2.7851085662841797\n",
      "Epoch 1, Iteration 52, Loss: 2.997345447540283\n",
      "Epoch 1, Iteration 53, Loss: 2.7082157135009766\n",
      "Epoch 1, Iteration 54, Loss: 2.926750421524048\n",
      "Epoch 1, Iteration 55, Loss: 3.011016368865967\n",
      "Epoch 1, Iteration 56, Loss: 2.5122487545013428\n",
      "Epoch 1, Iteration 57, Loss: 2.812964916229248\n",
      "Epoch 1, Iteration 58, Loss: 2.450106620788574\n",
      "Epoch 1, Iteration 59, Loss: 2.909555673599243\n",
      "Epoch 1, Iteration 60, Loss: 2.9468023777008057\n",
      "Epoch 1, Iteration 61, Loss: 2.3072948455810547\n",
      "Epoch 1, Iteration 62, Loss: 2.548079013824463\n",
      "Epoch 1, Iteration 63, Loss: 2.405674457550049\n",
      "Epoch 1, Iteration 64, Loss: 3.1688709259033203\n",
      "Epoch 1, Iteration 65, Loss: 2.819506883621216\n",
      "Epoch 1, Iteration 66, Loss: 2.823728084564209\n",
      "Epoch 1, Iteration 67, Loss: 2.512380599975586\n",
      "Epoch 1, Iteration 68, Loss: 2.807260036468506\n",
      "Epoch 1, Iteration 69, Loss: 2.7136073112487793\n",
      "Epoch 1, Iteration 70, Loss: 2.902906656265259\n",
      "Epoch 1, Iteration 71, Loss: 2.867306709289551\n",
      "Epoch 1, Iteration 72, Loss: 3.0494818687438965\n",
      "Epoch 1, Iteration 73, Loss: 2.355048656463623\n",
      "Epoch 1, Iteration 74, Loss: 2.8432528972625732\n",
      "Epoch 1, Iteration 75, Loss: 2.4165096282958984\n",
      "Epoch 1, Iteration 76, Loss: 2.5069260597229004\n",
      "Epoch 1, Iteration 77, Loss: 2.424684524536133\n",
      "Epoch 1, Iteration 78, Loss: 3.0372862815856934\n",
      "Epoch 1, Iteration 79, Loss: 2.987328052520752\n",
      "Epoch 1, Iteration 80, Loss: 2.346406936645508\n",
      "Epoch 1, Iteration 81, Loss: 2.8334836959838867\n",
      "Epoch 1, Iteration 82, Loss: 2.529773473739624\n",
      "Epoch 1, Iteration 83, Loss: 2.3993780612945557\n",
      "Epoch 1, Iteration 84, Loss: 2.6648712158203125\n",
      "Epoch 1, Iteration 85, Loss: 2.745170831680298\n",
      "Epoch 1, Iteration 86, Loss: 2.5569651126861572\n",
      "Epoch 1, Iteration 87, Loss: 2.429582118988037\n",
      "Epoch 1, Iteration 88, Loss: 2.544119358062744\n",
      "Epoch 1, Iteration 89, Loss: 2.609938621520996\n",
      "Epoch 1, Iteration 90, Loss: 2.2164387702941895\n",
      "Epoch 1, Iteration 91, Loss: 2.5206949710845947\n",
      "Epoch 1, Iteration 92, Loss: 2.1905784606933594\n",
      "Epoch 1, Iteration 93, Loss: 2.5010790824890137\n",
      "Epoch 1, Iteration 94, Loss: 3.0153677463531494\n",
      "Epoch 1, Iteration 95, Loss: 2.6962056159973145\n",
      "Epoch 1, Iteration 96, Loss: 2.601996898651123\n",
      "Epoch 1, Iteration 97, Loss: 2.3799147605895996\n",
      "Epoch 1, Iteration 98, Loss: 2.6112074851989746\n",
      "Epoch 1, Iteration 99, Loss: 2.6229066848754883\n",
      "Epoch 1, Iteration 100, Loss: 2.4185631275177\n",
      "Epoch 1, Iteration 100, Valid Loss: 1.9101494550704956\n",
      "Epoch 1, Iteration 101, Loss: 2.8278017044067383\n",
      "Epoch 1, Iteration 102, Loss: 2.6063194274902344\n",
      "Epoch 1, Iteration 103, Loss: 2.478332042694092\n",
      "Epoch 1, Iteration 104, Loss: 3.14284086227417\n",
      "Epoch 1, Iteration 105, Loss: 2.735675096511841\n",
      "Epoch 1, Iteration 106, Loss: 2.6115188598632812\n",
      "Epoch 1, Iteration 107, Loss: 3.472679853439331\n",
      "Epoch 1, Iteration 108, Loss: 2.5908114910125732\n",
      "Epoch 1, Iteration 109, Loss: 2.914862632751465\n",
      "Epoch 1, Iteration 110, Loss: 3.028566598892212\n",
      "Epoch 1, Iteration 111, Loss: 2.69329571723938\n",
      "Epoch 1, Iteration 112, Loss: 2.638523817062378\n",
      "Epoch 1, Iteration 113, Loss: 2.368875503540039\n",
      "Epoch 1, Iteration 114, Loss: 3.1091830730438232\n",
      "Epoch 1, Iteration 115, Loss: 3.0127921104431152\n",
      "Epoch 1, Iteration 116, Loss: 2.7085485458374023\n",
      "Epoch 1, Iteration 117, Loss: 2.3850600719451904\n",
      "Epoch 1, Iteration 118, Loss: 2.7669014930725098\n",
      "Epoch 1, Iteration 119, Loss: 2.6632113456726074\n",
      "Epoch 1, Iteration 120, Loss: 2.568939208984375\n",
      "Epoch 1, Iteration 121, Loss: 2.5103979110717773\n",
      "Epoch 1, Iteration 122, Loss: 2.602283239364624\n",
      "Epoch 1, Iteration 123, Loss: 2.4584782123565674\n",
      "Epoch 1, Iteration 124, Loss: 2.579658031463623\n",
      "Epoch 1, Iteration 125, Loss: 2.276992082595825\n",
      "Epoch 1, Iteration 126, Loss: 2.5218026638031006\n",
      "Epoch 1, Iteration 127, Loss: 2.8691017627716064\n",
      "Epoch 1, Iteration 128, Loss: 2.849830389022827\n",
      "Epoch 1, Iteration 129, Loss: 2.723959445953369\n",
      "Epoch 1, Iteration 130, Loss: 3.007742404937744\n",
      "Epoch 1, Iteration 131, Loss: 2.5338141918182373\n",
      "Epoch 1, Iteration 132, Loss: 2.516120433807373\n",
      "Epoch 1, Iteration 133, Loss: 2.746861457824707\n",
      "Epoch 1, Iteration 134, Loss: 2.381638288497925\n",
      "Epoch 1, Iteration 135, Loss: 2.3638134002685547\n",
      "Epoch 1, Iteration 136, Loss: 2.5450241565704346\n",
      "Epoch 1, Iteration 137, Loss: 2.4853262901306152\n",
      "Epoch 1, Iteration 138, Loss: 2.348574161529541\n",
      "Epoch 1, Iteration 139, Loss: 2.377197265625\n",
      "Epoch 1, Iteration 140, Loss: 2.331813335418701\n",
      "Epoch 1, Iteration 141, Loss: 2.6626241207122803\n",
      "Epoch 1, Iteration 142, Loss: 2.246328353881836\n",
      "Epoch 1, Iteration 143, Loss: 2.594076156616211\n",
      "Epoch 1, Iteration 144, Loss: 2.431610345840454\n",
      "Epoch 1, Iteration 145, Loss: 2.681023597717285\n",
      "Epoch 1, Iteration 146, Loss: 2.9591221809387207\n",
      "Epoch 1, Iteration 147, Loss: 2.5458357334136963\n",
      "Epoch 1, Iteration 148, Loss: 2.0877654552459717\n",
      "Epoch 1, Iteration 149, Loss: 2.260444164276123\n",
      "Epoch 1, Iteration 150, Loss: 2.622335910797119\n",
      "Epoch 1, Iteration 150, Valid Loss: 1.8300867080688477\n",
      "Epoch 1, Iteration 151, Loss: 2.719947099685669\n",
      "Epoch 1, Iteration 152, Loss: 2.5234878063201904\n",
      "Epoch 1, Iteration 153, Loss: 2.594094753265381\n",
      "Epoch 1, Iteration 154, Loss: 2.3481295108795166\n",
      "Epoch 1, Iteration 155, Loss: 2.488255500793457\n",
      "Epoch 1, Iteration 156, Loss: 2.5492169857025146\n",
      "Epoch 1, Iteration 157, Loss: 3.0227015018463135\n",
      "Epoch 1, Iteration 158, Loss: 2.678347110748291\n",
      "Epoch 1, Iteration 159, Loss: 2.657527208328247\n",
      "Epoch 1, Iteration 160, Loss: 2.4666953086853027\n",
      "Epoch 1, Iteration 161, Loss: 2.446859836578369\n",
      "Epoch 1, Iteration 162, Loss: 2.853679656982422\n",
      "Epoch 1, Iteration 163, Loss: 2.3126211166381836\n",
      "Epoch 1, Iteration 164, Loss: 2.161681652069092\n",
      "Epoch 1, Iteration 165, Loss: 2.3011436462402344\n",
      "Epoch 1, Iteration 166, Loss: 2.3050525188446045\n",
      "Epoch 1, Iteration 167, Loss: 2.8827528953552246\n",
      "Epoch 1, Iteration 168, Loss: 2.7398979663848877\n",
      "Epoch 1, Iteration 169, Loss: 2.66719388961792\n",
      "Epoch 1, Iteration 170, Loss: 2.6387219429016113\n",
      "Epoch 1, Iteration 171, Loss: 2.8514881134033203\n",
      "Epoch 1, Iteration 172, Loss: 2.3874597549438477\n",
      "Epoch 1, Iteration 173, Loss: 2.548152446746826\n",
      "Epoch 1, Iteration 174, Loss: 2.7036831378936768\n",
      "Epoch 1, Iteration 175, Loss: 2.59183406829834\n",
      "Epoch 1, Iteration 176, Loss: 2.2608559131622314\n",
      "Epoch 1, Iteration 177, Loss: 2.8548898696899414\n",
      "Epoch 1, Iteration 178, Loss: 2.7187507152557373\n",
      "Epoch 1, Iteration 179, Loss: 2.6329803466796875\n",
      "Epoch 1, Iteration 180, Loss: 2.4543182849884033\n",
      "Epoch 1, Iteration 181, Loss: 2.8478171825408936\n",
      "Epoch 1, Iteration 182, Loss: 2.7359471321105957\n",
      "Epoch 1, Iteration 183, Loss: 2.311985969543457\n",
      "Epoch 1, Iteration 184, Loss: 2.3338398933410645\n",
      "Epoch 1, Iteration 185, Loss: 2.5089516639709473\n",
      "Epoch 1, Iteration 186, Loss: 2.4026618003845215\n",
      "Epoch 1, Iteration 187, Loss: 2.3630547523498535\n",
      "Epoch 1, Iteration 188, Loss: 2.4718127250671387\n",
      "Epoch 1, Iteration 189, Loss: 2.4324283599853516\n",
      "Epoch 1, Iteration 190, Loss: 2.8206276893615723\n",
      "Epoch 1, Iteration 191, Loss: 2.6017861366271973\n",
      "Epoch 1, Iteration 192, Loss: 2.551754951477051\n",
      "Epoch 1, Iteration 193, Loss: 2.4984207153320312\n",
      "Epoch 1, Iteration 194, Loss: 2.2547950744628906\n",
      "Epoch 1, Iteration 195, Loss: 2.310366153717041\n",
      "Epoch 1, Iteration 196, Loss: 2.5423195362091064\n",
      "Epoch 1, Iteration 197, Loss: 2.5195939540863037\n",
      "Epoch 1, Iteration 198, Loss: 2.4872071743011475\n",
      "Epoch 1, Iteration 199, Loss: 2.655186891555786\n",
      "Epoch 1, Iteration 200, Loss: 2.381539821624756\n",
      "Epoch 1, Iteration 200, Valid Loss: 1.765332579612732\n",
      "Epoch 1, Iteration 201, Loss: 2.4440059661865234\n",
      "Epoch 1, Iteration 202, Loss: 2.622429847717285\n",
      "Epoch 1, Iteration 203, Loss: 2.3156964778900146\n",
      "Epoch 1, Iteration 204, Loss: 2.4730515480041504\n",
      "Epoch 1, Iteration 205, Loss: 2.182501792907715\n",
      "Epoch 1, Iteration 206, Loss: 2.269002914428711\n",
      "Epoch 1, Iteration 207, Loss: 2.673776388168335\n",
      "Epoch 1, Iteration 208, Loss: 3.020709991455078\n",
      "Epoch 1, Iteration 209, Loss: 3.3633275032043457\n",
      "Epoch 1, Iteration 210, Loss: 2.983981132507324\n",
      "Epoch 1, Iteration 211, Loss: 3.209458827972412\n",
      "Epoch 1, Iteration 212, Loss: 2.981060028076172\n",
      "Epoch 1, Iteration 213, Loss: 2.947214126586914\n",
      "Epoch 1, Iteration 214, Loss: 2.4936132431030273\n",
      "Epoch 1, Iteration 215, Loss: 2.49436354637146\n",
      "Epoch 1, Iteration 216, Loss: 2.3953919410705566\n",
      "Epoch 1, Iteration 217, Loss: 2.6458687782287598\n",
      "Epoch 1, Iteration 218, Loss: 2.816016912460327\n",
      "Epoch 1, Iteration 219, Loss: 2.8142762184143066\n",
      "Epoch 1, Iteration 220, Loss: 2.670410394668579\n",
      "Epoch 1, Iteration 221, Loss: 2.6329727172851562\n",
      "Epoch 1, Iteration 222, Loss: 2.5261390209198\n",
      "Epoch 1, Iteration 223, Loss: 2.5003604888916016\n",
      "Epoch 1, Iteration 224, Loss: 2.3236725330352783\n",
      "Epoch 1, Iteration 225, Loss: 2.6488590240478516\n",
      "Epoch 1, Iteration 226, Loss: 2.670156478881836\n",
      "Epoch 1, Iteration 227, Loss: 2.4873852729797363\n",
      "Epoch 1, Iteration 228, Loss: 2.2820401191711426\n",
      "Epoch 1, Iteration 229, Loss: 2.311140775680542\n",
      "Epoch 1, Iteration 230, Loss: 2.636190414428711\n",
      "Epoch 1, Iteration 231, Loss: 2.530656337738037\n",
      "Epoch 1, Iteration 232, Loss: 2.5059800148010254\n",
      "Epoch 1, Iteration 233, Loss: 2.9906015396118164\n",
      "Epoch 1, Iteration 234, Loss: 2.721343517303467\n",
      "Epoch 1, Iteration 235, Loss: 2.1888437271118164\n",
      "Epoch 1, Iteration 236, Loss: 2.5676398277282715\n",
      "Epoch 1, Iteration 237, Loss: 2.5557446479797363\n",
      "Epoch 1, Iteration 238, Loss: 2.236215591430664\n",
      "Epoch 1, Iteration 239, Loss: 2.573184013366699\n",
      "Epoch 1, Iteration 240, Loss: 2.102567195892334\n",
      "Epoch 1, Iteration 241, Loss: 2.206817150115967\n",
      "Epoch 1, Iteration 242, Loss: 2.1450085639953613\n",
      "Epoch 1, Iteration 243, Loss: 2.3601021766662598\n",
      "Epoch 1, Iteration 244, Loss: 2.382343292236328\n",
      "Epoch 1, Iteration 245, Loss: 2.1201744079589844\n",
      "Epoch 1, Iteration 246, Loss: 2.421661376953125\n",
      "Epoch 1, Iteration 247, Loss: 2.2631704807281494\n",
      "Epoch 1, Iteration 248, Loss: 2.4826433658599854\n",
      "Epoch 1, Iteration 249, Loss: 2.177189350128174\n",
      "Epoch 1, Iteration 250, Loss: 2.5035276412963867\n",
      "Epoch 1, Iteration 250, Valid Loss: 1.68952214717865\n",
      "Epoch 1, Iteration 251, Loss: 2.365046977996826\n",
      "Epoch 1, Iteration 252, Loss: 2.4270150661468506\n",
      "Epoch 1, Iteration 253, Loss: 2.5271384716033936\n",
      "Epoch 1, Iteration 254, Loss: 2.4443721771240234\n",
      "Epoch 1, Iteration 255, Loss: 2.49369478225708\n",
      "Epoch 1, Iteration 256, Loss: 2.6767992973327637\n",
      "Epoch 1, Iteration 257, Loss: 2.210475444793701\n",
      "Epoch 1, Iteration 258, Loss: 2.712580919265747\n",
      "Epoch 1, Iteration 259, Loss: 2.1237823963165283\n",
      "Epoch 1, Iteration 260, Loss: 2.41736102104187\n",
      "Epoch 1, Iteration 261, Loss: 2.522493839263916\n",
      "Epoch 1, Iteration 262, Loss: 2.1298024654388428\n",
      "Epoch 1, Iteration 263, Loss: 2.648317337036133\n",
      "Epoch 1, Iteration 264, Loss: 2.525376319885254\n",
      "Epoch 1, Iteration 265, Loss: 2.452493190765381\n",
      "Epoch 1, Iteration 266, Loss: 2.4815707206726074\n",
      "Epoch 1, Iteration 267, Loss: 2.3857083320617676\n",
      "Epoch 1, Iteration 268, Loss: 2.308835983276367\n",
      "Epoch 1, Iteration 269, Loss: 2.304215669631958\n",
      "Epoch 1, Iteration 270, Loss: 2.288381576538086\n",
      "Epoch 1, Iteration 271, Loss: 2.4869730472564697\n",
      "Epoch 1, Iteration 272, Loss: 2.22098970413208\n",
      "Epoch 1, Iteration 273, Loss: 2.120967149734497\n",
      "Epoch 1, Iteration 274, Loss: 2.165925979614258\n",
      "Epoch 1, Iteration 275, Loss: 2.1176483631134033\n",
      "Epoch 1, Iteration 276, Loss: 2.1733551025390625\n",
      "Epoch 1, Iteration 277, Loss: 2.1782917976379395\n",
      "Epoch 1, Iteration 278, Loss: 2.4260427951812744\n",
      "Epoch 1, Iteration 279, Loss: 2.2003870010375977\n",
      "Epoch 1, Iteration 280, Loss: 2.0047311782836914\n",
      "Epoch 1, Iteration 281, Loss: 2.081037759780884\n",
      "Epoch 1, Iteration 282, Loss: 2.390122890472412\n",
      "Epoch 1, Iteration 283, Loss: 2.3455092906951904\n",
      "Epoch 1, Iteration 284, Loss: 2.5062098503112793\n",
      "Epoch 1, Iteration 285, Loss: 2.2450950145721436\n",
      "Epoch 1, Iteration 286, Loss: 2.2038962841033936\n",
      "Epoch 1, Iteration 287, Loss: 2.179431200027466\n",
      "Epoch 1, Iteration 288, Loss: 2.088042736053467\n",
      "Epoch 1, Iteration 289, Loss: 2.270559787750244\n",
      "Epoch 1, Iteration 290, Loss: 2.112051248550415\n",
      "Epoch 1, Iteration 291, Loss: 2.307587146759033\n",
      "Epoch 1, Iteration 292, Loss: 2.6125452518463135\n",
      "Epoch 1, Iteration 293, Loss: 2.5254201889038086\n",
      "Epoch 1, Iteration 294, Loss: 2.123588800430298\n",
      "Epoch 1, Iteration 295, Loss: 2.160126209259033\n",
      "Epoch 1, Iteration 296, Loss: 2.6193137168884277\n",
      "Epoch 1, Iteration 297, Loss: 2.474640369415283\n",
      "Epoch 1, Iteration 298, Loss: 2.3382205963134766\n",
      "Epoch 1, Iteration 299, Loss: 1.9627776145935059\n",
      "Epoch 1, Iteration 300, Loss: 2.0814061164855957\n",
      "Epoch 1, Iteration 300, Valid Loss: 1.6283998489379883\n",
      "Epoch 1, Iteration 301, Loss: 2.3136165142059326\n",
      "Epoch 1, Iteration 302, Loss: 2.230719804763794\n",
      "Epoch 1, Iteration 303, Loss: 2.6891419887542725\n",
      "Epoch 1, Iteration 304, Loss: 2.0816075801849365\n",
      "Epoch 1, Iteration 305, Loss: 2.340463161468506\n",
      "Epoch 1, Iteration 306, Loss: 2.3947696685791016\n",
      "Epoch 1, Iteration 307, Loss: 2.093515634536743\n",
      "Epoch 1, Iteration 308, Loss: 2.166707992553711\n",
      "Epoch 1, Iteration 309, Loss: 2.1189637184143066\n",
      "Epoch 1, Iteration 310, Loss: 2.5303659439086914\n",
      "Epoch 1, Iteration 311, Loss: 2.018118381500244\n",
      "Epoch 1, Iteration 312, Loss: 2.0426554679870605\n",
      "Epoch 1, Iteration 313, Loss: 2.283726692199707\n",
      "Epoch 1, Iteration 314, Loss: 2.1380627155303955\n",
      "Epoch 1, Iteration 315, Loss: 2.0876429080963135\n",
      "Epoch 1, Iteration 316, Loss: 2.1197471618652344\n",
      "Epoch 1, Iteration 317, Loss: 2.132847309112549\n",
      "Epoch 1, Iteration 318, Loss: 2.1174373626708984\n",
      "Epoch 1, Iteration 319, Loss: 2.2444114685058594\n",
      "Epoch 1, Iteration 320, Loss: 1.9696483612060547\n",
      "Epoch 1, Iteration 321, Loss: 2.069786310195923\n",
      "Epoch 1, Iteration 322, Loss: 2.0206265449523926\n",
      "Epoch 1, Iteration 323, Loss: 2.152468204498291\n",
      "Epoch 1, Iteration 324, Loss: 2.3122990131378174\n",
      "Epoch 1, Iteration 325, Loss: 2.1322150230407715\n",
      "Epoch 1, Iteration 326, Loss: 2.1725566387176514\n",
      "Epoch 1, Iteration 327, Loss: 2.224168062210083\n",
      "Epoch 1, Iteration 328, Loss: 2.1094627380371094\n",
      "Epoch 1, Iteration 329, Loss: 2.4502780437469482\n",
      "Epoch 1, Iteration 330, Loss: 1.9047563076019287\n",
      "Epoch 1, Iteration 331, Loss: 1.9482529163360596\n",
      "Epoch 1, Iteration 332, Loss: 2.207941770553589\n",
      "Epoch 1, Iteration 333, Loss: 2.4555821418762207\n",
      "Epoch 1, Iteration 334, Loss: 2.228618621826172\n",
      "Epoch 1, Iteration 335, Loss: 2.341294765472412\n",
      "Epoch 1, Iteration 336, Loss: 2.2027230262756348\n",
      "Epoch 1, Iteration 337, Loss: 1.9787497520446777\n",
      "Epoch 1, Iteration 338, Loss: 2.2882418632507324\n",
      "Epoch 1, Iteration 339, Loss: 2.171067237854004\n",
      "Epoch 1, Iteration 340, Loss: 1.971839189529419\n",
      "Epoch 1, Iteration 341, Loss: 2.1621549129486084\n",
      "Epoch 1, Iteration 342, Loss: 1.9842110872268677\n",
      "Epoch 1, Iteration 343, Loss: 2.103533983230591\n",
      "Epoch 1, Iteration 344, Loss: 2.264626979827881\n",
      "Epoch 1, Iteration 345, Loss: 2.267334461212158\n",
      "Epoch 1, Iteration 346, Loss: 2.152834892272949\n",
      "Epoch 1, Iteration 347, Loss: 1.8919835090637207\n",
      "Epoch 1, Iteration 348, Loss: 1.9597171545028687\n",
      "Epoch 1, Iteration 349, Loss: 2.3375935554504395\n",
      "Epoch 1, Iteration 350, Loss: 1.9475200176239014\n",
      "Epoch 1, Iteration 350, Valid Loss: 1.5681591033935547\n",
      "Epoch 1, Iteration 351, Loss: 2.2800803184509277\n",
      "Epoch 1, Iteration 352, Loss: 2.022433280944824\n",
      "Epoch 1, Iteration 353, Loss: 2.3380930423736572\n",
      "Epoch 1, Iteration 354, Loss: 2.2082972526550293\n",
      "Epoch 1, Iteration 355, Loss: 2.011723518371582\n",
      "Epoch 1, Iteration 356, Loss: 2.172292947769165\n",
      "Epoch 1, Iteration 357, Loss: 2.1731116771698\n",
      "Epoch 1, Iteration 358, Loss: 2.116807699203491\n",
      "Epoch 1, Iteration 359, Loss: 2.0247178077697754\n",
      "Epoch 1, Iteration 360, Loss: 1.9530994892120361\n",
      "Epoch 1, Iteration 361, Loss: 2.0484204292297363\n",
      "Epoch 1, Iteration 362, Loss: 2.0422732830047607\n",
      "Epoch 1, Iteration 363, Loss: 2.1841890811920166\n",
      "Epoch 1, Iteration 364, Loss: 2.077766180038452\n",
      "Epoch 1, Iteration 365, Loss: 2.5500409603118896\n",
      "Epoch 1, Iteration 366, Loss: 2.45626163482666\n",
      "Epoch 1, Iteration 367, Loss: 2.2972538471221924\n",
      "Epoch 1, Iteration 368, Loss: 2.1800365447998047\n",
      "Epoch 1, Iteration 369, Loss: 2.341439723968506\n",
      "Epoch 1, Iteration 370, Loss: 2.264253616333008\n",
      "Epoch 1, Iteration 371, Loss: 2.2345190048217773\n",
      "Epoch 1, Iteration 372, Loss: 1.8584774732589722\n",
      "Epoch 1, Iteration 373, Loss: 2.0580334663391113\n",
      "Epoch 1, Iteration 374, Loss: 2.4043009281158447\n",
      "Epoch 1, Iteration 375, Loss: 2.0695583820343018\n",
      "Epoch 1, Iteration 376, Loss: 2.3634438514709473\n",
      "Epoch 1, Iteration 377, Loss: 2.277512550354004\n",
      "Epoch 1, Iteration 378, Loss: 2.0501742362976074\n",
      "Epoch 1, Iteration 379, Loss: 2.1528944969177246\n",
      "Epoch 1, Iteration 380, Loss: 2.096625328063965\n",
      "Epoch 1, Iteration 381, Loss: 1.9787874221801758\n",
      "Epoch 1, Iteration 382, Loss: 1.922613501548767\n",
      "Epoch 1, Iteration 383, Loss: 2.088258743286133\n",
      "Epoch 1, Iteration 384, Loss: 2.1069037914276123\n",
      "Epoch 1, Iteration 385, Loss: 1.9982714653015137\n",
      "Epoch 1, Iteration 386, Loss: 1.9964567422866821\n",
      "Epoch 1, Iteration 387, Loss: 2.650114059448242\n",
      "Epoch 1, Iteration 388, Loss: 2.1228854656219482\n",
      "Epoch 1, Iteration 389, Loss: 2.2169735431671143\n",
      "Epoch 1, Iteration 390, Loss: 2.320500135421753\n",
      "Epoch 1, Iteration 391, Loss: 2.0929336547851562\n",
      "Epoch 2/10, Loss: 2.501201459643792\n",
      "Epoch 2, Iteration 0, Loss: 2.286557674407959\n",
      "Epoch 2, Iteration 1, Loss: 2.614622116088867\n",
      "Epoch 2, Iteration 2, Loss: 2.4487786293029785\n",
      "Epoch 2, Iteration 3, Loss: 2.3690881729125977\n",
      "Epoch 2, Iteration 4, Loss: 2.3142921924591064\n",
      "Epoch 2, Iteration 5, Loss: 2.382401704788208\n",
      "Epoch 2, Iteration 6, Loss: 2.190772294998169\n",
      "Epoch 2, Iteration 7, Loss: 2.346938371658325\n",
      "Epoch 2, Iteration 8, Loss: 2.4393463134765625\n",
      "Epoch 2, Iteration 9, Loss: 2.6392405033111572\n",
      "Epoch 2, Iteration 10, Loss: 2.2874512672424316\n",
      "Epoch 2, Iteration 11, Loss: 2.157971143722534\n",
      "Epoch 2, Iteration 12, Loss: 2.2852015495300293\n",
      "Epoch 2, Iteration 13, Loss: 2.485421657562256\n",
      "Epoch 2, Iteration 14, Loss: 2.1346280574798584\n",
      "Epoch 2, Iteration 15, Loss: 2.393770694732666\n",
      "Epoch 2, Iteration 16, Loss: 2.1718499660491943\n",
      "Epoch 2, Iteration 17, Loss: 2.432003974914551\n",
      "Epoch 2, Iteration 18, Loss: 2.3318567276000977\n",
      "Epoch 2, Iteration 19, Loss: 2.3828635215759277\n",
      "Epoch 2, Iteration 20, Loss: 2.2084734439849854\n",
      "Epoch 2, Iteration 21, Loss: 2.2867214679718018\n",
      "Epoch 2, Iteration 22, Loss: 2.278371810913086\n",
      "Epoch 2, Iteration 23, Loss: 2.269719123840332\n",
      "Epoch 2, Iteration 24, Loss: 2.086919069290161\n",
      "Epoch 2, Iteration 25, Loss: 2.007523536682129\n",
      "Epoch 2, Iteration 26, Loss: 2.584179639816284\n",
      "Epoch 2, Iteration 27, Loss: 2.1719720363616943\n",
      "Epoch 2, Iteration 28, Loss: 2.194897174835205\n",
      "Epoch 2, Iteration 29, Loss: 2.4409170150756836\n",
      "Epoch 2, Iteration 30, Loss: 2.276454210281372\n",
      "Epoch 2, Iteration 31, Loss: 2.25176739692688\n",
      "Epoch 2, Iteration 32, Loss: 2.2687740325927734\n",
      "Epoch 2, Iteration 33, Loss: 2.1357016563415527\n",
      "Epoch 2, Iteration 34, Loss: 2.175964832305908\n",
      "Epoch 2, Iteration 35, Loss: 2.2780418395996094\n",
      "Epoch 2, Iteration 36, Loss: 1.948418140411377\n",
      "Epoch 2, Iteration 37, Loss: 1.9796761274337769\n",
      "Epoch 2, Iteration 38, Loss: 2.085378646850586\n",
      "Epoch 2, Iteration 39, Loss: 2.048222780227661\n",
      "Epoch 2, Iteration 40, Loss: 2.221381187438965\n",
      "Epoch 2, Iteration 41, Loss: 2.194794178009033\n",
      "Epoch 2, Iteration 42, Loss: 2.3149428367614746\n",
      "Epoch 2, Iteration 43, Loss: 2.1628475189208984\n",
      "Epoch 2, Iteration 44, Loss: 2.284600257873535\n",
      "Epoch 2, Iteration 45, Loss: 2.120267152786255\n",
      "Epoch 2, Iteration 46, Loss: 2.2481346130371094\n",
      "Epoch 2, Iteration 47, Loss: 2.304439067840576\n",
      "Epoch 2, Iteration 48, Loss: 2.4795522689819336\n",
      "Epoch 2, Iteration 49, Loss: 2.244668483734131\n",
      "Epoch 2, Iteration 50, Loss: 2.162907600402832\n",
      "Epoch 2, Iteration 50, Valid Loss: 1.4213221073150635\n",
      "Epoch 2, Iteration 51, Loss: 2.0878076553344727\n",
      "Epoch 2, Iteration 52, Loss: 2.2632603645324707\n",
      "Epoch 2, Iteration 53, Loss: 2.108198404312134\n",
      "Epoch 2, Iteration 54, Loss: 4.600985527038574\n",
      "Epoch 2, Iteration 55, Loss: 2.2870593070983887\n",
      "Epoch 2, Iteration 56, Loss: 1.9840397834777832\n",
      "Epoch 2, Iteration 57, Loss: 2.1654469966888428\n",
      "Epoch 2, Iteration 58, Loss: 1.908090353012085\n",
      "Epoch 2, Iteration 59, Loss: 2.1883792877197266\n",
      "Epoch 2, Iteration 60, Loss: 2.6085245609283447\n",
      "Epoch 2, Iteration 61, Loss: 1.8684709072113037\n",
      "Epoch 2, Iteration 62, Loss: 2.101689338684082\n",
      "Epoch 2, Iteration 63, Loss: 1.969943642616272\n",
      "Epoch 2, Iteration 64, Loss: 1.9301307201385498\n",
      "Epoch 2, Iteration 65, Loss: 2.2052161693573\n",
      "Epoch 2, Iteration 66, Loss: 2.1853466033935547\n",
      "Epoch 2, Iteration 67, Loss: 1.8913639783859253\n",
      "Epoch 2, Iteration 68, Loss: 2.4045915603637695\n",
      "Epoch 2, Iteration 69, Loss: 2.384411096572876\n",
      "Epoch 2, Iteration 70, Loss: 2.2909042835235596\n",
      "Epoch 2, Iteration 71, Loss: 2.2285218238830566\n",
      "Epoch 2, Iteration 72, Loss: 2.469818353652954\n",
      "Epoch 2, Iteration 73, Loss: 1.7943761348724365\n",
      "Epoch 2, Iteration 74, Loss: 2.350026845932007\n",
      "Epoch 2, Iteration 75, Loss: 2.044023275375366\n",
      "Epoch 2, Iteration 76, Loss: 2.080824375152588\n",
      "Epoch 2, Iteration 77, Loss: 1.7879729270935059\n",
      "Epoch 2, Iteration 78, Loss: 2.49055552482605\n",
      "Epoch 2, Iteration 79, Loss: 2.4874320030212402\n",
      "Epoch 2, Iteration 80, Loss: 2.0746617317199707\n",
      "Epoch 2, Iteration 81, Loss: 2.358713150024414\n",
      "Epoch 2, Iteration 82, Loss: 2.2870123386383057\n",
      "Epoch 2, Iteration 83, Loss: 1.8669553995132446\n",
      "Epoch 2, Iteration 84, Loss: 2.282327651977539\n",
      "Epoch 2, Iteration 85, Loss: 2.162407875061035\n",
      "Epoch 2, Iteration 86, Loss: 2.164499282836914\n",
      "Epoch 2, Iteration 87, Loss: 2.0206618309020996\n",
      "Epoch 2, Iteration 88, Loss: 2.1260759830474854\n",
      "Epoch 2, Iteration 89, Loss: 2.1906001567840576\n",
      "Epoch 2, Iteration 90, Loss: 1.639358401298523\n",
      "Epoch 2, Iteration 91, Loss: 2.035695791244507\n",
      "Epoch 2, Iteration 92, Loss: 1.6431154012680054\n",
      "Epoch 2, Iteration 93, Loss: 2.0404434204101562\n",
      "Epoch 2, Iteration 94, Loss: 2.5114688873291016\n",
      "Epoch 2, Iteration 95, Loss: 2.067798137664795\n",
      "Epoch 2, Iteration 96, Loss: 2.119192600250244\n",
      "Epoch 2, Iteration 97, Loss: 1.841003179550171\n",
      "Epoch 2, Iteration 98, Loss: 2.2519705295562744\n",
      "Epoch 2, Iteration 99, Loss: 2.0609309673309326\n",
      "Epoch 2, Iteration 100, Loss: 1.9864476919174194\n",
      "Epoch 2, Iteration 100, Valid Loss: 1.4327645301818848\n",
      "Epoch 2, Iteration 101, Loss: 2.433574676513672\n",
      "Epoch 2, Iteration 102, Loss: 2.2646055221557617\n",
      "Epoch 2, Iteration 103, Loss: 2.0611140727996826\n",
      "Epoch 2, Iteration 104, Loss: 2.56742525100708\n",
      "Epoch 2, Iteration 105, Loss: 2.2528176307678223\n",
      "Epoch 2, Iteration 106, Loss: 2.098330497741699\n",
      "Epoch 2, Iteration 107, Loss: 2.988760232925415\n",
      "Epoch 2, Iteration 108, Loss: 2.1841814517974854\n",
      "Epoch 2, Iteration 109, Loss: 2.3368499279022217\n",
      "Epoch 2, Iteration 110, Loss: 2.3855717182159424\n",
      "Epoch 2, Iteration 111, Loss: 2.2859115600585938\n",
      "Epoch 2, Iteration 112, Loss: 2.1882638931274414\n",
      "Epoch 2, Iteration 113, Loss: 1.8568365573883057\n",
      "Epoch 2, Iteration 114, Loss: 2.662118434906006\n",
      "Epoch 2, Iteration 115, Loss: 2.4008641242980957\n",
      "Epoch 2, Iteration 116, Loss: 2.1028356552124023\n",
      "Epoch 2, Iteration 117, Loss: 1.8301891088485718\n",
      "Epoch 2, Iteration 118, Loss: 2.270049810409546\n",
      "Epoch 2, Iteration 119, Loss: 2.1909773349761963\n",
      "Epoch 2, Iteration 120, Loss: 1.9862655401229858\n",
      "Epoch 2, Iteration 121, Loss: 1.989842414855957\n",
      "Epoch 2, Iteration 122, Loss: 2.065607786178589\n",
      "Epoch 2, Iteration 123, Loss: 1.8876595497131348\n",
      "Epoch 2, Iteration 124, Loss: 2.0782902240753174\n",
      "Epoch 2, Iteration 125, Loss: 1.894863486289978\n",
      "Epoch 2, Iteration 126, Loss: 1.8142839670181274\n",
      "Epoch 2, Iteration 127, Loss: 2.5226454734802246\n",
      "Epoch 2, Iteration 128, Loss: 2.306884288787842\n",
      "Epoch 2, Iteration 129, Loss: 2.178489923477173\n",
      "Epoch 2, Iteration 130, Loss: 2.4563405513763428\n",
      "Epoch 2, Iteration 131, Loss: 1.9575656652450562\n",
      "Epoch 2, Iteration 132, Loss: 1.9815584421157837\n",
      "Epoch 2, Iteration 133, Loss: 2.463592767715454\n",
      "Epoch 2, Iteration 134, Loss: 1.8528800010681152\n",
      "Epoch 2, Iteration 135, Loss: 2.1706197261810303\n",
      "Epoch 2, Iteration 136, Loss: 1.9732637405395508\n",
      "Epoch 2, Iteration 137, Loss: 1.9684380292892456\n",
      "Epoch 2, Iteration 138, Loss: 1.9924395084381104\n",
      "Epoch 2, Iteration 139, Loss: 1.9029819965362549\n",
      "Epoch 2, Iteration 140, Loss: 2.0194528102874756\n",
      "Epoch 2, Iteration 141, Loss: 2.517256736755371\n",
      "Epoch 2, Iteration 142, Loss: 1.8961538076400757\n",
      "Epoch 2, Iteration 143, Loss: 2.097055196762085\n",
      "Epoch 2, Iteration 144, Loss: 1.9962385892868042\n",
      "Epoch 2, Iteration 145, Loss: 2.2070603370666504\n",
      "Epoch 2, Iteration 146, Loss: 2.262887954711914\n",
      "Epoch 2, Iteration 147, Loss: 1.9534482955932617\n",
      "Epoch 2, Iteration 148, Loss: 1.715044617652893\n",
      "Epoch 2, Iteration 149, Loss: 1.9373698234558105\n",
      "Epoch 2, Iteration 150, Loss: 2.183907985687256\n",
      "Epoch 2, Iteration 150, Valid Loss: 1.3856003284454346\n",
      "Epoch 2, Iteration 151, Loss: 2.125739574432373\n",
      "Epoch 2, Iteration 152, Loss: 1.9286315441131592\n",
      "Epoch 2, Iteration 153, Loss: 2.1813530921936035\n",
      "Epoch 2, Iteration 154, Loss: 1.9181668758392334\n",
      "Epoch 2, Iteration 155, Loss: 2.22383975982666\n",
      "Epoch 2, Iteration 156, Loss: 1.9474704265594482\n",
      "Epoch 2, Iteration 157, Loss: 2.4764509201049805\n",
      "Epoch 2, Iteration 158, Loss: 2.1043734550476074\n",
      "Epoch 2, Iteration 159, Loss: 2.118313789367676\n",
      "Epoch 2, Iteration 160, Loss: 1.9635858535766602\n",
      "Epoch 2, Iteration 161, Loss: 2.0313117504119873\n",
      "Epoch 2, Iteration 162, Loss: 2.4122838973999023\n",
      "Epoch 2, Iteration 163, Loss: 1.861215591430664\n",
      "Epoch 2, Iteration 164, Loss: 1.9887120723724365\n",
      "Epoch 2, Iteration 165, Loss: 1.9791063070297241\n",
      "Epoch 2, Iteration 166, Loss: 1.9717398881912231\n",
      "Epoch 2, Iteration 167, Loss: 2.418872117996216\n",
      "Epoch 2, Iteration 168, Loss: 2.246973991394043\n",
      "Epoch 2, Iteration 169, Loss: 2.1547436714172363\n",
      "Epoch 2, Iteration 170, Loss: 2.230612277984619\n",
      "Epoch 2, Iteration 171, Loss: 2.136319875717163\n",
      "Epoch 2, Iteration 172, Loss: 1.9036263227462769\n",
      "Epoch 2, Iteration 173, Loss: 2.0495996475219727\n",
      "Epoch 2, Iteration 174, Loss: 2.1942877769470215\n",
      "Epoch 2, Iteration 175, Loss: 2.229475498199463\n",
      "Epoch 2, Iteration 176, Loss: 1.846193790435791\n",
      "Epoch 2, Iteration 177, Loss: 2.551244020462036\n",
      "Epoch 2, Iteration 178, Loss: 2.340250253677368\n",
      "Epoch 2, Iteration 179, Loss: 2.1640238761901855\n",
      "Epoch 2, Iteration 180, Loss: 2.035254955291748\n",
      "Epoch 2, Iteration 181, Loss: 2.455217123031616\n",
      "Epoch 2, Iteration 182, Loss: 2.3375377655029297\n",
      "Epoch 2, Iteration 183, Loss: 1.8851567506790161\n",
      "Epoch 2, Iteration 184, Loss: 1.9357702732086182\n",
      "Epoch 2, Iteration 185, Loss: 1.9999479055404663\n",
      "Epoch 2, Iteration 186, Loss: 1.9416370391845703\n",
      "Epoch 2, Iteration 187, Loss: 1.9533491134643555\n",
      "Epoch 2, Iteration 188, Loss: 2.025179624557495\n",
      "Epoch 2, Iteration 189, Loss: 2.0295631885528564\n",
      "Epoch 2, Iteration 190, Loss: 2.387248992919922\n",
      "Epoch 2, Iteration 191, Loss: 2.2403266429901123\n",
      "Epoch 2, Iteration 192, Loss: 2.171095848083496\n",
      "Epoch 2, Iteration 193, Loss: 2.080294609069824\n",
      "Epoch 2, Iteration 194, Loss: 2.048478841781616\n",
      "Epoch 2, Iteration 195, Loss: 1.802522897720337\n",
      "Epoch 2, Iteration 196, Loss: 2.0860238075256348\n",
      "Epoch 2, Iteration 197, Loss: 2.090195894241333\n",
      "Epoch 2, Iteration 198, Loss: 2.0340468883514404\n",
      "Epoch 2, Iteration 199, Loss: 2.171344041824341\n",
      "Epoch 2, Iteration 200, Loss: 1.9438915252685547\n",
      "Epoch 2, Iteration 200, Valid Loss: 1.3462024927139282\n",
      "Epoch 2, Iteration 201, Loss: 1.9121882915496826\n",
      "Epoch 2, Iteration 202, Loss: 2.148733139038086\n",
      "Epoch 2, Iteration 203, Loss: 1.8877822160720825\n",
      "Epoch 2, Iteration 204, Loss: 2.0536205768585205\n",
      "Epoch 2, Iteration 205, Loss: 1.729576587677002\n",
      "Epoch 2, Iteration 206, Loss: 1.8970060348510742\n",
      "Epoch 2, Iteration 207, Loss: 2.20676326751709\n",
      "Epoch 2, Iteration 208, Loss: 2.6616551876068115\n",
      "Epoch 2, Iteration 209, Loss: 3.0502357482910156\n",
      "Epoch 2, Iteration 210, Loss: 2.5110857486724854\n",
      "Epoch 2, Iteration 211, Loss: 2.770214557647705\n",
      "Epoch 2, Iteration 212, Loss: 2.4686129093170166\n",
      "Epoch 2, Iteration 213, Loss: 2.590967893600464\n",
      "Epoch 2, Iteration 214, Loss: 2.1375529766082764\n",
      "Epoch 2, Iteration 215, Loss: 1.9817966222763062\n",
      "Epoch 2, Iteration 216, Loss: 1.9952163696289062\n",
      "Epoch 2, Iteration 217, Loss: 2.445341110229492\n",
      "Epoch 2, Iteration 218, Loss: 2.3014204502105713\n",
      "Epoch 2, Iteration 219, Loss: 2.2331719398498535\n",
      "Epoch 2, Iteration 220, Loss: 2.177527904510498\n",
      "Epoch 2, Iteration 221, Loss: 2.1814441680908203\n",
      "Epoch 2, Iteration 222, Loss: 2.0641705989837646\n",
      "Epoch 2, Iteration 223, Loss: 2.1376614570617676\n",
      "Epoch 2, Iteration 224, Loss: 2.080907106399536\n",
      "Epoch 2, Iteration 225, Loss: 2.1908156871795654\n",
      "Epoch 2, Iteration 226, Loss: 2.295016050338745\n",
      "Epoch 2, Iteration 227, Loss: 2.128798484802246\n",
      "Epoch 2, Iteration 228, Loss: 1.9457005262374878\n",
      "Epoch 2, Iteration 229, Loss: 2.0115182399749756\n",
      "Epoch 2, Iteration 230, Loss: 2.226147174835205\n",
      "Epoch 2, Iteration 231, Loss: 2.0994460582733154\n",
      "Epoch 2, Iteration 232, Loss: 2.23634934425354\n",
      "Epoch 2, Iteration 233, Loss: 2.599374532699585\n",
      "Epoch 2, Iteration 234, Loss: 2.3429858684539795\n",
      "Epoch 2, Iteration 235, Loss: 1.8240832090377808\n",
      "Epoch 2, Iteration 236, Loss: 2.1357641220092773\n",
      "Epoch 2, Iteration 237, Loss: 2.0123982429504395\n",
      "Epoch 2, Iteration 238, Loss: 1.8875737190246582\n",
      "Epoch 2, Iteration 239, Loss: 2.1969351768493652\n",
      "Epoch 2, Iteration 240, Loss: 1.7195281982421875\n",
      "Epoch 2, Iteration 241, Loss: 1.8051626682281494\n",
      "Epoch 2, Iteration 242, Loss: 1.811455249786377\n",
      "Epoch 2, Iteration 243, Loss: 1.8903343677520752\n",
      "Epoch 2, Iteration 244, Loss: 1.885507583618164\n",
      "Epoch 2, Iteration 245, Loss: 1.6984268426895142\n",
      "Epoch 2, Iteration 246, Loss: 2.0087549686431885\n",
      "Epoch 2, Iteration 247, Loss: 1.8542749881744385\n",
      "Epoch 2, Iteration 248, Loss: 2.075773239135742\n",
      "Epoch 2, Iteration 249, Loss: 2.006000280380249\n",
      "Epoch 2, Iteration 250, Loss: 2.162698745727539\n",
      "Epoch 2, Iteration 250, Valid Loss: 1.3049259185791016\n",
      "Epoch 2, Iteration 251, Loss: 1.9742088317871094\n",
      "Epoch 2, Iteration 252, Loss: 2.1591460704803467\n",
      "Epoch 2, Iteration 253, Loss: 2.2223827838897705\n",
      "Epoch 2, Iteration 254, Loss: 2.072493553161621\n",
      "Epoch 2, Iteration 255, Loss: 2.1565940380096436\n",
      "Epoch 2, Iteration 256, Loss: 2.3472816944122314\n",
      "Epoch 2, Iteration 257, Loss: 2.024179220199585\n",
      "Epoch 2, Iteration 258, Loss: 2.3193531036376953\n",
      "Epoch 2, Iteration 259, Loss: 1.7896747589111328\n",
      "Epoch 2, Iteration 260, Loss: 2.1168811321258545\n",
      "Epoch 2, Iteration 261, Loss: 2.248678207397461\n",
      "Epoch 2, Iteration 262, Loss: 1.8478422164916992\n",
      "Epoch 2, Iteration 263, Loss: 2.2681431770324707\n",
      "Epoch 2, Iteration 264, Loss: 2.0776829719543457\n",
      "Epoch 2, Iteration 265, Loss: 2.0884451866149902\n",
      "Epoch 2, Iteration 266, Loss: 2.1199557781219482\n",
      "Epoch 2, Iteration 267, Loss: 2.0518977642059326\n",
      "Epoch 2, Iteration 268, Loss: 2.076777458190918\n",
      "Epoch 2, Iteration 269, Loss: 1.978698968887329\n",
      "Epoch 2, Iteration 270, Loss: 1.8976309299468994\n",
      "Epoch 2, Iteration 271, Loss: 2.1241865158081055\n",
      "Epoch 2, Iteration 272, Loss: 1.919883131980896\n",
      "Epoch 2, Iteration 273, Loss: 2.154454231262207\n",
      "Epoch 2, Iteration 274, Loss: 1.7748363018035889\n",
      "Epoch 2, Iteration 275, Loss: 1.7188624143600464\n",
      "Epoch 2, Iteration 276, Loss: 1.8906900882720947\n",
      "Epoch 2, Iteration 277, Loss: 1.900101661682129\n",
      "Epoch 2, Iteration 278, Loss: 2.137911319732666\n",
      "Epoch 2, Iteration 279, Loss: 1.8481857776641846\n",
      "Epoch 2, Iteration 280, Loss: 1.637709379196167\n",
      "Epoch 2, Iteration 281, Loss: 1.7629077434539795\n",
      "Epoch 2, Iteration 282, Loss: 2.0265326499938965\n",
      "Epoch 2, Iteration 283, Loss: 1.9493041038513184\n",
      "Epoch 2, Iteration 284, Loss: 2.0075225830078125\n",
      "Epoch 2, Iteration 285, Loss: 1.8201749324798584\n",
      "Epoch 2, Iteration 286, Loss: 1.9577265977859497\n",
      "Epoch 2, Iteration 287, Loss: 1.9044930934906006\n",
      "Epoch 2, Iteration 288, Loss: 1.818145751953125\n",
      "Epoch 2, Iteration 289, Loss: 1.9698574542999268\n",
      "Epoch 2, Iteration 290, Loss: 1.8352595567703247\n",
      "Epoch 2, Iteration 291, Loss: 1.8840535879135132\n",
      "Epoch 2, Iteration 292, Loss: 2.252629518508911\n",
      "Epoch 2, Iteration 293, Loss: 2.1793670654296875\n",
      "Epoch 2, Iteration 294, Loss: 1.865281581878662\n",
      "Epoch 2, Iteration 295, Loss: 1.9119187593460083\n",
      "Epoch 2, Iteration 296, Loss: 2.2657687664031982\n",
      "Epoch 2, Iteration 297, Loss: 2.1381402015686035\n",
      "Epoch 2, Iteration 298, Loss: 2.03503680229187\n",
      "Epoch 2, Iteration 299, Loss: 1.663654088973999\n",
      "Epoch 2, Iteration 300, Loss: 1.7111482620239258\n",
      "Epoch 2, Iteration 300, Valid Loss: 1.2681950330734253\n",
      "Epoch 2, Iteration 301, Loss: 1.9531176090240479\n",
      "Epoch 2, Iteration 302, Loss: 1.6202366352081299\n",
      "Epoch 2, Iteration 303, Loss: 2.353825569152832\n",
      "Epoch 2, Iteration 304, Loss: 1.8361741304397583\n",
      "Epoch 2, Iteration 305, Loss: 1.869588851928711\n",
      "Epoch 2, Iteration 306, Loss: 1.9020874500274658\n",
      "Epoch 2, Iteration 307, Loss: 1.7567496299743652\n",
      "Epoch 2, Iteration 308, Loss: 1.8369019031524658\n",
      "Epoch 2, Iteration 309, Loss: 1.8667778968811035\n",
      "Epoch 2, Iteration 310, Loss: 2.197144031524658\n",
      "Epoch 2, Iteration 311, Loss: 1.7186299562454224\n",
      "Epoch 2, Iteration 312, Loss: 1.7634892463684082\n",
      "Epoch 2, Iteration 313, Loss: 2.065945863723755\n",
      "Epoch 2, Iteration 314, Loss: 1.7706518173217773\n",
      "Epoch 2, Iteration 315, Loss: 1.636157751083374\n",
      "Epoch 2, Iteration 316, Loss: 1.924253225326538\n",
      "Epoch 2, Iteration 317, Loss: 1.8084675073623657\n",
      "Epoch 2, Iteration 318, Loss: 1.7598590850830078\n",
      "Epoch 2, Iteration 319, Loss: 2.036818742752075\n",
      "Epoch 2, Iteration 320, Loss: 1.6256771087646484\n",
      "Epoch 2, Iteration 321, Loss: 1.860236406326294\n",
      "Epoch 2, Iteration 322, Loss: 1.7952277660369873\n",
      "Epoch 2, Iteration 323, Loss: 1.8297789096832275\n",
      "Epoch 2, Iteration 324, Loss: 1.8872783184051514\n",
      "Epoch 2, Iteration 325, Loss: 1.8373396396636963\n",
      "Epoch 2, Iteration 326, Loss: 1.8843228816986084\n",
      "Epoch 2, Iteration 327, Loss: 1.8274425268173218\n",
      "Epoch 2, Iteration 328, Loss: 1.7584458589553833\n",
      "Epoch 2, Iteration 329, Loss: 1.9948744773864746\n",
      "Epoch 2, Iteration 330, Loss: 1.5826058387756348\n",
      "Epoch 2, Iteration 331, Loss: 1.7651195526123047\n",
      "Epoch 2, Iteration 332, Loss: 1.8706790208816528\n",
      "Epoch 2, Iteration 333, Loss: 2.1401162147521973\n",
      "Epoch 2, Iteration 334, Loss: 1.936783790588379\n",
      "Epoch 2, Iteration 335, Loss: 2.018296957015991\n",
      "Epoch 2, Iteration 336, Loss: 1.9182751178741455\n",
      "Epoch 2, Iteration 337, Loss: 1.660406470298767\n",
      "Epoch 2, Iteration 338, Loss: 1.9610058069229126\n",
      "Epoch 2, Iteration 339, Loss: 1.8439879417419434\n",
      "Epoch 2, Iteration 340, Loss: 1.5922224521636963\n",
      "Epoch 2, Iteration 341, Loss: 1.7403309345245361\n",
      "Epoch 2, Iteration 342, Loss: 1.8345891237258911\n",
      "Epoch 2, Iteration 343, Loss: 1.6946364641189575\n",
      "Epoch 2, Iteration 344, Loss: 1.983027458190918\n",
      "Epoch 2, Iteration 345, Loss: 1.885607123374939\n",
      "Epoch 2, Iteration 346, Loss: 1.8350143432617188\n",
      "Epoch 2, Iteration 347, Loss: 1.6449010372161865\n",
      "Epoch 2, Iteration 348, Loss: 1.6676836013793945\n",
      "Epoch 2, Iteration 349, Loss: 1.9327459335327148\n",
      "Epoch 2, Iteration 350, Loss: 1.7833105325698853\n",
      "Epoch 2, Iteration 350, Valid Loss: 1.2611229419708252\n",
      "Epoch 2, Iteration 351, Loss: 1.9248390197753906\n",
      "Epoch 2, Iteration 352, Loss: 1.7204817533493042\n",
      "Epoch 2, Iteration 353, Loss: 1.961958408355713\n",
      "Epoch 2, Iteration 354, Loss: 1.8910620212554932\n",
      "Epoch 2, Iteration 355, Loss: 1.765541911125183\n",
      "Epoch 2, Iteration 356, Loss: 2.1174516677856445\n",
      "Epoch 2, Iteration 357, Loss: 1.8061655759811401\n",
      "Epoch 2, Iteration 358, Loss: 1.8119597434997559\n",
      "Epoch 2, Iteration 359, Loss: 1.7856932878494263\n",
      "Epoch 2, Iteration 360, Loss: 1.683517575263977\n",
      "Epoch 2, Iteration 361, Loss: 1.6589863300323486\n",
      "Epoch 2, Iteration 362, Loss: 1.7694993019104004\n",
      "Epoch 2, Iteration 363, Loss: 1.7494306564331055\n",
      "Epoch 2, Iteration 364, Loss: 1.8723132610321045\n",
      "Epoch 2, Iteration 365, Loss: 2.034930944442749\n",
      "Epoch 2, Iteration 366, Loss: 2.1757054328918457\n",
      "Epoch 2, Iteration 367, Loss: 1.96094810962677\n",
      "Epoch 2, Iteration 368, Loss: 1.9410173892974854\n",
      "Epoch 2, Iteration 369, Loss: 2.0222625732421875\n",
      "Epoch 2, Iteration 370, Loss: 2.0003435611724854\n",
      "Epoch 2, Iteration 371, Loss: 1.8532301187515259\n",
      "Epoch 2, Iteration 372, Loss: 1.5402729511260986\n",
      "Epoch 2, Iteration 373, Loss: 1.7063719034194946\n",
      "Epoch 2, Iteration 374, Loss: 2.079665422439575\n",
      "Epoch 2, Iteration 375, Loss: 1.7901332378387451\n",
      "Epoch 2, Iteration 376, Loss: 2.0376358032226562\n",
      "Epoch 2, Iteration 377, Loss: 1.829662799835205\n",
      "Epoch 2, Iteration 378, Loss: 1.6777729988098145\n",
      "Epoch 2, Iteration 379, Loss: 1.835367202758789\n",
      "Epoch 2, Iteration 380, Loss: 1.7454185485839844\n",
      "Epoch 2, Iteration 381, Loss: 1.814885139465332\n",
      "Epoch 2, Iteration 382, Loss: 1.5941115617752075\n",
      "Epoch 2, Iteration 383, Loss: 1.735875129699707\n",
      "Epoch 2, Iteration 384, Loss: 1.8937029838562012\n",
      "Epoch 2, Iteration 385, Loss: 1.7100956439971924\n",
      "Epoch 2, Iteration 386, Loss: 1.682816505432129\n",
      "Epoch 2, Iteration 387, Loss: 2.180328607559204\n",
      "Epoch 2, Iteration 388, Loss: 1.839030146598816\n",
      "Epoch 2, Iteration 389, Loss: 1.8997719287872314\n",
      "Epoch 2, Iteration 390, Loss: 2.1287457942962646\n",
      "Epoch 2, Iteration 391, Loss: 1.778011679649353\n",
      "Epoch 3/10, Loss: 2.072728917306783\n",
      "Epoch 3, Iteration 0, Loss: 1.9927942752838135\n",
      "Epoch 3, Iteration 1, Loss: 2.313610076904297\n",
      "Epoch 3, Iteration 2, Loss: 2.131511926651001\n",
      "Epoch 3, Iteration 3, Loss: 2.0919196605682373\n",
      "Epoch 3, Iteration 4, Loss: 2.000335454940796\n",
      "Epoch 3, Iteration 5, Loss: 2.1329307556152344\n",
      "Epoch 3, Iteration 6, Loss: 1.8926513195037842\n",
      "Epoch 3, Iteration 7, Loss: 1.9635159969329834\n",
      "Epoch 3, Iteration 8, Loss: 2.0300393104553223\n",
      "Epoch 3, Iteration 9, Loss: 2.2980282306671143\n",
      "Epoch 3, Iteration 10, Loss: 1.973040223121643\n",
      "Epoch 3, Iteration 11, Loss: 1.769657850265503\n",
      "Epoch 3, Iteration 12, Loss: 1.7769505977630615\n",
      "Epoch 3, Iteration 13, Loss: 2.066627264022827\n",
      "Epoch 3, Iteration 14, Loss: 1.918713927268982\n",
      "Epoch 3, Iteration 15, Loss: 2.0777628421783447\n",
      "Epoch 3, Iteration 16, Loss: 1.8931849002838135\n",
      "Epoch 3, Iteration 17, Loss: 2.09836745262146\n",
      "Epoch 3, Iteration 18, Loss: 2.0840163230895996\n",
      "Epoch 3, Iteration 19, Loss: 2.1429989337921143\n",
      "Epoch 3, Iteration 20, Loss: 1.9330710172653198\n",
      "Epoch 3, Iteration 21, Loss: 2.02394962310791\n",
      "Epoch 3, Iteration 22, Loss: 1.8961416482925415\n",
      "Epoch 3, Iteration 23, Loss: 1.917567253112793\n",
      "Epoch 3, Iteration 24, Loss: 1.9065064191818237\n",
      "Epoch 3, Iteration 25, Loss: 1.7520986795425415\n",
      "Epoch 3, Iteration 26, Loss: 2.37825345993042\n",
      "Epoch 3, Iteration 27, Loss: 1.9772354364395142\n",
      "Epoch 3, Iteration 28, Loss: 1.946837067604065\n",
      "Epoch 3, Iteration 29, Loss: 2.223017930984497\n",
      "Epoch 3, Iteration 30, Loss: 1.9763761758804321\n",
      "Epoch 3, Iteration 31, Loss: 1.9637715816497803\n",
      "Epoch 3, Iteration 32, Loss: 1.8484781980514526\n",
      "Epoch 3, Iteration 33, Loss: 1.8776886463165283\n",
      "Epoch 3, Iteration 34, Loss: 1.8578386306762695\n",
      "Epoch 3, Iteration 35, Loss: 2.028228759765625\n",
      "Epoch 3, Iteration 36, Loss: 1.7140347957611084\n",
      "Epoch 3, Iteration 37, Loss: 1.6924439668655396\n",
      "Epoch 3, Iteration 38, Loss: 1.769873857498169\n",
      "Epoch 3, Iteration 39, Loss: 1.7443163394927979\n",
      "Epoch 3, Iteration 40, Loss: 1.9133431911468506\n",
      "Epoch 3, Iteration 41, Loss: 1.9798930883407593\n",
      "Epoch 3, Iteration 42, Loss: 2.050356388092041\n",
      "Epoch 3, Iteration 43, Loss: 1.9092060327529907\n",
      "Epoch 3, Iteration 44, Loss: 1.984686017036438\n",
      "Epoch 3, Iteration 45, Loss: 1.9179705381393433\n",
      "Epoch 3, Iteration 46, Loss: 1.9729692935943604\n",
      "Epoch 3, Iteration 47, Loss: 1.9160693883895874\n",
      "Epoch 3, Iteration 48, Loss: 2.2450814247131348\n",
      "Epoch 3, Iteration 49, Loss: 1.8909412622451782\n",
      "Epoch 3, Iteration 50, Loss: 1.956740140914917\n",
      "Epoch 3, Iteration 50, Valid Loss: 1.156657099723816\n",
      "Epoch 3, Iteration 51, Loss: 1.9632585048675537\n",
      "Epoch 3, Iteration 52, Loss: 1.9456977844238281\n",
      "Epoch 3, Iteration 53, Loss: 2.0356333255767822\n",
      "Epoch 3, Iteration 54, Loss: 2.201798439025879\n",
      "Epoch 3, Iteration 55, Loss: 2.013700008392334\n",
      "Epoch 3, Iteration 56, Loss: 1.7581605911254883\n",
      "Epoch 3, Iteration 57, Loss: 1.9946010112762451\n",
      "Epoch 3, Iteration 58, Loss: 1.8252058029174805\n",
      "Epoch 3, Iteration 59, Loss: 1.9493588209152222\n",
      "Epoch 3, Iteration 60, Loss: 1.9458057880401611\n",
      "Epoch 3, Iteration 61, Loss: 1.6145615577697754\n",
      "Epoch 3, Iteration 62, Loss: 1.8724726438522339\n",
      "Epoch 3, Iteration 63, Loss: 1.6819642782211304\n",
      "Epoch 3, Iteration 64, Loss: 1.6801173686981201\n",
      "Epoch 3, Iteration 65, Loss: 2.1819262504577637\n",
      "Epoch 3, Iteration 66, Loss: 1.94134521484375\n",
      "Epoch 3, Iteration 67, Loss: 1.5605683326721191\n",
      "Epoch 3, Iteration 68, Loss: 2.10931396484375\n",
      "Epoch 3, Iteration 69, Loss: 2.0747015476226807\n",
      "Epoch 3, Iteration 70, Loss: 1.9840757846832275\n",
      "Epoch 3, Iteration 71, Loss: 1.9145814180374146\n",
      "Epoch 3, Iteration 72, Loss: 2.0205135345458984\n",
      "Epoch 3, Iteration 73, Loss: 1.6122467517852783\n",
      "Epoch 3, Iteration 74, Loss: 2.1141624450683594\n",
      "Epoch 3, Iteration 75, Loss: 1.658858299255371\n",
      "Epoch 3, Iteration 76, Loss: 1.7542123794555664\n",
      "Epoch 3, Iteration 77, Loss: 1.50408935546875\n",
      "Epoch 3, Iteration 78, Loss: 2.273167848587036\n",
      "Epoch 3, Iteration 79, Loss: 2.1843271255493164\n",
      "Epoch 3, Iteration 80, Loss: 1.771363377571106\n",
      "Epoch 3, Iteration 81, Loss: 2.0994865894317627\n",
      "Epoch 3, Iteration 82, Loss: 2.08545184135437\n",
      "Epoch 3, Iteration 83, Loss: 1.6327458620071411\n",
      "Epoch 3, Iteration 84, Loss: 2.009229898452759\n",
      "Epoch 3, Iteration 85, Loss: 1.882491946220398\n",
      "Epoch 3, Iteration 86, Loss: 1.8411049842834473\n",
      "Epoch 3, Iteration 87, Loss: 1.761361837387085\n",
      "Epoch 3, Iteration 88, Loss: 1.8891770839691162\n",
      "Epoch 3, Iteration 89, Loss: 1.9336358308792114\n",
      "Epoch 3, Iteration 90, Loss: 1.5776596069335938\n",
      "Epoch 3, Iteration 91, Loss: 1.6835534572601318\n",
      "Epoch 3, Iteration 92, Loss: 1.4483628273010254\n",
      "Epoch 3, Iteration 93, Loss: 1.891270637512207\n",
      "Epoch 3, Iteration 94, Loss: 2.2009010314941406\n",
      "Epoch 3, Iteration 95, Loss: 1.7164616584777832\n",
      "Epoch 3, Iteration 96, Loss: 1.8232002258300781\n",
      "Epoch 3, Iteration 97, Loss: 1.6458890438079834\n",
      "Epoch 3, Iteration 98, Loss: 1.8398593664169312\n",
      "Epoch 3, Iteration 99, Loss: 1.8209967613220215\n",
      "Epoch 3, Iteration 100, Loss: 1.7787151336669922\n",
      "Epoch 3, Iteration 100, Valid Loss: 1.2064343690872192\n",
      "Epoch 3, Iteration 101, Loss: 2.174315929412842\n",
      "Epoch 3, Iteration 102, Loss: 1.9011406898498535\n",
      "Epoch 3, Iteration 103, Loss: 1.9683557748794556\n",
      "Epoch 3, Iteration 104, Loss: 2.617352247238159\n",
      "Epoch 3, Iteration 105, Loss: 1.9488072395324707\n",
      "Epoch 3, Iteration 106, Loss: 1.8975231647491455\n",
      "Epoch 3, Iteration 107, Loss: 2.427664041519165\n",
      "Epoch 3, Iteration 108, Loss: 1.779694676399231\n",
      "Epoch 3, Iteration 109, Loss: 2.1239733695983887\n",
      "Epoch 3, Iteration 110, Loss: 2.1016132831573486\n",
      "Epoch 3, Iteration 111, Loss: 2.247767686843872\n",
      "Epoch 3, Iteration 112, Loss: 1.8715368509292603\n",
      "Epoch 3, Iteration 113, Loss: 1.7820894718170166\n",
      "Epoch 3, Iteration 114, Loss: 2.252917766571045\n",
      "Epoch 3, Iteration 115, Loss: 2.1543338298797607\n",
      "Epoch 3, Iteration 116, Loss: 1.8294790983200073\n",
      "Epoch 3, Iteration 117, Loss: 1.554114580154419\n",
      "Epoch 3, Iteration 118, Loss: 1.966754674911499\n",
      "Epoch 3, Iteration 119, Loss: 1.9371412992477417\n",
      "Epoch 3, Iteration 120, Loss: 1.8997135162353516\n",
      "Epoch 3, Iteration 121, Loss: 1.9326725006103516\n",
      "Epoch 3, Iteration 122, Loss: 1.8119189739227295\n",
      "Epoch 3, Iteration 123, Loss: 1.8161064386367798\n",
      "Epoch 3, Iteration 124, Loss: 1.712233066558838\n",
      "Epoch 3, Iteration 125, Loss: 1.6075600385665894\n",
      "Epoch 3, Iteration 126, Loss: 2.430988311767578\n",
      "Epoch 3, Iteration 127, Loss: 2.361602783203125\n",
      "Epoch 3, Iteration 128, Loss: 1.7772935628890991\n",
      "Epoch 3, Iteration 129, Loss: 2.077261447906494\n",
      "Epoch 3, Iteration 130, Loss: 2.4234282970428467\n",
      "Epoch 3, Iteration 131, Loss: 1.6969358921051025\n",
      "Epoch 3, Iteration 132, Loss: 1.9033358097076416\n",
      "Epoch 3, Iteration 133, Loss: 2.255793571472168\n",
      "Epoch 3, Iteration 134, Loss: 1.6954063177108765\n",
      "Epoch 3, Iteration 135, Loss: 1.7891523838043213\n",
      "Epoch 3, Iteration 136, Loss: 1.8199973106384277\n",
      "Epoch 3, Iteration 137, Loss: 1.7587182521820068\n",
      "Epoch 3, Iteration 138, Loss: 1.668592929840088\n",
      "Epoch 3, Iteration 139, Loss: 1.7983640432357788\n",
      "Epoch 3, Iteration 140, Loss: 1.6356751918792725\n",
      "Epoch 3, Iteration 141, Loss: 2.092153549194336\n",
      "Epoch 3, Iteration 142, Loss: 1.934700846672058\n",
      "Epoch 3, Iteration 143, Loss: 1.901759147644043\n",
      "Epoch 3, Iteration 144, Loss: 1.7490874528884888\n",
      "Epoch 3, Iteration 145, Loss: 1.9153971672058105\n",
      "Epoch 3, Iteration 146, Loss: 2.140746831893921\n",
      "Epoch 3, Iteration 147, Loss: 1.822947382926941\n",
      "Epoch 3, Iteration 148, Loss: 1.5458335876464844\n",
      "Epoch 3, Iteration 149, Loss: 1.7252391576766968\n",
      "Epoch 3, Iteration 150, Loss: 2.024901866912842\n",
      "Epoch 3, Iteration 150, Valid Loss: 1.1686341762542725\n",
      "Epoch 3, Iteration 151, Loss: 1.98897385597229\n",
      "Epoch 3, Iteration 152, Loss: 2.1994898319244385\n",
      "Epoch 3, Iteration 153, Loss: 2.4468441009521484\n",
      "Epoch 3, Iteration 154, Loss: 1.6552664041519165\n",
      "Epoch 3, Iteration 155, Loss: 1.9403407573699951\n",
      "Epoch 3, Iteration 156, Loss: 1.7612380981445312\n",
      "Epoch 3, Iteration 157, Loss: 2.4807322025299072\n",
      "Epoch 3, Iteration 158, Loss: 1.7725906372070312\n",
      "Epoch 3, Iteration 159, Loss: 1.943507194519043\n",
      "Epoch 3, Iteration 160, Loss: 1.6937416791915894\n",
      "Epoch 3, Iteration 161, Loss: 1.815675139427185\n",
      "Epoch 3, Iteration 162, Loss: 2.0941052436828613\n",
      "Epoch 3, Iteration 163, Loss: 1.5442545413970947\n",
      "Epoch 3, Iteration 164, Loss: 1.8085620403289795\n",
      "Epoch 3, Iteration 165, Loss: 1.6505725383758545\n",
      "Epoch 3, Iteration 166, Loss: 1.651850700378418\n",
      "Epoch 3, Iteration 167, Loss: 2.116209030151367\n",
      "Epoch 3, Iteration 168, Loss: 1.9851937294006348\n",
      "Epoch 3, Iteration 169, Loss: 1.9768589735031128\n",
      "Epoch 3, Iteration 170, Loss: 2.0325241088867188\n",
      "Epoch 3, Iteration 171, Loss: 1.808887243270874\n",
      "Epoch 3, Iteration 172, Loss: 1.8627369403839111\n",
      "Epoch 3, Iteration 173, Loss: 1.8078782558441162\n",
      "Epoch 3, Iteration 174, Loss: 2.0543951988220215\n",
      "Epoch 3, Iteration 175, Loss: 2.0715389251708984\n",
      "Epoch 3, Iteration 176, Loss: 1.618523359298706\n",
      "Epoch 3, Iteration 177, Loss: 2.3048911094665527\n",
      "Epoch 3, Iteration 178, Loss: 2.062981605529785\n",
      "Epoch 3, Iteration 179, Loss: 1.911369800567627\n",
      "Epoch 3, Iteration 180, Loss: 1.8138242959976196\n",
      "Epoch 3, Iteration 181, Loss: 2.1670467853546143\n",
      "Epoch 3, Iteration 182, Loss: 2.111783027648926\n",
      "Epoch 3, Iteration 183, Loss: 1.6068415641784668\n",
      "Epoch 3, Iteration 184, Loss: 1.6763821840286255\n",
      "Epoch 3, Iteration 185, Loss: 1.7085886001586914\n",
      "Epoch 3, Iteration 186, Loss: 1.79966139793396\n",
      "Epoch 3, Iteration 187, Loss: 1.7152342796325684\n",
      "Epoch 3, Iteration 188, Loss: 1.801344394683838\n",
      "Epoch 3, Iteration 189, Loss: 1.7636334896087646\n",
      "Epoch 3, Iteration 190, Loss: 2.072964668273926\n",
      "Epoch 3, Iteration 191, Loss: 1.8992570638656616\n",
      "Epoch 3, Iteration 192, Loss: 1.907185435295105\n",
      "Epoch 3, Iteration 193, Loss: 1.787203311920166\n",
      "Epoch 3, Iteration 194, Loss: 1.737977385520935\n",
      "Epoch 3, Iteration 195, Loss: 1.659181833267212\n",
      "Epoch 3, Iteration 196, Loss: 1.966025471687317\n",
      "Epoch 3, Iteration 197, Loss: 1.8447505235671997\n",
      "Epoch 3, Iteration 198, Loss: 1.7759265899658203\n",
      "Epoch 3, Iteration 199, Loss: 2.0650887489318848\n",
      "Epoch 3, Iteration 200, Loss: 1.7492036819458008\n",
      "Epoch 3, Iteration 200, Valid Loss: 1.13078773021698\n",
      "Epoch 3, Iteration 201, Loss: 1.7438043355941772\n",
      "Epoch 3, Iteration 202, Loss: 1.845731496810913\n",
      "Epoch 3, Iteration 203, Loss: 1.7594127655029297\n",
      "Epoch 3, Iteration 204, Loss: 1.7547423839569092\n",
      "Epoch 3, Iteration 205, Loss: 1.47288978099823\n",
      "Epoch 3, Iteration 206, Loss: 1.7409740686416626\n",
      "Epoch 3, Iteration 207, Loss: 2.177246332168579\n",
      "Epoch 3, Iteration 208, Loss: 2.4528470039367676\n",
      "Epoch 3, Iteration 209, Loss: 2.7070791721343994\n",
      "Epoch 3, Iteration 210, Loss: 2.3394827842712402\n",
      "Epoch 3, Iteration 211, Loss: 2.4127988815307617\n",
      "Epoch 3, Iteration 212, Loss: 2.196847677230835\n",
      "Epoch 3, Iteration 213, Loss: 2.3568155765533447\n",
      "Epoch 3, Iteration 214, Loss: 1.950029969215393\n",
      "Epoch 3, Iteration 215, Loss: 1.7670464515686035\n",
      "Epoch 3, Iteration 216, Loss: 1.7748216390609741\n",
      "Epoch 3, Iteration 217, Loss: 1.8885133266448975\n",
      "Epoch 3, Iteration 218, Loss: 1.9876344203948975\n",
      "Epoch 3, Iteration 219, Loss: 1.9864122867584229\n",
      "Epoch 3, Iteration 220, Loss: 1.8293099403381348\n",
      "Epoch 3, Iteration 221, Loss: 2.0432357788085938\n",
      "Epoch 3, Iteration 222, Loss: 1.8334472179412842\n",
      "Epoch 3, Iteration 223, Loss: 1.852443814277649\n",
      "Epoch 3, Iteration 224, Loss: 1.6536130905151367\n",
      "Epoch 3, Iteration 225, Loss: 2.077669620513916\n",
      "Epoch 3, Iteration 226, Loss: 2.0321075916290283\n",
      "Epoch 3, Iteration 227, Loss: 1.9233558177947998\n",
      "Epoch 3, Iteration 228, Loss: 1.777366042137146\n",
      "Epoch 3, Iteration 229, Loss: 1.692683219909668\n",
      "Epoch 3, Iteration 230, Loss: 1.942657709121704\n",
      "Epoch 3, Iteration 231, Loss: 1.9337459802627563\n",
      "Epoch 3, Iteration 232, Loss: 1.9106109142303467\n",
      "Epoch 3, Iteration 233, Loss: 2.397695541381836\n",
      "Epoch 3, Iteration 234, Loss: 2.106095790863037\n",
      "Epoch 3, Iteration 235, Loss: 1.7115200757980347\n",
      "Epoch 3, Iteration 236, Loss: 1.9264174699783325\n",
      "Epoch 3, Iteration 237, Loss: 1.838548183441162\n",
      "Epoch 3, Iteration 238, Loss: 1.6443569660186768\n",
      "Epoch 3, Iteration 239, Loss: 1.996193289756775\n",
      "Epoch 3, Iteration 240, Loss: 1.578702449798584\n",
      "Epoch 3, Iteration 241, Loss: 1.7308874130249023\n",
      "Epoch 3, Iteration 242, Loss: 1.7123968601226807\n",
      "Epoch 3, Iteration 243, Loss: 1.6697251796722412\n",
      "Epoch 3, Iteration 244, Loss: 1.722550868988037\n",
      "Epoch 3, Iteration 245, Loss: 1.45345139503479\n",
      "Epoch 3, Iteration 246, Loss: 1.8810354471206665\n",
      "Epoch 3, Iteration 247, Loss: 1.6394472122192383\n",
      "Epoch 3, Iteration 248, Loss: 1.7942233085632324\n",
      "Epoch 3, Iteration 249, Loss: 1.6379371881484985\n",
      "Epoch 3, Iteration 250, Loss: 1.994032859802246\n",
      "Epoch 3, Iteration 250, Valid Loss: 1.0955371856689453\n",
      "Epoch 3, Iteration 251, Loss: 1.736013412475586\n",
      "Epoch 3, Iteration 252, Loss: 1.920964241027832\n",
      "Epoch 3, Iteration 253, Loss: 1.8854565620422363\n",
      "Epoch 3, Iteration 254, Loss: 1.872876763343811\n",
      "Epoch 3, Iteration 255, Loss: 1.9241784811019897\n",
      "Epoch 3, Iteration 256, Loss: 2.0432963371276855\n",
      "Epoch 3, Iteration 257, Loss: 1.738599181175232\n",
      "Epoch 3, Iteration 258, Loss: 2.249565839767456\n",
      "Epoch 3, Iteration 259, Loss: 1.6037826538085938\n",
      "Epoch 3, Iteration 260, Loss: 2.012678623199463\n",
      "Epoch 3, Iteration 261, Loss: 2.0508532524108887\n",
      "Epoch 3, Iteration 262, Loss: 1.5914626121520996\n",
      "Epoch 3, Iteration 263, Loss: 2.0654337406158447\n",
      "Epoch 3, Iteration 264, Loss: 1.8663495779037476\n",
      "Epoch 3, Iteration 265, Loss: 1.8947787284851074\n",
      "Epoch 3, Iteration 266, Loss: 2.042623996734619\n",
      "Epoch 3, Iteration 267, Loss: 1.799491286277771\n",
      "Epoch 3, Iteration 268, Loss: 1.804900884628296\n",
      "Epoch 3, Iteration 269, Loss: 1.7735953330993652\n",
      "Epoch 3, Iteration 270, Loss: 1.7778894901275635\n",
      "Epoch 3, Iteration 271, Loss: 1.84098482131958\n",
      "Epoch 3, Iteration 272, Loss: 1.6569342613220215\n",
      "Epoch 3, Iteration 273, Loss: 1.6504113674163818\n",
      "Epoch 3, Iteration 274, Loss: 1.6654481887817383\n",
      "Epoch 3, Iteration 275, Loss: 1.660343885421753\n",
      "Epoch 3, Iteration 276, Loss: 1.6480900049209595\n",
      "Epoch 3, Iteration 277, Loss: 1.6728366613388062\n",
      "Epoch 3, Iteration 278, Loss: 1.8827431201934814\n",
      "Epoch 3, Iteration 279, Loss: 1.6187487840652466\n",
      "Epoch 3, Iteration 280, Loss: 1.4718244075775146\n",
      "Epoch 3, Iteration 281, Loss: 1.601762294769287\n",
      "Epoch 3, Iteration 282, Loss: 1.877235770225525\n",
      "Epoch 3, Iteration 283, Loss: 1.7786920070648193\n",
      "Epoch 3, Iteration 284, Loss: 1.732607364654541\n",
      "Epoch 3, Iteration 285, Loss: 1.6622684001922607\n",
      "Epoch 3, Iteration 286, Loss: 1.701936960220337\n",
      "Epoch 3, Iteration 287, Loss: 1.6179049015045166\n",
      "Epoch 3, Iteration 288, Loss: 1.6014642715454102\n",
      "Epoch 3, Iteration 289, Loss: 1.721198558807373\n",
      "Epoch 3, Iteration 290, Loss: 1.5729647874832153\n",
      "Epoch 3, Iteration 291, Loss: 1.6347360610961914\n",
      "Epoch 3, Iteration 292, Loss: 2.042713165283203\n",
      "Epoch 3, Iteration 293, Loss: 2.1021862030029297\n",
      "Epoch 3, Iteration 294, Loss: 1.6528122425079346\n",
      "Epoch 3, Iteration 295, Loss: 1.7258021831512451\n",
      "Epoch 3, Iteration 296, Loss: 2.141329765319824\n",
      "Epoch 3, Iteration 297, Loss: 2.0481176376342773\n",
      "Epoch 3, Iteration 298, Loss: 1.785968542098999\n",
      "Epoch 3, Iteration 299, Loss: 1.6430717706680298\n",
      "Epoch 3, Iteration 300, Loss: 1.5627635717391968\n",
      "Epoch 3, Iteration 300, Valid Loss: 1.0983868837356567\n",
      "Epoch 3, Iteration 301, Loss: 1.7577723264694214\n",
      "Epoch 3, Iteration 302, Loss: 1.602607250213623\n",
      "Epoch 3, Iteration 303, Loss: 2.058422803878784\n",
      "Epoch 3, Iteration 304, Loss: 1.6647844314575195\n",
      "Epoch 3, Iteration 305, Loss: 1.5975656509399414\n",
      "Epoch 3, Iteration 306, Loss: 1.6623892784118652\n",
      "Epoch 3, Iteration 307, Loss: 1.6264479160308838\n",
      "Epoch 3, Iteration 308, Loss: 1.6394144296646118\n",
      "Epoch 3, Iteration 309, Loss: 1.6448880434036255\n",
      "Epoch 3, Iteration 310, Loss: 1.9471473693847656\n",
      "Epoch 3, Iteration 311, Loss: 1.7078747749328613\n",
      "Epoch 3, Iteration 312, Loss: 1.5132126808166504\n",
      "Epoch 3, Iteration 313, Loss: 1.862917423248291\n",
      "Epoch 3, Iteration 314, Loss: 1.6553610563278198\n",
      "Epoch 3, Iteration 315, Loss: 1.4707709550857544\n",
      "Epoch 3, Iteration 316, Loss: 1.8139252662658691\n",
      "Epoch 3, Iteration 317, Loss: 1.619817852973938\n",
      "Epoch 3, Iteration 318, Loss: 1.6532204151153564\n",
      "Epoch 3, Iteration 319, Loss: 1.8999017477035522\n",
      "Epoch 3, Iteration 320, Loss: 1.417925238609314\n",
      "Epoch 3, Iteration 321, Loss: 1.6736903190612793\n",
      "Epoch 3, Iteration 322, Loss: 1.5988843441009521\n",
      "Epoch 3, Iteration 323, Loss: 1.7035328149795532\n",
      "Epoch 3, Iteration 324, Loss: 1.679767370223999\n",
      "Epoch 3, Iteration 325, Loss: 2.1046552658081055\n",
      "Epoch 3, Iteration 326, Loss: 1.7618987560272217\n",
      "Epoch 3, Iteration 327, Loss: 1.5847413539886475\n",
      "Epoch 3, Iteration 328, Loss: 1.602769136428833\n",
      "Epoch 3, Iteration 329, Loss: 1.8733530044555664\n",
      "Epoch 3, Iteration 330, Loss: 1.3941093683242798\n",
      "Epoch 3, Iteration 331, Loss: 1.7306504249572754\n",
      "Epoch 3, Iteration 332, Loss: 1.7671396732330322\n",
      "Epoch 3, Iteration 333, Loss: 1.9647632837295532\n",
      "Epoch 3, Iteration 334, Loss: 1.6935429573059082\n",
      "Epoch 3, Iteration 335, Loss: 1.9206628799438477\n",
      "Epoch 3, Iteration 336, Loss: 1.8068559169769287\n",
      "Epoch 3, Iteration 337, Loss: 1.5576162338256836\n",
      "Epoch 3, Iteration 338, Loss: 1.7372061014175415\n",
      "Epoch 3, Iteration 339, Loss: 1.6559929847717285\n",
      "Epoch 3, Iteration 340, Loss: 1.5465463399887085\n",
      "Epoch 3, Iteration 341, Loss: 1.6479169130325317\n",
      "Epoch 3, Iteration 342, Loss: 1.594854474067688\n",
      "Epoch 3, Iteration 343, Loss: 1.6851208209991455\n",
      "Epoch 3, Iteration 344, Loss: 1.9209098815917969\n",
      "Epoch 3, Iteration 345, Loss: 1.8148581981658936\n",
      "Epoch 3, Iteration 346, Loss: 1.7150917053222656\n",
      "Epoch 3, Iteration 347, Loss: 1.5511120557785034\n",
      "Epoch 3, Iteration 348, Loss: 1.4644110202789307\n",
      "Epoch 3, Iteration 349, Loss: 1.7501939535140991\n",
      "Epoch 3, Iteration 350, Loss: 1.5646159648895264\n",
      "Epoch 3, Iteration 350, Valid Loss: 1.0996520519256592\n",
      "Epoch 3, Iteration 351, Loss: 1.752777338027954\n",
      "Epoch 3, Iteration 352, Loss: 1.5229406356811523\n",
      "Epoch 3, Iteration 353, Loss: 1.7696460485458374\n",
      "Epoch 3, Iteration 354, Loss: 1.6334785223007202\n",
      "Epoch 3, Iteration 355, Loss: 1.6130483150482178\n",
      "Epoch 3, Iteration 356, Loss: 1.7121529579162598\n",
      "Epoch 3, Iteration 357, Loss: 1.6076858043670654\n",
      "Epoch 3, Iteration 358, Loss: 1.679269790649414\n",
      "Epoch 3, Iteration 359, Loss: 1.6190524101257324\n",
      "Epoch 3, Iteration 360, Loss: 1.5148509740829468\n",
      "Epoch 3, Iteration 361, Loss: 1.6494715213775635\n",
      "Epoch 3, Iteration 362, Loss: 1.6699659824371338\n",
      "Epoch 3, Iteration 363, Loss: 1.5537974834442139\n",
      "Epoch 3, Iteration 364, Loss: 1.6513886451721191\n",
      "Epoch 3, Iteration 365, Loss: 1.902747392654419\n",
      "Epoch 3, Iteration 366, Loss: 1.906885027885437\n",
      "Epoch 3, Iteration 367, Loss: 1.8057219982147217\n",
      "Epoch 3, Iteration 368, Loss: 1.6478490829467773\n",
      "Epoch 3, Iteration 369, Loss: 1.8169952630996704\n",
      "Epoch 3, Iteration 370, Loss: 1.8315672874450684\n",
      "Epoch 3, Iteration 371, Loss: 1.7081458568572998\n",
      "Epoch 3, Iteration 372, Loss: 1.4391511678695679\n",
      "Epoch 3, Iteration 373, Loss: 1.4813532829284668\n",
      "Epoch 3, Iteration 374, Loss: 1.9271938800811768\n",
      "Epoch 3, Iteration 375, Loss: 1.673292875289917\n",
      "Epoch 3, Iteration 376, Loss: 1.9085577726364136\n",
      "Epoch 3, Iteration 377, Loss: 1.6818389892578125\n",
      "Epoch 3, Iteration 378, Loss: 1.863849401473999\n",
      "Epoch 3, Iteration 379, Loss: 1.5877892971038818\n",
      "Epoch 3, Iteration 380, Loss: 1.5354117155075073\n",
      "Epoch 3, Iteration 381, Loss: 1.6729965209960938\n",
      "Epoch 3, Iteration 382, Loss: 1.403804063796997\n",
      "Epoch 3, Iteration 383, Loss: 1.6546334028244019\n",
      "Epoch 3, Iteration 384, Loss: 1.6615684032440186\n",
      "Epoch 3, Iteration 385, Loss: 1.602301836013794\n",
      "Epoch 3, Iteration 386, Loss: 1.581606388092041\n",
      "Epoch 3, Iteration 387, Loss: 2.0073485374450684\n",
      "Epoch 3, Iteration 388, Loss: 1.6509928703308105\n",
      "Epoch 3, Iteration 389, Loss: 1.7343432903289795\n",
      "Epoch 3, Iteration 390, Loss: 1.847362995147705\n",
      "Epoch 3, Iteration 391, Loss: 1.5823321342468262\n",
      "Epoch 4/10, Loss: 1.8502658295388124\n",
      "Epoch 4, Iteration 0, Loss: 1.7371091842651367\n",
      "Epoch 4, Iteration 1, Loss: 2.126358985900879\n",
      "Epoch 4, Iteration 2, Loss: 1.9478178024291992\n",
      "Epoch 4, Iteration 3, Loss: 1.9553124904632568\n",
      "Epoch 4, Iteration 4, Loss: 1.8464140892028809\n",
      "Epoch 4, Iteration 5, Loss: 1.904242753982544\n",
      "Epoch 4, Iteration 6, Loss: 1.641303539276123\n",
      "Epoch 4, Iteration 7, Loss: 1.7755777835845947\n",
      "Epoch 4, Iteration 8, Loss: 1.8391495943069458\n",
      "Epoch 4, Iteration 9, Loss: 2.134030818939209\n",
      "Epoch 4, Iteration 10, Loss: 1.8715999126434326\n",
      "Epoch 4, Iteration 11, Loss: 1.6389050483703613\n",
      "Epoch 4, Iteration 12, Loss: 1.7212762832641602\n",
      "Epoch 4, Iteration 13, Loss: 1.9223878383636475\n",
      "Epoch 4, Iteration 14, Loss: 1.615178108215332\n",
      "Epoch 4, Iteration 15, Loss: 1.9018248319625854\n",
      "Epoch 4, Iteration 16, Loss: 1.756269097328186\n",
      "Epoch 4, Iteration 17, Loss: 1.9803504943847656\n",
      "Epoch 4, Iteration 18, Loss: 1.8195065259933472\n",
      "Epoch 4, Iteration 19, Loss: 1.9197276830673218\n",
      "Epoch 4, Iteration 20, Loss: 1.8515079021453857\n",
      "Epoch 4, Iteration 21, Loss: 1.773347020149231\n",
      "Epoch 4, Iteration 22, Loss: 1.8061922788619995\n",
      "Epoch 4, Iteration 23, Loss: 1.7466647624969482\n",
      "Epoch 4, Iteration 24, Loss: 1.6074597835540771\n",
      "Epoch 4, Iteration 25, Loss: 1.6351577043533325\n",
      "Epoch 4, Iteration 26, Loss: 2.130108594894409\n",
      "Epoch 4, Iteration 27, Loss: 1.7454354763031006\n",
      "Epoch 4, Iteration 28, Loss: 1.86592435836792\n",
      "Epoch 4, Iteration 29, Loss: 2.0464627742767334\n",
      "Epoch 4, Iteration 30, Loss: 1.8483660221099854\n",
      "Epoch 4, Iteration 31, Loss: 1.8773621320724487\n",
      "Epoch 4, Iteration 32, Loss: 1.794525146484375\n",
      "Epoch 4, Iteration 33, Loss: 1.744555950164795\n",
      "Epoch 4, Iteration 34, Loss: 1.8396472930908203\n",
      "Epoch 4, Iteration 35, Loss: 1.819650411605835\n",
      "Epoch 4, Iteration 36, Loss: 1.5689764022827148\n",
      "Epoch 4, Iteration 37, Loss: 1.6146910190582275\n",
      "Epoch 4, Iteration 38, Loss: 1.6479768753051758\n",
      "Epoch 4, Iteration 39, Loss: 1.606965184211731\n",
      "Epoch 4, Iteration 40, Loss: 1.7317010164260864\n",
      "Epoch 4, Iteration 41, Loss: 1.8417527675628662\n",
      "Epoch 4, Iteration 42, Loss: 1.817557692527771\n",
      "Epoch 4, Iteration 43, Loss: 1.6959985494613647\n",
      "Epoch 4, Iteration 44, Loss: 1.7360278367996216\n",
      "Epoch 4, Iteration 45, Loss: 1.7643296718597412\n",
      "Epoch 4, Iteration 46, Loss: 1.802947998046875\n",
      "Epoch 4, Iteration 47, Loss: 1.8732471466064453\n",
      "Epoch 4, Iteration 48, Loss: 1.8964965343475342\n",
      "Epoch 4, Iteration 49, Loss: 1.7432219982147217\n",
      "Epoch 4, Iteration 50, Loss: 1.7584269046783447\n",
      "Epoch 4, Iteration 50, Valid Loss: 1.025460124015808\n",
      "Epoch 4, Iteration 51, Loss: 1.7537527084350586\n",
      "Epoch 4, Iteration 52, Loss: 1.7641921043395996\n",
      "Epoch 4, Iteration 53, Loss: 1.7476274967193604\n",
      "Epoch 4, Iteration 54, Loss: 2.1407766342163086\n",
      "Epoch 4, Iteration 55, Loss: 2.019041061401367\n",
      "Epoch 4, Iteration 56, Loss: 1.6371092796325684\n",
      "Epoch 4, Iteration 57, Loss: 1.7951836585998535\n",
      "Epoch 4, Iteration 58, Loss: 1.6893545389175415\n",
      "Epoch 4, Iteration 59, Loss: 1.6823606491088867\n",
      "Epoch 4, Iteration 60, Loss: 1.803578495979309\n",
      "Epoch 4, Iteration 61, Loss: 1.4514050483703613\n",
      "Epoch 4, Iteration 62, Loss: 1.6577417850494385\n",
      "Epoch 4, Iteration 63, Loss: 1.437455177307129\n",
      "Epoch 4, Iteration 64, Loss: 1.5461342334747314\n",
      "Epoch 4, Iteration 65, Loss: 1.9184707403182983\n",
      "Epoch 4, Iteration 66, Loss: 1.7889940738677979\n",
      "Epoch 4, Iteration 67, Loss: 1.3864378929138184\n",
      "Epoch 4, Iteration 68, Loss: 1.9736331701278687\n",
      "Epoch 4, Iteration 69, Loss: 2.023160934448242\n",
      "Epoch 4, Iteration 70, Loss: 2.065265655517578\n",
      "Epoch 4, Iteration 71, Loss: 1.76806640625\n",
      "Epoch 4, Iteration 72, Loss: 1.9230971336364746\n",
      "Epoch 4, Iteration 73, Loss: 1.577357292175293\n",
      "Epoch 4, Iteration 74, Loss: 1.9433890581130981\n",
      "Epoch 4, Iteration 75, Loss: 1.7922616004943848\n",
      "Epoch 4, Iteration 76, Loss: 1.5923516750335693\n",
      "Epoch 4, Iteration 77, Loss: 1.5384975671768188\n",
      "Epoch 4, Iteration 78, Loss: 2.1912941932678223\n",
      "Epoch 4, Iteration 79, Loss: 2.0321242809295654\n",
      "Epoch 4, Iteration 80, Loss: 1.530853509902954\n",
      "Epoch 4, Iteration 81, Loss: 1.972400426864624\n",
      "Epoch 4, Iteration 82, Loss: 1.6734037399291992\n",
      "Epoch 4, Iteration 83, Loss: 1.4528169631958008\n",
      "Epoch 4, Iteration 84, Loss: 1.902522087097168\n",
      "Epoch 4, Iteration 85, Loss: 1.802321434020996\n",
      "Epoch 4, Iteration 86, Loss: 1.8693828582763672\n",
      "Epoch 4, Iteration 87, Loss: 1.911532998085022\n",
      "Epoch 4, Iteration 88, Loss: 1.9102160930633545\n",
      "Epoch 4, Iteration 89, Loss: 1.7146742343902588\n",
      "Epoch 4, Iteration 90, Loss: 1.5078102350234985\n",
      "Epoch 4, Iteration 91, Loss: 1.666885495185852\n",
      "Epoch 4, Iteration 92, Loss: 1.4355181455612183\n",
      "Epoch 4, Iteration 93, Loss: 1.7170125246047974\n",
      "Epoch 4, Iteration 94, Loss: 2.0884218215942383\n",
      "Epoch 4, Iteration 95, Loss: 1.6274406909942627\n",
      "Epoch 4, Iteration 96, Loss: 1.7362534999847412\n",
      "Epoch 4, Iteration 97, Loss: 1.5422587394714355\n",
      "Epoch 4, Iteration 98, Loss: 1.624557375907898\n",
      "Epoch 4, Iteration 99, Loss: 1.64383864402771\n",
      "Epoch 4, Iteration 100, Loss: 1.5723657608032227\n",
      "Epoch 4, Iteration 100, Valid Loss: 1.0552277565002441\n",
      "Epoch 4, Iteration 101, Loss: 2.166410207748413\n",
      "Epoch 4, Iteration 102, Loss: 1.6943670511245728\n",
      "Epoch 4, Iteration 103, Loss: 1.6125187873840332\n",
      "Epoch 4, Iteration 104, Loss: 2.090446949005127\n",
      "Epoch 4, Iteration 105, Loss: 1.7587512731552124\n",
      "Epoch 4, Iteration 106, Loss: 1.6900293827056885\n",
      "Epoch 4, Iteration 107, Loss: 2.458348274230957\n",
      "Epoch 4, Iteration 108, Loss: 1.7048673629760742\n",
      "Epoch 4, Iteration 109, Loss: 1.9642841815948486\n",
      "Epoch 4, Iteration 110, Loss: 1.9624030590057373\n",
      "Epoch 4, Iteration 111, Loss: 1.8025643825531006\n",
      "Epoch 4, Iteration 112, Loss: 1.7327003479003906\n",
      "Epoch 4, Iteration 113, Loss: 1.4991014003753662\n",
      "Epoch 4, Iteration 114, Loss: 2.283367872238159\n",
      "Epoch 4, Iteration 115, Loss: 2.0514168739318848\n",
      "Epoch 4, Iteration 116, Loss: 1.664703130722046\n",
      "Epoch 4, Iteration 117, Loss: 1.4540058374404907\n",
      "Epoch 4, Iteration 118, Loss: 1.8044593334197998\n",
      "Epoch 4, Iteration 119, Loss: 1.776540756225586\n",
      "Epoch 4, Iteration 120, Loss: 1.6901110410690308\n",
      "Epoch 4, Iteration 121, Loss: 1.566795825958252\n",
      "Epoch 4, Iteration 122, Loss: 1.6825312376022339\n",
      "Epoch 4, Iteration 123, Loss: 1.626997709274292\n",
      "Epoch 4, Iteration 124, Loss: 1.580359697341919\n",
      "Epoch 4, Iteration 125, Loss: 1.4601476192474365\n",
      "Epoch 4, Iteration 126, Loss: 1.5331989526748657\n",
      "Epoch 4, Iteration 127, Loss: 1.8953083753585815\n",
      "Epoch 4, Iteration 128, Loss: 2.035344123840332\n",
      "Epoch 4, Iteration 129, Loss: 1.7623176574707031\n",
      "Epoch 4, Iteration 130, Loss: 2.153329610824585\n",
      "Epoch 4, Iteration 131, Loss: 1.5995389223098755\n",
      "Epoch 4, Iteration 132, Loss: 1.6052497625350952\n",
      "Epoch 4, Iteration 133, Loss: 2.1817502975463867\n",
      "Epoch 4, Iteration 134, Loss: 1.7860727310180664\n",
      "Epoch 4, Iteration 135, Loss: 1.9269909858703613\n",
      "Epoch 4, Iteration 136, Loss: 1.775665044784546\n",
      "Epoch 4, Iteration 137, Loss: 1.6644901037216187\n",
      "Epoch 4, Iteration 138, Loss: 1.4536241292953491\n",
      "Epoch 4, Iteration 139, Loss: 1.5600838661193848\n",
      "Epoch 4, Iteration 140, Loss: 1.6655861139297485\n",
      "Epoch 4, Iteration 141, Loss: 2.028593063354492\n",
      "Epoch 4, Iteration 142, Loss: 1.6444640159606934\n",
      "Epoch 4, Iteration 143, Loss: 1.7683095932006836\n",
      "Epoch 4, Iteration 144, Loss: 1.624572515487671\n",
      "Epoch 4, Iteration 145, Loss: 1.7715866565704346\n",
      "Epoch 4, Iteration 146, Loss: 2.010902166366577\n",
      "Epoch 4, Iteration 147, Loss: 1.6404815912246704\n",
      "Epoch 4, Iteration 148, Loss: 1.4366565942764282\n",
      "Epoch 4, Iteration 149, Loss: 1.5941251516342163\n",
      "Epoch 4, Iteration 150, Loss: 1.8402225971221924\n",
      "Epoch 4, Iteration 150, Valid Loss: 1.0464338064193726\n",
      "Epoch 4, Iteration 151, Loss: 1.8037869930267334\n",
      "Epoch 4, Iteration 152, Loss: 1.6312655210494995\n",
      "Epoch 4, Iteration 153, Loss: 1.937288522720337\n",
      "Epoch 4, Iteration 154, Loss: 1.5540857315063477\n",
      "Epoch 4, Iteration 155, Loss: 1.6899232864379883\n",
      "Epoch 4, Iteration 156, Loss: 1.7266898155212402\n",
      "Epoch 4, Iteration 157, Loss: 2.2633845806121826\n",
      "Epoch 4, Iteration 158, Loss: 1.7873810529708862\n",
      "Epoch 4, Iteration 159, Loss: 1.8333075046539307\n",
      "Epoch 4, Iteration 160, Loss: 1.6348901987075806\n",
      "Epoch 4, Iteration 161, Loss: 1.5844477415084839\n",
      "Epoch 4, Iteration 162, Loss: 1.9682466983795166\n",
      "Epoch 4, Iteration 163, Loss: 1.3898885250091553\n",
      "Epoch 4, Iteration 164, Loss: 1.7451919317245483\n",
      "Epoch 4, Iteration 165, Loss: 1.640782356262207\n",
      "Epoch 4, Iteration 166, Loss: 1.4676682949066162\n",
      "Epoch 4, Iteration 167, Loss: 2.031811475753784\n",
      "Epoch 4, Iteration 168, Loss: 1.8574652671813965\n",
      "Epoch 4, Iteration 169, Loss: 1.8002036809921265\n",
      "Epoch 4, Iteration 170, Loss: 1.8455561399459839\n",
      "Epoch 4, Iteration 171, Loss: 1.7612351179122925\n",
      "Epoch 4, Iteration 172, Loss: 1.6474148035049438\n",
      "Epoch 4, Iteration 173, Loss: 1.7162790298461914\n",
      "Epoch 4, Iteration 174, Loss: 1.8614871501922607\n",
      "Epoch 4, Iteration 175, Loss: 1.8742104768753052\n",
      "Epoch 4, Iteration 176, Loss: 1.4329288005828857\n",
      "Epoch 4, Iteration 177, Loss: 2.2074811458587646\n",
      "Epoch 4, Iteration 178, Loss: 1.9841349124908447\n",
      "Epoch 4, Iteration 179, Loss: 1.7357583045959473\n",
      "Epoch 4, Iteration 180, Loss: 1.6002575159072876\n",
      "Epoch 4, Iteration 181, Loss: 1.9717028141021729\n",
      "Epoch 4, Iteration 182, Loss: 1.9357738494873047\n",
      "Epoch 4, Iteration 183, Loss: 1.5398030281066895\n",
      "Epoch 4, Iteration 184, Loss: 1.5337297916412354\n",
      "Epoch 4, Iteration 185, Loss: 1.5975613594055176\n",
      "Epoch 4, Iteration 186, Loss: 1.5878838300704956\n",
      "Epoch 4, Iteration 187, Loss: 1.5813754796981812\n",
      "Epoch 4, Iteration 188, Loss: 1.5640630722045898\n",
      "Epoch 4, Iteration 189, Loss: 1.6014901399612427\n",
      "Epoch 4, Iteration 190, Loss: 1.8846898078918457\n",
      "Epoch 4, Iteration 191, Loss: 1.8080157041549683\n",
      "Epoch 4, Iteration 192, Loss: 1.8148906230926514\n",
      "Epoch 4, Iteration 193, Loss: 1.748168706893921\n",
      "Epoch 4, Iteration 194, Loss: 1.6580495834350586\n",
      "Epoch 4, Iteration 195, Loss: 1.569737195968628\n",
      "Epoch 4, Iteration 196, Loss: 1.7612353563308716\n",
      "Epoch 4, Iteration 197, Loss: 1.7260738611221313\n",
      "Epoch 4, Iteration 198, Loss: 1.7425241470336914\n",
      "Epoch 4, Iteration 199, Loss: 1.8578791618347168\n",
      "Epoch 4, Iteration 200, Loss: 1.5212819576263428\n",
      "Epoch 4, Iteration 200, Valid Loss: 1.004392147064209\n",
      "Epoch 4, Iteration 201, Loss: 1.6015115976333618\n",
      "Epoch 4, Iteration 202, Loss: 1.7593227624893188\n",
      "Epoch 4, Iteration 203, Loss: 1.6118431091308594\n",
      "Epoch 4, Iteration 204, Loss: 1.6784005165100098\n",
      "Epoch 4, Iteration 205, Loss: 1.4110788106918335\n",
      "Epoch 4, Iteration 206, Loss: 1.6060712337493896\n",
      "Epoch 4, Iteration 207, Loss: 1.93414306640625\n",
      "Epoch 4, Iteration 208, Loss: 2.24261474609375\n",
      "Epoch 4, Iteration 209, Loss: 2.4531965255737305\n",
      "Epoch 4, Iteration 210, Loss: 2.144571304321289\n",
      "Epoch 4, Iteration 211, Loss: 2.378819704055786\n",
      "Epoch 4, Iteration 212, Loss: 2.083977699279785\n",
      "Epoch 4, Iteration 213, Loss: 2.172849655151367\n",
      "Epoch 4, Iteration 214, Loss: 1.814469575881958\n",
      "Epoch 4, Iteration 215, Loss: 1.6062883138656616\n",
      "Epoch 4, Iteration 216, Loss: 1.6161574125289917\n",
      "Epoch 4, Iteration 217, Loss: 1.7108925580978394\n",
      "Epoch 4, Iteration 218, Loss: 1.8405699729919434\n",
      "Epoch 4, Iteration 219, Loss: 1.775814175605774\n",
      "Epoch 4, Iteration 220, Loss: 1.7061131000518799\n",
      "Epoch 4, Iteration 221, Loss: 1.7960361242294312\n",
      "Epoch 4, Iteration 222, Loss: 1.6970449686050415\n",
      "Epoch 4, Iteration 223, Loss: 1.6560685634613037\n",
      "Epoch 4, Iteration 224, Loss: 1.5148783922195435\n",
      "Epoch 4, Iteration 225, Loss: 1.9045238494873047\n",
      "Epoch 4, Iteration 226, Loss: 1.9481619596481323\n",
      "Epoch 4, Iteration 227, Loss: 1.7880444526672363\n",
      "Epoch 4, Iteration 228, Loss: 1.5875537395477295\n",
      "Epoch 4, Iteration 229, Loss: 1.538806676864624\n",
      "Epoch 4, Iteration 230, Loss: 1.8740875720977783\n",
      "Epoch 4, Iteration 231, Loss: 1.7760671377182007\n",
      "Epoch 4, Iteration 232, Loss: 1.8892830610275269\n",
      "Epoch 4, Iteration 233, Loss: 2.2258567810058594\n",
      "Epoch 4, Iteration 234, Loss: 1.8941442966461182\n",
      "Epoch 4, Iteration 235, Loss: 1.6737004518508911\n",
      "Epoch 4, Iteration 236, Loss: 1.9178094863891602\n",
      "Epoch 4, Iteration 237, Loss: 1.7086340188980103\n",
      "Epoch 4, Iteration 238, Loss: 1.6256605386734009\n",
      "Epoch 4, Iteration 239, Loss: 1.6723438501358032\n",
      "Epoch 4, Iteration 240, Loss: 1.4530212879180908\n",
      "Epoch 4, Iteration 241, Loss: 1.5028891563415527\n",
      "Epoch 4, Iteration 242, Loss: 1.599487066268921\n",
      "Epoch 4, Iteration 243, Loss: 1.6333656311035156\n",
      "Epoch 4, Iteration 244, Loss: 1.6792707443237305\n",
      "Epoch 4, Iteration 245, Loss: 1.2929209470748901\n",
      "Epoch 4, Iteration 246, Loss: 1.791182518005371\n",
      "Epoch 4, Iteration 247, Loss: 1.5966219902038574\n",
      "Epoch 4, Iteration 248, Loss: 1.89718496799469\n",
      "Epoch 4, Iteration 249, Loss: 1.666701316833496\n",
      "Epoch 4, Iteration 250, Loss: 1.9048187732696533\n",
      "Epoch 4, Iteration 250, Valid Loss: 1.004701018333435\n",
      "Epoch 4, Iteration 251, Loss: 1.6420533657073975\n",
      "Epoch 4, Iteration 252, Loss: 1.8492460250854492\n",
      "Epoch 4, Iteration 253, Loss: 1.8313125371932983\n",
      "Epoch 4, Iteration 254, Loss: 1.7921607494354248\n",
      "Epoch 4, Iteration 255, Loss: 1.8101375102996826\n",
      "Epoch 4, Iteration 256, Loss: 1.8542872667312622\n",
      "Epoch 4, Iteration 257, Loss: 1.6086883544921875\n",
      "Epoch 4, Iteration 258, Loss: 1.932638168334961\n",
      "Epoch 4, Iteration 259, Loss: 1.504128098487854\n",
      "Epoch 4, Iteration 260, Loss: 1.804335355758667\n",
      "Epoch 4, Iteration 261, Loss: 1.7654438018798828\n",
      "Epoch 4, Iteration 262, Loss: 1.4725558757781982\n",
      "Epoch 4, Iteration 263, Loss: 2.0301694869995117\n",
      "Epoch 4, Iteration 264, Loss: 1.7159290313720703\n",
      "Epoch 4, Iteration 265, Loss: 1.7688121795654297\n",
      "Epoch 4, Iteration 266, Loss: 1.7789411544799805\n",
      "Epoch 4, Iteration 267, Loss: 1.6803373098373413\n",
      "Epoch 4, Iteration 268, Loss: 1.6350362300872803\n",
      "Epoch 4, Iteration 269, Loss: 1.7125533819198608\n",
      "Epoch 4, Iteration 270, Loss: 1.575933575630188\n",
      "Epoch 4, Iteration 271, Loss: 1.790830135345459\n",
      "Epoch 4, Iteration 272, Loss: 1.5724170207977295\n",
      "Epoch 4, Iteration 273, Loss: 1.707000732421875\n",
      "Epoch 4, Iteration 274, Loss: 1.547121286392212\n",
      "Epoch 4, Iteration 275, Loss: 1.4654598236083984\n",
      "Epoch 4, Iteration 276, Loss: 1.5116251707077026\n",
      "Epoch 4, Iteration 277, Loss: 1.6044139862060547\n",
      "Epoch 4, Iteration 278, Loss: 1.7927157878875732\n",
      "Epoch 4, Iteration 279, Loss: 1.5529897212982178\n",
      "Epoch 4, Iteration 280, Loss: 1.3282767534255981\n",
      "Epoch 4, Iteration 281, Loss: 1.6561501026153564\n",
      "Epoch 4, Iteration 282, Loss: 1.7079397439956665\n",
      "Epoch 4, Iteration 283, Loss: 1.6496586799621582\n",
      "Epoch 4, Iteration 284, Loss: 1.6731865406036377\n",
      "Epoch 4, Iteration 285, Loss: 1.5320916175842285\n",
      "Epoch 4, Iteration 286, Loss: 1.5072442293167114\n",
      "Epoch 4, Iteration 287, Loss: 1.7046982049942017\n",
      "Epoch 4, Iteration 288, Loss: 1.4877521991729736\n",
      "Epoch 4, Iteration 289, Loss: 1.6237916946411133\n",
      "Epoch 4, Iteration 290, Loss: 1.5298078060150146\n",
      "Epoch 4, Iteration 291, Loss: 1.5021257400512695\n",
      "Epoch 4, Iteration 292, Loss: 1.9849563837051392\n",
      "Epoch 4, Iteration 293, Loss: 2.040799379348755\n",
      "Epoch 4, Iteration 294, Loss: 1.4140994548797607\n",
      "Epoch 4, Iteration 295, Loss: 1.600494146347046\n",
      "Epoch 4, Iteration 296, Loss: 1.928253412246704\n",
      "Epoch 4, Iteration 297, Loss: 1.8607113361358643\n",
      "Epoch 4, Iteration 298, Loss: 1.6247831583023071\n",
      "Epoch 4, Iteration 299, Loss: 1.5832765102386475\n",
      "Epoch 4, Iteration 300, Loss: 1.4042730331420898\n",
      "Epoch 4, Iteration 300, Valid Loss: 0.986478328704834\n",
      "Epoch 4, Iteration 301, Loss: 1.672362208366394\n",
      "Epoch 4, Iteration 302, Loss: 1.6518237590789795\n",
      "Epoch 4, Iteration 303, Loss: 1.9938011169433594\n",
      "Epoch 4, Iteration 304, Loss: 1.572363257408142\n",
      "Epoch 4, Iteration 305, Loss: 1.5815578699111938\n",
      "Epoch 4, Iteration 306, Loss: 1.5854228734970093\n",
      "Epoch 4, Iteration 307, Loss: 1.4990463256835938\n",
      "Epoch 4, Iteration 308, Loss: 1.5339317321777344\n",
      "Epoch 4, Iteration 309, Loss: 1.5008820295333862\n",
      "Epoch 4, Iteration 310, Loss: 1.9270777702331543\n",
      "Epoch 4, Iteration 311, Loss: 1.5199360847473145\n",
      "Epoch 4, Iteration 312, Loss: 1.5261238813400269\n",
      "Epoch 4, Iteration 313, Loss: 1.7912695407867432\n",
      "Epoch 4, Iteration 314, Loss: 1.4734289646148682\n",
      "Epoch 4, Iteration 315, Loss: 1.3145684003829956\n",
      "Epoch 4, Iteration 316, Loss: 1.7872384786605835\n",
      "Epoch 4, Iteration 317, Loss: 1.6325249671936035\n",
      "Epoch 4, Iteration 318, Loss: 1.5154308080673218\n",
      "Epoch 4, Iteration 319, Loss: 1.710349202156067\n",
      "Epoch 4, Iteration 320, Loss: 1.4950320720672607\n",
      "Epoch 4, Iteration 321, Loss: 1.5893913507461548\n",
      "Epoch 4, Iteration 322, Loss: 1.5220527648925781\n",
      "Epoch 4, Iteration 323, Loss: 1.6246157884597778\n",
      "Epoch 4, Iteration 324, Loss: 1.5763312578201294\n",
      "Epoch 4, Iteration 325, Loss: 1.6577962636947632\n",
      "Epoch 4, Iteration 326, Loss: 1.5222772359848022\n",
      "Epoch 4, Iteration 327, Loss: 1.4752886295318604\n",
      "Epoch 4, Iteration 328, Loss: 1.4765735864639282\n",
      "Epoch 4, Iteration 329, Loss: 1.8364938497543335\n",
      "Epoch 4, Iteration 330, Loss: 1.3362257480621338\n",
      "Epoch 4, Iteration 331, Loss: 1.5642666816711426\n",
      "Epoch 4, Iteration 332, Loss: 1.626083254814148\n",
      "Epoch 4, Iteration 333, Loss: 1.8715981245040894\n",
      "Epoch 4, Iteration 334, Loss: 1.4287145137786865\n",
      "Epoch 4, Iteration 335, Loss: 1.7590844631195068\n",
      "Epoch 4, Iteration 336, Loss: 1.6506128311157227\n",
      "Epoch 4, Iteration 337, Loss: 1.366361379623413\n",
      "Epoch 4, Iteration 338, Loss: 1.6051197052001953\n",
      "Epoch 4, Iteration 339, Loss: 1.6250545978546143\n",
      "Epoch 4, Iteration 340, Loss: 1.4558595418930054\n",
      "Epoch 4, Iteration 341, Loss: 1.5372662544250488\n",
      "Epoch 4, Iteration 342, Loss: 1.6697297096252441\n",
      "Epoch 4, Iteration 343, Loss: 1.4767086505889893\n",
      "Epoch 4, Iteration 344, Loss: 1.6203720569610596\n",
      "Epoch 4, Iteration 345, Loss: 1.6436177492141724\n",
      "Epoch 4, Iteration 346, Loss: 1.6276865005493164\n",
      "Epoch 4, Iteration 347, Loss: 1.3081108331680298\n",
      "Epoch 4, Iteration 348, Loss: 1.3955174684524536\n",
      "Epoch 4, Iteration 349, Loss: 1.6210960149765015\n",
      "Epoch 4, Iteration 350, Loss: 1.4864141941070557\n",
      "Epoch 4, Iteration 350, Valid Loss: 0.9985077977180481\n",
      "Epoch 4, Iteration 351, Loss: 1.6482120752334595\n",
      "Epoch 4, Iteration 352, Loss: 1.4681026935577393\n",
      "Epoch 4, Iteration 353, Loss: 1.7654485702514648\n",
      "Epoch 4, Iteration 354, Loss: 1.5032603740692139\n",
      "Epoch 4, Iteration 355, Loss: 1.584737777709961\n",
      "Epoch 4, Iteration 356, Loss: 1.6695059537887573\n",
      "Epoch 4, Iteration 357, Loss: 1.536502718925476\n",
      "Epoch 4, Iteration 358, Loss: 1.538542628288269\n",
      "Epoch 4, Iteration 359, Loss: 1.5141630172729492\n",
      "Epoch 4, Iteration 360, Loss: 1.4005961418151855\n",
      "Epoch 4, Iteration 361, Loss: 1.5325021743774414\n",
      "Epoch 4, Iteration 362, Loss: 1.5127549171447754\n",
      "Epoch 4, Iteration 363, Loss: 1.458130955696106\n",
      "Epoch 4, Iteration 364, Loss: 1.7111159563064575\n",
      "Epoch 4, Iteration 365, Loss: 1.8508570194244385\n",
      "Epoch 4, Iteration 366, Loss: 1.7768343687057495\n",
      "Epoch 4, Iteration 367, Loss: 1.7582576274871826\n",
      "Epoch 4, Iteration 368, Loss: 1.5194547176361084\n",
      "Epoch 4, Iteration 369, Loss: 1.7112345695495605\n",
      "Epoch 4, Iteration 370, Loss: 1.6362160444259644\n",
      "Epoch 4, Iteration 371, Loss: 1.7528913021087646\n",
      "Epoch 4, Iteration 372, Loss: 1.4156267642974854\n",
      "Epoch 4, Iteration 373, Loss: 1.5308948755264282\n",
      "Epoch 4, Iteration 374, Loss: 1.854848861694336\n",
      "Epoch 4, Iteration 375, Loss: 1.4802041053771973\n",
      "Epoch 4, Iteration 376, Loss: 1.7082582712173462\n",
      "Epoch 4, Iteration 377, Loss: 1.6633937358856201\n",
      "Epoch 4, Iteration 378, Loss: 1.4325897693634033\n",
      "Epoch 4, Iteration 379, Loss: 1.5385715961456299\n",
      "Epoch 4, Iteration 380, Loss: 1.4012792110443115\n",
      "Epoch 4, Iteration 381, Loss: 1.4496872425079346\n",
      "Epoch 4, Iteration 382, Loss: 1.3094700574874878\n",
      "Epoch 4, Iteration 383, Loss: 1.5490217208862305\n",
      "Epoch 4, Iteration 384, Loss: 1.5772429704666138\n",
      "Epoch 4, Iteration 385, Loss: 1.4048466682434082\n",
      "Epoch 4, Iteration 386, Loss: 1.4115619659423828\n",
      "Epoch 4, Iteration 387, Loss: 1.9106844663619995\n",
      "Epoch 4, Iteration 388, Loss: 1.482727289199829\n",
      "Epoch 4, Iteration 389, Loss: 1.674148678779602\n",
      "Epoch 4, Iteration 390, Loss: 1.7321946620941162\n",
      "Epoch 4, Iteration 391, Loss: 1.4781132936477661\n",
      "Epoch 5/10, Loss: 1.7147714431796754\n",
      "Epoch 5, Iteration 0, Loss: 1.6241973638534546\n",
      "Epoch 5, Iteration 1, Loss: 1.963212251663208\n",
      "Epoch 5, Iteration 2, Loss: 1.7757047414779663\n",
      "Epoch 5, Iteration 3, Loss: 1.7659999132156372\n",
      "Epoch 5, Iteration 4, Loss: 1.6691088676452637\n",
      "Epoch 5, Iteration 5, Loss: 1.7627533674240112\n",
      "Epoch 5, Iteration 6, Loss: 1.541996717453003\n",
      "Epoch 5, Iteration 7, Loss: 1.6496121883392334\n",
      "Epoch 5, Iteration 8, Loss: 1.6970267295837402\n",
      "Epoch 5, Iteration 9, Loss: 1.9905030727386475\n",
      "Epoch 5, Iteration 10, Loss: 1.7962000370025635\n",
      "Epoch 5, Iteration 11, Loss: 1.532524585723877\n",
      "Epoch 5, Iteration 12, Loss: 1.5836600065231323\n",
      "Epoch 5, Iteration 13, Loss: 1.8448501825332642\n",
      "Epoch 5, Iteration 14, Loss: 1.591100811958313\n",
      "Epoch 5, Iteration 15, Loss: 2.036252498626709\n",
      "Epoch 5, Iteration 16, Loss: 1.6408731937408447\n",
      "Epoch 5, Iteration 17, Loss: 1.9250812530517578\n",
      "Epoch 5, Iteration 18, Loss: 1.82134211063385\n",
      "Epoch 5, Iteration 19, Loss: 1.7742199897766113\n",
      "Epoch 5, Iteration 20, Loss: 1.601891040802002\n",
      "Epoch 5, Iteration 21, Loss: 1.6324987411499023\n",
      "Epoch 5, Iteration 22, Loss: 1.6537991762161255\n",
      "Epoch 5, Iteration 23, Loss: 1.6548197269439697\n",
      "Epoch 5, Iteration 24, Loss: 1.570068120956421\n",
      "Epoch 5, Iteration 25, Loss: 1.4799270629882812\n",
      "Epoch 5, Iteration 26, Loss: 2.019129991531372\n",
      "Epoch 5, Iteration 27, Loss: 1.6259310245513916\n",
      "Epoch 5, Iteration 28, Loss: 1.6382135152816772\n",
      "Epoch 5, Iteration 29, Loss: 1.9445935487747192\n",
      "Epoch 5, Iteration 30, Loss: 1.7213358879089355\n",
      "Epoch 5, Iteration 31, Loss: 1.6889522075653076\n",
      "Epoch 5, Iteration 32, Loss: 1.5712909698486328\n",
      "Epoch 5, Iteration 33, Loss: 1.6248365640640259\n",
      "Epoch 5, Iteration 34, Loss: 1.5438814163208008\n",
      "Epoch 5, Iteration 35, Loss: 1.7334164381027222\n",
      "Epoch 5, Iteration 36, Loss: 1.3927234411239624\n",
      "Epoch 5, Iteration 37, Loss: 1.5105963945388794\n",
      "Epoch 5, Iteration 38, Loss: 1.5309550762176514\n",
      "Epoch 5, Iteration 39, Loss: 1.5630650520324707\n",
      "Epoch 5, Iteration 40, Loss: 1.6336628198623657\n",
      "Epoch 5, Iteration 41, Loss: 1.6937111616134644\n",
      "Epoch 5, Iteration 42, Loss: 1.6569857597351074\n",
      "Epoch 5, Iteration 43, Loss: 1.628631591796875\n",
      "Epoch 5, Iteration 44, Loss: 1.5997512340545654\n",
      "Epoch 5, Iteration 45, Loss: 1.6817376613616943\n",
      "Epoch 5, Iteration 46, Loss: 1.6963781118392944\n",
      "Epoch 5, Iteration 47, Loss: 1.6646173000335693\n",
      "Epoch 5, Iteration 48, Loss: 1.9106496572494507\n",
      "Epoch 5, Iteration 49, Loss: 1.6686073541641235\n",
      "Epoch 5, Iteration 50, Loss: 1.5482323169708252\n",
      "Epoch 5, Iteration 50, Valid Loss: 0.9428172707557678\n",
      "Epoch 5, Iteration 51, Loss: 1.5370311737060547\n",
      "Epoch 5, Iteration 52, Loss: 1.625463604927063\n",
      "Epoch 5, Iteration 53, Loss: 1.6707054376602173\n",
      "Epoch 5, Iteration 54, Loss: 1.9300870895385742\n",
      "Epoch 5, Iteration 55, Loss: 1.8402020931243896\n",
      "Epoch 5, Iteration 56, Loss: 1.7051985263824463\n",
      "Epoch 5, Iteration 57, Loss: 1.6634278297424316\n",
      "Epoch 5, Iteration 58, Loss: 1.439271330833435\n",
      "Epoch 5, Iteration 59, Loss: 1.6041339635849\n",
      "Epoch 5, Iteration 60, Loss: 1.7814266681671143\n",
      "Epoch 5, Iteration 61, Loss: 1.3284647464752197\n",
      "Epoch 5, Iteration 62, Loss: 1.61202073097229\n",
      "Epoch 5, Iteration 63, Loss: 1.3924486637115479\n",
      "Epoch 5, Iteration 64, Loss: 1.6407774686813354\n",
      "Epoch 5, Iteration 65, Loss: 1.725877046585083\n",
      "Epoch 5, Iteration 66, Loss: 1.7210885286331177\n",
      "Epoch 5, Iteration 67, Loss: 1.3523669242858887\n",
      "Epoch 5, Iteration 68, Loss: 1.8087865114212036\n",
      "Epoch 5, Iteration 69, Loss: 1.7957288026809692\n",
      "Epoch 5, Iteration 70, Loss: 1.7048250436782837\n",
      "Epoch 5, Iteration 71, Loss: 1.6886308193206787\n",
      "Epoch 5, Iteration 72, Loss: 1.671466588973999\n",
      "Epoch 5, Iteration 73, Loss: 1.416710615158081\n",
      "Epoch 5, Iteration 74, Loss: 1.8038126230239868\n",
      "Epoch 5, Iteration 75, Loss: 1.5792748928070068\n",
      "Epoch 5, Iteration 76, Loss: 1.4819995164871216\n",
      "Epoch 5, Iteration 77, Loss: 1.3582398891448975\n",
      "Epoch 5, Iteration 78, Loss: 1.9351550340652466\n",
      "Epoch 5, Iteration 79, Loss: 1.9541382789611816\n",
      "Epoch 5, Iteration 80, Loss: 1.392984390258789\n",
      "Epoch 5, Iteration 81, Loss: 1.7917760610580444\n",
      "Epoch 5, Iteration 82, Loss: 1.634090781211853\n",
      "Epoch 5, Iteration 83, Loss: 1.3501005172729492\n",
      "Epoch 5, Iteration 84, Loss: 1.800550937652588\n",
      "Epoch 5, Iteration 85, Loss: 1.5991591215133667\n",
      "Epoch 5, Iteration 86, Loss: 1.7859070301055908\n",
      "Epoch 5, Iteration 87, Loss: 1.541868805885315\n",
      "Epoch 5, Iteration 88, Loss: 1.6675137281417847\n",
      "Epoch 5, Iteration 89, Loss: 1.6866745948791504\n",
      "Epoch 5, Iteration 90, Loss: 1.2260140180587769\n",
      "Epoch 5, Iteration 91, Loss: 1.4662858247756958\n",
      "Epoch 5, Iteration 92, Loss: 1.2153279781341553\n",
      "Epoch 5, Iteration 93, Loss: 1.5963877439498901\n",
      "Epoch 5, Iteration 94, Loss: 1.890172004699707\n",
      "Epoch 5, Iteration 95, Loss: 1.570389747619629\n",
      "Epoch 5, Iteration 96, Loss: 1.5415947437286377\n",
      "Epoch 5, Iteration 97, Loss: 1.4527525901794434\n",
      "Epoch 5, Iteration 98, Loss: 1.5589313507080078\n",
      "Epoch 5, Iteration 99, Loss: 1.5037752389907837\n",
      "Epoch 5, Iteration 100, Loss: 1.4567193984985352\n",
      "Epoch 5, Iteration 100, Valid Loss: 0.9849002957344055\n",
      "Epoch 5, Iteration 101, Loss: 1.999730110168457\n",
      "Epoch 5, Iteration 102, Loss: 1.5697041749954224\n",
      "Epoch 5, Iteration 103, Loss: 1.546654224395752\n",
      "Epoch 5, Iteration 104, Loss: 2.0828795433044434\n",
      "Epoch 5, Iteration 105, Loss: 1.737540364265442\n",
      "Epoch 5, Iteration 106, Loss: 1.583606481552124\n",
      "Epoch 5, Iteration 107, Loss: 2.0096566677093506\n",
      "Epoch 5, Iteration 108, Loss: 1.7096853256225586\n",
      "Epoch 5, Iteration 109, Loss: 1.861893653869629\n",
      "Epoch 5, Iteration 110, Loss: 1.8252159357070923\n",
      "Epoch 5, Iteration 111, Loss: 1.7453417778015137\n",
      "Epoch 5, Iteration 112, Loss: 1.7058573961257935\n",
      "Epoch 5, Iteration 113, Loss: 1.4712061882019043\n",
      "Epoch 5, Iteration 114, Loss: 2.1340112686157227\n",
      "Epoch 5, Iteration 115, Loss: 1.8171031475067139\n",
      "Epoch 5, Iteration 116, Loss: 1.6143287420272827\n",
      "Epoch 5, Iteration 117, Loss: 1.3517941236495972\n",
      "Epoch 5, Iteration 118, Loss: 1.7236747741699219\n",
      "Epoch 5, Iteration 119, Loss: 1.6015905141830444\n",
      "Epoch 5, Iteration 120, Loss: 1.5429933071136475\n",
      "Epoch 5, Iteration 121, Loss: 1.6988325119018555\n",
      "Epoch 5, Iteration 122, Loss: 1.4729992151260376\n",
      "Epoch 5, Iteration 123, Loss: 1.5411944389343262\n",
      "Epoch 5, Iteration 124, Loss: 1.4468953609466553\n",
      "Epoch 5, Iteration 125, Loss: 1.7801587581634521\n",
      "Epoch 5, Iteration 126, Loss: 1.4491528272628784\n",
      "Epoch 5, Iteration 127, Loss: 1.7853271961212158\n",
      "Epoch 5, Iteration 128, Loss: 1.7998851537704468\n",
      "Epoch 5, Iteration 129, Loss: 2.0125153064727783\n",
      "Epoch 5, Iteration 130, Loss: 2.2243523597717285\n",
      "Epoch 5, Iteration 131, Loss: 1.4905214309692383\n",
      "Epoch 5, Iteration 132, Loss: 1.6286505460739136\n",
      "Epoch 5, Iteration 133, Loss: 1.9832286834716797\n",
      "Epoch 5, Iteration 134, Loss: 1.5174517631530762\n",
      "Epoch 5, Iteration 135, Loss: 1.7723411321640015\n",
      "Epoch 5, Iteration 136, Loss: 1.5006742477416992\n",
      "Epoch 5, Iteration 137, Loss: 1.5744872093200684\n",
      "Epoch 5, Iteration 138, Loss: 1.4604886770248413\n",
      "Epoch 5, Iteration 139, Loss: 1.4429399967193604\n",
      "Epoch 5, Iteration 140, Loss: 1.5391626358032227\n",
      "Epoch 5, Iteration 141, Loss: 1.8798168897628784\n",
      "Epoch 5, Iteration 142, Loss: 1.6000351905822754\n",
      "Epoch 5, Iteration 143, Loss: 1.644348382949829\n",
      "Epoch 5, Iteration 144, Loss: 1.4327524900436401\n",
      "Epoch 5, Iteration 145, Loss: 1.6659119129180908\n",
      "Epoch 5, Iteration 146, Loss: 1.8432841300964355\n",
      "Epoch 5, Iteration 147, Loss: 1.5099421739578247\n",
      "Epoch 5, Iteration 148, Loss: 1.250675916671753\n",
      "Epoch 5, Iteration 149, Loss: 1.4180662631988525\n",
      "Epoch 5, Iteration 150, Loss: 1.698464274406433\n",
      "Epoch 5, Iteration 150, Valid Loss: 0.9587625861167908\n",
      "Epoch 5, Iteration 151, Loss: 1.729225993156433\n",
      "Epoch 5, Iteration 152, Loss: 1.526397705078125\n",
      "Epoch 5, Iteration 153, Loss: 1.650132179260254\n",
      "Epoch 5, Iteration 154, Loss: 1.2905577421188354\n",
      "Epoch 5, Iteration 155, Loss: 1.6471542119979858\n",
      "Epoch 5, Iteration 156, Loss: 1.5161149501800537\n",
      "Epoch 5, Iteration 157, Loss: 2.1085798740386963\n",
      "Epoch 5, Iteration 158, Loss: 1.661340594291687\n",
      "Epoch 5, Iteration 159, Loss: 1.7068297863006592\n",
      "Epoch 5, Iteration 160, Loss: 1.4286911487579346\n",
      "Epoch 5, Iteration 161, Loss: 1.508253812789917\n",
      "Epoch 5, Iteration 162, Loss: 1.9515732526779175\n",
      "Epoch 5, Iteration 163, Loss: 1.3428151607513428\n",
      "Epoch 5, Iteration 164, Loss: 1.4895045757293701\n",
      "Epoch 5, Iteration 165, Loss: 1.4436308145523071\n",
      "Epoch 5, Iteration 166, Loss: 1.5094791650772095\n",
      "Epoch 5, Iteration 167, Loss: 1.922035813331604\n",
      "Epoch 5, Iteration 168, Loss: 1.7542234659194946\n",
      "Epoch 5, Iteration 169, Loss: 1.6911371946334839\n",
      "Epoch 5, Iteration 170, Loss: 1.8578126430511475\n",
      "Epoch 5, Iteration 171, Loss: 1.6885727643966675\n",
      "Epoch 5, Iteration 172, Loss: 1.747322678565979\n",
      "Epoch 5, Iteration 173, Loss: 1.4921616315841675\n",
      "Epoch 5, Iteration 174, Loss: 1.6713054180145264\n",
      "Epoch 5, Iteration 175, Loss: 1.7299220561981201\n",
      "Epoch 5, Iteration 176, Loss: 1.3603038787841797\n",
      "Epoch 5, Iteration 177, Loss: 2.063389778137207\n",
      "Epoch 5, Iteration 178, Loss: 1.827544093132019\n",
      "Epoch 5, Iteration 179, Loss: 1.6133270263671875\n",
      "Epoch 5, Iteration 180, Loss: 1.5445525646209717\n",
      "Epoch 5, Iteration 181, Loss: 2.077639579772949\n",
      "Epoch 5, Iteration 182, Loss: 1.8234347105026245\n",
      "Epoch 5, Iteration 183, Loss: 1.3829070329666138\n",
      "Epoch 5, Iteration 184, Loss: 1.4374535083770752\n",
      "Epoch 5, Iteration 185, Loss: 1.445854902267456\n",
      "Epoch 5, Iteration 186, Loss: 1.5209345817565918\n",
      "Epoch 5, Iteration 187, Loss: 1.5064618587493896\n",
      "Epoch 5, Iteration 188, Loss: 1.4045917987823486\n",
      "Epoch 5, Iteration 189, Loss: 1.4765143394470215\n",
      "Epoch 5, Iteration 190, Loss: 1.7641825675964355\n",
      "Epoch 5, Iteration 191, Loss: 1.7319798469543457\n",
      "Epoch 5, Iteration 192, Loss: 1.6670129299163818\n",
      "Epoch 5, Iteration 193, Loss: 1.637037754058838\n",
      "Epoch 5, Iteration 194, Loss: 1.5400385856628418\n",
      "Epoch 5, Iteration 195, Loss: 1.4451817274093628\n",
      "Epoch 5, Iteration 196, Loss: 1.6577129364013672\n",
      "Epoch 5, Iteration 197, Loss: 1.5556946992874146\n",
      "Epoch 5, Iteration 198, Loss: 1.623181939125061\n",
      "Epoch 5, Iteration 199, Loss: 1.752817988395691\n",
      "Epoch 5, Iteration 200, Loss: 1.5511069297790527\n",
      "Epoch 5, Iteration 200, Valid Loss: 0.9488080143928528\n",
      "Epoch 5, Iteration 201, Loss: 1.450032353401184\n",
      "Epoch 5, Iteration 202, Loss: 1.6190922260284424\n",
      "Epoch 5, Iteration 203, Loss: 1.5305896997451782\n",
      "Epoch 5, Iteration 204, Loss: 1.5388567447662354\n",
      "Epoch 5, Iteration 205, Loss: 1.369377613067627\n",
      "Epoch 5, Iteration 206, Loss: 1.5636882781982422\n",
      "Epoch 5, Iteration 207, Loss: 1.7714130878448486\n",
      "Epoch 5, Iteration 208, Loss: 2.1974287033081055\n",
      "Epoch 5, Iteration 209, Loss: 2.4718918800354004\n",
      "Epoch 5, Iteration 210, Loss: 2.0881121158599854\n",
      "Epoch 5, Iteration 211, Loss: 2.2081806659698486\n",
      "Epoch 5, Iteration 212, Loss: 1.9703913927078247\n",
      "Epoch 5, Iteration 213, Loss: 2.01556396484375\n",
      "Epoch 5, Iteration 214, Loss: 1.6103750467300415\n",
      "Epoch 5, Iteration 215, Loss: 1.523792028427124\n",
      "Epoch 5, Iteration 216, Loss: 1.4556301832199097\n",
      "Epoch 5, Iteration 217, Loss: 1.584694743156433\n",
      "Epoch 5, Iteration 218, Loss: 1.680633306503296\n",
      "Epoch 5, Iteration 219, Loss: 1.6826974153518677\n",
      "Epoch 5, Iteration 220, Loss: 1.559553623199463\n",
      "Epoch 5, Iteration 221, Loss: 1.6265040636062622\n",
      "Epoch 5, Iteration 222, Loss: 1.6638909578323364\n",
      "Epoch 5, Iteration 223, Loss: 1.5563740730285645\n",
      "Epoch 5, Iteration 224, Loss: 1.4225319623947144\n",
      "Epoch 5, Iteration 225, Loss: 1.8290632963180542\n",
      "Epoch 5, Iteration 226, Loss: 1.7514188289642334\n",
      "Epoch 5, Iteration 227, Loss: 1.7081576585769653\n",
      "Epoch 5, Iteration 228, Loss: 1.4905881881713867\n",
      "Epoch 5, Iteration 229, Loss: 1.8251848220825195\n",
      "Epoch 5, Iteration 230, Loss: 1.6662051677703857\n",
      "Epoch 5, Iteration 231, Loss: 1.6864131689071655\n",
      "Epoch 5, Iteration 232, Loss: 1.7253621816635132\n",
      "Epoch 5, Iteration 233, Loss: 2.160684585571289\n",
      "Epoch 5, Iteration 234, Loss: 1.8220239877700806\n",
      "Epoch 5, Iteration 235, Loss: 1.496199369430542\n",
      "Epoch 5, Iteration 236, Loss: 1.71034836769104\n",
      "Epoch 5, Iteration 237, Loss: 1.5125619173049927\n",
      "Epoch 5, Iteration 238, Loss: 1.4374662637710571\n",
      "Epoch 5, Iteration 239, Loss: 1.7302665710449219\n",
      "Epoch 5, Iteration 240, Loss: 1.4237899780273438\n",
      "Epoch 5, Iteration 241, Loss: 1.5157196521759033\n",
      "Epoch 5, Iteration 242, Loss: 1.5390409231185913\n",
      "Epoch 5, Iteration 243, Loss: 1.598778247833252\n",
      "Epoch 5, Iteration 244, Loss: 1.5074180364608765\n",
      "Epoch 5, Iteration 245, Loss: 1.275437355041504\n",
      "Epoch 5, Iteration 246, Loss: 1.6183373928070068\n",
      "Epoch 5, Iteration 247, Loss: 1.498618483543396\n",
      "Epoch 5, Iteration 248, Loss: 1.6181854009628296\n",
      "Epoch 5, Iteration 249, Loss: 1.4479094743728638\n",
      "Epoch 5, Iteration 250, Loss: 1.6858798265457153\n",
      "Epoch 5, Iteration 250, Valid Loss: 0.9225060939788818\n",
      "Epoch 5, Iteration 251, Loss: 1.6025629043579102\n",
      "Epoch 5, Iteration 252, Loss: 1.7668412923812866\n",
      "Epoch 5, Iteration 253, Loss: 1.6798031330108643\n",
      "Epoch 5, Iteration 254, Loss: 1.6857531070709229\n",
      "Epoch 5, Iteration 255, Loss: 1.8060219287872314\n",
      "Epoch 5, Iteration 256, Loss: 1.9080426692962646\n",
      "Epoch 5, Iteration 257, Loss: 1.4903898239135742\n",
      "Epoch 5, Iteration 258, Loss: 2.049928665161133\n",
      "Epoch 5, Iteration 259, Loss: 1.3544212579727173\n",
      "Epoch 5, Iteration 260, Loss: 1.8829609155654907\n",
      "Epoch 5, Iteration 261, Loss: 1.7180018424987793\n",
      "Epoch 5, Iteration 262, Loss: 1.365537166595459\n",
      "Epoch 5, Iteration 263, Loss: 1.8241095542907715\n",
      "Epoch 5, Iteration 264, Loss: 1.5782525539398193\n",
      "Epoch 5, Iteration 265, Loss: 1.6463662385940552\n",
      "Epoch 5, Iteration 266, Loss: 1.6888396739959717\n",
      "Epoch 5, Iteration 267, Loss: 1.581978678703308\n",
      "Epoch 5, Iteration 268, Loss: 1.570966124534607\n",
      "Epoch 5, Iteration 269, Loss: 1.5449402332305908\n",
      "Epoch 5, Iteration 270, Loss: 1.6477500200271606\n",
      "Epoch 5, Iteration 271, Loss: 1.7313425540924072\n",
      "Epoch 5, Iteration 272, Loss: 1.4217016696929932\n",
      "Epoch 5, Iteration 273, Loss: 1.7530094385147095\n",
      "Epoch 5, Iteration 274, Loss: 1.5008939504623413\n",
      "Epoch 5, Iteration 275, Loss: 1.438976764678955\n",
      "Epoch 5, Iteration 276, Loss: 1.4305530786514282\n",
      "Epoch 5, Iteration 277, Loss: 1.4764915704727173\n",
      "Epoch 5, Iteration 278, Loss: 1.7396612167358398\n",
      "Epoch 5, Iteration 279, Loss: 1.3985188007354736\n",
      "Epoch 5, Iteration 280, Loss: 1.3488212823867798\n",
      "Epoch 5, Iteration 281, Loss: 1.4411587715148926\n",
      "Epoch 5, Iteration 282, Loss: 1.67073655128479\n",
      "Epoch 5, Iteration 283, Loss: 1.6278018951416016\n",
      "Epoch 5, Iteration 284, Loss: 1.546694278717041\n",
      "Epoch 5, Iteration 285, Loss: 1.5157146453857422\n",
      "Epoch 5, Iteration 286, Loss: 1.4832850694656372\n",
      "Epoch 5, Iteration 287, Loss: 1.588942289352417\n",
      "Epoch 5, Iteration 288, Loss: 1.4132490158081055\n",
      "Epoch 5, Iteration 289, Loss: 1.4577360153198242\n",
      "Epoch 5, Iteration 290, Loss: 1.4472326040267944\n",
      "Epoch 5, Iteration 291, Loss: 1.4432587623596191\n",
      "Epoch 5, Iteration 292, Loss: 1.806030511856079\n",
      "Epoch 5, Iteration 293, Loss: 1.851209282875061\n",
      "Epoch 5, Iteration 294, Loss: 1.373218297958374\n",
      "Epoch 5, Iteration 295, Loss: 1.4433236122131348\n",
      "Epoch 5, Iteration 296, Loss: 1.8701752424240112\n",
      "Epoch 5, Iteration 297, Loss: 1.7918519973754883\n",
      "Epoch 5, Iteration 298, Loss: 1.6074343919754028\n",
      "Epoch 5, Iteration 299, Loss: 1.271769642829895\n",
      "Epoch 5, Iteration 300, Loss: 1.3090121746063232\n",
      "Epoch 5, Iteration 300, Valid Loss: 0.9176764488220215\n",
      "Epoch 5, Iteration 301, Loss: 1.549169898033142\n",
      "Epoch 5, Iteration 302, Loss: 1.4348331689834595\n",
      "Epoch 5, Iteration 303, Loss: 1.8500018119812012\n",
      "Epoch 5, Iteration 304, Loss: 1.451108694076538\n",
      "Epoch 5, Iteration 305, Loss: 1.4190924167633057\n",
      "Epoch 5, Iteration 306, Loss: 1.457453727722168\n",
      "Epoch 5, Iteration 307, Loss: 1.4100300073623657\n",
      "Epoch 5, Iteration 308, Loss: 1.4512654542922974\n",
      "Epoch 5, Iteration 309, Loss: 1.6157324314117432\n",
      "Epoch 5, Iteration 310, Loss: 1.854191780090332\n",
      "Epoch 5, Iteration 311, Loss: 1.5261479616165161\n",
      "Epoch 5, Iteration 312, Loss: 1.2916206121444702\n",
      "Epoch 5, Iteration 313, Loss: 1.7311545610427856\n",
      "Epoch 5, Iteration 314, Loss: 1.4270477294921875\n",
      "Epoch 5, Iteration 315, Loss: 1.2871392965316772\n",
      "Epoch 5, Iteration 316, Loss: 1.6991900205612183\n",
      "Epoch 5, Iteration 317, Loss: 1.497772216796875\n",
      "Epoch 5, Iteration 318, Loss: 1.4348313808441162\n",
      "Epoch 5, Iteration 319, Loss: 1.6371804475784302\n",
      "Epoch 5, Iteration 320, Loss: 1.256373643875122\n",
      "Epoch 5, Iteration 321, Loss: 1.5885767936706543\n",
      "Epoch 5, Iteration 322, Loss: 1.478087067604065\n",
      "Epoch 5, Iteration 323, Loss: 1.5652291774749756\n",
      "Epoch 5, Iteration 324, Loss: 1.5709517002105713\n",
      "Epoch 5, Iteration 325, Loss: 1.5193835496902466\n",
      "Epoch 5, Iteration 326, Loss: 1.5100078582763672\n",
      "Epoch 5, Iteration 327, Loss: 1.457986831665039\n",
      "Epoch 5, Iteration 328, Loss: 1.5297892093658447\n",
      "Epoch 5, Iteration 329, Loss: 1.584195852279663\n",
      "Epoch 5, Iteration 330, Loss: 1.2303707599639893\n",
      "Epoch 5, Iteration 331, Loss: 1.4284725189208984\n",
      "Epoch 5, Iteration 332, Loss: 1.5218212604522705\n",
      "Epoch 5, Iteration 333, Loss: 1.7115068435668945\n",
      "Epoch 5, Iteration 334, Loss: 1.4609233140945435\n",
      "Epoch 5, Iteration 335, Loss: 1.5551788806915283\n",
      "Epoch 5, Iteration 336, Loss: 1.593967080116272\n",
      "Epoch 5, Iteration 337, Loss: 1.2515661716461182\n",
      "Epoch 5, Iteration 338, Loss: 1.6314823627471924\n",
      "Epoch 5, Iteration 339, Loss: 1.439786672592163\n",
      "Epoch 5, Iteration 340, Loss: 1.362583875656128\n",
      "Epoch 5, Iteration 341, Loss: 1.4734538793563843\n",
      "Epoch 5, Iteration 342, Loss: 1.4099359512329102\n",
      "Epoch 5, Iteration 343, Loss: 1.3991708755493164\n",
      "Epoch 5, Iteration 344, Loss: 1.650292992591858\n",
      "Epoch 5, Iteration 345, Loss: 1.6740976572036743\n",
      "Epoch 5, Iteration 346, Loss: 1.5752472877502441\n",
      "Epoch 5, Iteration 347, Loss: 1.2533700466156006\n",
      "Epoch 5, Iteration 348, Loss: 1.34756338596344\n",
      "Epoch 5, Iteration 349, Loss: 1.5934185981750488\n",
      "Epoch 5, Iteration 350, Loss: 1.4331225156784058\n",
      "Epoch 5, Iteration 350, Valid Loss: 0.9296104907989502\n",
      "Epoch 5, Iteration 351, Loss: 1.5181796550750732\n",
      "Epoch 5, Iteration 352, Loss: 1.4747213125228882\n",
      "Epoch 5, Iteration 353, Loss: 1.5352963209152222\n",
      "Epoch 5, Iteration 354, Loss: 1.3996578454971313\n",
      "Epoch 5, Iteration 355, Loss: 1.5225749015808105\n",
      "Epoch 5, Iteration 356, Loss: 1.5642588138580322\n",
      "Epoch 5, Iteration 357, Loss: 1.3757166862487793\n",
      "Epoch 5, Iteration 358, Loss: 1.4726537466049194\n",
      "Epoch 5, Iteration 359, Loss: 1.5025755167007446\n",
      "Epoch 5, Iteration 360, Loss: 1.4207279682159424\n",
      "Epoch 5, Iteration 361, Loss: 1.450935959815979\n",
      "Epoch 5, Iteration 362, Loss: 1.446498155593872\n",
      "Epoch 5, Iteration 363, Loss: 1.465457797050476\n",
      "Epoch 5, Iteration 364, Loss: 1.489612340927124\n",
      "Epoch 5, Iteration 365, Loss: 1.6371926069259644\n",
      "Epoch 5, Iteration 366, Loss: 1.6911771297454834\n",
      "Epoch 5, Iteration 367, Loss: 1.5758883953094482\n",
      "Epoch 5, Iteration 368, Loss: 1.5197315216064453\n",
      "Epoch 5, Iteration 369, Loss: 1.6710612773895264\n",
      "Epoch 5, Iteration 370, Loss: 1.6399503946304321\n",
      "Epoch 5, Iteration 371, Loss: 1.5268973112106323\n",
      "Epoch 5, Iteration 372, Loss: 1.3054159879684448\n",
      "Epoch 5, Iteration 373, Loss: 1.3553645610809326\n",
      "Epoch 5, Iteration 374, Loss: 1.7647312879562378\n",
      "Epoch 5, Iteration 375, Loss: 1.5279526710510254\n",
      "Epoch 5, Iteration 376, Loss: 1.6568537950515747\n",
      "Epoch 5, Iteration 377, Loss: 1.5262727737426758\n",
      "Epoch 5, Iteration 378, Loss: 1.394017219543457\n",
      "Epoch 5, Iteration 379, Loss: 1.4247462749481201\n",
      "Epoch 5, Iteration 380, Loss: 1.4443881511688232\n",
      "Epoch 5, Iteration 381, Loss: 1.3863593339920044\n",
      "Epoch 5, Iteration 382, Loss: 1.139230489730835\n",
      "Epoch 5, Iteration 383, Loss: 1.4424571990966797\n",
      "Epoch 5, Iteration 384, Loss: 1.4292678833007812\n",
      "Epoch 5, Iteration 385, Loss: 1.365984320640564\n",
      "Epoch 5, Iteration 386, Loss: 1.3070930242538452\n",
      "Epoch 5, Iteration 387, Loss: 1.7878165245056152\n",
      "Epoch 5, Iteration 388, Loss: 1.4664231538772583\n",
      "Epoch 5, Iteration 389, Loss: 1.589698314666748\n",
      "Epoch 5, Iteration 390, Loss: 1.764993667602539\n",
      "Epoch 5, Iteration 391, Loss: 1.3075919151306152\n",
      "Epoch 6/10, Loss: 1.610075089700368\n",
      "Epoch 6, Iteration 0, Loss: 1.6199774742126465\n",
      "Epoch 6, Iteration 1, Loss: 1.9752928018569946\n",
      "Epoch 6, Iteration 2, Loss: 1.6939550638198853\n",
      "Epoch 6, Iteration 3, Loss: 1.722096562385559\n",
      "Epoch 6, Iteration 4, Loss: 1.57614004611969\n",
      "Epoch 6, Iteration 5, Loss: 1.701351284980774\n",
      "Epoch 6, Iteration 6, Loss: 1.5209085941314697\n",
      "Epoch 6, Iteration 7, Loss: 1.5276198387145996\n",
      "Epoch 6, Iteration 8, Loss: 1.5916011333465576\n",
      "Epoch 6, Iteration 9, Loss: 1.8126264810562134\n",
      "Epoch 6, Iteration 10, Loss: 1.6002862453460693\n",
      "Epoch 6, Iteration 11, Loss: 1.4392764568328857\n",
      "Epoch 6, Iteration 12, Loss: 1.4769902229309082\n",
      "Epoch 6, Iteration 13, Loss: 1.6957480907440186\n",
      "Epoch 6, Iteration 14, Loss: 1.5315653085708618\n",
      "Epoch 6, Iteration 15, Loss: 1.6574445962905884\n",
      "Epoch 6, Iteration 16, Loss: 1.585288405418396\n",
      "Epoch 6, Iteration 17, Loss: 1.7523553371429443\n",
      "Epoch 6, Iteration 18, Loss: 1.6332430839538574\n",
      "Epoch 6, Iteration 19, Loss: 1.7007032632827759\n",
      "Epoch 6, Iteration 20, Loss: 1.5388457775115967\n",
      "Epoch 6, Iteration 21, Loss: 1.6277046203613281\n",
      "Epoch 6, Iteration 22, Loss: 1.5918887853622437\n",
      "Epoch 6, Iteration 23, Loss: 1.6447951793670654\n",
      "Epoch 6, Iteration 24, Loss: 1.5157431364059448\n",
      "Epoch 6, Iteration 25, Loss: 1.3695793151855469\n",
      "Epoch 6, Iteration 26, Loss: 1.917839527130127\n",
      "Epoch 6, Iteration 27, Loss: 1.5510177612304688\n",
      "Epoch 6, Iteration 28, Loss: 1.5950958728790283\n",
      "Epoch 6, Iteration 29, Loss: 1.774978518486023\n",
      "Epoch 6, Iteration 30, Loss: 1.653422236442566\n",
      "Epoch 6, Iteration 31, Loss: 1.611724615097046\n",
      "Epoch 6, Iteration 32, Loss: 1.6420629024505615\n",
      "Epoch 6, Iteration 33, Loss: 1.5167860984802246\n",
      "Epoch 6, Iteration 34, Loss: 1.518810510635376\n",
      "Epoch 6, Iteration 35, Loss: 1.7654571533203125\n",
      "Epoch 6, Iteration 36, Loss: 1.348781943321228\n",
      "Epoch 6, Iteration 37, Loss: 1.4671165943145752\n",
      "Epoch 6, Iteration 38, Loss: 1.5064862966537476\n",
      "Epoch 6, Iteration 39, Loss: 1.4275003671646118\n",
      "Epoch 6, Iteration 40, Loss: 1.4698586463928223\n",
      "Epoch 6, Iteration 41, Loss: 1.630218267440796\n",
      "Epoch 6, Iteration 42, Loss: 1.519860029220581\n",
      "Epoch 6, Iteration 43, Loss: 1.5286273956298828\n",
      "Epoch 6, Iteration 44, Loss: 1.508428692817688\n",
      "Epoch 6, Iteration 45, Loss: 1.5316461324691772\n",
      "Epoch 6, Iteration 46, Loss: 1.6069648265838623\n",
      "Epoch 6, Iteration 47, Loss: 1.6345535516738892\n",
      "Epoch 6, Iteration 48, Loss: 1.7910077571868896\n",
      "Epoch 6, Iteration 49, Loss: 1.6145765781402588\n",
      "Epoch 6, Iteration 50, Loss: 1.5419031381607056\n",
      "Epoch 6, Iteration 50, Valid Loss: 0.8801066875457764\n",
      "Epoch 6, Iteration 51, Loss: 1.5248162746429443\n",
      "Epoch 6, Iteration 52, Loss: 1.4592008590698242\n",
      "Epoch 6, Iteration 53, Loss: 1.8820598125457764\n",
      "Epoch 6, Iteration 54, Loss: 1.8643913269042969\n",
      "Epoch 6, Iteration 55, Loss: 1.8089256286621094\n",
      "Epoch 6, Iteration 56, Loss: 1.4671275615692139\n",
      "Epoch 6, Iteration 57, Loss: 1.8304493427276611\n",
      "Epoch 6, Iteration 58, Loss: 1.43105149269104\n",
      "Epoch 6, Iteration 59, Loss: 1.5173838138580322\n",
      "Epoch 6, Iteration 60, Loss: 1.595081090927124\n",
      "Epoch 6, Iteration 61, Loss: 1.268319010734558\n",
      "Epoch 6, Iteration 62, Loss: 1.5163404941558838\n",
      "Epoch 6, Iteration 63, Loss: 1.375815510749817\n",
      "Epoch 6, Iteration 64, Loss: 1.4243367910385132\n",
      "Epoch 6, Iteration 65, Loss: 1.834952712059021\n",
      "Epoch 6, Iteration 66, Loss: 1.4665634632110596\n",
      "Epoch 6, Iteration 67, Loss: 1.2455077171325684\n",
      "Epoch 6, Iteration 68, Loss: 1.797236680984497\n",
      "Epoch 6, Iteration 69, Loss: 1.9584975242614746\n",
      "Epoch 6, Iteration 70, Loss: 1.7993382215499878\n",
      "Epoch 6, Iteration 71, Loss: 1.6500751972198486\n",
      "Epoch 6, Iteration 72, Loss: 1.7802550792694092\n",
      "Epoch 6, Iteration 73, Loss: 1.38499116897583\n",
      "Epoch 6, Iteration 74, Loss: 1.8708425760269165\n",
      "Epoch 6, Iteration 75, Loss: 1.639788269996643\n",
      "Epoch 6, Iteration 76, Loss: 1.4574744701385498\n",
      "Epoch 6, Iteration 77, Loss: 1.2456490993499756\n",
      "Epoch 6, Iteration 78, Loss: 1.8526241779327393\n",
      "Epoch 6, Iteration 79, Loss: 1.8638877868652344\n",
      "Epoch 6, Iteration 80, Loss: 1.3100886344909668\n",
      "Epoch 6, Iteration 81, Loss: 1.8885821104049683\n",
      "Epoch 6, Iteration 82, Loss: 1.4150880575180054\n",
      "Epoch 6, Iteration 83, Loss: 1.2991806268692017\n",
      "Epoch 6, Iteration 84, Loss: 1.768306851387024\n",
      "Epoch 6, Iteration 85, Loss: 1.6327557563781738\n",
      "Epoch 6, Iteration 86, Loss: 1.7086223363876343\n",
      "Epoch 6, Iteration 87, Loss: 1.4709006547927856\n",
      "Epoch 6, Iteration 88, Loss: 1.6663978099822998\n",
      "Epoch 6, Iteration 89, Loss: 1.6517815589904785\n",
      "Epoch 6, Iteration 90, Loss: 1.2594797611236572\n",
      "Epoch 6, Iteration 91, Loss: 1.5023152828216553\n",
      "Epoch 6, Iteration 92, Loss: 1.2363882064819336\n",
      "Epoch 6, Iteration 93, Loss: 1.660900592803955\n",
      "Epoch 6, Iteration 94, Loss: 1.9668269157409668\n",
      "Epoch 6, Iteration 95, Loss: 1.342360496520996\n",
      "Epoch 6, Iteration 96, Loss: 1.586355209350586\n",
      "Epoch 6, Iteration 97, Loss: 1.3347020149230957\n",
      "Epoch 6, Iteration 98, Loss: 1.5126726627349854\n",
      "Epoch 6, Iteration 99, Loss: 1.431624412536621\n",
      "Epoch 6, Iteration 100, Loss: 1.349648356437683\n",
      "Epoch 6, Iteration 100, Valid Loss: 0.9077703952789307\n",
      "Epoch 6, Iteration 101, Loss: 1.9472973346710205\n",
      "Epoch 6, Iteration 102, Loss: 1.4677927494049072\n",
      "Epoch 6, Iteration 103, Loss: 1.6908758878707886\n",
      "Epoch 6, Iteration 104, Loss: 1.9260406494140625\n",
      "Epoch 6, Iteration 105, Loss: 1.6798138618469238\n",
      "Epoch 6, Iteration 106, Loss: 1.5099396705627441\n",
      "Epoch 6, Iteration 107, Loss: 1.9842350482940674\n",
      "Epoch 6, Iteration 108, Loss: 1.5039970874786377\n",
      "Epoch 6, Iteration 109, Loss: 1.8017916679382324\n",
      "Epoch 6, Iteration 110, Loss: 1.6946724653244019\n",
      "Epoch 6, Iteration 111, Loss: 1.7168257236480713\n",
      "Epoch 6, Iteration 112, Loss: 1.7467677593231201\n",
      "Epoch 6, Iteration 113, Loss: 1.6069656610488892\n",
      "Epoch 6, Iteration 114, Loss: 1.8182008266448975\n",
      "Epoch 6, Iteration 115, Loss: 1.7952830791473389\n",
      "Epoch 6, Iteration 116, Loss: 1.507143259048462\n",
      "Epoch 6, Iteration 117, Loss: 1.2312302589416504\n",
      "Epoch 6, Iteration 118, Loss: 1.7613332271575928\n",
      "Epoch 6, Iteration 119, Loss: 1.5167096853256226\n",
      "Epoch 6, Iteration 120, Loss: 1.539642095565796\n",
      "Epoch 6, Iteration 121, Loss: 1.437103509902954\n",
      "Epoch 6, Iteration 122, Loss: 1.504991888999939\n",
      "Epoch 6, Iteration 123, Loss: 1.4682892560958862\n",
      "Epoch 6, Iteration 124, Loss: 1.4118834733963013\n",
      "Epoch 6, Iteration 125, Loss: 1.4099677801132202\n",
      "Epoch 6, Iteration 126, Loss: 1.6281824111938477\n",
      "Epoch 6, Iteration 127, Loss: 1.7660911083221436\n",
      "Epoch 6, Iteration 128, Loss: 1.6210098266601562\n",
      "Epoch 6, Iteration 129, Loss: 1.6805026531219482\n",
      "Epoch 6, Iteration 130, Loss: 2.0623703002929688\n",
      "Epoch 6, Iteration 131, Loss: 1.3849362134933472\n",
      "Epoch 6, Iteration 132, Loss: 1.3292641639709473\n",
      "Epoch 6, Iteration 133, Loss: 2.0101428031921387\n",
      "Epoch 6, Iteration 134, Loss: 1.4277002811431885\n",
      "Epoch 6, Iteration 135, Loss: 1.7068655490875244\n",
      "Epoch 6, Iteration 136, Loss: 1.4823964834213257\n",
      "Epoch 6, Iteration 137, Loss: 1.5091005563735962\n",
      "Epoch 6, Iteration 138, Loss: 1.294032096862793\n",
      "Epoch 6, Iteration 139, Loss: 1.5137851238250732\n",
      "Epoch 6, Iteration 140, Loss: 1.5053246021270752\n",
      "Epoch 6, Iteration 141, Loss: 1.9657225608825684\n",
      "Epoch 6, Iteration 142, Loss: 1.3840432167053223\n",
      "Epoch 6, Iteration 143, Loss: 1.621026635169983\n",
      "Epoch 6, Iteration 144, Loss: 1.3804835081100464\n",
      "Epoch 6, Iteration 145, Loss: 1.5712579488754272\n",
      "Epoch 6, Iteration 146, Loss: 1.822765827178955\n",
      "Epoch 6, Iteration 147, Loss: 1.5091968774795532\n",
      "Epoch 6, Iteration 148, Loss: 1.3077669143676758\n",
      "Epoch 6, Iteration 149, Loss: 1.5717713832855225\n",
      "Epoch 6, Iteration 150, Loss: 1.6728101968765259\n",
      "Epoch 6, Iteration 150, Valid Loss: 0.8900068402290344\n",
      "Epoch 6, Iteration 151, Loss: 1.6467156410217285\n",
      "Epoch 6, Iteration 152, Loss: 1.4174944162368774\n",
      "Epoch 6, Iteration 153, Loss: 1.5098551511764526\n",
      "Epoch 6, Iteration 154, Loss: 1.2643811702728271\n",
      "Epoch 6, Iteration 155, Loss: 1.460545539855957\n",
      "Epoch 6, Iteration 156, Loss: 1.4667834043502808\n",
      "Epoch 6, Iteration 157, Loss: 2.0621538162231445\n",
      "Epoch 6, Iteration 158, Loss: 1.5493929386138916\n",
      "Epoch 6, Iteration 159, Loss: 1.6079069375991821\n",
      "Epoch 6, Iteration 160, Loss: 1.3924989700317383\n",
      "Epoch 6, Iteration 161, Loss: 1.4226629734039307\n",
      "Epoch 6, Iteration 162, Loss: 1.966302752494812\n",
      "Epoch 6, Iteration 163, Loss: 1.3185315132141113\n",
      "Epoch 6, Iteration 164, Loss: 1.4695051908493042\n",
      "Epoch 6, Iteration 165, Loss: 1.3623703718185425\n",
      "Epoch 6, Iteration 166, Loss: 1.430393934249878\n",
      "Epoch 6, Iteration 167, Loss: 1.7870765924453735\n",
      "Epoch 6, Iteration 168, Loss: 1.672603964805603\n",
      "Epoch 6, Iteration 169, Loss: 1.5805009603500366\n",
      "Epoch 6, Iteration 170, Loss: 1.8130018711090088\n",
      "Epoch 6, Iteration 171, Loss: 1.6844273805618286\n",
      "Epoch 6, Iteration 172, Loss: 1.681754469871521\n",
      "Epoch 6, Iteration 173, Loss: 1.4997889995574951\n",
      "Epoch 6, Iteration 174, Loss: 1.6853007078170776\n",
      "Epoch 6, Iteration 175, Loss: 1.7438760995864868\n",
      "Epoch 6, Iteration 176, Loss: 1.2812011241912842\n",
      "Epoch 6, Iteration 177, Loss: 1.9700212478637695\n",
      "Epoch 6, Iteration 178, Loss: 1.7303378582000732\n",
      "Epoch 6, Iteration 179, Loss: 1.5392810106277466\n",
      "Epoch 6, Iteration 180, Loss: 1.4631752967834473\n",
      "Epoch 6, Iteration 181, Loss: 1.8209939002990723\n",
      "Epoch 6, Iteration 182, Loss: 1.6672279834747314\n",
      "Epoch 6, Iteration 183, Loss: 1.3285036087036133\n",
      "Epoch 6, Iteration 184, Loss: 1.4732544422149658\n",
      "Epoch 6, Iteration 185, Loss: 1.390442132949829\n",
      "Epoch 6, Iteration 186, Loss: 1.4472277164459229\n",
      "Epoch 6, Iteration 187, Loss: 1.4445841312408447\n",
      "Epoch 6, Iteration 188, Loss: 1.432904839515686\n",
      "Epoch 6, Iteration 189, Loss: 1.4102249145507812\n",
      "Epoch 6, Iteration 190, Loss: 1.6740726232528687\n",
      "Epoch 6, Iteration 191, Loss: 1.6596860885620117\n",
      "Epoch 6, Iteration 192, Loss: 1.6061774492263794\n",
      "Epoch 6, Iteration 193, Loss: 1.5020365715026855\n",
      "Epoch 6, Iteration 194, Loss: 1.6505844593048096\n",
      "Epoch 6, Iteration 195, Loss: 1.4477304220199585\n",
      "Epoch 6, Iteration 196, Loss: 1.6577105522155762\n",
      "Epoch 6, Iteration 197, Loss: 1.4947516918182373\n",
      "Epoch 6, Iteration 198, Loss: 1.5032143592834473\n",
      "Epoch 6, Iteration 199, Loss: 1.6648792028427124\n",
      "Epoch 6, Iteration 200, Loss: 1.374205470085144\n",
      "Epoch 6, Iteration 200, Valid Loss: 0.8835152387619019\n",
      "Epoch 6, Iteration 201, Loss: 1.3960838317871094\n",
      "Epoch 6, Iteration 202, Loss: 1.6325286626815796\n",
      "Epoch 6, Iteration 203, Loss: 1.3655840158462524\n",
      "Epoch 6, Iteration 204, Loss: 1.3991329669952393\n",
      "Epoch 6, Iteration 205, Loss: 1.2249974012374878\n",
      "Epoch 6, Iteration 206, Loss: 1.537030577659607\n",
      "Epoch 6, Iteration 207, Loss: 1.756842017173767\n",
      "Epoch 6, Iteration 208, Loss: 2.237668752670288\n",
      "Epoch 6, Iteration 209, Loss: 2.2318427562713623\n",
      "Epoch 6, Iteration 210, Loss: 1.9342153072357178\n",
      "Epoch 6, Iteration 211, Loss: 2.07342529296875\n",
      "Epoch 6, Iteration 212, Loss: 1.8778975009918213\n",
      "Epoch 6, Iteration 213, Loss: 1.924814224243164\n",
      "Epoch 6, Iteration 214, Loss: 1.6485130786895752\n",
      "Epoch 6, Iteration 215, Loss: 1.4320183992385864\n",
      "Epoch 6, Iteration 216, Loss: 1.4425418376922607\n",
      "Epoch 6, Iteration 217, Loss: 1.4405066967010498\n",
      "Epoch 6, Iteration 218, Loss: 1.5139672756195068\n",
      "Epoch 6, Iteration 219, Loss: 1.574571132659912\n",
      "Epoch 6, Iteration 220, Loss: 1.498403549194336\n",
      "Epoch 6, Iteration 221, Loss: 1.5495511293411255\n",
      "Epoch 6, Iteration 222, Loss: 1.5290782451629639\n",
      "Epoch 6, Iteration 223, Loss: 1.4733175039291382\n",
      "Epoch 6, Iteration 224, Loss: 1.3704843521118164\n",
      "Epoch 6, Iteration 225, Loss: 1.6893842220306396\n",
      "Epoch 6, Iteration 226, Loss: 1.7678959369659424\n",
      "Epoch 6, Iteration 227, Loss: 1.5492870807647705\n",
      "Epoch 6, Iteration 228, Loss: 1.5126879215240479\n",
      "Epoch 6, Iteration 229, Loss: 1.6243575811386108\n",
      "Epoch 6, Iteration 230, Loss: 1.7354897260665894\n",
      "Epoch 6, Iteration 231, Loss: 1.5668001174926758\n",
      "Epoch 6, Iteration 232, Loss: 1.833617925643921\n",
      "Epoch 6, Iteration 233, Loss: 2.0582361221313477\n",
      "Epoch 6, Iteration 234, Loss: 1.756669044494629\n",
      "Epoch 6, Iteration 235, Loss: 1.3859987258911133\n",
      "Epoch 6, Iteration 236, Loss: 1.7243491411209106\n",
      "Epoch 6, Iteration 237, Loss: 1.620750069618225\n",
      "Epoch 6, Iteration 238, Loss: 1.5057439804077148\n",
      "Epoch 6, Iteration 239, Loss: 1.703249216079712\n",
      "Epoch 6, Iteration 240, Loss: 1.4177664518356323\n",
      "Epoch 6, Iteration 241, Loss: 1.4284911155700684\n",
      "Epoch 6, Iteration 242, Loss: 1.4465550184249878\n",
      "Epoch 6, Iteration 243, Loss: 1.5001354217529297\n",
      "Epoch 6, Iteration 244, Loss: 1.446664571762085\n",
      "Epoch 6, Iteration 245, Loss: 1.3101412057876587\n",
      "Epoch 6, Iteration 246, Loss: 1.5314809083938599\n",
      "Epoch 6, Iteration 247, Loss: 1.4800751209259033\n",
      "Epoch 6, Iteration 248, Loss: 1.615035891532898\n",
      "Epoch 6, Iteration 249, Loss: 1.6256024837493896\n",
      "Epoch 6, Iteration 250, Loss: 1.6550995111465454\n",
      "Epoch 6, Iteration 250, Valid Loss: 0.8846786022186279\n",
      "Epoch 6, Iteration 251, Loss: 1.4536815881729126\n",
      "Epoch 6, Iteration 252, Loss: 1.7094496488571167\n",
      "Epoch 6, Iteration 253, Loss: 1.6200472116470337\n",
      "Epoch 6, Iteration 254, Loss: 1.624190092086792\n",
      "Epoch 6, Iteration 255, Loss: 1.675429344177246\n",
      "Epoch 6, Iteration 256, Loss: 1.768024206161499\n",
      "Epoch 6, Iteration 257, Loss: 1.4466201066970825\n",
      "Epoch 6, Iteration 258, Loss: 1.866119384765625\n",
      "Epoch 6, Iteration 259, Loss: 1.5152325630187988\n",
      "Epoch 6, Iteration 260, Loss: 1.7421846389770508\n",
      "Epoch 6, Iteration 261, Loss: 1.6729971170425415\n",
      "Epoch 6, Iteration 262, Loss: 1.3566968441009521\n",
      "Epoch 6, Iteration 263, Loss: 1.794919729232788\n",
      "Epoch 6, Iteration 264, Loss: 1.617527961730957\n",
      "Epoch 6, Iteration 265, Loss: 1.5097886323928833\n",
      "Epoch 6, Iteration 266, Loss: 1.6631226539611816\n",
      "Epoch 6, Iteration 267, Loss: 1.524803638458252\n",
      "Epoch 6, Iteration 268, Loss: 1.5041667222976685\n",
      "Epoch 6, Iteration 269, Loss: 1.6495808362960815\n",
      "Epoch 6, Iteration 270, Loss: 1.533400297164917\n",
      "Epoch 6, Iteration 271, Loss: 1.6865164041519165\n",
      "Epoch 6, Iteration 272, Loss: 1.4459902048110962\n",
      "Epoch 6, Iteration 273, Loss: 1.3200833797454834\n",
      "Epoch 6, Iteration 274, Loss: 1.6013141870498657\n",
      "Epoch 6, Iteration 275, Loss: 1.37417733669281\n",
      "Epoch 6, Iteration 276, Loss: 1.5154613256454468\n",
      "Epoch 6, Iteration 277, Loss: 1.3789782524108887\n",
      "Epoch 6, Iteration 278, Loss: 1.6475672721862793\n",
      "Epoch 6, Iteration 279, Loss: 1.3184257745742798\n",
      "Epoch 6, Iteration 280, Loss: 1.1966501474380493\n",
      "Epoch 6, Iteration 281, Loss: 1.3146158456802368\n",
      "Epoch 6, Iteration 282, Loss: 1.6328426599502563\n",
      "Epoch 6, Iteration 283, Loss: 1.5473178625106812\n",
      "Epoch 6, Iteration 284, Loss: 1.5762349367141724\n",
      "Epoch 6, Iteration 285, Loss: 1.4658817052841187\n",
      "Epoch 6, Iteration 286, Loss: 1.4126553535461426\n",
      "Epoch 6, Iteration 287, Loss: 1.5005011558532715\n",
      "Epoch 6, Iteration 288, Loss: 1.364957332611084\n",
      "Epoch 6, Iteration 289, Loss: 1.545977234840393\n",
      "Epoch 6, Iteration 290, Loss: 1.3789973258972168\n",
      "Epoch 6, Iteration 291, Loss: 1.4217307567596436\n",
      "Epoch 6, Iteration 292, Loss: 1.7549631595611572\n",
      "Epoch 6, Iteration 293, Loss: 1.7469594478607178\n",
      "Epoch 6, Iteration 294, Loss: 1.4498441219329834\n",
      "Epoch 6, Iteration 295, Loss: 1.4749081134796143\n",
      "Epoch 6, Iteration 296, Loss: 1.7957133054733276\n",
      "Epoch 6, Iteration 297, Loss: 1.7204983234405518\n",
      "Epoch 6, Iteration 298, Loss: 1.476863145828247\n",
      "Epoch 6, Iteration 299, Loss: 1.3488072156906128\n",
      "Epoch 6, Iteration 300, Loss: 1.3291943073272705\n",
      "Epoch 6, Iteration 300, Valid Loss: 0.8756405711174011\n",
      "Epoch 6, Iteration 301, Loss: 1.4513570070266724\n",
      "Epoch 6, Iteration 302, Loss: 1.2998287677764893\n",
      "Epoch 6, Iteration 303, Loss: 1.7815475463867188\n",
      "Epoch 6, Iteration 304, Loss: 1.379940390586853\n",
      "Epoch 6, Iteration 305, Loss: 1.422245979309082\n",
      "Epoch 6, Iteration 306, Loss: 1.3752119541168213\n",
      "Epoch 6, Iteration 307, Loss: 1.4798264503479004\n",
      "Epoch 6, Iteration 308, Loss: 1.3930978775024414\n",
      "Epoch 6, Iteration 309, Loss: 1.4156060218811035\n",
      "Epoch 6, Iteration 310, Loss: 1.7274553775787354\n",
      "Epoch 6, Iteration 311, Loss: 1.3887453079223633\n",
      "Epoch 6, Iteration 312, Loss: 1.3895567655563354\n",
      "Epoch 6, Iteration 313, Loss: 1.7248284816741943\n",
      "Epoch 6, Iteration 314, Loss: 1.417992115020752\n",
      "Epoch 6, Iteration 315, Loss: 1.3605964183807373\n",
      "Epoch 6, Iteration 316, Loss: 1.6675742864608765\n",
      "Epoch 6, Iteration 317, Loss: 1.4778106212615967\n",
      "Epoch 6, Iteration 318, Loss: 1.3206040859222412\n",
      "Epoch 6, Iteration 319, Loss: 1.8175328969955444\n",
      "Epoch 6, Iteration 320, Loss: 1.2203521728515625\n",
      "Epoch 6, Iteration 321, Loss: 1.4299731254577637\n",
      "Epoch 6, Iteration 322, Loss: 1.3866820335388184\n",
      "Epoch 6, Iteration 323, Loss: 1.4304001331329346\n",
      "Epoch 6, Iteration 324, Loss: 1.438102126121521\n",
      "Epoch 6, Iteration 325, Loss: 1.3719102144241333\n",
      "Epoch 6, Iteration 326, Loss: 1.4943572282791138\n",
      "Epoch 6, Iteration 327, Loss: 1.3884832859039307\n",
      "Epoch 6, Iteration 328, Loss: 1.2918565273284912\n",
      "Epoch 6, Iteration 329, Loss: 1.5959079265594482\n",
      "Epoch 6, Iteration 330, Loss: 1.1310347318649292\n",
      "Epoch 6, Iteration 331, Loss: 1.3569104671478271\n",
      "Epoch 6, Iteration 332, Loss: 1.514693260192871\n",
      "Epoch 6, Iteration 333, Loss: 1.757453203201294\n",
      "Epoch 6, Iteration 334, Loss: 1.411563515663147\n",
      "Epoch 6, Iteration 335, Loss: 1.7097175121307373\n",
      "Epoch 6, Iteration 336, Loss: 1.5805790424346924\n",
      "Epoch 6, Iteration 337, Loss: 1.2158348560333252\n",
      "Epoch 6, Iteration 338, Loss: 1.5348010063171387\n",
      "Epoch 6, Iteration 339, Loss: 1.550653100013733\n",
      "Epoch 6, Iteration 340, Loss: 1.3498198986053467\n",
      "Epoch 6, Iteration 341, Loss: 1.3559339046478271\n",
      "Epoch 6, Iteration 342, Loss: 1.4237744808197021\n",
      "Epoch 6, Iteration 343, Loss: 1.3745876550674438\n",
      "Epoch 6, Iteration 344, Loss: 1.4811877012252808\n",
      "Epoch 6, Iteration 345, Loss: 1.4941662549972534\n",
      "Epoch 6, Iteration 346, Loss: 1.4697924852371216\n",
      "Epoch 6, Iteration 347, Loss: 1.3302743434906006\n",
      "Epoch 6, Iteration 348, Loss: 1.2765297889709473\n",
      "Epoch 6, Iteration 349, Loss: 1.575301170349121\n",
      "Epoch 6, Iteration 350, Loss: 1.313553810119629\n",
      "Epoch 6, Iteration 350, Valid Loss: 0.8903225660324097\n",
      "Epoch 6, Iteration 351, Loss: 1.5969749689102173\n",
      "Epoch 6, Iteration 352, Loss: 1.271857500076294\n",
      "Epoch 6, Iteration 353, Loss: 1.5514142513275146\n",
      "Epoch 6, Iteration 354, Loss: 1.4968317747116089\n",
      "Epoch 6, Iteration 355, Loss: 1.4342539310455322\n",
      "Epoch 6, Iteration 356, Loss: 1.4962825775146484\n",
      "Epoch 6, Iteration 357, Loss: 1.4084193706512451\n",
      "Epoch 6, Iteration 358, Loss: 1.4336577653884888\n",
      "Epoch 6, Iteration 359, Loss: 1.4181315898895264\n",
      "Epoch 6, Iteration 360, Loss: 1.2694729566574097\n",
      "Epoch 6, Iteration 361, Loss: 1.4546488523483276\n",
      "Epoch 6, Iteration 362, Loss: 1.384979486465454\n",
      "Epoch 6, Iteration 363, Loss: 1.3814640045166016\n",
      "Epoch 6, Iteration 364, Loss: 1.4989975690841675\n",
      "Epoch 6, Iteration 365, Loss: 1.6247286796569824\n",
      "Epoch 6, Iteration 366, Loss: 1.640358805656433\n",
      "Epoch 6, Iteration 367, Loss: 1.6232037544250488\n",
      "Epoch 6, Iteration 368, Loss: 1.378652572631836\n",
      "Epoch 6, Iteration 369, Loss: 1.643567681312561\n",
      "Epoch 6, Iteration 370, Loss: 1.5579239130020142\n",
      "Epoch 6, Iteration 371, Loss: 1.404334545135498\n",
      "Epoch 6, Iteration 372, Loss: 1.2610243558883667\n",
      "Epoch 6, Iteration 373, Loss: 1.3536263704299927\n",
      "Epoch 6, Iteration 374, Loss: 1.7440602779388428\n",
      "Epoch 6, Iteration 375, Loss: 1.4313160181045532\n",
      "Epoch 6, Iteration 376, Loss: 1.5415066480636597\n",
      "Epoch 6, Iteration 377, Loss: 1.5006128549575806\n",
      "Epoch 6, Iteration 378, Loss: 1.3354153633117676\n",
      "Epoch 6, Iteration 379, Loss: 1.3872712850570679\n",
      "Epoch 6, Iteration 380, Loss: 1.334791898727417\n",
      "Epoch 6, Iteration 381, Loss: 1.3750548362731934\n",
      "Epoch 6, Iteration 382, Loss: 1.1354877948760986\n",
      "Epoch 6, Iteration 383, Loss: 1.3043265342712402\n",
      "Epoch 6, Iteration 384, Loss: 1.4615356922149658\n",
      "Epoch 6, Iteration 385, Loss: 1.334808588027954\n",
      "Epoch 6, Iteration 386, Loss: 1.320015788078308\n",
      "Epoch 6, Iteration 387, Loss: 1.8003954887390137\n",
      "Epoch 6, Iteration 388, Loss: 1.408442497253418\n",
      "Epoch 6, Iteration 389, Loss: 1.4810298681259155\n",
      "Epoch 6, Iteration 390, Loss: 1.5447837114334106\n",
      "Epoch 6, Iteration 391, Loss: 1.3616070747375488\n",
      "Epoch 7/10, Loss: 1.552569690407539\n",
      "Epoch 7, Iteration 0, Loss: 1.5364042520523071\n",
      "Epoch 7, Iteration 1, Loss: 1.8069727420806885\n",
      "Epoch 7, Iteration 2, Loss: 1.5839309692382812\n",
      "Epoch 7, Iteration 3, Loss: 1.6996214389801025\n",
      "Epoch 7, Iteration 4, Loss: 1.5201318264007568\n",
      "Epoch 7, Iteration 5, Loss: 1.772413969039917\n",
      "Epoch 7, Iteration 6, Loss: 1.4731824398040771\n",
      "Epoch 7, Iteration 7, Loss: 1.5280439853668213\n",
      "Epoch 7, Iteration 8, Loss: 1.535683274269104\n",
      "Epoch 7, Iteration 9, Loss: 1.834026575088501\n",
      "Epoch 7, Iteration 10, Loss: 1.612262487411499\n",
      "Epoch 7, Iteration 11, Loss: 1.3830910921096802\n",
      "Epoch 7, Iteration 12, Loss: 1.4403996467590332\n",
      "Epoch 7, Iteration 13, Loss: 1.636704683303833\n",
      "Epoch 7, Iteration 14, Loss: 1.522611379623413\n",
      "Epoch 7, Iteration 15, Loss: 1.8805503845214844\n",
      "Epoch 7, Iteration 16, Loss: 1.4223806858062744\n",
      "Epoch 7, Iteration 17, Loss: 1.6550254821777344\n",
      "Epoch 7, Iteration 18, Loss: 1.5299267768859863\n",
      "Epoch 7, Iteration 19, Loss: 1.6456334590911865\n",
      "Epoch 7, Iteration 20, Loss: 1.4915447235107422\n",
      "Epoch 7, Iteration 21, Loss: 1.4573338031768799\n",
      "Epoch 7, Iteration 22, Loss: 1.4851800203323364\n",
      "Epoch 7, Iteration 23, Loss: 1.565545916557312\n",
      "Epoch 7, Iteration 24, Loss: 1.3903381824493408\n",
      "Epoch 7, Iteration 25, Loss: 1.3031795024871826\n",
      "Epoch 7, Iteration 26, Loss: 1.859615445137024\n",
      "Epoch 7, Iteration 27, Loss: 1.4108502864837646\n",
      "Epoch 7, Iteration 28, Loss: 1.5862480401992798\n",
      "Epoch 7, Iteration 29, Loss: 1.6826770305633545\n",
      "Epoch 7, Iteration 30, Loss: 1.524187684059143\n",
      "Epoch 7, Iteration 31, Loss: 1.7087258100509644\n",
      "Epoch 7, Iteration 32, Loss: 1.4362863302230835\n",
      "Epoch 7, Iteration 33, Loss: 1.5465748310089111\n",
      "Epoch 7, Iteration 34, Loss: 1.4138437509536743\n",
      "Epoch 7, Iteration 35, Loss: 1.5235779285430908\n",
      "Epoch 7, Iteration 36, Loss: 1.2702566385269165\n",
      "Epoch 7, Iteration 37, Loss: 1.3949508666992188\n",
      "Epoch 7, Iteration 38, Loss: 1.443190336227417\n",
      "Epoch 7, Iteration 39, Loss: 1.4515658617019653\n",
      "Epoch 7, Iteration 40, Loss: 1.4742549657821655\n",
      "Epoch 7, Iteration 41, Loss: 1.5856633186340332\n",
      "Epoch 7, Iteration 42, Loss: 1.5358591079711914\n",
      "Epoch 7, Iteration 43, Loss: 1.4563450813293457\n",
      "Epoch 7, Iteration 44, Loss: 1.4677205085754395\n",
      "Epoch 7, Iteration 45, Loss: 1.5384495258331299\n",
      "Epoch 7, Iteration 46, Loss: 1.512183427810669\n",
      "Epoch 7, Iteration 47, Loss: 1.5485618114471436\n",
      "Epoch 7, Iteration 48, Loss: 1.6368772983551025\n",
      "Epoch 7, Iteration 49, Loss: 1.505608081817627\n",
      "Epoch 7, Iteration 50, Loss: 1.4159218072891235\n",
      "Epoch 7, Iteration 50, Valid Loss: 0.83391273021698\n",
      "Epoch 7, Iteration 51, Loss: 1.3667972087860107\n",
      "Epoch 7, Iteration 52, Loss: 1.4188916683197021\n",
      "Epoch 7, Iteration 53, Loss: 1.5639739036560059\n",
      "Epoch 7, Iteration 54, Loss: 1.7462739944458008\n",
      "Epoch 7, Iteration 55, Loss: 1.6693553924560547\n",
      "Epoch 7, Iteration 56, Loss: 1.5015265941619873\n",
      "Epoch 7, Iteration 57, Loss: 1.879530668258667\n",
      "Epoch 7, Iteration 58, Loss: 1.4212104082107544\n",
      "Epoch 7, Iteration 59, Loss: 1.5024888515472412\n",
      "Epoch 7, Iteration 60, Loss: 1.5320227146148682\n",
      "Epoch 7, Iteration 61, Loss: 1.1462322473526\n",
      "Epoch 7, Iteration 62, Loss: 1.505871057510376\n",
      "Epoch 7, Iteration 63, Loss: 1.3024036884307861\n",
      "Epoch 7, Iteration 64, Loss: 1.319401502609253\n",
      "Epoch 7, Iteration 65, Loss: 1.6046713590621948\n",
      "Epoch 7, Iteration 66, Loss: 1.4976153373718262\n",
      "Epoch 7, Iteration 67, Loss: 1.1966201066970825\n",
      "Epoch 7, Iteration 68, Loss: 1.6865694522857666\n",
      "Epoch 7, Iteration 69, Loss: 1.695443868637085\n",
      "Epoch 7, Iteration 70, Loss: 1.5221256017684937\n",
      "Epoch 7, Iteration 71, Loss: 1.665388822555542\n",
      "Epoch 7, Iteration 72, Loss: 1.6800636053085327\n",
      "Epoch 7, Iteration 73, Loss: 1.313042163848877\n",
      "Epoch 7, Iteration 74, Loss: 1.7743771076202393\n",
      "Epoch 7, Iteration 75, Loss: 1.4799778461456299\n",
      "Epoch 7, Iteration 76, Loss: 1.400665283203125\n",
      "Epoch 7, Iteration 77, Loss: 1.2386932373046875\n",
      "Epoch 7, Iteration 78, Loss: 1.8199423551559448\n",
      "Epoch 7, Iteration 79, Loss: 1.753056287765503\n",
      "Epoch 7, Iteration 80, Loss: 1.3100736141204834\n",
      "Epoch 7, Iteration 81, Loss: 1.6508173942565918\n",
      "Epoch 7, Iteration 82, Loss: 1.7525875568389893\n",
      "Epoch 7, Iteration 83, Loss: 1.2975974082946777\n",
      "Epoch 7, Iteration 84, Loss: 1.6318672895431519\n",
      "Epoch 7, Iteration 85, Loss: 1.5775004625320435\n",
      "Epoch 7, Iteration 86, Loss: 1.590164303779602\n",
      "Epoch 7, Iteration 87, Loss: 1.4440494775772095\n",
      "Epoch 7, Iteration 88, Loss: 1.5936675071716309\n",
      "Epoch 7, Iteration 89, Loss: 1.4662792682647705\n",
      "Epoch 7, Iteration 90, Loss: 1.173667073249817\n",
      "Epoch 7, Iteration 91, Loss: 1.3596712350845337\n",
      "Epoch 7, Iteration 92, Loss: 1.1211730241775513\n",
      "Epoch 7, Iteration 93, Loss: 1.4592901468276978\n",
      "Epoch 7, Iteration 94, Loss: 2.029233694076538\n",
      "Epoch 7, Iteration 95, Loss: 1.3324891328811646\n",
      "Epoch 7, Iteration 96, Loss: 1.5255156755447388\n",
      "Epoch 7, Iteration 97, Loss: 1.3092734813690186\n",
      "Epoch 7, Iteration 98, Loss: 1.4298205375671387\n",
      "Epoch 7, Iteration 99, Loss: 1.513092279434204\n",
      "Epoch 7, Iteration 100, Loss: 1.3181430101394653\n",
      "Epoch 7, Iteration 100, Valid Loss: 0.8532846570014954\n",
      "Epoch 7, Iteration 101, Loss: 1.8426382541656494\n",
      "Epoch 7, Iteration 102, Loss: 1.5977833271026611\n",
      "Epoch 7, Iteration 103, Loss: 1.4208849668502808\n",
      "Epoch 7, Iteration 104, Loss: 1.8742687702178955\n",
      "Epoch 7, Iteration 105, Loss: 1.65004301071167\n",
      "Epoch 7, Iteration 106, Loss: 1.382920265197754\n",
      "Epoch 7, Iteration 107, Loss: 1.8511983156204224\n",
      "Epoch 7, Iteration 108, Loss: 1.4457041025161743\n",
      "Epoch 7, Iteration 109, Loss: 1.757643461227417\n",
      "Epoch 7, Iteration 110, Loss: 1.7592123746871948\n",
      "Epoch 7, Iteration 111, Loss: 1.6111232042312622\n",
      "Epoch 7, Iteration 112, Loss: 1.512965440750122\n",
      "Epoch 7, Iteration 113, Loss: 1.343505859375\n",
      "Epoch 7, Iteration 114, Loss: 1.8154540061950684\n",
      "Epoch 7, Iteration 115, Loss: 1.6921472549438477\n",
      "Epoch 7, Iteration 116, Loss: 1.4661509990692139\n",
      "Epoch 7, Iteration 117, Loss: 1.1301360130310059\n",
      "Epoch 7, Iteration 118, Loss: 1.562126874923706\n",
      "Epoch 7, Iteration 119, Loss: 1.5060672760009766\n",
      "Epoch 7, Iteration 120, Loss: 1.888845682144165\n",
      "Epoch 7, Iteration 121, Loss: 1.4274879693984985\n",
      "Epoch 7, Iteration 122, Loss: 1.3917781114578247\n",
      "Epoch 7, Iteration 123, Loss: 1.6006063222885132\n",
      "Epoch 7, Iteration 124, Loss: 1.2917358875274658\n",
      "Epoch 7, Iteration 125, Loss: 1.323639988899231\n",
      "Epoch 7, Iteration 126, Loss: 1.2359473705291748\n",
      "Epoch 7, Iteration 127, Loss: 1.6675896644592285\n",
      "Epoch 7, Iteration 128, Loss: 1.6345332860946655\n",
      "Epoch 7, Iteration 129, Loss: 1.5754061937332153\n",
      "Epoch 7, Iteration 130, Loss: 1.8727376461029053\n",
      "Epoch 7, Iteration 131, Loss: 1.3276910781860352\n",
      "Epoch 7, Iteration 132, Loss: 1.3883315324783325\n",
      "Epoch 7, Iteration 133, Loss: 1.956460952758789\n",
      "Epoch 7, Iteration 134, Loss: 1.3473459482192993\n",
      "Epoch 7, Iteration 135, Loss: 1.663020372390747\n",
      "Epoch 7, Iteration 136, Loss: 1.7486552000045776\n",
      "Epoch 7, Iteration 137, Loss: 1.4985401630401611\n",
      "Epoch 7, Iteration 138, Loss: 1.4357879161834717\n",
      "Epoch 7, Iteration 139, Loss: 1.3256046772003174\n",
      "Epoch 7, Iteration 140, Loss: 1.4099270105361938\n",
      "Epoch 7, Iteration 141, Loss: 1.7452785968780518\n",
      "Epoch 7, Iteration 142, Loss: 1.530281662940979\n",
      "Epoch 7, Iteration 143, Loss: 1.5285375118255615\n",
      "Epoch 7, Iteration 144, Loss: 1.3158942461013794\n",
      "Epoch 7, Iteration 145, Loss: 1.480312466621399\n",
      "Epoch 7, Iteration 146, Loss: 1.6985737085342407\n",
      "Epoch 7, Iteration 147, Loss: 1.5178812742233276\n",
      "Epoch 7, Iteration 148, Loss: 1.1991907358169556\n",
      "Epoch 7, Iteration 149, Loss: 1.423799753189087\n",
      "Epoch 7, Iteration 150, Loss: 1.620507001876831\n",
      "Epoch 7, Iteration 150, Valid Loss: 0.860694944858551\n",
      "Epoch 7, Iteration 151, Loss: 1.539823055267334\n",
      "Epoch 7, Iteration 152, Loss: 1.4677436351776123\n",
      "Epoch 7, Iteration 153, Loss: 1.44652259349823\n",
      "Epoch 7, Iteration 154, Loss: 1.3046598434448242\n",
      "Epoch 7, Iteration 155, Loss: 1.33828866481781\n",
      "Epoch 7, Iteration 156, Loss: 1.5588719844818115\n",
      "Epoch 7, Iteration 157, Loss: 2.223271131515503\n",
      "Epoch 7, Iteration 158, Loss: 1.515028476715088\n",
      "Epoch 7, Iteration 159, Loss: 1.6467763185501099\n",
      "Epoch 7, Iteration 160, Loss: 1.3063749074935913\n",
      "Epoch 7, Iteration 161, Loss: 1.4073184728622437\n",
      "Epoch 7, Iteration 162, Loss: 1.8064128160476685\n",
      "Epoch 7, Iteration 163, Loss: 1.1900001764297485\n",
      "Epoch 7, Iteration 164, Loss: 1.345703125\n",
      "Epoch 7, Iteration 165, Loss: 1.2959198951721191\n",
      "Epoch 7, Iteration 166, Loss: 1.3821094036102295\n",
      "Epoch 7, Iteration 167, Loss: 1.6840463876724243\n",
      "Epoch 7, Iteration 168, Loss: 1.5880907773971558\n",
      "Epoch 7, Iteration 169, Loss: 1.5326006412506104\n",
      "Epoch 7, Iteration 170, Loss: 1.719942569732666\n",
      "Epoch 7, Iteration 171, Loss: 1.661402702331543\n",
      "Epoch 7, Iteration 172, Loss: 1.471421241760254\n",
      "Epoch 7, Iteration 173, Loss: 1.4497238397598267\n",
      "Epoch 7, Iteration 174, Loss: 1.528917908668518\n",
      "Epoch 7, Iteration 175, Loss: 1.7608438730239868\n",
      "Epoch 7, Iteration 176, Loss: 1.211942434310913\n",
      "Epoch 7, Iteration 177, Loss: 1.931076169013977\n",
      "Epoch 7, Iteration 178, Loss: 1.7882360219955444\n",
      "Epoch 7, Iteration 179, Loss: 1.4212496280670166\n",
      "Epoch 7, Iteration 180, Loss: 1.4209967851638794\n",
      "Epoch 7, Iteration 181, Loss: 1.7749449014663696\n",
      "Epoch 7, Iteration 182, Loss: 1.723534345626831\n",
      "Epoch 7, Iteration 183, Loss: 1.3300647735595703\n",
      "Epoch 7, Iteration 184, Loss: 1.4241092205047607\n",
      "Epoch 7, Iteration 185, Loss: 1.297806739807129\n",
      "Epoch 7, Iteration 186, Loss: 1.4223074913024902\n",
      "Epoch 7, Iteration 187, Loss: 1.4744025468826294\n",
      "Epoch 7, Iteration 188, Loss: 1.367459774017334\n",
      "Epoch 7, Iteration 189, Loss: 1.4432966709136963\n",
      "Epoch 7, Iteration 190, Loss: 1.6838912963867188\n",
      "Epoch 7, Iteration 191, Loss: 1.6596890687942505\n",
      "Epoch 7, Iteration 192, Loss: 1.5636622905731201\n",
      "Epoch 7, Iteration 193, Loss: 1.4855843782424927\n",
      "Epoch 7, Iteration 194, Loss: 1.41678786277771\n",
      "Epoch 7, Iteration 195, Loss: 1.3968698978424072\n",
      "Epoch 7, Iteration 196, Loss: 1.5871665477752686\n",
      "Epoch 7, Iteration 197, Loss: 1.4205816984176636\n",
      "Epoch 7, Iteration 198, Loss: 1.5193690061569214\n",
      "Epoch 7, Iteration 199, Loss: 1.5808026790618896\n",
      "Epoch 7, Iteration 200, Loss: 1.3629164695739746\n",
      "Epoch 7, Iteration 200, Valid Loss: 0.8607769012451172\n",
      "Epoch 7, Iteration 201, Loss: 1.3785291910171509\n",
      "Epoch 7, Iteration 202, Loss: 1.5226616859436035\n",
      "Epoch 7, Iteration 203, Loss: 1.3506578207015991\n",
      "Epoch 7, Iteration 204, Loss: 1.392716407775879\n",
      "Epoch 7, Iteration 205, Loss: 1.1865379810333252\n",
      "Epoch 7, Iteration 206, Loss: 1.3191202878952026\n",
      "Epoch 7, Iteration 207, Loss: 1.752992033958435\n",
      "Epoch 7, Iteration 208, Loss: 2.0793964862823486\n",
      "Epoch 7, Iteration 209, Loss: 2.217776298522949\n",
      "Epoch 7, Iteration 210, Loss: 1.900684118270874\n",
      "Epoch 7, Iteration 211, Loss: 1.985004186630249\n",
      "Epoch 7, Iteration 212, Loss: 1.7656002044677734\n",
      "Epoch 7, Iteration 213, Loss: 1.8681000471115112\n",
      "Epoch 7, Iteration 214, Loss: 1.5166524648666382\n",
      "Epoch 7, Iteration 215, Loss: 1.3472480773925781\n",
      "Epoch 7, Iteration 216, Loss: 1.3077459335327148\n",
      "Epoch 7, Iteration 217, Loss: 1.4729052782058716\n",
      "Epoch 7, Iteration 218, Loss: 1.5044605731964111\n",
      "Epoch 7, Iteration 219, Loss: 1.6277565956115723\n",
      "Epoch 7, Iteration 220, Loss: 1.38688325881958\n",
      "Epoch 7, Iteration 221, Loss: 1.562766432762146\n",
      "Epoch 7, Iteration 222, Loss: 1.5341863632202148\n",
      "Epoch 7, Iteration 223, Loss: 1.4911425113677979\n",
      "Epoch 7, Iteration 224, Loss: 1.361806035041809\n",
      "Epoch 7, Iteration 225, Loss: 1.619752287864685\n",
      "Epoch 7, Iteration 226, Loss: 1.7849459648132324\n",
      "Epoch 7, Iteration 227, Loss: 1.4326069355010986\n",
      "Epoch 7, Iteration 228, Loss: 1.4383165836334229\n",
      "Epoch 7, Iteration 229, Loss: 1.4270853996276855\n",
      "Epoch 7, Iteration 230, Loss: 1.631999135017395\n",
      "Epoch 7, Iteration 231, Loss: 1.510425329208374\n",
      "Epoch 7, Iteration 232, Loss: 1.6339576244354248\n",
      "Epoch 7, Iteration 233, Loss: 1.9589160680770874\n",
      "Epoch 7, Iteration 234, Loss: 1.6949964761734009\n",
      "Epoch 7, Iteration 235, Loss: 1.4066003561019897\n",
      "Epoch 7, Iteration 236, Loss: 1.6314820051193237\n",
      "Epoch 7, Iteration 237, Loss: 1.4407709836959839\n",
      "Epoch 7, Iteration 238, Loss: 1.3717234134674072\n",
      "Epoch 7, Iteration 239, Loss: 1.7211480140686035\n",
      "Epoch 7, Iteration 240, Loss: 1.2572238445281982\n",
      "Epoch 7, Iteration 241, Loss: 1.3879342079162598\n",
      "Epoch 7, Iteration 242, Loss: 1.433213233947754\n",
      "Epoch 7, Iteration 243, Loss: 1.499159336090088\n",
      "Epoch 7, Iteration 244, Loss: 1.3115283250808716\n",
      "Epoch 7, Iteration 245, Loss: 1.2269222736358643\n",
      "Epoch 7, Iteration 246, Loss: 1.6066133975982666\n",
      "Epoch 7, Iteration 247, Loss: 1.425289511680603\n",
      "Epoch 7, Iteration 248, Loss: 1.4713006019592285\n",
      "Epoch 7, Iteration 249, Loss: 1.4296848773956299\n",
      "Epoch 7, Iteration 250, Loss: 1.5912766456604004\n",
      "Epoch 7, Iteration 250, Valid Loss: 0.8594619035720825\n",
      "Epoch 7, Iteration 251, Loss: 1.4305832386016846\n",
      "Epoch 7, Iteration 252, Loss: 1.733934998512268\n",
      "Epoch 7, Iteration 253, Loss: 1.630379557609558\n",
      "Epoch 7, Iteration 254, Loss: 1.6707360744476318\n",
      "Epoch 7, Iteration 255, Loss: 1.640630841255188\n",
      "Epoch 7, Iteration 256, Loss: 1.721746802330017\n",
      "Epoch 7, Iteration 257, Loss: 1.552176833152771\n",
      "Epoch 7, Iteration 258, Loss: 1.7994579076766968\n",
      "Epoch 7, Iteration 259, Loss: 1.276687741279602\n",
      "Epoch 7, Iteration 260, Loss: 1.6748861074447632\n",
      "Epoch 7, Iteration 261, Loss: 1.6760450601577759\n",
      "Epoch 7, Iteration 262, Loss: 1.3635364770889282\n",
      "Epoch 7, Iteration 263, Loss: 1.7043724060058594\n",
      "Epoch 7, Iteration 264, Loss: 1.5922389030456543\n",
      "Epoch 7, Iteration 265, Loss: 1.5649546384811401\n",
      "Epoch 7, Iteration 266, Loss: 1.6091581583023071\n",
      "Epoch 7, Iteration 267, Loss: 1.5414801836013794\n",
      "Epoch 7, Iteration 268, Loss: 1.4567064046859741\n",
      "Epoch 7, Iteration 269, Loss: 1.480970025062561\n",
      "Epoch 7, Iteration 270, Loss: 1.5141252279281616\n",
      "Epoch 7, Iteration 271, Loss: 1.5396217107772827\n",
      "Epoch 7, Iteration 272, Loss: 1.340606451034546\n",
      "Epoch 7, Iteration 273, Loss: 1.430370807647705\n",
      "Epoch 7, Iteration 274, Loss: 1.3962594270706177\n",
      "Epoch 7, Iteration 275, Loss: 1.4324934482574463\n",
      "Epoch 7, Iteration 276, Loss: 1.4026076793670654\n",
      "Epoch 7, Iteration 277, Loss: 1.4414656162261963\n",
      "Epoch 7, Iteration 278, Loss: 1.628364086151123\n",
      "Epoch 7, Iteration 279, Loss: 1.315476655960083\n",
      "Epoch 7, Iteration 280, Loss: 1.121764063835144\n",
      "Epoch 7, Iteration 281, Loss: 1.3478070497512817\n",
      "Epoch 7, Iteration 282, Loss: 1.4567859172821045\n",
      "Epoch 7, Iteration 283, Loss: 1.482600212097168\n",
      "Epoch 7, Iteration 284, Loss: 1.5350918769836426\n",
      "Epoch 7, Iteration 285, Loss: 1.4178235530853271\n",
      "Epoch 7, Iteration 286, Loss: 1.4640103578567505\n",
      "Epoch 7, Iteration 287, Loss: 1.48512601852417\n",
      "Epoch 7, Iteration 288, Loss: 1.2622572183609009\n",
      "Epoch 7, Iteration 289, Loss: 1.4218385219573975\n",
      "Epoch 7, Iteration 290, Loss: 1.3407418727874756\n",
      "Epoch 7, Iteration 291, Loss: 1.1485275030136108\n",
      "Epoch 7, Iteration 292, Loss: 1.6877105236053467\n",
      "Epoch 7, Iteration 293, Loss: 1.778075933456421\n",
      "Epoch 7, Iteration 294, Loss: 1.3780243396759033\n",
      "Epoch 7, Iteration 295, Loss: 1.3348298072814941\n",
      "Epoch 7, Iteration 296, Loss: 1.7831591367721558\n",
      "Epoch 7, Iteration 297, Loss: 1.6726912260055542\n",
      "Epoch 7, Iteration 298, Loss: 1.4546699523925781\n",
      "Epoch 7, Iteration 299, Loss: 1.3145053386688232\n",
      "Epoch 7, Iteration 300, Loss: 1.2116385698318481\n",
      "Epoch 7, Iteration 300, Valid Loss: 0.8372480869293213\n",
      "Epoch 7, Iteration 301, Loss: 1.5077885389328003\n",
      "Epoch 7, Iteration 302, Loss: 1.3403589725494385\n",
      "Epoch 7, Iteration 303, Loss: 1.757131814956665\n",
      "Epoch 7, Iteration 304, Loss: 1.400667428970337\n",
      "Epoch 7, Iteration 305, Loss: 1.2768350839614868\n",
      "Epoch 7, Iteration 306, Loss: 1.3736684322357178\n",
      "Epoch 7, Iteration 307, Loss: 1.3842341899871826\n",
      "Epoch 7, Iteration 308, Loss: 1.4099645614624023\n",
      "Epoch 7, Iteration 309, Loss: 1.369411587715149\n",
      "Epoch 7, Iteration 310, Loss: 1.6874823570251465\n",
      "Epoch 7, Iteration 311, Loss: 1.2992111444473267\n",
      "Epoch 7, Iteration 312, Loss: 1.2526355981826782\n",
      "Epoch 7, Iteration 313, Loss: 1.6189556121826172\n",
      "Epoch 7, Iteration 314, Loss: 1.2962532043457031\n",
      "Epoch 7, Iteration 315, Loss: 1.2348759174346924\n",
      "Epoch 7, Iteration 316, Loss: 1.4935319423675537\n",
      "Epoch 7, Iteration 317, Loss: 1.4165706634521484\n",
      "Epoch 7, Iteration 318, Loss: 1.4370167255401611\n",
      "Epoch 7, Iteration 319, Loss: 1.6696996688842773\n",
      "Epoch 7, Iteration 320, Loss: 1.2322158813476562\n",
      "Epoch 7, Iteration 321, Loss: 1.4784128665924072\n",
      "Epoch 7, Iteration 322, Loss: 1.2901453971862793\n",
      "Epoch 7, Iteration 323, Loss: 1.3501096963882446\n",
      "Epoch 7, Iteration 324, Loss: 1.4763202667236328\n",
      "Epoch 7, Iteration 325, Loss: 1.358790397644043\n",
      "Epoch 7, Iteration 326, Loss: 1.41684889793396\n",
      "Epoch 7, Iteration 327, Loss: 1.3331732749938965\n",
      "Epoch 7, Iteration 328, Loss: 1.2312146425247192\n",
      "Epoch 7, Iteration 329, Loss: 1.5873734951019287\n",
      "Epoch 7, Iteration 330, Loss: 1.2047247886657715\n",
      "Epoch 7, Iteration 331, Loss: 1.3280105590820312\n",
      "Epoch 7, Iteration 332, Loss: 1.5391995906829834\n",
      "Epoch 7, Iteration 333, Loss: 1.7032504081726074\n",
      "Epoch 7, Iteration 334, Loss: 1.5606976747512817\n",
      "Epoch 7, Iteration 335, Loss: 1.6208271980285645\n",
      "Epoch 7, Iteration 336, Loss: 1.4094313383102417\n",
      "Epoch 7, Iteration 337, Loss: 1.3146817684173584\n",
      "Epoch 7, Iteration 338, Loss: 1.3967971801757812\n",
      "Epoch 7, Iteration 339, Loss: 1.3928180932998657\n",
      "Epoch 7, Iteration 340, Loss: 1.2907458543777466\n",
      "Epoch 7, Iteration 341, Loss: 1.379958152770996\n",
      "Epoch 7, Iteration 342, Loss: 1.2487354278564453\n",
      "Epoch 7, Iteration 343, Loss: 1.299283504486084\n",
      "Epoch 7, Iteration 344, Loss: 1.481992244720459\n",
      "Epoch 7, Iteration 345, Loss: 1.5135042667388916\n",
      "Epoch 7, Iteration 346, Loss: 1.3945543766021729\n",
      "Epoch 7, Iteration 347, Loss: 1.2520854473114014\n",
      "Epoch 7, Iteration 348, Loss: 1.250410795211792\n",
      "Epoch 7, Iteration 349, Loss: 1.3433477878570557\n",
      "Epoch 7, Iteration 350, Loss: 1.293210506439209\n",
      "Epoch 7, Iteration 350, Valid Loss: 0.8525695204734802\n",
      "Epoch 7, Iteration 351, Loss: 1.648637294769287\n",
      "Epoch 7, Iteration 352, Loss: 1.2877535820007324\n",
      "Epoch 7, Iteration 353, Loss: 1.515759825706482\n",
      "Epoch 7, Iteration 354, Loss: 1.234400987625122\n",
      "Epoch 7, Iteration 355, Loss: 1.3510807752609253\n",
      "Epoch 7, Iteration 356, Loss: 1.390845537185669\n",
      "Epoch 7, Iteration 357, Loss: 1.3133699893951416\n",
      "Epoch 7, Iteration 358, Loss: 1.3347254991531372\n",
      "Epoch 7, Iteration 359, Loss: 1.3623448610305786\n",
      "Epoch 7, Iteration 360, Loss: 1.3388794660568237\n",
      "Epoch 7, Iteration 361, Loss: 1.3688161373138428\n",
      "Epoch 7, Iteration 362, Loss: 1.3684812784194946\n",
      "Epoch 7, Iteration 363, Loss: 1.325979232788086\n",
      "Epoch 7, Iteration 364, Loss: 1.352188229560852\n",
      "Epoch 7, Iteration 365, Loss: 1.5552858114242554\n",
      "Epoch 7, Iteration 366, Loss: 1.5967679023742676\n",
      "Epoch 7, Iteration 367, Loss: 1.4464483261108398\n",
      "Epoch 7, Iteration 368, Loss: 1.4300975799560547\n",
      "Epoch 7, Iteration 369, Loss: 1.49965500831604\n",
      "Epoch 7, Iteration 370, Loss: 1.4779882431030273\n",
      "Epoch 7, Iteration 371, Loss: 1.4459489583969116\n",
      "Epoch 7, Iteration 372, Loss: 1.1498998403549194\n",
      "Epoch 7, Iteration 373, Loss: 1.2484537363052368\n",
      "Epoch 7, Iteration 374, Loss: 1.6576699018478394\n",
      "Epoch 7, Iteration 375, Loss: 1.3904669284820557\n",
      "Epoch 7, Iteration 376, Loss: 1.477550983428955\n",
      "Epoch 7, Iteration 377, Loss: 1.4823521375656128\n",
      "Epoch 7, Iteration 378, Loss: 1.2617837190628052\n",
      "Epoch 7, Iteration 379, Loss: 1.333933711051941\n",
      "Epoch 7, Iteration 380, Loss: 1.276319980621338\n",
      "Epoch 7, Iteration 381, Loss: 1.2719659805297852\n",
      "Epoch 7, Iteration 382, Loss: 1.0900455713272095\n",
      "Epoch 7, Iteration 383, Loss: 1.3700652122497559\n",
      "Epoch 7, Iteration 384, Loss: 1.3247740268707275\n",
      "Epoch 7, Iteration 385, Loss: 1.390204668045044\n",
      "Epoch 7, Iteration 386, Loss: 1.3264415264129639\n",
      "Epoch 7, Iteration 387, Loss: 1.6846771240234375\n",
      "Epoch 7, Iteration 388, Loss: 1.302365779876709\n",
      "Epoch 7, Iteration 389, Loss: 1.4911448955535889\n",
      "Epoch 7, Iteration 390, Loss: 1.5014781951904297\n",
      "Epoch 7, Iteration 391, Loss: 1.366663932800293\n",
      "Epoch 8/10, Loss: 1.4948714831653906\n",
      "Epoch 8, Iteration 0, Loss: 1.4393656253814697\n",
      "Epoch 8, Iteration 1, Loss: 1.749307632446289\n",
      "Epoch 8, Iteration 2, Loss: 1.619559407234192\n",
      "Epoch 8, Iteration 3, Loss: 1.6098167896270752\n",
      "Epoch 8, Iteration 4, Loss: 1.445092797279358\n",
      "Epoch 8, Iteration 5, Loss: 1.6445002555847168\n",
      "Epoch 8, Iteration 6, Loss: 1.3997108936309814\n",
      "Epoch 8, Iteration 7, Loss: 1.4300384521484375\n",
      "Epoch 8, Iteration 8, Loss: 1.4215648174285889\n",
      "Epoch 8, Iteration 9, Loss: 1.7518935203552246\n",
      "Epoch 8, Iteration 10, Loss: 1.5694869756698608\n",
      "Epoch 8, Iteration 11, Loss: 1.3507561683654785\n",
      "Epoch 8, Iteration 12, Loss: 1.3842344284057617\n",
      "Epoch 8, Iteration 13, Loss: 1.6089829206466675\n",
      "Epoch 8, Iteration 14, Loss: 1.4212747812271118\n",
      "Epoch 8, Iteration 15, Loss: 1.5174354314804077\n",
      "Epoch 8, Iteration 16, Loss: 1.4856479167938232\n",
      "Epoch 8, Iteration 17, Loss: 1.6202976703643799\n",
      "Epoch 8, Iteration 18, Loss: 1.5395195484161377\n",
      "Epoch 8, Iteration 19, Loss: 1.587661862373352\n",
      "Epoch 8, Iteration 20, Loss: 1.390952706336975\n",
      "Epoch 8, Iteration 21, Loss: 1.4542813301086426\n",
      "Epoch 8, Iteration 22, Loss: 1.4128124713897705\n",
      "Epoch 8, Iteration 23, Loss: 1.4754559993743896\n",
      "Epoch 8, Iteration 24, Loss: 1.3225899934768677\n",
      "Epoch 8, Iteration 25, Loss: 1.2508559226989746\n",
      "Epoch 8, Iteration 26, Loss: 1.735918402671814\n",
      "Epoch 8, Iteration 27, Loss: 1.4011512994766235\n",
      "Epoch 8, Iteration 28, Loss: 1.4844341278076172\n",
      "Epoch 8, Iteration 29, Loss: 1.7016100883483887\n",
      "Epoch 8, Iteration 30, Loss: 1.4806609153747559\n",
      "Epoch 8, Iteration 31, Loss: 1.5237555503845215\n",
      "Epoch 8, Iteration 32, Loss: 1.4608161449432373\n",
      "Epoch 8, Iteration 33, Loss: 1.4671906232833862\n",
      "Epoch 8, Iteration 34, Loss: 1.312432050704956\n",
      "Epoch 8, Iteration 35, Loss: 1.5441200733184814\n",
      "Epoch 8, Iteration 36, Loss: 1.2542717456817627\n",
      "Epoch 8, Iteration 37, Loss: 1.3586506843566895\n",
      "Epoch 8, Iteration 38, Loss: 1.4289895296096802\n",
      "Epoch 8, Iteration 39, Loss: 1.377084732055664\n",
      "Epoch 8, Iteration 40, Loss: 1.4122065305709839\n",
      "Epoch 8, Iteration 41, Loss: 1.5219085216522217\n",
      "Epoch 8, Iteration 42, Loss: 1.503842830657959\n",
      "Epoch 8, Iteration 43, Loss: 1.3867956399917603\n",
      "Epoch 8, Iteration 44, Loss: 1.4417328834533691\n",
      "Epoch 8, Iteration 45, Loss: 1.527442216873169\n",
      "Epoch 8, Iteration 46, Loss: 1.4141278266906738\n",
      "Epoch 8, Iteration 47, Loss: 1.4786288738250732\n",
      "Epoch 8, Iteration 48, Loss: 1.70689058303833\n",
      "Epoch 8, Iteration 49, Loss: 1.4417004585266113\n",
      "Epoch 8, Iteration 50, Loss: 1.3697521686553955\n",
      "Epoch 8, Iteration 50, Valid Loss: 0.8142073154449463\n",
      "Epoch 8, Iteration 51, Loss: 1.327920913696289\n",
      "Epoch 8, Iteration 52, Loss: 1.3846054077148438\n",
      "Epoch 8, Iteration 53, Loss: 1.4683520793914795\n",
      "Epoch 8, Iteration 54, Loss: 1.905142903327942\n",
      "Epoch 8, Iteration 55, Loss: 1.559916377067566\n",
      "Epoch 8, Iteration 56, Loss: 1.4018763303756714\n",
      "Epoch 8, Iteration 57, Loss: 1.703188419342041\n",
      "Epoch 8, Iteration 58, Loss: 1.3453912734985352\n",
      "Epoch 8, Iteration 59, Loss: 1.4615615606307983\n",
      "Epoch 8, Iteration 60, Loss: 1.4819310903549194\n",
      "Epoch 8, Iteration 61, Loss: 1.2399073839187622\n",
      "Epoch 8, Iteration 62, Loss: 1.4502267837524414\n",
      "Epoch 8, Iteration 63, Loss: 1.190110683441162\n",
      "Epoch 8, Iteration 64, Loss: 1.428169846534729\n",
      "Epoch 8, Iteration 65, Loss: 1.52140212059021\n",
      "Epoch 8, Iteration 66, Loss: 1.489134430885315\n",
      "Epoch 8, Iteration 67, Loss: 1.1362155675888062\n",
      "Epoch 8, Iteration 68, Loss: 1.7221672534942627\n",
      "Epoch 8, Iteration 69, Loss: 1.6757407188415527\n",
      "Epoch 8, Iteration 70, Loss: 1.642999529838562\n",
      "Epoch 8, Iteration 71, Loss: 1.3782751560211182\n",
      "Epoch 8, Iteration 72, Loss: 1.5857903957366943\n",
      "Epoch 8, Iteration 73, Loss: 1.2198508977890015\n",
      "Epoch 8, Iteration 74, Loss: 1.5751879215240479\n",
      "Epoch 8, Iteration 75, Loss: 1.506572961807251\n",
      "Epoch 8, Iteration 76, Loss: 1.3940833806991577\n",
      "Epoch 8, Iteration 77, Loss: 1.250809907913208\n",
      "Epoch 8, Iteration 78, Loss: 1.683335781097412\n",
      "Epoch 8, Iteration 79, Loss: 1.764510154724121\n",
      "Epoch 8, Iteration 80, Loss: 1.2985246181488037\n",
      "Epoch 8, Iteration 81, Loss: 1.7511258125305176\n",
      "Epoch 8, Iteration 82, Loss: 1.4603161811828613\n",
      "Epoch 8, Iteration 83, Loss: 1.2765015363693237\n",
      "Epoch 8, Iteration 84, Loss: 1.6057634353637695\n",
      "Epoch 8, Iteration 85, Loss: 1.4768340587615967\n",
      "Epoch 8, Iteration 86, Loss: 1.4635337591171265\n",
      "Epoch 8, Iteration 87, Loss: 1.4716765880584717\n",
      "Epoch 8, Iteration 88, Loss: 1.5998427867889404\n",
      "Epoch 8, Iteration 89, Loss: 1.4176008701324463\n",
      "Epoch 8, Iteration 90, Loss: 1.124244213104248\n",
      "Epoch 8, Iteration 91, Loss: 1.5466810464859009\n",
      "Epoch 8, Iteration 92, Loss: 1.1251397132873535\n",
      "Epoch 8, Iteration 93, Loss: 1.5199849605560303\n",
      "Epoch 8, Iteration 94, Loss: 1.712134838104248\n",
      "Epoch 8, Iteration 95, Loss: 1.3028590679168701\n",
      "Epoch 8, Iteration 96, Loss: 1.4864552021026611\n",
      "Epoch 8, Iteration 97, Loss: 1.285569190979004\n",
      "Epoch 8, Iteration 98, Loss: 1.3698313236236572\n",
      "Epoch 8, Iteration 99, Loss: 1.3162766695022583\n",
      "Epoch 8, Iteration 100, Loss: 1.255401372909546\n",
      "Epoch 8, Iteration 100, Valid Loss: 0.8328921794891357\n",
      "Epoch 8, Iteration 101, Loss: 1.9012844562530518\n",
      "Epoch 8, Iteration 102, Loss: 1.4711846113204956\n",
      "Epoch 8, Iteration 103, Loss: 1.54902982711792\n",
      "Epoch 8, Iteration 104, Loss: 1.6159932613372803\n",
      "Epoch 8, Iteration 105, Loss: 1.4966495037078857\n",
      "Epoch 8, Iteration 106, Loss: 1.4268882274627686\n",
      "Epoch 8, Iteration 107, Loss: 2.1595935821533203\n",
      "Epoch 8, Iteration 108, Loss: 1.4506843090057373\n",
      "Epoch 8, Iteration 109, Loss: 1.6761221885681152\n",
      "Epoch 8, Iteration 110, Loss: 1.7237591743469238\n",
      "Epoch 8, Iteration 111, Loss: 1.6180825233459473\n",
      "Epoch 8, Iteration 112, Loss: 1.5635175704956055\n",
      "Epoch 8, Iteration 113, Loss: 1.3622355461120605\n",
      "Epoch 8, Iteration 114, Loss: 1.7846330404281616\n",
      "Epoch 8, Iteration 115, Loss: 1.729926586151123\n",
      "Epoch 8, Iteration 116, Loss: 1.4229888916015625\n",
      "Epoch 8, Iteration 117, Loss: 1.119889259338379\n",
      "Epoch 8, Iteration 118, Loss: 1.4335529804229736\n",
      "Epoch 8, Iteration 119, Loss: 1.5178775787353516\n",
      "Epoch 8, Iteration 120, Loss: 1.423566460609436\n",
      "Epoch 8, Iteration 121, Loss: 1.3452779054641724\n",
      "Epoch 8, Iteration 122, Loss: 1.3426618576049805\n",
      "Epoch 8, Iteration 123, Loss: 1.3955308198928833\n",
      "Epoch 8, Iteration 124, Loss: 1.3595011234283447\n",
      "Epoch 8, Iteration 125, Loss: 1.2509636878967285\n",
      "Epoch 8, Iteration 126, Loss: 1.2911298274993896\n",
      "Epoch 8, Iteration 127, Loss: 1.4781854152679443\n",
      "Epoch 8, Iteration 128, Loss: 1.6847320795059204\n",
      "Epoch 8, Iteration 129, Loss: 1.6362817287445068\n",
      "Epoch 8, Iteration 130, Loss: 1.9033818244934082\n",
      "Epoch 8, Iteration 131, Loss: 1.2914756536483765\n",
      "Epoch 8, Iteration 132, Loss: 1.3270022869110107\n",
      "Epoch 8, Iteration 133, Loss: 1.7974798679351807\n",
      "Epoch 8, Iteration 134, Loss: 1.2722746133804321\n",
      "Epoch 8, Iteration 135, Loss: 1.3472386598587036\n",
      "Epoch 8, Iteration 136, Loss: 1.3197059631347656\n",
      "Epoch 8, Iteration 137, Loss: 1.3179067373275757\n",
      "Epoch 8, Iteration 138, Loss: 1.342419981956482\n",
      "Epoch 8, Iteration 139, Loss: 1.3281686305999756\n",
      "Epoch 8, Iteration 140, Loss: 1.339903712272644\n",
      "Epoch 8, Iteration 141, Loss: 1.7370318174362183\n",
      "Epoch 8, Iteration 142, Loss: 1.3032565116882324\n",
      "Epoch 8, Iteration 143, Loss: 1.492177963256836\n",
      "Epoch 8, Iteration 144, Loss: 1.4045606851577759\n",
      "Epoch 8, Iteration 145, Loss: 1.5269674062728882\n",
      "Epoch 8, Iteration 146, Loss: 1.5436789989471436\n",
      "Epoch 8, Iteration 147, Loss: 1.4678387641906738\n",
      "Epoch 8, Iteration 148, Loss: 1.1284492015838623\n",
      "Epoch 8, Iteration 149, Loss: 1.3140754699707031\n",
      "Epoch 8, Iteration 150, Loss: 1.420262336730957\n",
      "Epoch 8, Iteration 150, Valid Loss: 0.8365446925163269\n",
      "Epoch 8, Iteration 151, Loss: 1.5658708810806274\n",
      "Epoch 8, Iteration 152, Loss: 1.3772087097167969\n",
      "Epoch 8, Iteration 153, Loss: 1.6380558013916016\n",
      "Epoch 8, Iteration 154, Loss: 1.1398038864135742\n",
      "Epoch 8, Iteration 155, Loss: 1.3853527307510376\n",
      "Epoch 8, Iteration 156, Loss: 1.3150633573532104\n",
      "Epoch 8, Iteration 157, Loss: 1.999753475189209\n",
      "Epoch 8, Iteration 158, Loss: 1.502272367477417\n",
      "Epoch 8, Iteration 159, Loss: 1.5365874767303467\n",
      "Epoch 8, Iteration 160, Loss: 1.4063152074813843\n",
      "Epoch 8, Iteration 161, Loss: 1.473973035812378\n",
      "Epoch 8, Iteration 162, Loss: 1.760183334350586\n",
      "Epoch 8, Iteration 163, Loss: 1.1503713130950928\n",
      "Epoch 8, Iteration 164, Loss: 1.4243618249893188\n",
      "Epoch 8, Iteration 165, Loss: 1.3183990716934204\n",
      "Epoch 8, Iteration 166, Loss: 2.1079976558685303\n",
      "Epoch 8, Iteration 167, Loss: 1.6974807977676392\n",
      "Epoch 8, Iteration 168, Loss: 1.5609649419784546\n",
      "Epoch 8, Iteration 169, Loss: 1.3858206272125244\n",
      "Epoch 8, Iteration 170, Loss: 1.716569185256958\n",
      "Epoch 8, Iteration 171, Loss: 1.4928340911865234\n",
      "Epoch 8, Iteration 172, Loss: 1.4374504089355469\n",
      "Epoch 8, Iteration 173, Loss: 1.3223860263824463\n",
      "Epoch 8, Iteration 174, Loss: 1.5515375137329102\n",
      "Epoch 8, Iteration 175, Loss: 1.6635844707489014\n",
      "Epoch 8, Iteration 176, Loss: 1.185982346534729\n",
      "Epoch 8, Iteration 177, Loss: 1.9051772356033325\n",
      "Epoch 8, Iteration 178, Loss: 1.6359890699386597\n",
      "Epoch 8, Iteration 179, Loss: 1.4315474033355713\n",
      "Epoch 8, Iteration 180, Loss: 1.4180126190185547\n",
      "Epoch 8, Iteration 181, Loss: 1.6520657539367676\n",
      "Epoch 8, Iteration 182, Loss: 1.589085578918457\n",
      "Epoch 8, Iteration 183, Loss: 1.2963566780090332\n",
      "Epoch 8, Iteration 184, Loss: 1.3298134803771973\n",
      "Epoch 8, Iteration 185, Loss: 1.235674262046814\n",
      "Epoch 8, Iteration 186, Loss: 1.3702569007873535\n",
      "Epoch 8, Iteration 187, Loss: 1.3678654432296753\n",
      "Epoch 8, Iteration 188, Loss: 1.33292818069458\n",
      "Epoch 8, Iteration 189, Loss: 1.3165028095245361\n",
      "Epoch 8, Iteration 190, Loss: 1.6609970331192017\n",
      "Epoch 8, Iteration 191, Loss: 1.4856085777282715\n",
      "Epoch 8, Iteration 192, Loss: 1.510599136352539\n",
      "Epoch 8, Iteration 193, Loss: 1.4616944789886475\n",
      "Epoch 8, Iteration 194, Loss: 1.4290618896484375\n",
      "Epoch 8, Iteration 195, Loss: 1.382369875907898\n",
      "Epoch 8, Iteration 196, Loss: 1.598785400390625\n",
      "Epoch 8, Iteration 197, Loss: 1.365872859954834\n",
      "Epoch 8, Iteration 198, Loss: 1.5219300985336304\n",
      "Epoch 8, Iteration 199, Loss: 1.4938350915908813\n",
      "Epoch 8, Iteration 200, Loss: 1.356231927871704\n",
      "Epoch 8, Iteration 200, Valid Loss: 0.8136895298957825\n",
      "Epoch 8, Iteration 201, Loss: 1.380087971687317\n",
      "Epoch 8, Iteration 202, Loss: 1.5078399181365967\n",
      "Epoch 8, Iteration 203, Loss: 1.2740119695663452\n",
      "Epoch 8, Iteration 204, Loss: 1.5201716423034668\n",
      "Epoch 8, Iteration 205, Loss: 1.1465911865234375\n",
      "Epoch 8, Iteration 206, Loss: 1.3958700895309448\n",
      "Epoch 8, Iteration 207, Loss: 1.6810295581817627\n",
      "Epoch 8, Iteration 208, Loss: 1.976500153541565\n",
      "Epoch 8, Iteration 209, Loss: 2.1842174530029297\n",
      "Epoch 8, Iteration 210, Loss: 1.8045377731323242\n",
      "Epoch 8, Iteration 211, Loss: 1.9806667566299438\n",
      "Epoch 8, Iteration 212, Loss: 1.669787049293518\n",
      "Epoch 8, Iteration 213, Loss: 1.7943029403686523\n",
      "Epoch 8, Iteration 214, Loss: 1.5752582550048828\n",
      "Epoch 8, Iteration 215, Loss: 1.281163215637207\n",
      "Epoch 8, Iteration 216, Loss: 1.3518118858337402\n",
      "Epoch 8, Iteration 217, Loss: 1.4186139106750488\n",
      "Epoch 8, Iteration 218, Loss: 1.3963794708251953\n",
      "Epoch 8, Iteration 219, Loss: 1.5260313749313354\n",
      "Epoch 8, Iteration 220, Loss: 1.364668369293213\n",
      "Epoch 8, Iteration 221, Loss: 1.369138240814209\n",
      "Epoch 8, Iteration 222, Loss: 1.4474046230316162\n",
      "Epoch 8, Iteration 223, Loss: 1.3310123682022095\n",
      "Epoch 8, Iteration 224, Loss: 1.3062105178833008\n",
      "Epoch 8, Iteration 225, Loss: 1.706632375717163\n",
      "Epoch 8, Iteration 226, Loss: 1.6176680326461792\n",
      "Epoch 8, Iteration 227, Loss: 1.6180717945098877\n",
      "Epoch 8, Iteration 228, Loss: 1.3636350631713867\n",
      "Epoch 8, Iteration 229, Loss: 1.418032169342041\n",
      "Epoch 8, Iteration 230, Loss: 1.4355151653289795\n",
      "Epoch 8, Iteration 231, Loss: 1.5273534059524536\n",
      "Epoch 8, Iteration 232, Loss: 1.5685410499572754\n",
      "Epoch 8, Iteration 233, Loss: 1.8433127403259277\n",
      "Epoch 8, Iteration 234, Loss: 1.7894924879074097\n",
      "Epoch 8, Iteration 235, Loss: 1.2781779766082764\n",
      "Epoch 8, Iteration 236, Loss: 1.6115330457687378\n",
      "Epoch 8, Iteration 237, Loss: 1.3914077281951904\n",
      "Epoch 8, Iteration 238, Loss: 1.2664414644241333\n",
      "Epoch 8, Iteration 239, Loss: 1.7323219776153564\n",
      "Epoch 8, Iteration 240, Loss: 1.2715274095535278\n",
      "Epoch 8, Iteration 241, Loss: 1.3328114748001099\n",
      "Epoch 8, Iteration 242, Loss: 1.3214268684387207\n",
      "Epoch 8, Iteration 243, Loss: 1.4016236066818237\n",
      "Epoch 8, Iteration 244, Loss: 1.3166565895080566\n",
      "Epoch 8, Iteration 245, Loss: 1.1092288494110107\n",
      "Epoch 8, Iteration 246, Loss: 1.5040010213851929\n",
      "Epoch 8, Iteration 247, Loss: 1.3571982383728027\n",
      "Epoch 8, Iteration 248, Loss: 1.4038856029510498\n",
      "Epoch 8, Iteration 249, Loss: 1.3785985708236694\n",
      "Epoch 8, Iteration 250, Loss: 1.5818884372711182\n",
      "Epoch 8, Iteration 250, Valid Loss: 0.818796694278717\n",
      "Epoch 8, Iteration 251, Loss: 1.3336812257766724\n",
      "Epoch 8, Iteration 252, Loss: 1.5830342769622803\n",
      "Epoch 8, Iteration 253, Loss: 1.609312653541565\n",
      "Epoch 8, Iteration 254, Loss: 1.565932273864746\n",
      "Epoch 8, Iteration 255, Loss: 1.5759886503219604\n",
      "Epoch 8, Iteration 256, Loss: 1.6925079822540283\n",
      "Epoch 8, Iteration 257, Loss: 1.3768601417541504\n",
      "Epoch 8, Iteration 258, Loss: 1.6672887802124023\n",
      "Epoch 8, Iteration 259, Loss: 1.191219687461853\n",
      "Epoch 8, Iteration 260, Loss: 1.7063173055648804\n",
      "Epoch 8, Iteration 261, Loss: 1.4853249788284302\n",
      "Epoch 8, Iteration 262, Loss: 1.3375139236450195\n",
      "Epoch 8, Iteration 263, Loss: 1.7756961584091187\n",
      "Epoch 8, Iteration 264, Loss: 1.4887639284133911\n",
      "Epoch 8, Iteration 265, Loss: 1.4984776973724365\n",
      "Epoch 8, Iteration 266, Loss: 1.574129581451416\n",
      "Epoch 8, Iteration 267, Loss: 1.4836547374725342\n",
      "Epoch 8, Iteration 268, Loss: 1.3672380447387695\n",
      "Epoch 8, Iteration 269, Loss: 1.3942408561706543\n",
      "Epoch 8, Iteration 270, Loss: 1.4789018630981445\n",
      "Epoch 8, Iteration 271, Loss: 1.7812353372573853\n",
      "Epoch 8, Iteration 272, Loss: 1.3330246210098267\n",
      "Epoch 8, Iteration 273, Loss: 1.33529794216156\n",
      "Epoch 8, Iteration 274, Loss: 1.3749226331710815\n",
      "Epoch 8, Iteration 275, Loss: 1.353347659111023\n",
      "Epoch 8, Iteration 276, Loss: 1.329105019569397\n",
      "Epoch 8, Iteration 277, Loss: 1.302438497543335\n",
      "Epoch 8, Iteration 278, Loss: 1.6135163307189941\n",
      "Epoch 8, Iteration 279, Loss: 1.2673285007476807\n",
      "Epoch 8, Iteration 280, Loss: 1.081084966659546\n",
      "Epoch 8, Iteration 281, Loss: 1.2946090698242188\n",
      "Epoch 8, Iteration 282, Loss: 1.384164571762085\n",
      "Epoch 8, Iteration 283, Loss: 1.409653902053833\n",
      "Epoch 8, Iteration 284, Loss: 1.5127785205841064\n",
      "Epoch 8, Iteration 285, Loss: 1.3284696340560913\n",
      "Epoch 8, Iteration 286, Loss: 1.2922934293746948\n",
      "Epoch 8, Iteration 287, Loss: 1.5543532371520996\n",
      "Epoch 8, Iteration 288, Loss: 1.2384096384048462\n",
      "Epoch 8, Iteration 289, Loss: 1.2936992645263672\n",
      "Epoch 8, Iteration 290, Loss: 1.2740414142608643\n",
      "Epoch 8, Iteration 291, Loss: 1.2369639873504639\n",
      "Epoch 8, Iteration 292, Loss: 1.7208929061889648\n",
      "Epoch 8, Iteration 293, Loss: 1.7672300338745117\n",
      "Epoch 8, Iteration 294, Loss: 1.3467220067977905\n",
      "Epoch 8, Iteration 295, Loss: 1.3362317085266113\n",
      "Epoch 8, Iteration 296, Loss: 1.6673945188522339\n",
      "Epoch 8, Iteration 297, Loss: 1.6823673248291016\n",
      "Epoch 8, Iteration 298, Loss: 1.4619226455688477\n",
      "Epoch 8, Iteration 299, Loss: 1.3258981704711914\n",
      "Epoch 8, Iteration 300, Loss: 1.215512990951538\n",
      "Epoch 8, Iteration 300, Valid Loss: 0.8014295697212219\n",
      "Epoch 8, Iteration 301, Loss: 1.37364661693573\n",
      "Epoch 8, Iteration 302, Loss: 1.2319819927215576\n",
      "Epoch 8, Iteration 303, Loss: 1.6969226598739624\n",
      "Epoch 8, Iteration 304, Loss: 1.273331642150879\n",
      "Epoch 8, Iteration 305, Loss: 1.2675058841705322\n",
      "Epoch 8, Iteration 306, Loss: 1.358703851699829\n",
      "Epoch 8, Iteration 307, Loss: 1.3040680885314941\n",
      "Epoch 8, Iteration 308, Loss: 1.228568434715271\n",
      "Epoch 8, Iteration 309, Loss: 1.3408762216567993\n",
      "Epoch 8, Iteration 310, Loss: 1.6419155597686768\n",
      "Epoch 8, Iteration 311, Loss: 1.2638887166976929\n",
      "Epoch 8, Iteration 312, Loss: 1.2610951662063599\n",
      "Epoch 8, Iteration 313, Loss: 1.657314419746399\n",
      "Epoch 8, Iteration 314, Loss: 1.3921769857406616\n",
      "Epoch 8, Iteration 315, Loss: 1.2208983898162842\n",
      "Epoch 8, Iteration 316, Loss: 1.5785413980484009\n",
      "Epoch 8, Iteration 317, Loss: 1.5096721649169922\n",
      "Epoch 8, Iteration 318, Loss: 1.228377103805542\n",
      "Epoch 8, Iteration 319, Loss: 1.5814241170883179\n",
      "Epoch 8, Iteration 320, Loss: 1.2059462070465088\n",
      "Epoch 8, Iteration 321, Loss: 1.4600269794464111\n",
      "Epoch 8, Iteration 322, Loss: 1.3228093385696411\n",
      "Epoch 8, Iteration 323, Loss: 1.3948911428451538\n",
      "Epoch 8, Iteration 324, Loss: 1.418845534324646\n",
      "Epoch 8, Iteration 325, Loss: 1.3993473052978516\n",
      "Epoch 8, Iteration 326, Loss: 1.333662509918213\n",
      "Epoch 8, Iteration 327, Loss: 1.2963999509811401\n",
      "Epoch 8, Iteration 328, Loss: 1.1970399618148804\n",
      "Epoch 8, Iteration 329, Loss: 1.5907562971115112\n",
      "Epoch 8, Iteration 330, Loss: 1.1401777267456055\n",
      "Epoch 8, Iteration 331, Loss: 1.2814931869506836\n",
      "Epoch 8, Iteration 332, Loss: 1.3689743280410767\n",
      "Epoch 8, Iteration 333, Loss: 1.552196979522705\n",
      "Epoch 8, Iteration 334, Loss: 1.2402868270874023\n",
      "Epoch 8, Iteration 335, Loss: 1.6287086009979248\n",
      "Epoch 8, Iteration 336, Loss: 1.4322322607040405\n",
      "Epoch 8, Iteration 337, Loss: 1.2011687755584717\n",
      "Epoch 8, Iteration 338, Loss: 1.5510497093200684\n",
      "Epoch 8, Iteration 339, Loss: 1.475487470626831\n",
      "Epoch 8, Iteration 340, Loss: 1.2206122875213623\n",
      "Epoch 8, Iteration 341, Loss: 1.4118802547454834\n",
      "Epoch 8, Iteration 342, Loss: 1.2807338237762451\n",
      "Epoch 8, Iteration 343, Loss: 1.3128540515899658\n",
      "Epoch 8, Iteration 344, Loss: 1.4593578577041626\n",
      "Epoch 8, Iteration 345, Loss: 1.4253696203231812\n",
      "Epoch 8, Iteration 346, Loss: 1.309037446975708\n",
      "Epoch 8, Iteration 347, Loss: 1.2492514848709106\n",
      "Epoch 8, Iteration 348, Loss: 1.234269618988037\n",
      "Epoch 8, Iteration 349, Loss: 1.3828213214874268\n",
      "Epoch 8, Iteration 350, Loss: 1.3575525283813477\n",
      "Epoch 8, Iteration 350, Valid Loss: 0.830708920955658\n",
      "Epoch 8, Iteration 351, Loss: 1.511652946472168\n",
      "Epoch 8, Iteration 352, Loss: 1.3057756423950195\n",
      "Epoch 8, Iteration 353, Loss: 1.429250717163086\n",
      "Epoch 8, Iteration 354, Loss: 1.3560984134674072\n",
      "Epoch 8, Iteration 355, Loss: 1.2704479694366455\n",
      "Epoch 8, Iteration 356, Loss: 1.2711501121520996\n",
      "Epoch 8, Iteration 357, Loss: 1.2344800233840942\n",
      "Epoch 8, Iteration 358, Loss: 1.3544447422027588\n",
      "Epoch 8, Iteration 359, Loss: 1.3995583057403564\n",
      "Epoch 8, Iteration 360, Loss: 1.201523780822754\n",
      "Epoch 8, Iteration 361, Loss: 1.1661036014556885\n",
      "Epoch 8, Iteration 362, Loss: 1.2919906377792358\n",
      "Epoch 8, Iteration 363, Loss: 1.2997891902923584\n",
      "Epoch 8, Iteration 364, Loss: 1.5171459913253784\n",
      "Epoch 8, Iteration 365, Loss: 1.5732061862945557\n",
      "Epoch 8, Iteration 366, Loss: 1.5418556928634644\n",
      "Epoch 8, Iteration 367, Loss: 1.4914216995239258\n",
      "Epoch 8, Iteration 368, Loss: 1.348759651184082\n",
      "Epoch 8, Iteration 369, Loss: 1.525402545928955\n",
      "Epoch 8, Iteration 370, Loss: 1.4487409591674805\n",
      "Epoch 8, Iteration 371, Loss: 1.395687460899353\n",
      "Epoch 8, Iteration 372, Loss: 1.2660819292068481\n",
      "Epoch 8, Iteration 373, Loss: 1.22538161277771\n",
      "Epoch 8, Iteration 374, Loss: 1.5359010696411133\n",
      "Epoch 8, Iteration 375, Loss: 1.405929684638977\n",
      "Epoch 8, Iteration 376, Loss: 1.5663930177688599\n",
      "Epoch 8, Iteration 377, Loss: 1.3881818056106567\n",
      "Epoch 8, Iteration 378, Loss: 1.2151463031768799\n",
      "Epoch 8, Iteration 379, Loss: 1.2234786748886108\n",
      "Epoch 8, Iteration 380, Loss: 1.2484883069992065\n",
      "Epoch 8, Iteration 381, Loss: 1.2789976596832275\n",
      "Epoch 8, Iteration 382, Loss: 1.110854983329773\n",
      "Epoch 8, Iteration 383, Loss: 1.2658593654632568\n",
      "Epoch 8, Iteration 384, Loss: 1.3611443042755127\n",
      "Epoch 8, Iteration 385, Loss: 1.2153513431549072\n",
      "Epoch 8, Iteration 386, Loss: 1.197295069694519\n",
      "Epoch 8, Iteration 387, Loss: 1.7118903398513794\n",
      "Epoch 8, Iteration 388, Loss: 1.2375954389572144\n",
      "Epoch 8, Iteration 389, Loss: 1.370826005935669\n",
      "Epoch 8, Iteration 390, Loss: 1.6258788108825684\n",
      "Epoch 8, Iteration 391, Loss: 1.267673373222351\n",
      "Epoch 9/10, Loss: 1.4493662754491883\n",
      "Epoch 9, Iteration 0, Loss: 1.4190019369125366\n",
      "Epoch 9, Iteration 1, Loss: 1.6678229570388794\n",
      "Epoch 9, Iteration 2, Loss: 1.5624536275863647\n",
      "Epoch 9, Iteration 3, Loss: 1.6376588344573975\n",
      "Epoch 9, Iteration 4, Loss: 1.3865503072738647\n",
      "Epoch 9, Iteration 5, Loss: 1.501921534538269\n",
      "Epoch 9, Iteration 6, Loss: 1.3373445272445679\n",
      "Epoch 9, Iteration 7, Loss: 1.3903498649597168\n",
      "Epoch 9, Iteration 8, Loss: 1.3458861112594604\n",
      "Epoch 9, Iteration 9, Loss: 1.6689820289611816\n",
      "Epoch 9, Iteration 10, Loss: 1.4673335552215576\n",
      "Epoch 9, Iteration 11, Loss: 1.274507761001587\n",
      "Epoch 9, Iteration 12, Loss: 1.3109683990478516\n",
      "Epoch 9, Iteration 13, Loss: 1.5159189701080322\n",
      "Epoch 9, Iteration 14, Loss: 1.3909316062927246\n",
      "Epoch 9, Iteration 15, Loss: 1.4832422733306885\n",
      "Epoch 9, Iteration 16, Loss: 1.2988488674163818\n",
      "Epoch 9, Iteration 17, Loss: 1.58353853225708\n",
      "Epoch 9, Iteration 18, Loss: 1.4418280124664307\n",
      "Epoch 9, Iteration 19, Loss: 1.5768640041351318\n",
      "Epoch 9, Iteration 20, Loss: 1.393585443496704\n",
      "Epoch 9, Iteration 21, Loss: 1.4125055074691772\n",
      "Epoch 9, Iteration 22, Loss: 1.4289298057556152\n",
      "Epoch 9, Iteration 23, Loss: 1.5695505142211914\n",
      "Epoch 9, Iteration 24, Loss: 1.262903094291687\n",
      "Epoch 9, Iteration 25, Loss: 1.1742537021636963\n",
      "Epoch 9, Iteration 26, Loss: 1.7576768398284912\n",
      "Epoch 9, Iteration 27, Loss: 1.3321698904037476\n",
      "Epoch 9, Iteration 28, Loss: 1.4999444484710693\n",
      "Epoch 9, Iteration 29, Loss: 1.6928555965423584\n",
      "Epoch 9, Iteration 30, Loss: 1.46009361743927\n",
      "Epoch 9, Iteration 31, Loss: 1.4277325868606567\n",
      "Epoch 9, Iteration 32, Loss: 1.3933098316192627\n",
      "Epoch 9, Iteration 33, Loss: 1.3823776245117188\n",
      "Epoch 9, Iteration 34, Loss: 1.3488729000091553\n",
      "Epoch 9, Iteration 35, Loss: 1.5763962268829346\n",
      "Epoch 9, Iteration 36, Loss: 1.1922333240509033\n",
      "Epoch 9, Iteration 37, Loss: 1.2769726514816284\n",
      "Epoch 9, Iteration 38, Loss: 1.2868902683258057\n",
      "Epoch 9, Iteration 39, Loss: 1.3184149265289307\n",
      "Epoch 9, Iteration 40, Loss: 1.3710565567016602\n",
      "Epoch 9, Iteration 41, Loss: 1.5256539583206177\n",
      "Epoch 9, Iteration 42, Loss: 1.3979108333587646\n",
      "Epoch 9, Iteration 43, Loss: 1.411049485206604\n",
      "Epoch 9, Iteration 44, Loss: 1.3796111345291138\n",
      "Epoch 9, Iteration 45, Loss: 1.3918449878692627\n",
      "Epoch 9, Iteration 46, Loss: 1.4331579208374023\n",
      "Epoch 9, Iteration 47, Loss: 1.5099258422851562\n",
      "Epoch 9, Iteration 48, Loss: 1.5577951669692993\n",
      "Epoch 9, Iteration 49, Loss: 1.3896297216415405\n",
      "Epoch 9, Iteration 50, Loss: 1.3005259037017822\n",
      "Epoch 9, Iteration 50, Valid Loss: 0.7849835753440857\n",
      "Epoch 9, Iteration 51, Loss: 1.4431955814361572\n",
      "Epoch 9, Iteration 52, Loss: 1.2651309967041016\n",
      "Epoch 9, Iteration 53, Loss: 1.4650492668151855\n",
      "Epoch 9, Iteration 54, Loss: 1.9141803979873657\n",
      "Epoch 9, Iteration 55, Loss: 1.5673633813858032\n",
      "Epoch 9, Iteration 56, Loss: 1.462011694908142\n",
      "Epoch 9, Iteration 57, Loss: 1.5916647911071777\n",
      "Epoch 9, Iteration 58, Loss: 1.2627547979354858\n",
      "Epoch 9, Iteration 59, Loss: 1.2665127515792847\n",
      "Epoch 9, Iteration 60, Loss: 1.424279808998108\n",
      "Epoch 9, Iteration 61, Loss: 1.2091315984725952\n",
      "Epoch 9, Iteration 62, Loss: 1.3397941589355469\n",
      "Epoch 9, Iteration 63, Loss: 1.142366886138916\n",
      "Epoch 9, Iteration 64, Loss: 1.395816683769226\n",
      "Epoch 9, Iteration 65, Loss: 1.7463512420654297\n",
      "Epoch 9, Iteration 66, Loss: 1.4525737762451172\n",
      "Epoch 9, Iteration 67, Loss: 1.066644549369812\n",
      "Epoch 9, Iteration 68, Loss: 1.8254789113998413\n",
      "Epoch 9, Iteration 69, Loss: 1.5741422176361084\n",
      "Epoch 9, Iteration 70, Loss: 1.495194911956787\n",
      "Epoch 9, Iteration 71, Loss: 1.4407414197921753\n",
      "Epoch 9, Iteration 72, Loss: 1.4747889041900635\n",
      "Epoch 9, Iteration 73, Loss: 1.2637864351272583\n",
      "Epoch 9, Iteration 74, Loss: 1.654082179069519\n",
      "Epoch 9, Iteration 75, Loss: 1.331043004989624\n",
      "Epoch 9, Iteration 76, Loss: 1.3922600746154785\n",
      "Epoch 9, Iteration 77, Loss: 2.6368472576141357\n",
      "Epoch 9, Iteration 78, Loss: 1.689424753189087\n",
      "Epoch 9, Iteration 79, Loss: 1.6141750812530518\n",
      "Epoch 9, Iteration 80, Loss: 1.1870262622833252\n",
      "Epoch 9, Iteration 81, Loss: 1.652380347251892\n",
      "Epoch 9, Iteration 82, Loss: 1.4608736038208008\n",
      "Epoch 9, Iteration 83, Loss: 1.2275687456130981\n",
      "Epoch 9, Iteration 84, Loss: 1.55076265335083\n",
      "Epoch 9, Iteration 85, Loss: 1.3575410842895508\n",
      "Epoch 9, Iteration 86, Loss: 1.5002729892730713\n",
      "Epoch 9, Iteration 87, Loss: 1.3782978057861328\n",
      "Epoch 9, Iteration 88, Loss: 1.46652352809906\n",
      "Epoch 9, Iteration 89, Loss: 1.4991540908813477\n",
      "Epoch 9, Iteration 90, Loss: 1.0839512348175049\n",
      "Epoch 9, Iteration 91, Loss: 1.2965677976608276\n",
      "Epoch 9, Iteration 92, Loss: 1.1759670972824097\n",
      "Epoch 9, Iteration 93, Loss: 1.388593316078186\n",
      "Epoch 9, Iteration 94, Loss: 1.8111412525177002\n",
      "Epoch 9, Iteration 95, Loss: 1.1998732089996338\n",
      "Epoch 9, Iteration 96, Loss: 1.4266644716262817\n",
      "Epoch 9, Iteration 97, Loss: 1.2327173948287964\n",
      "Epoch 9, Iteration 98, Loss: 1.5148429870605469\n",
      "Epoch 9, Iteration 99, Loss: 1.3451381921768188\n",
      "Epoch 9, Iteration 100, Loss: 1.3132104873657227\n",
      "Epoch 9, Iteration 100, Valid Loss: 0.8280003666877747\n",
      "Epoch 9, Iteration 101, Loss: 1.8089020252227783\n",
      "Epoch 9, Iteration 102, Loss: 1.3842121362686157\n",
      "Epoch 9, Iteration 103, Loss: 1.3909515142440796\n",
      "Epoch 9, Iteration 104, Loss: 1.9482202529907227\n",
      "Epoch 9, Iteration 105, Loss: 1.5218716859817505\n",
      "Epoch 9, Iteration 106, Loss: 1.3924609422683716\n",
      "Epoch 9, Iteration 107, Loss: 1.5056670904159546\n",
      "Epoch 9, Iteration 108, Loss: 1.3861420154571533\n",
      "Epoch 9, Iteration 109, Loss: 1.7374829053878784\n",
      "Epoch 9, Iteration 110, Loss: 1.6782736778259277\n",
      "Epoch 9, Iteration 111, Loss: 1.6536738872528076\n",
      "Epoch 9, Iteration 112, Loss: 1.4890046119689941\n",
      "Epoch 9, Iteration 113, Loss: 1.2781367301940918\n",
      "Epoch 9, Iteration 114, Loss: 1.7813427448272705\n",
      "Epoch 9, Iteration 115, Loss: 1.6448886394500732\n",
      "Epoch 9, Iteration 116, Loss: 1.3103582859039307\n",
      "Epoch 9, Iteration 117, Loss: 1.0626662969589233\n",
      "Epoch 9, Iteration 118, Loss: 1.4866437911987305\n",
      "Epoch 9, Iteration 119, Loss: 1.3932693004608154\n",
      "Epoch 9, Iteration 120, Loss: 1.3984849452972412\n",
      "Epoch 9, Iteration 121, Loss: 1.278580904006958\n",
      "Epoch 9, Iteration 122, Loss: 1.632873773574829\n",
      "Epoch 9, Iteration 123, Loss: 1.3822617530822754\n",
      "Epoch 9, Iteration 124, Loss: 1.3198797702789307\n",
      "Epoch 9, Iteration 125, Loss: 1.2305052280426025\n",
      "Epoch 9, Iteration 126, Loss: 1.1562451124191284\n",
      "Epoch 9, Iteration 127, Loss: 1.5675194263458252\n",
      "Epoch 9, Iteration 128, Loss: 1.450805425643921\n",
      "Epoch 9, Iteration 129, Loss: 1.6761071681976318\n",
      "Epoch 9, Iteration 130, Loss: 1.8835705518722534\n",
      "Epoch 9, Iteration 131, Loss: 1.2749435901641846\n",
      "Epoch 9, Iteration 132, Loss: 1.3326234817504883\n",
      "Epoch 9, Iteration 133, Loss: 1.7904927730560303\n",
      "Epoch 9, Iteration 134, Loss: 1.2449692487716675\n",
      "Epoch 9, Iteration 135, Loss: 1.5083760023117065\n",
      "Epoch 9, Iteration 136, Loss: 1.3153557777404785\n",
      "Epoch 9, Iteration 137, Loss: 1.3637171983718872\n",
      "Epoch 9, Iteration 138, Loss: 1.4730933904647827\n",
      "Epoch 9, Iteration 139, Loss: 1.2949639558792114\n",
      "Epoch 9, Iteration 140, Loss: 1.2995529174804688\n",
      "Epoch 9, Iteration 141, Loss: 1.5435734987258911\n",
      "Epoch 9, Iteration 142, Loss: 1.3894681930541992\n",
      "Epoch 9, Iteration 143, Loss: 1.4384011030197144\n",
      "Epoch 9, Iteration 144, Loss: 1.2931286096572876\n",
      "Epoch 9, Iteration 145, Loss: 1.4219043254852295\n",
      "Epoch 9, Iteration 146, Loss: 1.5924078226089478\n",
      "Epoch 9, Iteration 147, Loss: 1.43877375125885\n",
      "Epoch 9, Iteration 148, Loss: 1.1683177947998047\n",
      "Epoch 9, Iteration 149, Loss: 1.2747983932495117\n",
      "Epoch 9, Iteration 150, Loss: 1.54348886013031\n",
      "Epoch 9, Iteration 150, Valid Loss: 0.8128789067268372\n",
      "Epoch 9, Iteration 151, Loss: 1.5017849206924438\n",
      "Epoch 9, Iteration 152, Loss: 1.2742891311645508\n",
      "Epoch 9, Iteration 153, Loss: 1.3283048868179321\n",
      "Epoch 9, Iteration 154, Loss: 1.077863097190857\n",
      "Epoch 9, Iteration 155, Loss: 1.3917696475982666\n",
      "Epoch 9, Iteration 156, Loss: 1.3980902433395386\n",
      "Epoch 9, Iteration 157, Loss: 1.9668841361999512\n",
      "Epoch 9, Iteration 158, Loss: 1.4268035888671875\n",
      "Epoch 9, Iteration 159, Loss: 1.4096120595932007\n",
      "Epoch 9, Iteration 160, Loss: 1.3086421489715576\n",
      "Epoch 9, Iteration 161, Loss: 1.4094302654266357\n",
      "Epoch 9, Iteration 162, Loss: 1.7684775590896606\n",
      "Epoch 9, Iteration 163, Loss: 1.1567333936691284\n",
      "Epoch 9, Iteration 164, Loss: 2.542574882507324\n",
      "Epoch 9, Iteration 165, Loss: 1.316420078277588\n",
      "Epoch 9, Iteration 166, Loss: 1.265669345855713\n",
      "Epoch 9, Iteration 167, Loss: 1.7348363399505615\n",
      "Epoch 9, Iteration 168, Loss: 1.5332651138305664\n",
      "Epoch 9, Iteration 169, Loss: 1.3951289653778076\n",
      "Epoch 9, Iteration 170, Loss: 1.6752887964248657\n",
      "Epoch 9, Iteration 171, Loss: 1.4704086780548096\n",
      "Epoch 9, Iteration 172, Loss: 1.456965446472168\n",
      "Epoch 9, Iteration 173, Loss: 1.2828093767166138\n",
      "Epoch 9, Iteration 174, Loss: 1.4910417795181274\n",
      "Epoch 9, Iteration 175, Loss: 1.6175440549850464\n",
      "Epoch 9, Iteration 176, Loss: 1.162591576576233\n",
      "Epoch 9, Iteration 177, Loss: 1.7588715553283691\n",
      "Epoch 9, Iteration 178, Loss: 1.562511920928955\n",
      "Epoch 9, Iteration 179, Loss: 1.4710365533828735\n",
      "Epoch 9, Iteration 180, Loss: 1.3461543321609497\n",
      "Epoch 9, Iteration 181, Loss: 1.7562358379364014\n",
      "Epoch 9, Iteration 182, Loss: 1.6179008483886719\n",
      "Epoch 9, Iteration 183, Loss: 1.28679621219635\n",
      "Epoch 9, Iteration 184, Loss: 1.346968412399292\n",
      "Epoch 9, Iteration 185, Loss: 1.2824108600616455\n",
      "Epoch 9, Iteration 186, Loss: 1.3958839178085327\n",
      "Epoch 9, Iteration 187, Loss: 1.333539366722107\n",
      "Epoch 9, Iteration 188, Loss: 1.3044579029083252\n",
      "Epoch 9, Iteration 189, Loss: 1.2841517925262451\n",
      "Epoch 9, Iteration 190, Loss: 1.528052806854248\n",
      "Epoch 9, Iteration 191, Loss: 1.4950495958328247\n",
      "Epoch 9, Iteration 192, Loss: 1.54655921459198\n",
      "Epoch 9, Iteration 193, Loss: 1.3348572254180908\n",
      "Epoch 9, Iteration 194, Loss: 1.4162429571151733\n",
      "Epoch 9, Iteration 195, Loss: 1.3726085424423218\n",
      "Epoch 9, Iteration 196, Loss: 1.5571889877319336\n",
      "Epoch 9, Iteration 197, Loss: 1.3745536804199219\n",
      "Epoch 9, Iteration 198, Loss: 1.398815393447876\n",
      "Epoch 9, Iteration 199, Loss: 1.501975655555725\n",
      "Epoch 9, Iteration 200, Loss: 1.2963647842407227\n",
      "Epoch 9, Iteration 200, Valid Loss: 0.8030509948730469\n",
      "Epoch 9, Iteration 201, Loss: 1.2804629802703857\n",
      "Epoch 9, Iteration 202, Loss: 1.494346022605896\n",
      "Epoch 9, Iteration 203, Loss: 1.314789056777954\n",
      "Epoch 9, Iteration 204, Loss: 1.5140466690063477\n",
      "Epoch 9, Iteration 205, Loss: 1.2023165225982666\n",
      "Epoch 9, Iteration 206, Loss: 1.3440556526184082\n",
      "Epoch 9, Iteration 207, Loss: 1.6732187271118164\n",
      "Epoch 9, Iteration 208, Loss: 1.9684853553771973\n",
      "Epoch 9, Iteration 209, Loss: 2.1286208629608154\n",
      "Epoch 9, Iteration 210, Loss: 1.7884618043899536\n",
      "Epoch 9, Iteration 211, Loss: 1.9467988014221191\n",
      "Epoch 9, Iteration 212, Loss: 1.7111070156097412\n",
      "Epoch 9, Iteration 213, Loss: 1.779000997543335\n",
      "Epoch 9, Iteration 214, Loss: 1.5318877696990967\n",
      "Epoch 9, Iteration 215, Loss: 1.2701929807662964\n",
      "Epoch 9, Iteration 216, Loss: 1.2501310110092163\n",
      "Epoch 9, Iteration 217, Loss: 1.399592638015747\n",
      "Epoch 9, Iteration 218, Loss: 1.3931922912597656\n",
      "Epoch 9, Iteration 219, Loss: 1.622056484222412\n",
      "Epoch 9, Iteration 220, Loss: 1.289209008216858\n",
      "Epoch 9, Iteration 221, Loss: 1.384772777557373\n",
      "Epoch 9, Iteration 222, Loss: 1.42800772190094\n",
      "Epoch 9, Iteration 223, Loss: 1.3452988862991333\n",
      "Epoch 9, Iteration 224, Loss: 1.1864007711410522\n",
      "Epoch 9, Iteration 225, Loss: 1.602111577987671\n",
      "Epoch 9, Iteration 226, Loss: 1.5325839519500732\n",
      "Epoch 9, Iteration 227, Loss: 1.5406993627548218\n",
      "Epoch 9, Iteration 228, Loss: 1.265689492225647\n",
      "Epoch 9, Iteration 229, Loss: 1.377777338027954\n",
      "Epoch 9, Iteration 230, Loss: 1.4940121173858643\n",
      "Epoch 9, Iteration 231, Loss: 1.438992977142334\n",
      "Epoch 9, Iteration 232, Loss: 1.5263736248016357\n",
      "Epoch 9, Iteration 233, Loss: 1.8519511222839355\n",
      "Epoch 9, Iteration 234, Loss: 1.6092109680175781\n",
      "Epoch 9, Iteration 235, Loss: 1.2497249841690063\n",
      "Epoch 9, Iteration 236, Loss: 1.5269708633422852\n",
      "Epoch 9, Iteration 237, Loss: 1.379347801208496\n",
      "Epoch 9, Iteration 238, Loss: 1.221745491027832\n",
      "Epoch 9, Iteration 239, Loss: 1.3724308013916016\n",
      "Epoch 9, Iteration 240, Loss: 1.2168596982955933\n",
      "Epoch 9, Iteration 241, Loss: 1.2439018487930298\n",
      "Epoch 9, Iteration 242, Loss: 1.3814566135406494\n",
      "Epoch 9, Iteration 243, Loss: 1.3204550743103027\n",
      "Epoch 9, Iteration 244, Loss: 1.356824517250061\n",
      "Epoch 9, Iteration 245, Loss: 1.0416145324707031\n",
      "Epoch 9, Iteration 246, Loss: 1.4764769077301025\n",
      "Epoch 9, Iteration 247, Loss: 1.3640680313110352\n",
      "Epoch 9, Iteration 248, Loss: 1.7447030544281006\n",
      "Epoch 9, Iteration 249, Loss: 1.3857622146606445\n",
      "Epoch 9, Iteration 250, Loss: 1.4681951999664307\n",
      "Epoch 9, Iteration 250, Valid Loss: 0.7900123596191406\n",
      "Epoch 9, Iteration 251, Loss: 1.3727232217788696\n",
      "Epoch 9, Iteration 252, Loss: 1.5906593799591064\n",
      "Epoch 9, Iteration 253, Loss: 1.5250396728515625\n",
      "Epoch 9, Iteration 254, Loss: 1.5672903060913086\n",
      "Epoch 9, Iteration 255, Loss: 1.5739502906799316\n",
      "Epoch 9, Iteration 256, Loss: 1.6524009704589844\n",
      "Epoch 9, Iteration 257, Loss: 1.370086431503296\n",
      "Epoch 9, Iteration 258, Loss: 1.777796745300293\n",
      "Epoch 9, Iteration 259, Loss: 1.2884572744369507\n",
      "Epoch 9, Iteration 260, Loss: 1.4895884990692139\n",
      "Epoch 9, Iteration 261, Loss: 1.4452033042907715\n",
      "Epoch 9, Iteration 262, Loss: 1.2259595394134521\n",
      "Epoch 9, Iteration 263, Loss: 1.645209550857544\n",
      "Epoch 9, Iteration 264, Loss: 1.496997356414795\n",
      "Epoch 9, Iteration 265, Loss: 1.4791933298110962\n",
      "Epoch 9, Iteration 266, Loss: 1.556034803390503\n",
      "Epoch 9, Iteration 267, Loss: 1.3982244729995728\n",
      "Epoch 9, Iteration 268, Loss: 1.3608936071395874\n",
      "Epoch 9, Iteration 269, Loss: 1.2973467111587524\n",
      "Epoch 9, Iteration 270, Loss: 1.4126930236816406\n",
      "Epoch 9, Iteration 271, Loss: 1.4439764022827148\n",
      "Epoch 9, Iteration 272, Loss: 1.316035270690918\n",
      "Epoch 9, Iteration 273, Loss: 1.4264492988586426\n",
      "Epoch 9, Iteration 274, Loss: 1.3504571914672852\n",
      "Epoch 9, Iteration 275, Loss: 1.2250850200653076\n",
      "Epoch 9, Iteration 276, Loss: 1.4178094863891602\n",
      "Epoch 9, Iteration 277, Loss: 1.340114951133728\n",
      "Epoch 9, Iteration 278, Loss: 1.6817773580551147\n",
      "Epoch 9, Iteration 279, Loss: 1.2447001934051514\n",
      "Epoch 9, Iteration 280, Loss: 1.121953010559082\n",
      "Epoch 9, Iteration 281, Loss: 1.3751417398452759\n",
      "Epoch 9, Iteration 282, Loss: 1.3604165315628052\n",
      "Epoch 9, Iteration 283, Loss: 1.4023836851119995\n",
      "Epoch 9, Iteration 284, Loss: 1.363822340965271\n",
      "Epoch 9, Iteration 285, Loss: 1.278154969215393\n",
      "Epoch 9, Iteration 286, Loss: 1.3132747411727905\n",
      "Epoch 9, Iteration 287, Loss: 1.457972764968872\n",
      "Epoch 9, Iteration 288, Loss: 1.2142549753189087\n",
      "Epoch 9, Iteration 289, Loss: 1.2920889854431152\n",
      "Epoch 9, Iteration 290, Loss: 1.3218201398849487\n",
      "Epoch 9, Iteration 291, Loss: 1.1988680362701416\n",
      "Epoch 9, Iteration 292, Loss: 1.5647292137145996\n",
      "Epoch 9, Iteration 293, Loss: 1.594014048576355\n",
      "Epoch 9, Iteration 294, Loss: 1.2580474615097046\n",
      "Epoch 9, Iteration 295, Loss: 1.2466771602630615\n",
      "Epoch 9, Iteration 296, Loss: 1.5991408824920654\n",
      "Epoch 9, Iteration 297, Loss: 1.6060692071914673\n",
      "Epoch 9, Iteration 298, Loss: 1.4174120426177979\n",
      "Epoch 9, Iteration 299, Loss: 1.2325971126556396\n",
      "Epoch 9, Iteration 300, Loss: 1.1692687273025513\n",
      "Epoch 9, Iteration 300, Valid Loss: 0.7829958200454712\n",
      "Epoch 9, Iteration 301, Loss: 1.363530158996582\n",
      "Epoch 9, Iteration 302, Loss: 1.2792961597442627\n",
      "Epoch 9, Iteration 303, Loss: 1.7194998264312744\n",
      "Epoch 9, Iteration 304, Loss: 1.318787932395935\n",
      "Epoch 9, Iteration 305, Loss: 1.2382304668426514\n",
      "Epoch 9, Iteration 306, Loss: 1.3760796785354614\n",
      "Epoch 9, Iteration 307, Loss: 1.2310556173324585\n",
      "Epoch 9, Iteration 308, Loss: 1.2611351013183594\n",
      "Epoch 9, Iteration 309, Loss: 1.3756368160247803\n",
      "Epoch 9, Iteration 310, Loss: 1.6826181411743164\n",
      "Epoch 9, Iteration 311, Loss: 1.2766928672790527\n",
      "Epoch 9, Iteration 312, Loss: 1.217157006263733\n",
      "Epoch 9, Iteration 313, Loss: 1.6197330951690674\n",
      "Epoch 9, Iteration 314, Loss: 1.238403558731079\n",
      "Epoch 9, Iteration 315, Loss: 1.1635477542877197\n",
      "Epoch 9, Iteration 316, Loss: 1.4682676792144775\n",
      "Epoch 9, Iteration 317, Loss: 1.238741159439087\n",
      "Epoch 9, Iteration 318, Loss: 1.3183268308639526\n",
      "Epoch 9, Iteration 319, Loss: 1.531143069267273\n",
      "Epoch 9, Iteration 320, Loss: 1.1308104991912842\n",
      "Epoch 9, Iteration 321, Loss: 1.3986748456954956\n",
      "Epoch 9, Iteration 322, Loss: 1.2713452577590942\n",
      "Epoch 9, Iteration 323, Loss: 1.3544834852218628\n",
      "Epoch 9, Iteration 324, Loss: 1.3651328086853027\n",
      "Epoch 9, Iteration 325, Loss: 1.3014825582504272\n",
      "Epoch 9, Iteration 326, Loss: 1.3630539178848267\n",
      "Epoch 9, Iteration 327, Loss: 1.2223440408706665\n",
      "Epoch 9, Iteration 328, Loss: 1.3118140697479248\n",
      "Epoch 9, Iteration 329, Loss: 1.491673231124878\n",
      "Epoch 9, Iteration 330, Loss: 1.1141217947006226\n",
      "Epoch 9, Iteration 331, Loss: 1.3674430847167969\n",
      "Epoch 9, Iteration 332, Loss: 1.4625592231750488\n",
      "Epoch 9, Iteration 333, Loss: 1.577138900756836\n",
      "Epoch 9, Iteration 334, Loss: 1.257855772972107\n",
      "Epoch 9, Iteration 335, Loss: 1.582724928855896\n",
      "Epoch 9, Iteration 336, Loss: 1.344641089439392\n",
      "Epoch 9, Iteration 337, Loss: 1.2347785234451294\n",
      "Epoch 9, Iteration 338, Loss: 1.491646647453308\n",
      "Epoch 9, Iteration 339, Loss: 1.342214584350586\n",
      "Epoch 9, Iteration 340, Loss: 1.1733399629592896\n",
      "Epoch 9, Iteration 341, Loss: 1.3009965419769287\n",
      "Epoch 9, Iteration 342, Loss: 1.2873454093933105\n",
      "Epoch 9, Iteration 343, Loss: 1.2621715068817139\n",
      "Epoch 9, Iteration 344, Loss: 1.385965347290039\n",
      "Epoch 9, Iteration 345, Loss: 1.404354453086853\n",
      "Epoch 9, Iteration 346, Loss: 1.3246417045593262\n",
      "Epoch 9, Iteration 347, Loss: 1.1077098846435547\n",
      "Epoch 9, Iteration 348, Loss: 1.2453962564468384\n",
      "Epoch 9, Iteration 349, Loss: 1.300994634628296\n",
      "Epoch 9, Iteration 350, Loss: 1.2715940475463867\n",
      "Epoch 9, Iteration 350, Valid Loss: 0.800444483757019\n",
      "Epoch 9, Iteration 351, Loss: 1.4597824811935425\n",
      "Epoch 9, Iteration 352, Loss: 1.2731527090072632\n",
      "Epoch 9, Iteration 353, Loss: 1.4373162984848022\n",
      "Epoch 9, Iteration 354, Loss: 1.451431393623352\n",
      "Epoch 9, Iteration 355, Loss: 1.2932658195495605\n",
      "Epoch 9, Iteration 356, Loss: 1.3680179119110107\n",
      "Epoch 9, Iteration 357, Loss: 1.2089743614196777\n",
      "Epoch 9, Iteration 358, Loss: 1.2266782522201538\n",
      "Epoch 9, Iteration 359, Loss: 1.221789836883545\n",
      "Epoch 9, Iteration 360, Loss: 1.1673219203948975\n",
      "Epoch 9, Iteration 361, Loss: 1.4138561487197876\n",
      "Epoch 9, Iteration 362, Loss: 1.2037696838378906\n",
      "Epoch 9, Iteration 363, Loss: 1.2910650968551636\n",
      "Epoch 9, Iteration 364, Loss: 1.3591971397399902\n",
      "Epoch 9, Iteration 365, Loss: 1.4860423803329468\n",
      "Epoch 9, Iteration 366, Loss: 1.5768814086914062\n",
      "Epoch 9, Iteration 367, Loss: 1.3832780122756958\n",
      "Epoch 9, Iteration 368, Loss: 1.4255926609039307\n",
      "Epoch 9, Iteration 369, Loss: 1.485979676246643\n",
      "Epoch 9, Iteration 370, Loss: 1.4631835222244263\n",
      "Epoch 9, Iteration 371, Loss: 1.3195242881774902\n",
      "Epoch 9, Iteration 372, Loss: 1.1398322582244873\n",
      "Epoch 9, Iteration 373, Loss: 1.2332851886749268\n",
      "Epoch 9, Iteration 374, Loss: 1.6524927616119385\n",
      "Epoch 9, Iteration 375, Loss: 1.4044651985168457\n",
      "Epoch 9, Iteration 376, Loss: 1.399505615234375\n",
      "Epoch 9, Iteration 377, Loss: 1.4422587156295776\n",
      "Epoch 9, Iteration 378, Loss: 1.173766016960144\n",
      "Epoch 9, Iteration 379, Loss: 1.3219201564788818\n",
      "Epoch 9, Iteration 380, Loss: 1.1788979768753052\n",
      "Epoch 9, Iteration 381, Loss: 1.243704080581665\n",
      "Epoch 9, Iteration 382, Loss: 1.0883216857910156\n",
      "Epoch 9, Iteration 383, Loss: 1.203870177268982\n",
      "Epoch 9, Iteration 384, Loss: 1.3974683284759521\n",
      "Epoch 9, Iteration 385, Loss: 1.2745839357376099\n",
      "Epoch 9, Iteration 386, Loss: 1.2142212390899658\n",
      "Epoch 9, Iteration 387, Loss: 1.5820064544677734\n",
      "Epoch 9, Iteration 388, Loss: 1.318082571029663\n",
      "Epoch 9, Iteration 389, Loss: 1.3726059198379517\n",
      "Epoch 9, Iteration 390, Loss: 1.4738065004348755\n",
      "Epoch 9, Iteration 391, Loss: 1.2505515813827515\n",
      "Epoch 10/10, Loss: 1.42042383916524\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>head_tagging_accuracy</td><td>▁▄▆▆▇▇████</td></tr><tr><td>train_loss</td><td>██▆▅▅▅▄▄▃▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▂▂▂</td></tr><tr><td>unlabeled_attachment_score</td><td>▁▅▆▇▇█████</td></tr><tr><td>valid_loss</td><td>█▆▆▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>head_tagging_accuracy</td><td>0.75891</td></tr><tr><td>train_loss</td><td>1.25055</td></tr><tr><td>unlabeled_attachment_score</td><td>0.8013</td></tr><tr><td>valid_loss</td><td>0.80044</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dependency-parsing</strong> at: <a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/er1khkx3' target=\"_blank\">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing/runs/er1khkx3</a><br> View project at: <a href='https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing' target=\"_blank\">https://wandb.ai/maxewang10-saarland-informatics-campus/dependency-parsing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250113_225223-er1khkx3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batch: 65\n",
      "Batch 0\n",
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n",
      "Batch 11\n",
      "Batch 12\n",
      "Batch 13\n",
      "Batch 14\n",
      "Batch 15\n",
      "Batch 16\n",
      "Batch 17\n",
      "Batch 18\n",
      "Batch 19\n",
      "Batch 20\n",
      "Batch 21\n",
      "Batch 22\n",
      "Batch 23\n",
      "Batch 24\n",
      "Batch 25\n",
      "Batch 26\n",
      "Batch 27\n",
      "Batch 28\n",
      "Batch 29\n",
      "Batch 30\n",
      "Batch 31\n",
      "Batch 32\n",
      "Batch 33\n",
      "Batch 34\n",
      "Batch 35\n",
      "Batch 36\n",
      "Batch 37\n",
      "Batch 38\n",
      "Batch 39\n",
      "Batch 40\n",
      "Batch 41\n",
      "Batch 42\n",
      "Batch 43\n",
      "Batch 44\n",
      "Batch 45\n",
      "Batch 46\n",
      "Batch 47\n",
      "Batch 48\n",
      "Batch 49\n",
      "Batch 50\n",
      "Batch 51\n",
      "Batch 52\n",
      "Batch 53\n",
      "Batch 54\n",
      "Batch 55\n",
      "Batch 56\n",
      "Batch 57\n",
      "Batch 58\n",
      "Batch 59\n",
      "Batch 60\n",
      "Batch 61\n",
      "Batch 62\n",
      "Batch 63\n",
      "Batch 64\n",
      "LAS Accuracy: 0.7171660822441823\n"
     ]
    }
   ],
   "source": [
    "# Extra: training label predicting and evaluation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = DependencyParserModel(label_predicting=True)\n",
    "\n",
    "train(model, train_dataloader, valid_dataloader, device, lr=1e-3, num_epochs=10)\n",
    "\n",
    "# Evaluate Head prediction\n",
    "model.eval()\n",
    "total_count = 0\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    print(f\"total batch: {len(test_dataloader)}\")\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        x = data[\"input_ids\"].to(device)\n",
    "        mask = data[\"attention_mask\"].to(device)\n",
    "        head = data[\"head\"].to(device)\n",
    "        deprel = data[\"deprel_ids\"].to(device)\n",
    "\n",
    "        H_head, H_dep, L_head, L_dep = model(x, mask)\n",
    "        edge_scores = model.score_edges(H_head, H_dep)  # Shape: (batch_size, seq_len, seq_len)\n",
    "        label_scores = model.score_labels(L_head, L_dep)  # Shape: (batch_size, seq_len, seq_len, num_labels)\n",
    "\n",
    "        # best score heads\n",
    "        predicted_heads = edge_scores.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Shape: (batch_size, seq_len, seq_len, num_labels)\n",
    "        batch_size, n, _, num_labels = label_scores.shape\n",
    "\n",
    "        # batch_indices: (batch_size, 1)\n",
    "        # dep_indices:   (1,   n)\n",
    "        batch_indices = torch.arange(batch_size).unsqueeze(1).to(device)  # batch index\n",
    "        dep_indices = torch.arange(n).unsqueeze(0).to(device)  # dep index\n",
    "\n",
    "        # gather label_scores according to the gold head\n",
    "        # Shape: (batch_size, seq_len, num_labels)\n",
    "        label_scores_for_gold_edge = label_scores[batch_indices, dep_indices, head, :]\n",
    "\n",
    "        # best score labels\n",
    "        predicted_labels = label_scores_for_gold_edge.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "\n",
    "        print(f\"Batch {i}\")\n",
    "\n",
    "        valid_positions = (head != -100) & (mask == 1)  # Mask for valid positions # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # calculate correct predictions and total count\n",
    "        total_correct += (((predicted_heads == head) & (predicted_labels == deprel)) & valid_positions).sum().item()\n",
    "        total_count += valid_positions.sum().item()\n",
    "\n",
    "print(f\"LAS Accuracy: {total_correct / total_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892f70c2cd2df37",
   "metadata": {},
   "source": [
    "# Observation\n",
    "I trained 2 neural network models with different hyperparameters and architectures.\n",
    "The models are:\n",
    "- Base Model: RoBERTa + Linear Layer\n",
    "    - Learning Rate: 1e-3\n",
    "    - Epochs: 10\n",
    "    - edge dimension: 500\n",
    "    - Final UAS: 0.8089\n",
    "- Pro Model: RoBERTa + 2 Linear Layers with residual connection and more edge dimension\n",
    "    - Learning Rate: 6e-4\n",
    "    - Epochs: 15\n",
    "    - edge dimension: 1024\n",
    "    - Final UAS: 0.8636\n",
    "\n",
    "All of the models are trained with a 32 batch size.\n",
    "\n",
    "The Head Tagging Accuracy was achieved by computing the accuracy of the predicted heads against the gold standard annotations, while masking invalid positions (e.g., head = -100 and mask = 0). This value was computed after each training epoch on the validation set.\n",
    "\n",
    "The UAS score was evaluated using the MST-parsed dependency tree, ensuring the predicted edges can form a tree. The UAS score was recorded on the validation set also after training epoch. Compared to the base model, more linear layers and more edge dimension gives more improvement. However, it also takes a longer time for a more complex model to be trained.\n",
    "\n",
    "From the train loss and valid loss figures we can see the pro model has a much more better performance than the base model. The pro model has a lower loss and a higher accuracy. The pro model has a better performance on the validation set, which means it has a better generalization ability. Correspondingly, the UAS score of the pro model is higher than the base model.\n",
    "\n",
    "# Extra: Label Prediction\n",
    "Additionally, I trained a label prediction model by combining the edge prediction loss and the label prediction loss.\n",
    "The final LAS is 0.7171660822441823.\n",
    "\n",
    "# Training figures\n",
    "![train_loss](./figures/train_loss.png \"train_loss\")\n",
    "![valid_loss](./figures/valid_loss.png \"valid_loss\")\n",
    "![head_tagging_accuracy](./figures/head_tagging_accuracy.png \"head_tagging_accuracy\")\n",
    "![unlabeled_attachment_score](./figures/UAS.png \"unlabeled_attachment_score\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
