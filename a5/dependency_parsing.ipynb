{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T14:02:31.224227Z",
     "start_time": "2025-01-09T14:02:31.219915Z"
    }
   },
   "source": [
    "import torch\n",
    "import sys\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, XLMRobertaModel"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "246be7d70e3db82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:02:33.385087Z",
     "start_time": "2025-01-09T14:02:31.250086Z"
    }
   },
   "source": [
    "# load the dataset\n",
    "dataset = datasets.load_dataset(path=\"universal_dependencies\", name=\"en_ewt\", trust_remote_code=True)\n",
    "print(dataset)\n",
    "train_dataset = dataset[\"train\"]\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[\"text\"][:10])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 12543\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 2002\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 2077\n",
      "    })\n",
      "})\n",
      "['Al-Zaman : American forces killed Shaikh Abdullah al-Ani, the preacher at the mosque in the town of Qaim, near the Syrian border.', '[This killing of a respected cleric will be causing us trouble for years to come.]', 'DPA: Iraqi authorities announced that they had busted up 3 terrorist cells operating in Baghdad.', 'Two of them were being run by 2 officials of the Ministry of the Interior!', 'The MoI in Iraq is equivalent to the US FBI, so this would be like having J. Edgar Hoover unwittingly employ at a high level members of the Weathermen bombers back in the 1960s.', 'The third was being run by the head of an investment firm.', 'You wonder if he was manipulating the market with his bombing targets.', 'The cells were operating in the Ghazaliyah and al-Jihad districts of the capital.', \"Although the announcement was probably made to show progress in identifying and breaking up terror cells, I don't find the news that the Baathists continue to penetrate the Iraqi government very hopeful.\", 'It reminds me too much of the ARVN officers who were secretly working for the other side in Vietnam.']\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "907365ee16f491c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:02:33.919820Z",
     "start_time": "2025-01-09T14:02:33.914815Z"
    }
   },
   "source": [
    "all_deprels = [\n",
    "    # these are the default UD dependency relations according to https://universaldependencies.org/u/dep/\n",
    "    \"acl\", \"acl:relcl\", \"advcl\", \"advcl:relcl\", \"advmod\", \"advmod:emph\", \"advmod:lmod\", \"amod\", \"appos\",\n",
    "    \"aux\", \"aux:pass\", \"case\", \"cc\", \"cc:preconj\", \"ccomp\", \"clf\", \"compound\", \"compound:lvc\",\n",
    "    \"compound:prt\", \"compound:redup\", \"compound:svc\", \"conj\", \"cop\", \"csubj\", \"csubj:outer\",\n",
    "    \"csubj:pass\", \"dep\", \"det\", \"det:numgov\", \"det:nummod\", \"det:poss\", \"discourse\", \"dislocated\",\n",
    "    \"expl\", \"expl:impers\", \"expl:pass\", \"expl:pv\", \"fixed\", \"flat\", \"flat:foreign\", \"flat:name\",\n",
    "    \"goeswith\", \"iobj\", \"list\", \"mark\", \"nmod\", \"nmod:poss\", \"nmod:tmod\", \"nsubj\", \"nsubj:outer\",\n",
    "    \"nsubj:pass\", \"nummod\", \"nummod:gov\", \"obj\", \"obl\", \"obl:agent\", \"obl:arg\", \"obl:lmod\",\n",
    "    \"obl:tmod\", \"orphan\", \"parataxis\", \"punct\", \"reparandum\", \"root\", \"vocative\", \"xcomp\",\n",
    "\n",
    "    # we need some more for en_ewt\n",
    "    \"det:predet\", \"obl:npmod\", \"nmod:npmod\"\n",
    "]\n",
    "\n",
    "# construct deprel to ID mapping\n",
    "deprel_to_id = {rel: idx for idx, rel in enumerate(all_deprels)}"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "fd3b37577b6ef9f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:02:34.427060Z",
     "start_time": "2025-01-09T14:02:34.418701Z"
    }
   },
   "source": [
    "# Code for the assignment in https://github.com/coli-saar/cl/wiki/Assignment:-Dependency-parsing\n",
    "# Alexander Koller, December 2023\n",
    "\n",
    "def strip_none_heads(examples, i):\n",
    "    tokens = examples[\"tokens\"][i]\n",
    "    heads = examples[\"head\"][i]\n",
    "    deprels = examples[\"deprel\"][i]\n",
    "\n",
    "    non_none = [(t, h, d) for t, h, d in zip(tokens, heads, deprels) if h != \"None\"]\n",
    "    return zip(*non_none)\n",
    "\n",
    "\n",
    "def map_first_occurrence(nums):\n",
    "    \"\"\"\n",
    "    Maps a list of numbers to a dictionary that assigns each unique number the position of its first occurrence.\n",
    "\n",
    "    Example:\n",
    "    > map_first_occurrence([0,1,2,3,3,3,4])\n",
    "    {0: 0, 1: 1, 2: 2, 3: 3, 4: 6}\n",
    "\n",
    "    :param nums:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    return {num: i for i, num in enumerate(nums) if num is not None and num not in seen and not seen.add(num)}\n",
    "\n",
    "\n",
    "def pad_to_same_size(lists, padding_symbol):\n",
    "    maxlen = max([len(l) for l in lists])\n",
    "    return [l + (padding_symbol,) * (maxlen - len(l)) for l in lists]\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples, deprel_to_id, tokenizer, skip_index=-100):\n",
    "    # delete tokens with \"None\" head and their annotations\n",
    "    examples_tokens, examples_heads, examples_deprels = [], [], []\n",
    "    for sentence_id in range(len(examples[\"tokens\"])):\n",
    "        tt, hh, dd = strip_none_heads(examples, sentence_id)\n",
    "        examples_tokens.append(tt)\n",
    "        examples_heads.append(hh)\n",
    "        examples_deprels.append(dd)\n",
    "\n",
    "    tokenized_inputs = tokenizer(examples_tokens, truncation=True, is_split_into_words=True,\n",
    "                                 padding=True)  # get \"tokenizer\" from global variable\n",
    "    # tokenized_inputs is a dictionary with keys input_ids and attention_mask;\n",
    "    # each is a list (per sentence) of lists (per token).\n",
    "\n",
    "    remapped_heads = []  # these will be lists (per sentence) of lists (per token)\n",
    "    deprel_ids = []\n",
    "    tokens_representing_words = []\n",
    "    num_words: list[int] = []\n",
    "    maxlen_t2w = 0  # max length of a token_to_word_here list\n",
    "\n",
    "    for sentence_id, annotated_heads in enumerate(examples_heads):\n",
    "        deprels = examples_deprels[sentence_id]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=sentence_id)\n",
    "        word_pos_to_token_pos = map_first_occurrence(\n",
    "            word_ids)  # word-pos to first token-pos; both start at 0 for first word (actual) / first token (BOS)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        heads_here: list[int] = []\n",
    "        deprel_ids_here: list[int] = []\n",
    "\n",
    "        # list of token positions that map to words (first token of each word)\n",
    "        # token 0 -> word 0 (BOS)\n",
    "        tokens_representing_word_here: list[int] = [0]\n",
    "\n",
    "        for sentence_position, word_idx in enumerate(word_ids):\n",
    "            # Special tokens (BOS, EOS) have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                heads_here.append(skip_index)\n",
    "                deprel_ids_here.append(skip_index)\n",
    "\n",
    "            # We set the label for the first token of each word;\n",
    "            # subsequent tokens of the same word will have the same word_idx.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                if annotated_heads[word_idx] == \"None\":  # added by padding\n",
    "                    print(\"A 'None' head survived!\")\n",
    "                    sys.exit(0)\n",
    "                else:\n",
    "                    # Map HEAD annotation to position of first token of head word.\n",
    "                    # HEAD = 0 => map it to first token (BOS)\n",
    "                    # Otherwise, look up first token for HEAD-1 (HEAD is 1-based, word positions are 0-based)\n",
    "                    head_word_pos = int(annotated_heads[word_idx])\n",
    "                    head_token_pos = 0 if head_word_pos == 0 else word_pos_to_token_pos[head_word_pos - 1]\n",
    "\n",
    "                    heads_here.append(head_token_pos)\n",
    "                    deprel_ids_here.append(deprel_to_id[deprels[word_idx]])\n",
    "\n",
    "                    tokens_representing_word_here.append(sentence_position)  # first word is index 1; index 0 is BOS\n",
    "\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                heads_here.append(skip_index)\n",
    "                deprel_ids_here.append(skip_index)\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        remapped_heads.append(heads_here)\n",
    "        deprel_ids.append(deprel_ids_here)\n",
    "        tokens_representing_words.append(tokens_representing_word_here)\n",
    "\n",
    "        num_words.append(len(tokens_representing_word_here))\n",
    "        if len(tokens_representing_word_here) > maxlen_t2w:\n",
    "            maxlen_t2w = len(tokens_representing_word_here)\n",
    "\n",
    "    # pad t2w lists to same length\n",
    "    for t2w in tokens_representing_words:\n",
    "        t2w += [-1] * (maxlen_t2w - len(t2w))\n",
    "\n",
    "    tokenized_inputs[\"head\"] = remapped_heads\n",
    "    tokenized_inputs[\"deprel_ids\"] = deprel_ids\n",
    "    tokenized_inputs[\"tokens_representing_words\"] = tokens_representing_words\n",
    "    tokenized_inputs[\"num_words\"] = num_words\n",
    "    tokenized_inputs[\"tokenid_to_wordid\"] = [tokenized_inputs.word_ids(batch_index=i) for i in\n",
    "                                             range(len(examples_heads))]  # map token ID to word ID\n",
    "\n",
    "    return tokenized_inputs"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "id": "99990e156c16a774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:02:35.724704Z",
     "start_time": "2025-01-09T14:02:34.934321Z"
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "# test tokenization\n",
    "tokenized_inputs = tokenize_and_align_labels(train_dataset[:10], deprel_to_id, tokenizer)\n",
    "\n",
    "for i in range(10):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][i])  # i 是句子的索引\n",
    "    word_ids = tokenized_inputs[\"tokenid_to_wordid\"][i]\n",
    "\n",
    "    print(f\"Example {i + 1}\")\n",
    "    print(f\"{'Token':<15}{'Token ID':<10}{'Head':<10}{'Deprel':<15}{'Word Mapping':<15}\")\n",
    "    for j, token in enumerate(tokens):\n",
    "        if token == tokenizer.pad_token:\n",
    "            break\n",
    "        token_id = tokenized_inputs[\"input_ids\"][i][j]\n",
    "        head = tokenized_inputs[\"head\"][i][j]\n",
    "        deprel = all_deprels[tokenized_inputs[\"deprel_ids\"][i][j]] if tokenized_inputs[\"deprel_ids\"][i][\n",
    "                                                                          j] != -100 else \"None\"\n",
    "        word_mapping = word_ids[j]\n",
    "\n",
    "        token_str = token if token else \"None\"\n",
    "        token_id = str(token_id)\n",
    "        head_str = str(head)\n",
    "        deprel_str = deprel if deprel else \"None\"\n",
    "        word_mapping_str = str(word_mapping) if word_mapping is not None else \"None\"\n",
    "\n",
    "        print(f\"{token_str:<15}{token_id:<10}{head_str:<10}{deprel_str:<15}{word_mapping_str:<15}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WangEntang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁Al            884       0         root           0              \n",
      "▁-             20        1         punct          1              \n",
      "▁Zaman         53113     1         flat           2              \n",
      "▁:             152       1         punct          3              \n",
      "▁American      15672     6         amod           4              \n",
      "▁forces        84616     7         nsubj          5              \n",
      "▁killed        152388    1         parataxis      6              \n",
      "▁Sha           7224      7         obj            7              \n",
      "ikh            41336     -100      None           7              \n",
      "▁Abdullah      34490     8         flat           8              \n",
      "▁al            144       8         flat           9              \n",
      "▁-             20        8         punct          10             \n",
      "▁Ani           32340     8         flat           11             \n",
      "▁              6         8         punct          12             \n",
      ",              4         -100      None           12             \n",
      "▁the           70        17        det            13             \n",
      "▁prea          19542     8         appos          14             \n",
      "cher           5372      -100      None           14             \n",
      "▁at            99        21        case           15             \n",
      "▁the           70        21        det            16             \n",
      "▁mos           7304      7         obl            17             \n",
      "que            944       -100      None           17             \n",
      "▁in            23        25        case           18             \n",
      "▁the           70        25        det            19             \n",
      "▁town          59444     21        nmod           20             \n",
      "▁of            111       27        case           21             \n",
      "▁Qa            16785     25        nmod           22             \n",
      "im             464       -100      None           22             \n",
      "▁              6         25        punct          23             \n",
      ",              4         -100      None           23             \n",
      "▁near          43573     35        case           24             \n",
      "▁the           70        35        det            25             \n",
      "▁Syria         51712     35        amod           26             \n",
      "n              19        -100      None           26             \n",
      "▁border        132988    25        nmod           27             \n",
      "▁              6         1         punct          28             \n",
      ".              5         -100      None           28             \n",
      "</s>           2         -100      None           None           \n",
      "Example 2\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁[             378       12        punct          0              \n",
      "▁This          3293      3         det            1              \n",
      "▁ki            200       12        nsubj          2              \n",
      "lling          30319     -100      None           2              \n",
      "▁of            111       9         case           3              \n",
      "▁a             10        9         det            4              \n",
      "▁respect       15072     9         amod           5              \n",
      "ed             297       -100      None           5              \n",
      "▁cleric        181186    3         nmod           6              \n",
      "▁will          1221      12        aux            7              \n",
      "▁be            186       12        aux            8              \n",
      "▁causing       216806    0         root           9              \n",
      "▁us            1821      12        iobj           10             \n",
      "▁trouble       63134     12        obj            11             \n",
      "▁for           100       16        case           12             \n",
      "▁years         5369      12        obl            13             \n",
      "▁to            47        18        mark           14             \n",
      "▁come          1380      16        acl            15             \n",
      "▁              6         12        punct          16             \n",
      ".              5         -100      None           16             \n",
      "▁]             10114     12        punct          17             \n",
      "</s>           2         -100      None           None           \n",
      "Example 3\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁D             391       0         root           0              \n",
      "PA             12236     -100      None           0              \n",
      "▁:             152       1         punct          1              \n",
      "▁Iraq          69496     6         amod           2              \n",
      "i              14        -100      None           2              \n",
      "▁authorities   207048    7         nsubj          3              \n",
      "▁announced     171530    1         parataxis      4              \n",
      "▁that          450       11        mark           5              \n",
      "▁they          1836      11        nsubj          6              \n",
      "▁had           1902      11        aux            7              \n",
      "▁bu            373       7         ccomp          8              \n",
      "sted           14437     -100      None           8              \n",
      "▁up            1257      11        compound:prt   9              \n",
      "▁3             138       16        nummod         10             \n",
      "▁terrorist     42998     16        amod           11             \n",
      "▁cell          38750     11        obj            12             \n",
      "s              7         -100      None           12             \n",
      "▁operating     172852    16        acl            13             \n",
      "▁in            23        20        case           14             \n",
      "▁Bagh          177161    18        obl            15             \n",
      "dad            12409     -100      None           15             \n",
      "▁              6         1         punct          16             \n",
      ".              5         -100      None           16             \n",
      "</s>           2         -100      None           None           \n",
      "Example 4\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁Two           32964     6         nsubj:pass     0              \n",
      "▁of            111       3         case           1              \n",
      "▁them          2856      1         nmod           2              \n",
      "▁were          3542      6         aux            3              \n",
      "▁being         8035      6         aux:pass       4              \n",
      "▁run           11675     0         root           5              \n",
      "▁by            390       9         case           6              \n",
      "▁2             116       9         nummod         7              \n",
      "▁official      51521     6         obl            8              \n",
      "s              7         -100      None           8              \n",
      "▁of            111       13        case           9              \n",
      "▁the           70        13        det            10             \n",
      "▁Ministry      185236    9         nmod           11             \n",
      "▁of            111       16        case           12             \n",
      "▁the           70        16        det            13             \n",
      "▁Interior      123024    13        nmod           14             \n",
      "▁!             711       6         punct          15             \n",
      "</s>           2         -100      None           None           \n",
      "Example 5\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁The           581       2         det            0              \n",
      "▁Mo            2501      7         nsubj          1              \n",
      "I              568       -100      None           1              \n",
      "▁in            23        5         case           2              \n",
      "▁Iraq          69496     2         nmod           3              \n",
      "▁is            83        7         cop            4              \n",
      "▁equivalent    183234    0         root           5              \n",
      "▁to            47        11        case           6              \n",
      "▁the           70        11        det            7              \n",
      "▁US            7082      11        compound       8              \n",
      "▁FBI           83085     7         obl            9              \n",
      "▁              6         7         punct          10             \n",
      ",              4         -100      None           10             \n",
      "▁so            221       19        advmod         11             \n",
      "▁this          903       17        nsubj          12             \n",
      "▁would         2806      17        aux            13             \n",
      "▁be            186       7         parataxis      14             \n",
      "▁like          1884      19        mark           15             \n",
      "▁having        19441     17        advcl          16             \n",
      "▁J             821       29        nsubj          17             \n",
      ".              5         -100      None           17             \n",
      "▁Edgar         111954    20        flat           18             \n",
      "▁Ho            2291      20        flat           19             \n",
      "over           5465      -100      None           19             \n",
      "▁un            51        29        advmod         20             \n",
      "wit            14817     -100      None           20             \n",
      "ting           1916      -100      None           20             \n",
      "ly             538       -100      None           20             \n",
      "▁employ        187016    19        ccomp          21             \n",
      "▁at            99        33        case           22             \n",
      "▁a             10        33        det            23             \n",
      "▁high          11192     33        amod           24             \n",
      "▁level         17366     29        obl            25             \n",
      "▁members       43032     29        obj            26             \n",
      "▁of            111       39        case           27             \n",
      "▁the           70        39        det            28             \n",
      "▁Weather       214526    39        compound       29             \n",
      "men            1055      -100      None           29             \n",
      "▁bomb          54330     34        nmod           30             \n",
      "ers            1314      -100      None           30             \n",
      "▁back          4420      44        advmod         31             \n",
      "▁in            23        44        case           32             \n",
      "▁the           70        44        det            33             \n",
      "▁1960          23936     29        obl            34             \n",
      "s              7         -100      None           34             \n",
      "▁              6         7         punct          35             \n",
      ".              5         -100      None           35             \n",
      "</s>           2         -100      None           None           \n",
      "Example 6\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁The           581       2         det            0              \n",
      "▁third         50960     5         nsubj:pass     1              \n",
      "▁was           509       5         aux            2              \n",
      "▁being         8035      5         aux:pass       3              \n",
      "▁run           11675     0         root           4              \n",
      "▁by            390       8         case           5              \n",
      "▁the           70        8         det            6              \n",
      "▁head          10336     5         obl            7              \n",
      "▁of            111       12        case           8              \n",
      "▁an            142       12        det            9              \n",
      "▁investment    77021     12        compound       10             \n",
      "▁firm          11037     8         nmod           11             \n",
      "▁              6         5         punct          12             \n",
      ".              5         -100      None           12             \n",
      "</s>           2         -100      None           None           \n",
      "Example 7\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁You           2583      2         nsubj          0              \n",
      "▁wonder        32195     0         root           1              \n",
      "▁if            2174      6         mark           2              \n",
      "▁he            764       6         nsubj          3              \n",
      "▁was           509       6         aux            4              \n",
      "▁manipula      45258     2         ccomp          5              \n",
      "ting           1916      -100      None           5              \n",
      "▁the           70        9         det            6              \n",
      "▁market        16839     6         obj            7              \n",
      "▁with          678       14        case           8              \n",
      "▁his           1919      14        nmod:poss      9              \n",
      "▁bomb          54330     14        compound       10             \n",
      "ing            214       -100      None           10             \n",
      "▁target        30388     6         obl            11             \n",
      "s              7         -100      None           11             \n",
      "▁              6         2         punct          12             \n",
      ".              5         -100      None           12             \n",
      "</s>           2         -100      None           None           \n",
      "Example 8\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁The           581       2         det            0              \n",
      "▁cell          38750     5         nsubj          1              \n",
      "s              7         -100      None           1              \n",
      "▁were          3542      5         aux            2              \n",
      "▁operating     172852    0         root           3              \n",
      "▁in            23        15        case           4              \n",
      "▁the           70        15        det            5              \n",
      "▁Ghazal        220428    15        compound       6              \n",
      "iyah           62776     -100      None           6              \n",
      "▁and           136       13        cc             7              \n",
      "▁al            144       13        compound       8              \n",
      "▁-             20        13        punct          9              \n",
      "▁Ji            3291      8         conj           10             \n",
      "had            8408      -100      None           10             \n",
      "▁district      103724    5         obl            11             \n",
      "s              7         -100      None           11             \n",
      "▁of            111       19        case           12             \n",
      "▁the           70        19        det            13             \n",
      "▁capital       10323     15        nmod           14             \n",
      "▁              6         5         punct          15             \n",
      ".              5         -100      None           15             \n",
      "</s>           2         -100      None           None           \n",
      "Example 9\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁Although      106073    8         mark           0              \n",
      "▁the           70        3         det            1              \n",
      "▁ann           3398      8         nsubj:pass     2              \n",
      "ounce          85018     -100      None           2              \n",
      "ment           674       -100      None           2              \n",
      "▁was           509       8         aux:pass       3              \n",
      "▁probably      31895     8         advmod         4              \n",
      "▁made          7228      28        advcl          5              \n",
      "▁to            47        10        mark           6              \n",
      "▁show          7639      8         xcomp          7              \n",
      "▁progress      42658     10        obj            8              \n",
      "▁in            23        13        mark           9              \n",
      "▁identify      135812    11        acl            10             \n",
      "ing            214       -100      None           10             \n",
      "▁and           136       16        cc             11             \n",
      "▁breaking      116987    13        conj           12             \n",
      "▁up            1257      16        compound:prt   13             \n",
      "▁terror        20870     19        compound       14             \n",
      "▁cell          38750     13        obj            15             \n",
      "s              7         -100      None           15             \n",
      "▁              6         28        punct          16             \n",
      ",              4         -100      None           16             \n",
      "▁I             87        28        nsubj          17             \n",
      "▁do            54        28        aux            18             \n",
      "▁n             653       28        advmod         19             \n",
      "'              25        -100      None           19             \n",
      "t              18        -100      None           19             \n",
      "▁find          7413      0         root           20             \n",
      "▁the           70        30        det            21             \n",
      "▁news          7123      28        obj            22             \n",
      "▁that          450       36        mark           23             \n",
      "▁the           70        33        det            24             \n",
      "▁Baat          160062    36        nsubj          25             \n",
      "hist           49063     -100      None           25             \n",
      "s              7         -100      None           25             \n",
      "▁continue      21342     30        acl            26             \n",
      "▁to            47        38        mark           27             \n",
      "▁penetra       75076     36        xcomp          28             \n",
      "te             67        -100      None           28             \n",
      "▁the           70        43        det            29             \n",
      "▁Iraq          69496     43        amod           30             \n",
      "i              14        -100      None           30             \n",
      "▁government    27759     38        obj            31             \n",
      "▁very          4552      45        advmod         32             \n",
      "▁hope          15673     28        xcomp          33             \n",
      "ful            7844      -100      None           33             \n",
      "▁              6         28        punct          34             \n",
      ".              5         -100      None           34             \n",
      "</s>           2         -100      None           None           \n",
      "Example 10\n",
      "Token          Token ID  Head      Deprel         Word Mapping   \n",
      "<s>            0         -100      None           None           \n",
      "▁It            1650      2         nsubj          0              \n",
      "▁remind        98911     0         root           1              \n",
      "s              7         -100      None           1              \n",
      "▁me            163       2         obj            2              \n",
      "▁too           5792      6         advmod         3              \n",
      "▁much          5045      2         advmod         4              \n",
      "▁of            111       11        case           5              \n",
      "▁the           70        11        det            6              \n",
      "▁AR            13685     11        compound       7              \n",
      "VN             40711     -100      None           7              \n",
      "▁officer       93324     2         obl            8              \n",
      "s              7         -100      None           8              \n",
      "▁who           2750      17        nsubj          9              \n",
      "▁were          3542      17        aux            10             \n",
      "▁secret        23410     17        advmod         11             \n",
      "ly             538       -100      None           11             \n",
      "▁working       20697     11        acl:relcl      12             \n",
      "▁for           100       21        case           13             \n",
      "▁the           70        21        det            14             \n",
      "▁other         3789      21        amod           15             \n",
      "▁side          5609      17        obl            16             \n",
      "▁in            23        23        case           17             \n",
      "▁Vietnam       39272     17        obl            18             \n",
      "▁              6         2         punct          19             \n",
      ".              5         -100      None           19             \n",
      "</s>           2         -100      None           None           \n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:02:40.423607Z",
     "start_time": "2025-01-09T14:02:36.250483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# tokenized dataset and construct dataloader\n",
    "train_tokenized_inputs = tokenize_and_align_labels(train_dataset[:], deprel_to_id, tokenizer)\n",
    "# Convert BatchEncoding to Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized_inputs.data)\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'head', 'deprel_ids'])\n",
    "print(train_dataset)\n",
    "\n",
    "valid_tokenized_inputs = tokenize_and_align_labels(valid_dataset[:], deprel_to_id, tokenizer)\n",
    "valid_dataset = Dataset.from_dict(valid_tokenized_inputs.data)\n",
    "valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'head', 'deprel_ids'])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32)\n",
    "next(iter(train_dataloader))"
   ],
   "id": "9bb1acfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'head', 'deprel_ids', 'tokens_representing_words', 'num_words', 'tokenid_to_wordid'],\n",
      "    num_rows: 12543\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   884,    20,  ...,     1,     1,     1],\n",
       "         [    0,   378,  3293,  ...,     1,     1,     1],\n",
       "         [    0,   391, 12236,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,   581,   262,  ...,     1,     1,     1],\n",
       "         [    0, 56645, 14508,  ...,     1,     1,     1],\n",
       "         [    0,  1529,    83,  ...,     1,     1,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'head': tensor([[-100,    0,    1,  ..., -100, -100, -100],\n",
       "         [-100,   12,    3,  ..., -100, -100, -100],\n",
       "         [-100,    0, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100,    6,    6,  ..., -100, -100, -100],\n",
       "         [-100,    7, -100,  ..., -100, -100, -100],\n",
       "         [-100,   10,   10,  ..., -100, -100, -100]]),\n",
       " 'deprel_ids': tensor([[-100,   63,   61,  ..., -100, -100, -100],\n",
       "         [-100,   61,   27,  ..., -100, -100, -100],\n",
       "         [-100,   63, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100,   27,   16,  ..., -100, -100, -100],\n",
       "         [-100,   50, -100,  ..., -100, -100, -100],\n",
       "         [-100,   48,   22,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:02:41.557727Z",
     "start_time": "2025-01-09T14:02:41.546588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DependencyParserModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size=768,  # xlm-roberta-base hidden size\n",
    "                 edge_mlp_dim=500,  # Dozat&Manning recommend 500\n",
    "                 label_mlp_dim=100,  # Dozat&Manning recommend 100\n",
    "                 num_labels=len(all_deprels),  # number of dependency labels\n",
    "                 edge_predicting=True,\n",
    "                 label_predicting=False,\n",
    "                 dropout_prob=0.33\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.edge_predicting = edge_predicting\n",
    "        self.label_predicting = label_predicting\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # Load pre-trained XLM-RoBERTa model\n",
    "        self.roberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "        # Freeze RoBERTa parameters\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define MLP for edge head and dependency projections\n",
    "        if self.edge_predicting:\n",
    "            self.edge_mlp_head = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, edge_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "            self.edge_mlp_dep = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, edge_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "\n",
    "            # Define U1 and u2 for edge scoring\n",
    "            self.U1 = torch.nn.Parameter(torch.empty(edge_mlp_dim, edge_mlp_dim))\n",
    "            self.u2 = torch.nn.Parameter(torch.empty(edge_mlp_dim))\n",
    "            # Apply Xavier initialization\n",
    "            torch.nn.init.xavier_uniform_(self.U1)\n",
    "            torch.nn.init.xavier_uniform_(self.u2.unsqueeze(0))\n",
    "            self.u2.squeeze(0)\n",
    "\n",
    "        # Extra: Edge labels predicting\n",
    "        # Define MLP for label head and dependency projections\n",
    "        if self.label_predicting:\n",
    "            self.label_mlp_head = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, label_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "            self.label_mlp_dep = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, label_mlp_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(dropout_prob)\n",
    "            )\n",
    "\n",
    "            # Define parameters for label scoring\n",
    "            self.U_label = torch.nn.Parameter(torch.empty(label_mlp_dim, label_mlp_dim))\n",
    "            self.W1_label = torch.nn.Parameter(torch.empty(label_mlp_dim, num_labels))\n",
    "            self.W2_label = torch.nn.Parameter(torch.empty(label_mlp_dim, num_labels))\n",
    "            self.b_label = torch.nn.Parameter(torch.empty(num_labels))\n",
    "\n",
    "            # Apply Xavier initialization\n",
    "            torch.nn.init.xavier_uniform_(self.U_label)\n",
    "            torch.nn.init.xavier_uniform_(self.W1_label)\n",
    "            torch.nn.init.xavier_uniform_(self.W2_label)\n",
    "            torch.nn.init.xavier_uniform_(self.b_label.unsqueeze(0))\n",
    "            self.b_label.squeeze(0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        returns:\n",
    "          H_head (edge MLP): [batch_size, seq_len, edge_mlp_dim]\n",
    "          H_dep (edge MLP):  [batch_size, seq_len, edge_mlp_dim]\n",
    "          L_head (label MLP): [batch_size, seq_len, label_mlp_dim]\n",
    "          L_dep (label MLP):  [batch_size, seq_len, label_mlp_dim]\n",
    "        \"\"\"\n",
    "        # initialize as None\n",
    "        H_head = H_dep = L_head = L_dep = None\n",
    "\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Shape: (batch_size, seq_length, hidden_size: 768)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        if self.edge_predicting:\n",
    "            # edge MLP projections\n",
    "            H_head = self.edge_mlp_head(last_hidden_state)\n",
    "            H_dep = self.edge_mlp_dep(last_hidden_state)\n",
    "\n",
    "        if self.label_predicting:\n",
    "            # Label MLP projections\n",
    "            L_head = self.label_mlp_head(last_hidden_state)\n",
    "            L_dep = self.label_mlp_dep(last_hidden_state)\n",
    "\n",
    "        return H_head, H_dep, L_head, L_dep\n",
    "\n",
    "    def score_edges(self, H_head, H_dep):\n",
    "        # score[i, j] = H_head[i] * U1 * H_dep[j].T + H_head[i] * u2\n",
    "        # b: batch_size, s: seq_len, d: edge_mlp_dim\n",
    "        H_head_U1 = torch.einsum(\"bsd,dd->bsd\", H_head, self.U1)\n",
    "        H_head_U1_H_dep = torch.einsum(\"bim,bjm->bij\", H_head_U1, H_dep)\n",
    "        H_head_u2 = torch.einsum(\"bid,d->bi\", H_head, self.u2)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # Shape: (batch_size, seq_len, seq_len) + (batch_size, seq_len, 1) broadcasting\n",
    "        scores = H_head_U1_H_dep + H_head_u2.unsqueeze(2)\n",
    "        return scores\n",
    "\n",
    "    def score_labels(self, L_head, L_dep):\n",
    "        # Compute biaffine term: x1 U x2^T\n",
    "        L_head_U_L_dep = torch.einsum(\"bid,dd,bjd->bij\", L_head, self.U_label,\n",
    "                                      L_dep)  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # Compute linear terms: W1 x1 and W2 x2\n",
    "        W1_L_head = torch.einsum(\"bid,dn->bin\", L_head, self.W1_label)  # Shape: (batch_size, seq_len, num_labels)\n",
    "        W2_L_dep = torch.einsum(\"bid,dn->bin\", L_dep, self.W2_label)  # Shape: (batch_size, seq_len, num_labels)\n",
    "\n",
    "        # Add bias term\n",
    "        bias = self.b_label.unsqueeze(0).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 1, num_labels)\n",
    "\n",
    "        # Combine all terms and expand dimensions\n",
    "        # Shape: (batch_size, seq_len, seq_len, num_labels)\n",
    "        label_scores = L_head_U_L_dep.unsqueeze(-1) + W1_L_head.unsqueeze(2) + W2_L_dep.unsqueeze(1) + bias\n",
    "\n",
    "        return label_scores"
   ],
   "id": "d90ff586065664ae",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:02:41.571019Z",
     "start_time": "2025-01-09T14:02:41.562714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "# train the model\n",
    "def train(model, train_dataloader, valid_dataloader, device, num_epochs=5, lr=2e-3, weight_decay=1e-2, alpha=1, beta=1):\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"dependency-parsing\", name=\"dependency-parsing\", config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\": beta\n",
    "    })\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # ignore the padding tokens\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    valid_dataset = next(iter(valid_dataloader))\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    print(\"batch num\", len(train_dataloader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            x = data[\"input_ids\"].to(device)\n",
    "            mask = data[\"attention_mask\"].to(device)\n",
    "            head = data[\"head\"].to(device)\n",
    "\n",
    "            # Extra: Train edge labels predicting\n",
    "            if model.label_predicting:\n",
    "                label = data[\"deprel_ids\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            H_head, H_dep, L_head, L_dep = model(x, mask)\n",
    "\n",
    "            edge_scores = model.score_edges(H_head, H_dep)  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "            # Extra: Train edge labels predicting\n",
    "            if model.label_predicting:\n",
    "                label_scores = model.score_labels(L_head, L_dep)  # Shape: (batch_size, seq_len, seq_len, num_labels)\n",
    "\n",
    "            loss_edge = criterion(\n",
    "                edge_scores.view(-1, edge_scores.size(-1)),  # Shape: (batch_size * seq_len, seq_len)\n",
    "                head.view(-1)  # Shape: (batch_size, seq_len)\n",
    "            )\n",
    "\n",
    "            # Extra: Train edge labels predicting\n",
    "            loss_label = 0\n",
    "            if model.label_predicting:\n",
    "                # Shape: (batch_size, seq_len, seq_len, num_labels)\n",
    "                batch_size, n, _, num_labels = label_scores.shape\n",
    "\n",
    "                # batch_indices: (batch_size, 1)\n",
    "                # dep_indices:   (1,   n)\n",
    "                batch_indices = torch.arange(batch_size).unsqueeze(1).to(device)  # batch index\n",
    "                dep_indices = torch.arange(n).unsqueeze(0).to(device)  # dep index\n",
    "\n",
    "                # gather label_scores according to the gold head\n",
    "                # Shape: (batch_size, seq_len, num_labels)\n",
    "                label_scores_for_gold_edge = label_scores[batch_indices, dep_indices, head, :]\n",
    "\n",
    "                loss_label = criterion(label_scores_for_gold_edge.view(-1, num_labels), label.view(-1))\n",
    "\n",
    "            loss = alpha * loss_edge + beta * loss_label\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # log the loss curve\n",
    "            total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch}, Iteration {i}, Loss: {loss.item()}\")\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                # verify the model\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    x = valid_dataset[\"input_ids\"].to(device)\n",
    "                    mask = valid_dataset[\"attention_mask\"].to(device)\n",
    "                    head = valid_dataset[\"head\"].to(device)\n",
    "\n",
    "                    H_head, H_dep, L_head, L_dep = model(x, mask)\n",
    "                    edge_scores = model.score_edges(H_head, H_dep)\n",
    "\n",
    "                    valid_loss = criterion(edge_scores.view(-1, edge_scores.size(-1)), head.view(-1))\n",
    "                    print(f\"Epoch {epoch}, Iteration {i}, Valid Loss: {valid_loss}\")\n",
    "                    wandb.log({\"valid_loss\": valid_loss.item()})\n",
    "                model.train()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "        # Evaluate Head prediction\n",
    "        model.eval()\n",
    "        total_count = 0\n",
    "        total_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valid_dataloader):\n",
    "                x = data[\"input_ids\"].to(device)\n",
    "                mask = data[\"attention_mask\"].to(device)\n",
    "                head = data[\"head\"].to(device)\n",
    "\n",
    "                H_head, H_dep, L_head, L_dep = model(x, mask)\n",
    "                edge_scores = model.score_edges(H_head, H_dep)  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "                # best score heads\n",
    "                predicted_heads = edge_scores.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "\n",
    "                # construct valid mask\n",
    "                valid_positions = (head != -100) & (mask == 1)  # (batch_size, seq_len)\n",
    "\n",
    "                # count accurate predictions\n",
    "                total_correct += ((predicted_heads == head) & valid_positions).sum().item()\n",
    "                total_count += valid_positions.sum().item()\n",
    "\n",
    "        model.train()\n",
    "        wandb.log({\"head_tagging_accuracy\": valid_loss.item()})\n",
    "\n",
    "    torch.save(model.state_dict(), \"dependency_parser.pth\")"
   ],
   "id": "c0a34ab69953ac7f",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:04:35.582593Z",
     "start_time": "2025-01-09T14:02:42.603003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = DependencyParserModel()\n",
    "\n",
    "train(model, train_dataloader, valid_dataloader, device, lr=1e-3, num_epochs=10)"
   ],
   "id": "d296b44d6de0f75f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WangEntang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch num 392\n",
      "Epoch 0, Iteration 0, Loss: 5.301342487335205\n",
      "Epoch 0, Iteration 1, Loss: 5.2024030685424805\n",
      "Epoch 0, Iteration 2, Loss: 5.091856002807617\n",
      "Epoch 0, Iteration 3, Loss: 5.038632869720459\n",
      "Epoch 0, Iteration 4, Loss: 4.88973331451416\n",
      "Epoch 0, Iteration 5, Loss: 4.840819358825684\n",
      "Epoch 0, Iteration 6, Loss: 4.679852485656738\n",
      "Epoch 0, Iteration 7, Loss: 4.613685607910156\n",
      "Epoch 0, Iteration 8, Loss: 4.587449073791504\n",
      "Epoch 0, Iteration 9, Loss: 4.483029365539551\n",
      "Epoch 0, Iteration 10, Loss: 4.398560523986816\n",
      "Epoch 0, Iteration 11, Loss: 4.249656677246094\n",
      "Epoch 0, Iteration 12, Loss: 4.075197219848633\n",
      "Epoch 0, Iteration 13, Loss: 4.199347972869873\n",
      "Epoch 0, Iteration 14, Loss: 3.9807143211364746\n",
      "Epoch 0, Iteration 15, Loss: 3.9997308254241943\n",
      "Epoch 0, Iteration 16, Loss: 3.808586359024048\n",
      "Epoch 0, Iteration 17, Loss: 3.9000091552734375\n",
      "Epoch 0, Iteration 18, Loss: 3.731304168701172\n",
      "Epoch 0, Iteration 19, Loss: 3.666053295135498\n",
      "Epoch 0, Iteration 20, Loss: 3.627490282058716\n",
      "Epoch 0, Iteration 21, Loss: 3.590188980102539\n",
      "Epoch 0, Iteration 22, Loss: 3.419027328491211\n",
      "Epoch 0, Iteration 23, Loss: 3.292759418487549\n",
      "Epoch 0, Iteration 24, Loss: 3.305698871612549\n",
      "Epoch 0, Iteration 25, Loss: 3.2726776599884033\n",
      "Epoch 0, Iteration 26, Loss: 3.4785828590393066\n",
      "Epoch 0, Iteration 27, Loss: 3.2280709743499756\n",
      "Epoch 0, Iteration 28, Loss: 3.1998238563537598\n",
      "Epoch 0, Iteration 29, Loss: 3.077390432357788\n",
      "Epoch 0, Iteration 30, Loss: 3.00095272064209\n",
      "Epoch 0, Iteration 31, Loss: 3.012566089630127\n",
      "Epoch 0, Iteration 32, Loss: 2.931203842163086\n",
      "Epoch 0, Iteration 33, Loss: 2.8965036869049072\n",
      "Epoch 0, Iteration 34, Loss: 3.1094579696655273\n",
      "Epoch 0, Iteration 35, Loss: 3.1661312580108643\n",
      "Epoch 0, Iteration 36, Loss: 2.9670803546905518\n",
      "Epoch 0, Iteration 37, Loss: 2.9114701747894287\n",
      "Epoch 0, Iteration 38, Loss: 2.9320015907287598\n",
      "Epoch 0, Iteration 39, Loss: 2.996936559677124\n",
      "Epoch 0, Iteration 40, Loss: 3.0317771434783936\n",
      "Epoch 0, Iteration 41, Loss: 2.9212095737457275\n",
      "Epoch 0, Iteration 42, Loss: 3.211667776107788\n",
      "Epoch 0, Iteration 43, Loss: 2.9574475288391113\n",
      "Epoch 0, Iteration 44, Loss: 3.0795018672943115\n",
      "Epoch 0, Iteration 45, Loss: 2.8359274864196777\n",
      "Epoch 0, Iteration 46, Loss: 3.026718854904175\n",
      "Epoch 0, Iteration 47, Loss: 3.0111734867095947\n",
      "Epoch 0, Iteration 48, Loss: 3.0332541465759277\n",
      "Epoch 0, Iteration 49, Loss: 2.914290189743042\n",
      "Epoch 0, Iteration 50, Loss: 2.8542985916137695\n",
      "Epoch 0, Iteration 50, Valid Loss: 2.7438228130340576\n",
      "Epoch 0, Iteration 51, Loss: 2.7312419414520264\n",
      "Epoch 0, Iteration 52, Loss: 3.062718391418457\n",
      "Epoch 0, Iteration 53, Loss: 2.6046464443206787\n",
      "Epoch 0, Iteration 54, Loss: 2.7669148445129395\n",
      "Epoch 0, Iteration 55, Loss: 2.6687026023864746\n",
      "Epoch 0, Iteration 56, Loss: 2.5590550899505615\n",
      "Epoch 0, Iteration 57, Loss: 2.590651512145996\n",
      "Epoch 0, Iteration 58, Loss: 2.3881075382232666\n",
      "Epoch 0, Iteration 59, Loss: 2.764352321624756\n",
      "Epoch 0, Iteration 60, Loss: 2.5725200176239014\n",
      "Epoch 0, Iteration 61, Loss: 2.476736545562744\n",
      "Epoch 0, Iteration 62, Loss: 2.4824917316436768\n",
      "Epoch 0, Iteration 63, Loss: 2.489856481552124\n",
      "Epoch 0, Iteration 64, Loss: 2.4141197204589844\n",
      "Epoch 0, Iteration 65, Loss: 2.6087284088134766\n",
      "Epoch 0, Iteration 66, Loss: 2.7645039558410645\n",
      "Epoch 0, Iteration 67, Loss: 2.6212375164031982\n",
      "Epoch 0, Iteration 68, Loss: 2.607701539993286\n",
      "Epoch 0, Iteration 69, Loss: 2.711066484451294\n",
      "Epoch 0, Iteration 70, Loss: 2.82538104057312\n",
      "Epoch 0, Iteration 71, Loss: 2.488370418548584\n",
      "Epoch 0, Iteration 72, Loss: 2.508263111114502\n",
      "Epoch 0, Iteration 73, Loss: 2.367891311645508\n",
      "Epoch 0, Iteration 74, Loss: 2.435106039047241\n",
      "Epoch 0, Iteration 75, Loss: 2.286566972732544\n",
      "Epoch 0, Iteration 76, Loss: 2.540757179260254\n",
      "Epoch 0, Iteration 77, Loss: 2.3875787258148193\n",
      "Epoch 0, Iteration 78, Loss: 2.9345648288726807\n",
      "Epoch 0, Iteration 79, Loss: 2.8535850048065186\n",
      "Epoch 0, Iteration 80, Loss: 2.317364454269409\n",
      "Epoch 0, Iteration 81, Loss: 2.3621487617492676\n",
      "Epoch 0, Iteration 82, Loss: 2.434690237045288\n",
      "Epoch 0, Iteration 83, Loss: 2.3714122772216797\n",
      "Epoch 0, Iteration 84, Loss: 2.7083523273468018\n",
      "Epoch 0, Iteration 85, Loss: 2.626178741455078\n",
      "Epoch 0, Iteration 86, Loss: 2.3645036220550537\n",
      "Epoch 0, Iteration 87, Loss: 2.41296648979187\n",
      "Epoch 0, Iteration 88, Loss: 2.2594988346099854\n",
      "Epoch 0, Iteration 89, Loss: 2.4992241859436035\n",
      "Epoch 0, Iteration 90, Loss: 2.2394497394561768\n",
      "Epoch 0, Iteration 91, Loss: 2.2836849689483643\n",
      "Epoch 0, Iteration 92, Loss: 2.2950234413146973\n",
      "Epoch 0, Iteration 93, Loss: 2.3743178844451904\n",
      "Epoch 0, Iteration 94, Loss: 2.64626145362854\n",
      "Epoch 0, Iteration 95, Loss: 2.5946621894836426\n",
      "Epoch 0, Iteration 96, Loss: 2.5674355030059814\n",
      "Epoch 0, Iteration 97, Loss: 2.455699920654297\n",
      "Epoch 0, Iteration 98, Loss: 2.575943946838379\n",
      "Epoch 0, Iteration 99, Loss: 2.4460530281066895\n",
      "Epoch 0, Iteration 100, Loss: 2.4397220611572266\n",
      "Epoch 0, Iteration 100, Valid Loss: 2.5850062370300293\n",
      "Epoch 0, Iteration 101, Loss: 2.4497804641723633\n",
      "Epoch 0, Iteration 102, Loss: 2.509589910507202\n",
      "Epoch 0, Iteration 103, Loss: 2.473050832748413\n",
      "Epoch 0, Iteration 104, Loss: 2.572366237640381\n",
      "Epoch 0, Iteration 105, Loss: 2.52901291847229\n",
      "Epoch 0, Iteration 106, Loss: 2.518662214279175\n",
      "Epoch 0, Iteration 107, Loss: 2.706648111343384\n",
      "Epoch 0, Iteration 108, Loss: 2.4676332473754883\n",
      "Epoch 0, Iteration 109, Loss: 2.5657846927642822\n",
      "Epoch 0, Iteration 110, Loss: 2.76700758934021\n",
      "Epoch 0, Iteration 111, Loss: 2.4641261100769043\n",
      "Epoch 0, Iteration 112, Loss: 2.606077194213867\n",
      "Epoch 0, Iteration 113, Loss: 2.4592952728271484\n",
      "Epoch 0, Iteration 114, Loss: 2.942396879196167\n",
      "Epoch 0, Iteration 115, Loss: 2.7781505584716797\n",
      "Epoch 0, Iteration 116, Loss: 2.586613893508911\n",
      "Epoch 0, Iteration 117, Loss: 2.4554383754730225\n",
      "Epoch 0, Iteration 118, Loss: 2.7135307788848877\n",
      "Epoch 0, Iteration 119, Loss: 2.691244125366211\n",
      "Epoch 0, Iteration 120, Loss: 2.3491604328155518\n",
      "Epoch 0, Iteration 121, Loss: 2.4070770740509033\n",
      "Epoch 0, Iteration 122, Loss: 2.424654722213745\n",
      "Epoch 0, Iteration 123, Loss: 2.389435291290283\n",
      "Epoch 0, Iteration 124, Loss: 2.519123077392578\n",
      "Epoch 0, Iteration 125, Loss: 2.1583027839660645\n",
      "Epoch 0, Iteration 126, Loss: 2.3462047576904297\n",
      "Epoch 0, Iteration 127, Loss: 2.4143893718719482\n",
      "Epoch 0, Iteration 128, Loss: 2.2909271717071533\n",
      "Epoch 0, Iteration 129, Loss: 2.4998364448547363\n",
      "Epoch 0, Iteration 130, Loss: 2.5526294708251953\n",
      "Epoch 0, Iteration 131, Loss: 2.528052568435669\n",
      "Epoch 0, Iteration 132, Loss: 2.3271148204803467\n",
      "Epoch 0, Iteration 133, Loss: 2.3656187057495117\n",
      "Epoch 0, Iteration 134, Loss: 2.152374505996704\n",
      "Epoch 0, Iteration 135, Loss: 2.340216875076294\n",
      "Epoch 0, Iteration 136, Loss: 2.442438840866089\n",
      "Epoch 0, Iteration 137, Loss: 2.437819242477417\n",
      "Epoch 0, Iteration 138, Loss: 2.3130457401275635\n",
      "Epoch 0, Iteration 139, Loss: 2.363863706588745\n",
      "Epoch 0, Iteration 140, Loss: 2.3677761554718018\n",
      "Epoch 0, Iteration 141, Loss: 2.608182668685913\n",
      "Epoch 0, Iteration 142, Loss: 2.281095266342163\n",
      "Epoch 0, Iteration 143, Loss: 2.533337116241455\n",
      "Epoch 0, Iteration 144, Loss: 2.334481954574585\n",
      "Epoch 0, Iteration 145, Loss: 2.341691017150879\n",
      "Epoch 0, Iteration 146, Loss: 2.6106109619140625\n",
      "Epoch 0, Iteration 147, Loss: 2.332024335861206\n",
      "Epoch 0, Iteration 148, Loss: 2.1460118293762207\n",
      "Epoch 0, Iteration 149, Loss: 2.3128390312194824\n",
      "Epoch 0, Iteration 150, Loss: 2.2824690341949463\n",
      "Epoch 0, Iteration 150, Valid Loss: 2.5117053985595703\n",
      "Epoch 0, Iteration 151, Loss: 2.6050119400024414\n",
      "Epoch 0, Iteration 152, Loss: 2.4450135231018066\n",
      "Epoch 0, Iteration 153, Loss: 2.185774087905884\n",
      "Epoch 0, Iteration 154, Loss: 2.379530191421509\n",
      "Epoch 0, Iteration 155, Loss: 2.366767644882202\n",
      "Epoch 0, Iteration 156, Loss: 2.397944927215576\n",
      "Epoch 0, Iteration 157, Loss: 2.6987478733062744\n",
      "Epoch 0, Iteration 158, Loss: 2.4701802730560303\n",
      "Epoch 0, Iteration 159, Loss: 2.602431297302246\n",
      "Epoch 0, Iteration 160, Loss: 2.412116527557373\n",
      "Epoch 0, Iteration 161, Loss: 2.4905991554260254\n",
      "Epoch 0, Iteration 162, Loss: 2.6300582885742188\n",
      "Epoch 0, Iteration 163, Loss: 2.443495988845825\n",
      "Epoch 0, Iteration 164, Loss: 2.0695154666900635\n",
      "Epoch 0, Iteration 165, Loss: 2.2734920978546143\n",
      "Epoch 0, Iteration 166, Loss: 2.400275230407715\n",
      "Epoch 0, Iteration 167, Loss: 2.5752720832824707\n",
      "Epoch 0, Iteration 168, Loss: 2.6578962802886963\n",
      "Epoch 0, Iteration 169, Loss: 2.3707892894744873\n",
      "Epoch 0, Iteration 170, Loss: 2.4247891902923584\n",
      "Epoch 0, Iteration 171, Loss: 2.3427915573120117\n",
      "Epoch 0, Iteration 172, Loss: 2.1989760398864746\n",
      "Epoch 0, Iteration 173, Loss: 2.5046231746673584\n",
      "Epoch 0, Iteration 174, Loss: 2.5393154621124268\n",
      "Epoch 0, Iteration 175, Loss: 2.548959493637085\n",
      "Epoch 0, Iteration 176, Loss: 2.112286329269409\n",
      "Epoch 0, Iteration 177, Loss: 2.52785587310791\n",
      "Epoch 0, Iteration 178, Loss: 2.4678542613983154\n",
      "Epoch 0, Iteration 179, Loss: 2.589205265045166\n",
      "Epoch 0, Iteration 180, Loss: 2.4160892963409424\n",
      "Epoch 0, Iteration 181, Loss: 2.412895441055298\n",
      "Epoch 0, Iteration 182, Loss: 2.4945051670074463\n",
      "Epoch 0, Iteration 183, Loss: 2.4389142990112305\n",
      "Epoch 0, Iteration 184, Loss: 2.215719699859619\n",
      "Epoch 0, Iteration 185, Loss: 2.535130739212036\n",
      "Epoch 0, Iteration 186, Loss: 2.3729944229125977\n",
      "Epoch 0, Iteration 187, Loss: 2.2867376804351807\n",
      "Epoch 0, Iteration 188, Loss: 2.4668993949890137\n",
      "Epoch 0, Iteration 189, Loss: 2.4587206840515137\n",
      "Epoch 0, Iteration 190, Loss: 2.8332366943359375\n",
      "Epoch 0, Iteration 191, Loss: 2.5977635383605957\n",
      "Epoch 0, Iteration 192, Loss: 2.5378293991088867\n",
      "Epoch 0, Iteration 193, Loss: 2.409755229949951\n",
      "Epoch 0, Iteration 194, Loss: 2.120903491973877\n",
      "Epoch 0, Iteration 195, Loss: 2.165266990661621\n",
      "Epoch 0, Iteration 196, Loss: 2.3790998458862305\n",
      "Epoch 0, Iteration 197, Loss: 2.3622114658355713\n",
      "Epoch 0, Iteration 198, Loss: 2.3846254348754883\n",
      "Epoch 0, Iteration 199, Loss: 2.5226101875305176\n",
      "Epoch 0, Iteration 200, Loss: 2.4907777309417725\n",
      "Epoch 0, Iteration 200, Valid Loss: 2.392343282699585\n",
      "Epoch 0, Iteration 201, Loss: 2.4353585243225098\n",
      "Epoch 0, Iteration 202, Loss: 2.492546319961548\n",
      "Epoch 0, Iteration 203, Loss: 2.317458391189575\n",
      "Epoch 0, Iteration 204, Loss: 2.3757591247558594\n",
      "Epoch 0, Iteration 205, Loss: 2.1234042644500732\n",
      "Epoch 0, Iteration 206, Loss: 2.16023850440979\n",
      "Epoch 0, Iteration 207, Loss: 2.2667155265808105\n",
      "Epoch 0, Iteration 208, Loss: 2.6897008419036865\n",
      "Epoch 0, Iteration 209, Loss: 2.563178539276123\n",
      "Epoch 0, Iteration 210, Loss: 2.8279175758361816\n",
      "Epoch 0, Iteration 211, Loss: 3.038966417312622\n",
      "Epoch 0, Iteration 212, Loss: 2.8616292476654053\n",
      "Epoch 0, Iteration 213, Loss: 2.7933969497680664\n",
      "Epoch 0, Iteration 214, Loss: 2.458878755569458\n",
      "Epoch 0, Iteration 215, Loss: 2.408607006072998\n",
      "Epoch 0, Iteration 216, Loss: 2.4299392700195312\n",
      "Epoch 0, Iteration 217, Loss: 2.4513297080993652\n",
      "Epoch 0, Iteration 218, Loss: 2.699286937713623\n",
      "Epoch 0, Iteration 219, Loss: 2.660865306854248\n",
      "Epoch 0, Iteration 220, Loss: 2.4989352226257324\n",
      "Epoch 0, Iteration 221, Loss: 2.457033634185791\n",
      "Epoch 0, Iteration 222, Loss: 2.470796585083008\n",
      "Epoch 0, Iteration 223, Loss: 2.519024610519409\n",
      "Epoch 0, Iteration 224, Loss: 2.324363946914673\n",
      "Epoch 0, Iteration 225, Loss: 2.45146107673645\n",
      "Epoch 0, Iteration 226, Loss: 2.4320783615112305\n",
      "Epoch 0, Iteration 227, Loss: 2.3331642150878906\n",
      "Epoch 0, Iteration 228, Loss: 2.1831564903259277\n",
      "Epoch 0, Iteration 229, Loss: 2.1047444343566895\n",
      "Epoch 0, Iteration 230, Loss: 2.1923420429229736\n",
      "Epoch 0, Iteration 231, Loss: 2.250762462615967\n",
      "Epoch 0, Iteration 232, Loss: 2.3424129486083984\n",
      "Epoch 0, Iteration 233, Loss: 2.7493975162506104\n",
      "Epoch 0, Iteration 234, Loss: 2.6539065837860107\n",
      "Epoch 0, Iteration 235, Loss: 1.983585000038147\n",
      "Epoch 0, Iteration 236, Loss: 2.271522283554077\n",
      "Epoch 0, Iteration 237, Loss: 2.3972864151000977\n",
      "Epoch 0, Iteration 238, Loss: 2.123087167739868\n",
      "Epoch 0, Iteration 239, Loss: 2.1947524547576904\n",
      "Epoch 0, Iteration 240, Loss: 1.8116765022277832\n",
      "Epoch 0, Iteration 241, Loss: 2.097313404083252\n",
      "Epoch 0, Iteration 242, Loss: 1.9898895025253296\n",
      "Epoch 0, Iteration 243, Loss: 2.1525752544403076\n",
      "Epoch 0, Iteration 244, Loss: 2.0738303661346436\n",
      "Epoch 0, Iteration 245, Loss: 2.041043996810913\n",
      "Epoch 0, Iteration 246, Loss: 2.140988349914551\n",
      "Epoch 0, Iteration 247, Loss: 1.9936476945877075\n",
      "Epoch 0, Iteration 248, Loss: 2.2845633029937744\n",
      "Epoch 0, Iteration 249, Loss: 2.0903818607330322\n",
      "Epoch 0, Iteration 250, Loss: 2.3310978412628174\n",
      "Epoch 0, Iteration 250, Valid Loss: 2.3001458644866943\n",
      "Epoch 0, Iteration 251, Loss: 2.322420835494995\n",
      "Epoch 0, Iteration 252, Loss: 2.100607395172119\n",
      "Epoch 0, Iteration 253, Loss: 2.3043160438537598\n",
      "Epoch 0, Iteration 254, Loss: 2.318267822265625\n",
      "Epoch 0, Iteration 255, Loss: 2.1173243522644043\n",
      "Epoch 0, Iteration 256, Loss: 2.462442398071289\n",
      "Epoch 0, Iteration 257, Loss: 2.096977710723877\n",
      "Epoch 0, Iteration 258, Loss: 2.3886477947235107\n",
      "Epoch 0, Iteration 259, Loss: 2.0032663345336914\n",
      "Epoch 0, Iteration 260, Loss: 2.197256565093994\n",
      "Epoch 0, Iteration 261, Loss: 2.4311904907226562\n",
      "Epoch 0, Iteration 262, Loss: 2.0351622104644775\n",
      "Epoch 0, Iteration 263, Loss: 2.2372043132781982\n",
      "Epoch 0, Iteration 264, Loss: 2.3135855197906494\n",
      "Epoch 0, Iteration 265, Loss: 2.3196299076080322\n",
      "Epoch 0, Iteration 266, Loss: 2.431205987930298\n",
      "Epoch 0, Iteration 267, Loss: 2.2817108631134033\n",
      "Epoch 0, Iteration 268, Loss: 2.2915971279144287\n",
      "Epoch 0, Iteration 269, Loss: 2.139660120010376\n",
      "Epoch 0, Iteration 270, Loss: 2.115009069442749\n",
      "Epoch 0, Iteration 271, Loss: 2.190633535385132\n",
      "Epoch 0, Iteration 272, Loss: 2.125035524368286\n",
      "Epoch 0, Iteration 273, Loss: 2.0042624473571777\n",
      "Epoch 0, Iteration 274, Loss: 1.9910763502120972\n",
      "Epoch 0, Iteration 275, Loss: 1.9017032384872437\n",
      "Epoch 0, Iteration 276, Loss: 2.0570175647735596\n",
      "Epoch 0, Iteration 277, Loss: 2.0153393745422363\n",
      "Epoch 0, Iteration 278, Loss: 2.042515754699707\n",
      "Epoch 0, Iteration 279, Loss: 2.066559314727783\n",
      "Epoch 0, Iteration 280, Loss: 1.9599494934082031\n",
      "Epoch 0, Iteration 281, Loss: 1.827108383178711\n",
      "Epoch 0, Iteration 282, Loss: 2.2588396072387695\n",
      "Epoch 0, Iteration 283, Loss: 2.083890199661255\n",
      "Epoch 0, Iteration 284, Loss: 2.0259764194488525\n",
      "Epoch 0, Iteration 285, Loss: 2.1447904109954834\n",
      "Epoch 0, Iteration 286, Loss: 1.9818768501281738\n",
      "Epoch 0, Iteration 287, Loss: 1.9225106239318848\n",
      "Epoch 0, Iteration 288, Loss: 1.9338550567626953\n",
      "Epoch 0, Iteration 289, Loss: 2.1250815391540527\n",
      "Epoch 0, Iteration 290, Loss: 2.023282766342163\n",
      "Epoch 0, Iteration 291, Loss: 2.1610891819000244\n",
      "Epoch 0, Iteration 292, Loss: 2.40193772315979\n",
      "Epoch 0, Iteration 293, Loss: 2.283637762069702\n",
      "Epoch 0, Iteration 294, Loss: 1.892772912979126\n",
      "Epoch 0, Iteration 295, Loss: 2.1377079486846924\n",
      "Epoch 0, Iteration 296, Loss: 2.489732027053833\n",
      "Epoch 0, Iteration 297, Loss: 2.2199301719665527\n",
      "Epoch 0, Iteration 298, Loss: 2.203545093536377\n",
      "Epoch 0, Iteration 299, Loss: 1.9267632961273193\n",
      "Epoch 0, Iteration 300, Loss: 2.033569574356079\n",
      "Epoch 0, Iteration 300, Valid Loss: 2.2040905952453613\n",
      "Epoch 0, Iteration 301, Loss: 2.246593475341797\n",
      "Epoch 0, Iteration 302, Loss: 1.8818459510803223\n",
      "Epoch 0, Iteration 303, Loss: 2.3381927013397217\n",
      "Epoch 0, Iteration 304, Loss: 2.0583083629608154\n",
      "Epoch 0, Iteration 305, Loss: 2.1031851768493652\n",
      "Epoch 0, Iteration 306, Loss: 2.103055477142334\n",
      "Epoch 0, Iteration 307, Loss: 1.9003764390945435\n",
      "Epoch 0, Iteration 308, Loss: 2.0219783782958984\n",
      "Epoch 0, Iteration 309, Loss: 2.0501949787139893\n",
      "Epoch 0, Iteration 310, Loss: 2.177978277206421\n",
      "Epoch 0, Iteration 311, Loss: 1.9700703620910645\n",
      "Epoch 0, Iteration 312, Loss: 1.9252674579620361\n",
      "Epoch 0, Iteration 313, Loss: 2.018533706665039\n",
      "Epoch 0, Iteration 314, Loss: 2.0029892921447754\n",
      "Epoch 0, Iteration 315, Loss: 1.9892985820770264\n",
      "Epoch 0, Iteration 316, Loss: 1.9790467023849487\n",
      "Epoch 0, Iteration 317, Loss: 1.9957683086395264\n",
      "Epoch 0, Iteration 318, Loss: 1.9729002714157104\n",
      "Epoch 0, Iteration 319, Loss: 2.0484836101531982\n",
      "Epoch 0, Iteration 320, Loss: 1.8748631477355957\n",
      "Epoch 0, Iteration 321, Loss: 1.992504596710205\n",
      "Epoch 0, Iteration 322, Loss: 1.855346441268921\n",
      "Epoch 0, Iteration 323, Loss: 1.8887325525283813\n",
      "Epoch 0, Iteration 324, Loss: 2.1380271911621094\n",
      "Epoch 0, Iteration 325, Loss: 2.02158522605896\n",
      "Epoch 0, Iteration 326, Loss: 1.933244228363037\n",
      "Epoch 0, Iteration 327, Loss: 2.007408380508423\n",
      "Epoch 0, Iteration 328, Loss: 1.9618009328842163\n",
      "Epoch 0, Iteration 329, Loss: 2.1645047664642334\n",
      "Epoch 0, Iteration 330, Loss: 1.7363258600234985\n",
      "Epoch 0, Iteration 331, Loss: 1.8524386882781982\n",
      "Epoch 0, Iteration 332, Loss: 2.1003096103668213\n",
      "Epoch 0, Iteration 333, Loss: 2.1613621711730957\n",
      "Epoch 0, Iteration 334, Loss: 1.9628052711486816\n",
      "Epoch 0, Iteration 335, Loss: 2.088409423828125\n",
      "Epoch 0, Iteration 336, Loss: 2.193127155303955\n",
      "Epoch 0, Iteration 337, Loss: 1.9358316659927368\n",
      "Epoch 0, Iteration 338, Loss: 2.223001718521118\n",
      "Epoch 0, Iteration 339, Loss: 1.9178154468536377\n",
      "Epoch 0, Iteration 340, Loss: 1.9277769327163696\n",
      "Epoch 0, Iteration 341, Loss: 2.0855588912963867\n",
      "Epoch 0, Iteration 342, Loss: 1.9076377153396606\n",
      "Epoch 0, Iteration 343, Loss: 1.9849907159805298\n",
      "Epoch 0, Iteration 344, Loss: 2.1265549659729004\n",
      "Epoch 0, Iteration 345, Loss: 2.1573033332824707\n",
      "Epoch 0, Iteration 346, Loss: 2.0551657676696777\n",
      "Epoch 0, Iteration 347, Loss: 1.9459407329559326\n",
      "Epoch 0, Iteration 348, Loss: 1.8303935527801514\n",
      "Epoch 0, Iteration 349, Loss: 2.110103130340576\n",
      "Epoch 0, Iteration 350, Loss: 1.8078690767288208\n",
      "Epoch 0, Iteration 350, Valid Loss: 2.138679027557373\n",
      "Epoch 0, Iteration 351, Loss: 2.067136526107788\n",
      "Epoch 0, Iteration 352, Loss: 2.022465229034424\n",
      "Epoch 0, Iteration 353, Loss: 2.135566234588623\n",
      "Epoch 0, Iteration 354, Loss: 2.0021510124206543\n",
      "Epoch 0, Iteration 355, Loss: 2.0636603832244873\n",
      "Epoch 0, Iteration 356, Loss: 2.0473029613494873\n",
      "Epoch 0, Iteration 357, Loss: 2.071887969970703\n",
      "Epoch 0, Iteration 358, Loss: 2.0416347980499268\n",
      "Epoch 0, Iteration 359, Loss: 1.9605414867401123\n",
      "Epoch 0, Iteration 360, Loss: 1.8080600500106812\n",
      "Epoch 0, Iteration 361, Loss: 1.8872593641281128\n",
      "Epoch 0, Iteration 362, Loss: 1.9361655712127686\n",
      "Epoch 0, Iteration 363, Loss: 1.9989430904388428\n",
      "Epoch 0, Iteration 364, Loss: 1.9594367742538452\n",
      "Epoch 0, Iteration 365, Loss: 2.30765962600708\n",
      "Epoch 0, Iteration 366, Loss: 2.205789566040039\n",
      "Epoch 0, Iteration 367, Loss: 2.139564275741577\n",
      "Epoch 0, Iteration 368, Loss: 2.0939013957977295\n",
      "Epoch 0, Iteration 369, Loss: 2.0744168758392334\n",
      "Epoch 0, Iteration 370, Loss: 2.142699718475342\n",
      "Epoch 0, Iteration 371, Loss: 2.1293585300445557\n",
      "Epoch 0, Iteration 372, Loss: 1.8243186473846436\n",
      "Epoch 0, Iteration 373, Loss: 1.914842128753662\n",
      "Epoch 0, Iteration 374, Loss: 2.079885482788086\n",
      "Epoch 0, Iteration 375, Loss: 1.91824471950531\n",
      "Epoch 0, Iteration 376, Loss: 2.2737960815429688\n",
      "Epoch 0, Iteration 377, Loss: 2.0097286701202393\n",
      "Epoch 0, Iteration 378, Loss: 1.9532920122146606\n",
      "Epoch 0, Iteration 379, Loss: 2.0928893089294434\n",
      "Epoch 0, Iteration 380, Loss: 1.9873939752578735\n",
      "Epoch 0, Iteration 381, Loss: 1.9434620141983032\n",
      "Epoch 0, Iteration 382, Loss: 1.8977915048599243\n",
      "Epoch 0, Iteration 383, Loss: 1.9550172090530396\n",
      "Epoch 0, Iteration 384, Loss: 2.0108649730682373\n",
      "Epoch 0, Iteration 385, Loss: 1.8662723302841187\n",
      "Epoch 0, Iteration 386, Loss: 1.9566552639007568\n",
      "Epoch 0, Iteration 387, Loss: 2.2005741596221924\n",
      "Epoch 0, Iteration 388, Loss: 2.0004045963287354\n",
      "Epoch 0, Iteration 389, Loss: 2.0686519145965576\n",
      "Epoch 0, Iteration 390, Loss: 2.143855333328247\n",
      "Epoch 0, Iteration 391, Loss: 2.030458688735962\n",
      "Epoch 1/10, Loss: 2.46371175135885\n",
      "Epoch 1, Iteration 0, Loss: 2.2060890197753906\n",
      "Epoch 1, Iteration 1, Loss: 2.4273455142974854\n",
      "Epoch 1, Iteration 2, Loss: 2.4249820709228516\n",
      "Epoch 1, Iteration 3, Loss: 2.2894651889801025\n",
      "Epoch 1, Iteration 4, Loss: 2.237640619277954\n",
      "Epoch 1, Iteration 5, Loss: 2.2955117225646973\n",
      "Epoch 1, Iteration 6, Loss: 2.1124653816223145\n",
      "Epoch 1, Iteration 7, Loss: 2.2327840328216553\n",
      "Epoch 1, Iteration 8, Loss: 2.3616974353790283\n",
      "Epoch 1, Iteration 9, Loss: 2.4860758781433105\n",
      "Epoch 1, Iteration 10, Loss: 2.275393486022949\n",
      "Epoch 1, Iteration 11, Loss: 1.9935942888259888\n",
      "Epoch 1, Iteration 12, Loss: 2.01599383354187\n",
      "Epoch 1, Iteration 13, Loss: 2.3155083656311035\n",
      "Epoch 1, Iteration 14, Loss: 2.0150279998779297\n",
      "Epoch 1, Iteration 15, Loss: 2.192326784133911\n",
      "Epoch 1, Iteration 16, Loss: 1.9909321069717407\n",
      "Epoch 1, Iteration 17, Loss: 2.3639094829559326\n",
      "Epoch 1, Iteration 18, Loss: 2.2530386447906494\n",
      "Epoch 1, Iteration 19, Loss: 2.2684381008148193\n",
      "Epoch 1, Iteration 20, Loss: 2.1981725692749023\n",
      "Epoch 1, Iteration 21, Loss: 2.250910997390747\n",
      "Epoch 1, Iteration 22, Loss: 2.0717556476593018\n",
      "Epoch 1, Iteration 23, Loss: 2.083394765853882\n",
      "Epoch 1, Iteration 24, Loss: 1.9979225397109985\n",
      "Epoch 1, Iteration 25, Loss: 1.9663575887680054\n",
      "Epoch 1, Iteration 26, Loss: 2.460374116897583\n",
      "Epoch 1, Iteration 27, Loss: 2.078366756439209\n",
      "Epoch 1, Iteration 28, Loss: 2.1328163146972656\n",
      "Epoch 1, Iteration 29, Loss: 2.1365506649017334\n",
      "Epoch 1, Iteration 30, Loss: 2.1043221950531006\n",
      "Epoch 1, Iteration 31, Loss: 2.2032692432403564\n",
      "Epoch 1, Iteration 32, Loss: 1.9042773246765137\n",
      "Epoch 1, Iteration 33, Loss: 1.8981943130493164\n",
      "Epoch 1, Iteration 34, Loss: 2.131895065307617\n",
      "Epoch 1, Iteration 35, Loss: 2.251056432723999\n",
      "Epoch 1, Iteration 36, Loss: 1.8755106925964355\n",
      "Epoch 1, Iteration 37, Loss: 1.9704688787460327\n",
      "Epoch 1, Iteration 38, Loss: 2.099881172180176\n",
      "Epoch 1, Iteration 39, Loss: 2.0065231323242188\n",
      "Epoch 1, Iteration 40, Loss: 2.1877291202545166\n",
      "Epoch 1, Iteration 41, Loss: 2.1768369674682617\n",
      "Epoch 1, Iteration 42, Loss: 2.3267934322357178\n",
      "Epoch 1, Iteration 43, Loss: 2.087716579437256\n",
      "Epoch 1, Iteration 44, Loss: 2.2035813331604004\n",
      "Epoch 1, Iteration 45, Loss: 2.0307135581970215\n",
      "Epoch 1, Iteration 46, Loss: 2.1633663177490234\n",
      "Epoch 1, Iteration 47, Loss: 2.1715593338012695\n",
      "Epoch 1, Iteration 48, Loss: 2.2624149322509766\n",
      "Epoch 1, Iteration 49, Loss: 2.1141974925994873\n",
      "Epoch 1, Iteration 50, Loss: 2.107719659805298\n",
      "Epoch 1, Iteration 50, Valid Loss: 1.9372949600219727\n",
      "Epoch 1, Iteration 51, Loss: 2.0445590019226074\n",
      "Epoch 1, Iteration 52, Loss: 2.3225839138031006\n",
      "Epoch 1, Iteration 53, Loss: 1.8905450105667114\n",
      "Epoch 1, Iteration 54, Loss: 2.0735349655151367\n",
      "Epoch 1, Iteration 55, Loss: 1.961072325706482\n",
      "Epoch 1, Iteration 56, Loss: 1.9160255193710327\n",
      "Epoch 1, Iteration 57, Loss: 1.7979505062103271\n",
      "Epoch 1, Iteration 58, Loss: 1.7108908891677856\n",
      "Epoch 1, Iteration 59, Loss: 2.0922858715057373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[59], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m DependencyParserModel()\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[58], line 76\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_dataloader, valid_dataloader, device, num_epochs, lr, weight_decay, alpha, beta)\u001B[0m\n\u001B[0;32m     73\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m# log the loss curve\u001B[39;00m\n\u001B[1;32m---> 76\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Iteration \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     78\u001B[0m wandb\u001B[38;5;241m.\u001B[39mlog({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m: loss\u001B[38;5;241m.\u001B[39mitem()})\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 59
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
